{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#let us import all the necessary packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import pi\n",
    "import cv2\n",
    "import scipy.misc\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"driving_dataset/\"\n",
    "DATA_FILE = os.path.join(DATA_FOLDER, \"data.txt\")\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "train_batch_pointer = 0\n",
    "test_batch_pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45406 45406\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_FILE) as f:\n",
    "    for line in f:\n",
    "        image_name, angle = line.split()\n",
    "        \n",
    "        image_path = os.path.join(DATA_FOLDER, image_name)\n",
    "        x.append(image_path)\n",
    "        \n",
    "        angle_radians = float(angle) * (pi / 180)  #converting angle into radians\n",
    "        y.append(angle_radians)\n",
    "y = np.array(y)\n",
    "print(str(len(x))+\" \"+str(len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting train and test data (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split ratio\n",
      "--------------------------------------------------\n",
      "Train dataset: 69.9995595295776 %\n",
      " size: 31784\n",
      "Test dataset: 30.00044047042241 %\n",
      " size: 13622\n"
     ]
    }
   ],
   "source": [
    "#using 70-30 split of train and test data\n",
    "split_ratio = int(len(x) * 0.7)\n",
    "\n",
    "train_x = x[:split_ratio]\n",
    "train_y = y[:split_ratio]\n",
    "\n",
    "test_x = x[split_ratio:]\n",
    "test_y = y[split_ratio:]\n",
    "\n",
    "print(\"Split ratio\")\n",
    "print('-'*50)\n",
    "print('Train dataset:',len(train_x)/len(x)*100,'%\\n','size:',len(train_x))\n",
    "\n",
    "print('Test dataset:',len(test_x)/len(x)*100,'%\\n','size:',len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#distrubution of steer angle parameter in train and test\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "plt.hist(train_y, bins = 50, histtype = \"step\")\n",
    "plt.hist(test_y, bins = 50, histtype = \"step\")   #test-orange\n",
    "plt.title(\"Steering Wheel angle in train and test\")\n",
    "plt.xlabel(\"Angle\")\n",
    "plt.ylabel(\"Bin count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the steering angle values are 0,indicating that the car has been driving mostly on straight roads rather than curvy roads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of the train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from driving_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(train_x[(train_batch_pointer + i) % len(train_x)]) \n",
    "        read_image_road = read_image[-150:] \n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) \n",
    "        read_image_final = read_image_resize/255.0  \n",
    "        \n",
    "        x_result.append(read_image_final) \n",
    "        \n",
    "        y_result.append(train_y[(train_batch_pointer + i) % len(train_y)])\n",
    "        \n",
    "    train_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestBatch(batch_size):\n",
    "    global test_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(test_x[(test_batch_pointer + i) % len(test_x)]) \n",
    "        read_image_road = read_image[-150:] \n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) \n",
    "        read_image_final = read_image_resize/255.0\n",
    "        x_result.append(read_image_final) \n",
    "        \n",
    "        y_result.append(test_y[(test_batch_pointer + i) % len(test_y)]) \n",
    "        \n",
    "    test_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape = shape, stddev = 0.1)\n",
    "    return tf.Variable(initial) \n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(previous_input, filter_input, strides,name):\n",
    "    print(name,filter_input.shape,filter_input.shape[-1])\n",
    "    return tf.nn.conv2d(previous_input, filter_input, strides = [1, strides, strides, 1], padding = \"VALID\")\n",
    "\n",
    "\n",
    "# def Dense(X, size, name):\n",
    "#     w = weight_variable(shape=size)\n",
    "#     b = weight_variable(shape=[size[-1]])\n",
    "    \n",
    "#     dense = tf.matmul(X, w) + b\n",
    "#     print(name, size, size[-1])\n",
    "#     ## Applying activation\n",
    "\n",
    "    \n",
    "#     h_fc = tf.nn.relu(dense)\n",
    "    \n",
    "    \n",
    "#     return h_fc\n",
    "\n",
    "\n",
    "# def flatten(X, size):\n",
    "#     return tf.reshape(X, [-1, size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(tf.float32, shape = [None, 66, 200, 3], name = \"Plc_1\")\n",
    "y_true = tf.placeholder(tf.float32,  name = \"Plc_2\")\n",
    "\n",
    "input_image = x_input\n",
    "#keep_prob = tf.placeholder(tf.float32)\n",
    "# # Flatten layer\n",
    "\n",
    "# h_conv5_flatten = flatten(Conv5, size=1152)\n",
    "\n",
    "\n",
    "# # Dense layer 1\n",
    "# h_fc1 = Dense(h_conv5_flatten, (1152, 1164), name='dense1')\n",
    "# # Dropout 1\n",
    "# h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# # Dense Layer 2\n",
    "# h_fc2 = Dense(h_fc1_drop, (1164, 100), name='dense2')\n",
    "# # Dropout 2\n",
    "# h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "# # Dense Layer 3\n",
    "# h_fc3 = Dense(h_fc2_drop, (100, 50), name='dense3')\n",
    "# # Dropout 3\n",
    "# h_fc3_drop = tf.nn.dropout(h_fc3, keep_prob)\n",
    "\n",
    "# # Dense Layer 4\n",
    "# h_fc4 = Dense(h_fc3_drop, (50, 10), name='dense4')\n",
    "\n",
    "# # Dropout 4\n",
    "# h_fc4_drop = tf.nn.dropout(h_fc4, keep_prob)\n",
    "\n",
    "\n",
    "# # Output\n",
    "# W_fc5 = weight_variable(shape = [10, 1])\n",
    "# b_fc5 = bias_variable(shape = [1])\n",
    "# y = tf.matmul(h_fc4_drop, W_fc5) + b_fc5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using linear output tf.matmul() instead of atan in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_1 (5, 5, 3, 24) 24\n",
      "conv2d_2 (5, 5, 24, 36) 36\n",
      "conv2d_3 (5, 5, 36, 48) 48\n",
      "conv2d_4 (3, 3, 48, 64) 64\n",
      "conv2d_5 (3, 3, 64, 64) 64\n",
      "WARNING:tensorflow:From <ipython-input-10-d5545d1a50ea>:41: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#Convolution Layers\n",
    "#First convolution layer\n",
    "W_Conv1 = weight_variable([5,5,3,24])\n",
    "B_Conv1 = bias_variable([24])\n",
    "Conv1 = tf.nn.elu(conv2d(input_image, W_Conv1, 2,name='conv2d_1') + B_Conv1)\n",
    "#tf.nn.conv2d(previous_input, filter_input, strides = [1, strides, strides, 1], padding = \"VALID\")\n",
    "\n",
    "#Second convolution layer\n",
    "W_Conv2 = weight_variable([5,5,24,36])\n",
    "B_Conv2 = bias_variable([36])\n",
    "Conv2 = tf.nn.elu(conv2d(Conv1, W_Conv2, 2,name='conv2d_2') + B_Conv2)\n",
    "\n",
    "\n",
    "#Third convolution layer\n",
    "W_Conv3 = weight_variable([5,5,36,48])\n",
    "B_Conv3 = bias_variable([48])\n",
    "Conv3 = tf.nn.elu(conv2d(Conv2, W_Conv3, 2,name='conv2d_3') + B_Conv3)\n",
    "\n",
    "\n",
    "#Fourth convolution layer\n",
    "W_Conv4 = weight_variable([3,3,48,64])\n",
    "B_Conv4 = bias_variable([64])\n",
    "Conv4 = tf.nn.elu(conv2d(Conv3, W_Conv4, 1,name='conv2d_4') + B_Conv4)\n",
    "\n",
    "\n",
    "\n",
    "#Fifth convolution layer\n",
    "W_Conv5 = weight_variable([3,3,64,64])\n",
    "B_Conv5 = bias_variable([64])\n",
    "Conv5 = tf.nn.elu(conv2d(Conv4, W_Conv5, 1,name='conv2d_5') + B_Conv5)\n",
    "\n",
    "\n",
    "#Fully-Connected Dense Layers\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W_FC1 = weight_variable([1152, 1164])\n",
    "B_FC1 = bias_variable([1164])\n",
    "FC1_Flatten = tf.reshape(Conv5, [-1, 1152]) \n",
    "Output_FC1 = tf.nn.elu(tf.matmul(FC1_Flatten, W_FC1) + B_FC1) \n",
    "\n",
    "Output_FC1_drop = tf.nn.dropout(Output_FC1, keep_prob)\n",
    "\n",
    "\n",
    "W_FC2 = weight_variable([1164, 100])\n",
    "B_FC2 = bias_variable([100])\n",
    "Output_FC2 = tf.nn.elu(tf.matmul(Output_FC1_drop, W_FC2) + B_FC2) \n",
    "\n",
    "Output_FC2_drop = tf.nn.dropout(Output_FC2, keep_prob)\n",
    "\n",
    "\n",
    "W_FC3 = weight_variable([100, 50])\n",
    "B_FC3 = bias_variable([50])\n",
    "Output_FC3 = tf.nn.elu(tf.matmul(Output_FC2_drop, W_FC3) + B_FC3) \n",
    "\n",
    "Output_FC3_drop = tf.nn.dropout(Output_FC3, keep_prob)\n",
    "\n",
    "\n",
    "\n",
    "W_FC4 = weight_variable([50, 10])\n",
    "B_FC4 = bias_variable([10])\n",
    "Output_FC4 = tf.nn.elu(tf.matmul(Output_FC3_drop, W_FC4) + B_FC4) #\n",
    "Output_FC4_drop = tf.nn.dropout(Output_FC4, keep_prob)\n",
    "\n",
    "\n",
    "W_FC5 = weight_variable([10, 1])\n",
    "B_FC5 = bias_variable([1])\n",
    "#y_predicted = tf.identity(tf.matmul(Output_FC4_drop, W_FC5) + B_FC5)\n",
    "y_predicted = tf.matmul(Output_FC4_drop, W_FC5) + B_FC5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "#### changing keep_prob(dropout) to 0.5 and learning rate of Adam optimizer to 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SELF DRIVING CAR MODEL\n",
      "Epoch: 1, Train_Loss: 17.1971492767334, Test_Loss: 21.44792938232422 *\n",
      "Epoch: 1, Train_Loss: 18.275070190429688, Test_Loss: 16.155990600585938 *\n",
      "Epoch: 1, Train_Loss: 32.9098014831543, Test_Loss: 21.228309631347656\n",
      "Epoch: 1, Train_Loss: 15.947515487670898, Test_Loss: 15.081491470336914 *\n",
      "Epoch: 1, Train_Loss: 11.379778861999512, Test_Loss: 13.640983581542969 *\n",
      "Epoch: 1, Train_Loss: 11.016790390014648, Test_Loss: 12.498842239379883 *\n",
      "Epoch: 1, Train_Loss: 10.67526626586914, Test_Loss: 12.365436553955078 *\n",
      "Epoch: 1, Train_Loss: 10.416671752929688, Test_Loss: 9.45344066619873 *\n",
      "Epoch: 1, Train_Loss: 11.029698371887207, Test_Loss: 8.8639554977417 *\n",
      "Epoch: 1, Train_Loss: 24.070764541625977, Test_Loss: 8.616769790649414 *\n",
      "Epoch: 1, Train_Loss: 11.349949836730957, Test_Loss: 8.322336196899414 *\n",
      "Epoch: 1, Train_Loss: 8.550683975219727, Test_Loss: 8.312333106994629 *\n",
      "Epoch: 1, Train_Loss: 14.66134262084961, Test_Loss: 8.037471771240234 *\n",
      "Epoch: 1, Train_Loss: 10.591028213500977, Test_Loss: 8.375833511352539\n",
      "Epoch: 1, Train_Loss: 8.633628845214844, Test_Loss: 8.776063919067383\n",
      "Epoch: 1, Train_Loss: 7.428146839141846, Test_Loss: 8.156299591064453 *\n",
      "Epoch: 1, Train_Loss: 7.370809555053711, Test_Loss: 8.676047325134277\n",
      "Epoch: 1, Train_Loss: 8.021344184875488, Test_Loss: 7.904350757598877 *\n",
      "Epoch: 1, Train_Loss: 7.316504955291748, Test_Loss: 7.7837677001953125 *\n",
      "Epoch: 1, Train_Loss: 7.40452241897583, Test_Loss: 7.720595836639404 *\n",
      "Epoch: 1, Train_Loss: 7.245510578155518, Test_Loss: 7.6451568603515625 *\n",
      "Epoch: 1, Train_Loss: 7.109016418457031, Test_Loss: 7.229194641113281 *\n",
      "Epoch: 1, Train_Loss: 6.7354936599731445, Test_Loss: 6.976587772369385 *\n",
      "Epoch: 1, Train_Loss: 7.29718017578125, Test_Loss: 7.268176555633545\n",
      "Epoch: 1, Train_Loss: 7.113767623901367, Test_Loss: 7.903298377990723\n",
      "Epoch: 1, Train_Loss: 7.613099575042725, Test_Loss: 7.0824055671691895 *\n",
      "Epoch: 1, Train_Loss: 6.716910362243652, Test_Loss: 9.04616641998291\n",
      "Epoch: 1, Train_Loss: 7.673214912414551, Test_Loss: 7.407237529754639 *\n",
      "Epoch: 1, Train_Loss: 7.087119102478027, Test_Loss: 8.231664657592773\n",
      "Epoch: 1, Train_Loss: 6.594246864318848, Test_Loss: 6.705004692077637 *\n",
      "Epoch: 1, Train_Loss: 6.5811333656311035, Test_Loss: 6.841149806976318\n",
      "Epoch: 1, Train_Loss: 6.732287406921387, Test_Loss: 6.997899055480957\n",
      "Epoch: 1, Train_Loss: 6.804567337036133, Test_Loss: 8.709086418151855\n",
      "Epoch: 1, Train_Loss: 6.573734283447266, Test_Loss: 7.977856636047363 *\n",
      "Epoch: 1, Train_Loss: 6.6770501136779785, Test_Loss: 6.801569938659668 *\n",
      "Epoch: 1, Train_Loss: 6.782909870147705, Test_Loss: 6.692112445831299 *\n",
      "Epoch: 1, Train_Loss: 6.6709489822387695, Test_Loss: 6.87178373336792\n",
      "Epoch: 1, Train_Loss: 6.825088024139404, Test_Loss: 7.386335372924805\n",
      "Epoch: 1, Train_Loss: 6.494589805603027, Test_Loss: 6.927436351776123 *\n",
      "Epoch: 1, Train_Loss: 6.658318519592285, Test_Loss: 7.738122463226318\n",
      "Epoch: 1, Train_Loss: 7.153392314910889, Test_Loss: 7.710530757904053 *\n",
      "Epoch: 1, Train_Loss: 6.637250900268555, Test_Loss: 6.853819847106934 *\n",
      "Epoch: 1, Train_Loss: 6.692354679107666, Test_Loss: 6.750904083251953 *\n",
      "Epoch: 1, Train_Loss: 6.763777256011963, Test_Loss: 6.8867411613464355\n",
      "Epoch: 1, Train_Loss: 15.996528625488281, Test_Loss: 6.793889045715332 *\n",
      "Epoch: 1, Train_Loss: 6.853341102600098, Test_Loss: 6.721014976501465 *\n",
      "Epoch: 1, Train_Loss: 6.5259480476379395, Test_Loss: 7.045742034912109\n",
      "Epoch: 1, Train_Loss: 6.578948020935059, Test_Loss: 7.352752685546875\n",
      "Epoch: 1, Train_Loss: 6.510647296905518, Test_Loss: 6.658944129943848 *\n",
      "Epoch: 1, Train_Loss: 6.481070041656494, Test_Loss: 6.585750102996826 *\n",
      "Epoch: 1, Train_Loss: 6.648102283477783, Test_Loss: 6.654448509216309\n",
      "Epoch: 1, Train_Loss: 6.65095329284668, Test_Loss: 6.692033767700195\n",
      "Epoch: 1, Train_Loss: 6.746879577636719, Test_Loss: 6.95895528793335\n",
      "Epoch: 1, Train_Loss: 6.779660224914551, Test_Loss: 8.201236724853516\n",
      "Epoch: 1, Train_Loss: 6.624180793762207, Test_Loss: 7.6080522537231445 *\n",
      "Epoch: 1, Train_Loss: 6.460154056549072, Test_Loss: 6.578480243682861 *\n",
      "Epoch: 1, Train_Loss: 6.583014011383057, Test_Loss: 6.624907493591309\n",
      "Epoch: 1, Train_Loss: 6.636862277984619, Test_Loss: 6.804537773132324\n",
      "Epoch: 1, Train_Loss: 6.628963470458984, Test_Loss: 6.454049110412598 *\n",
      "Epoch: 1, Train_Loss: 6.714931964874268, Test_Loss: 6.587832927703857\n",
      "Epoch: 1, Train_Loss: 6.661003112792969, Test_Loss: 6.77232027053833\n",
      "Epoch: 1, Train_Loss: 6.718843460083008, Test_Loss: 6.547859191894531 *\n",
      "Epoch: 1, Train_Loss: 6.621023178100586, Test_Loss: 6.438295841217041 *\n",
      "Epoch: 1, Train_Loss: 6.599667549133301, Test_Loss: 6.531038284301758\n",
      "Epoch: 1, Train_Loss: 6.578073024749756, Test_Loss: 6.749029159545898\n",
      "Epoch: 1, Train_Loss: 6.558867931365967, Test_Loss: 6.794005870819092\n",
      "Epoch: 1, Train_Loss: 6.723668575286865, Test_Loss: 6.804879665374756\n",
      "Epoch: 1, Train_Loss: 6.520353317260742, Test_Loss: 6.4623494148254395 *\n",
      "Epoch: 1, Train_Loss: 6.553831100463867, Test_Loss: 6.738678932189941\n",
      "Epoch: 1, Train_Loss: 11.76539134979248, Test_Loss: 6.466287136077881 *\n",
      "Epoch: 1, Train_Loss: 6.4186296463012695, Test_Loss: 6.474944591522217\n",
      "Epoch: 1, Train_Loss: 6.37546968460083, Test_Loss: 6.796921730041504\n",
      "Epoch: 1, Train_Loss: 6.397491455078125, Test_Loss: 8.062067031860352\n",
      "Epoch: 1, Train_Loss: 6.46317195892334, Test_Loss: 10.334321975708008\n",
      "Epoch: 1, Train_Loss: 6.39677619934082, Test_Loss: 6.4872941970825195 *\n",
      "Epoch: 1, Train_Loss: 6.3592448234558105, Test_Loss: 6.452261924743652 *\n",
      "Epoch: 1, Train_Loss: 6.396297931671143, Test_Loss: 6.4089274406433105 *\n",
      "Epoch: 1, Train_Loss: 6.4857587814331055, Test_Loss: 6.50654935836792\n",
      "Epoch: 1, Train_Loss: 6.550912380218506, Test_Loss: 6.409342288970947 *\n",
      "Epoch: 1, Train_Loss: 6.392656326293945, Test_Loss: 6.342165946960449 *\n",
      "Epoch: 1, Train_Loss: 6.497431755065918, Test_Loss: 6.443525314331055\n",
      "Epoch: 1, Train_Loss: 6.360618591308594, Test_Loss: 6.350499629974365 *\n",
      "Epoch: 1, Train_Loss: 6.469033718109131, Test_Loss: 6.381556987762451\n",
      "Epoch: 1, Train_Loss: 6.53892183303833, Test_Loss: 6.365784645080566 *\n",
      "Epoch: 1, Train_Loss: 6.56157112121582, Test_Loss: 6.502816200256348\n",
      "Epoch: 1, Train_Loss: 6.471107482910156, Test_Loss: 6.39675235748291 *\n",
      "Epoch: 1, Train_Loss: 6.4247965812683105, Test_Loss: 6.465932846069336\n",
      "Epoch: 1, Train_Loss: 6.390616416931152, Test_Loss: 6.491168022155762\n",
      "Epoch: 1, Train_Loss: 6.399233818054199, Test_Loss: 6.470703125 *\n",
      "Epoch: 1, Train_Loss: 6.5520195960998535, Test_Loss: 6.461082458496094 *\n",
      "Epoch: 1, Train_Loss: 6.385527610778809, Test_Loss: 6.373656749725342 *\n",
      "Epoch: 1, Train_Loss: 6.5485711097717285, Test_Loss: 6.505201816558838\n",
      "Epoch: 1, Train_Loss: 6.459033489227295, Test_Loss: 6.4078755378723145 *\n",
      "Epoch: 1, Train_Loss: 6.460144519805908, Test_Loss: 6.379354953765869 *\n",
      "Epoch: 1, Train_Loss: 6.564437389373779, Test_Loss: 6.439202308654785\n",
      "Epoch: 1, Train_Loss: 6.5431084632873535, Test_Loss: 6.417673110961914 *\n",
      "Epoch: 1, Train_Loss: 6.440526008605957, Test_Loss: 6.514477252960205\n",
      "Epoch: 1, Train_Loss: 6.4168925285339355, Test_Loss: 6.374192237854004 *\n",
      "Epoch: 1, Train_Loss: 6.463372230529785, Test_Loss: 6.621138572692871\n",
      "Epoch: 1, Train_Loss: 6.414214611053467, Test_Loss: 6.3736138343811035 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 6.537015914916992, Test_Loss: 6.401463508605957\n",
      "Epoch: 1, Train_Loss: 6.533891201019287, Test_Loss: 6.440732955932617\n",
      "Epoch: 1, Train_Loss: 6.384316921234131, Test_Loss: 6.613478183746338\n",
      "Epoch: 1, Train_Loss: 6.48750114440918, Test_Loss: 6.589946746826172 *\n",
      "Epoch: 1, Train_Loss: 6.406257152557373, Test_Loss: 10.587054252624512\n",
      "Epoch: 1, Train_Loss: 6.569697380065918, Test_Loss: 8.175972938537598 *\n",
      "Epoch: 1, Train_Loss: 11.547931671142578, Test_Loss: 6.46592378616333 *\n",
      "Epoch: 1, Train_Loss: 6.448026657104492, Test_Loss: 6.430534362792969 *\n",
      "Epoch: 1, Train_Loss: 6.362834930419922, Test_Loss: 6.429443359375 *\n",
      "Epoch: 1, Train_Loss: 6.391574382781982, Test_Loss: 6.430756092071533\n",
      "Epoch: 1, Train_Loss: 6.442096710205078, Test_Loss: 6.360219955444336 *\n",
      "Epoch: 1, Train_Loss: 6.428073883056641, Test_Loss: 6.520174503326416\n",
      "Epoch: 1, Train_Loss: 6.328887462615967, Test_Loss: 6.448414325714111 *\n",
      "Epoch: 1, Train_Loss: 6.4452104568481445, Test_Loss: 6.329817771911621 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 6.405048370361328, Test_Loss: 6.4150519371032715\n",
      "Epoch: 1, Train_Loss: 6.406662464141846, Test_Loss: 6.436625957489014\n",
      "Epoch: 1, Train_Loss: 6.368953704833984, Test_Loss: 6.349560737609863 *\n",
      "Epoch: 1, Train_Loss: 6.388218402862549, Test_Loss: 6.378956317901611\n",
      "Epoch: 1, Train_Loss: 6.489948272705078, Test_Loss: 6.521711349487305\n",
      "Epoch: 1, Train_Loss: 6.538998126983643, Test_Loss: 6.405981540679932 *\n",
      "Epoch: 1, Train_Loss: 6.400766372680664, Test_Loss: 6.427011013031006\n",
      "Epoch: 1, Train_Loss: 6.4189772605896, Test_Loss: 6.443938255310059\n",
      "Epoch: 1, Train_Loss: 6.4551615715026855, Test_Loss: 6.377431392669678 *\n",
      "Epoch: 1, Train_Loss: 6.422369003295898, Test_Loss: 6.359339237213135 *\n",
      "Epoch: 1, Train_Loss: 6.375151634216309, Test_Loss: 6.356565952301025 *\n",
      "Epoch: 1, Train_Loss: 6.346109390258789, Test_Loss: 6.373000144958496\n",
      "Epoch: 1, Train_Loss: 6.335890769958496, Test_Loss: 6.4321980476379395\n",
      "Epoch: 1, Train_Loss: 6.289059638977051, Test_Loss: 6.453332424163818\n",
      "Epoch: 1, Train_Loss: 6.282973766326904, Test_Loss: 6.332361221313477 *\n",
      "Epoch: 1, Train_Loss: 6.301525115966797, Test_Loss: 6.533318519592285\n",
      "Epoch: 1, Train_Loss: 6.322388172149658, Test_Loss: 6.47599983215332 *\n",
      "Epoch: 1, Train_Loss: 6.281675815582275, Test_Loss: 6.361362457275391 *\n",
      "Epoch: 1, Train_Loss: 6.245672225952148, Test_Loss: 6.321225643157959 *\n",
      "Epoch: 1, Train_Loss: 6.3383870124816895, Test_Loss: 6.329767227172852\n",
      "Epoch: 1, Train_Loss: 6.350632667541504, Test_Loss: 6.432301998138428\n",
      "Epoch: 1, Train_Loss: 6.387134552001953, Test_Loss: 6.304652690887451 *\n",
      "Epoch: 1, Train_Loss: 6.466813087463379, Test_Loss: 6.612699031829834\n",
      "Epoch: 1, Train_Loss: 6.593156814575195, Test_Loss: 6.442235469818115 *\n",
      "Epoch: 1, Train_Loss: 6.382216930389404, Test_Loss: 6.575722694396973\n",
      "Epoch: 1, Train_Loss: 6.525687217712402, Test_Loss: 6.548437595367432 *\n",
      "Epoch: 1, Train_Loss: 6.44973611831665, Test_Loss: 6.654110431671143\n",
      "Epoch: 1, Train_Loss: 6.527981281280518, Test_Loss: 6.782655239105225\n",
      "Epoch: 1, Train_Loss: 6.477003574371338, Test_Loss: 6.476498603820801 *\n",
      "Epoch: 1, Train_Loss: 6.652526378631592, Test_Loss: 6.633091926574707\n",
      "Epoch: 1, Train_Loss: 6.335587501525879, Test_Loss: 6.73799991607666\n",
      "Epoch: 1, Train_Loss: 6.340887069702148, Test_Loss: 6.364491939544678 *\n",
      "Epoch: 1, Train_Loss: 8.717609405517578, Test_Loss: 6.422910213470459\n",
      "Epoch: 1, Train_Loss: 7.297018527984619, Test_Loss: 6.396805763244629 *\n",
      "Epoch: 1, Train_Loss: 6.338770866394043, Test_Loss: 6.77569580078125\n",
      "Epoch: 1, Train_Loss: 6.348033428192139, Test_Loss: 6.50873327255249 *\n",
      "Epoch: 1, Train_Loss: 6.312028408050537, Test_Loss: 7.138799667358398\n",
      "Epoch: 1, Train_Loss: 6.329488754272461, Test_Loss: 6.517806529998779 *\n",
      "Epoch: 1, Train_Loss: 6.313722133636475, Test_Loss: 7.248430252075195\n",
      "Epoch: 1, Train_Loss: 6.41987943649292, Test_Loss: 6.854053020477295 *\n",
      "Epoch: 1, Train_Loss: 6.504058361053467, Test_Loss: 6.697482109069824 *\n",
      "Epoch: 1, Train_Loss: 6.396084785461426, Test_Loss: 6.872640132904053\n",
      "Epoch: 1, Train_Loss: 6.476641654968262, Test_Loss: 6.3227715492248535 *\n",
      "Epoch: 1, Train_Loss: 6.482052803039551, Test_Loss: 6.42117166519165\n",
      "Epoch: 1, Train_Loss: 6.3227219581604, Test_Loss: 6.382349014282227 *\n",
      "Epoch: 1, Train_Loss: 6.3685078620910645, Test_Loss: 6.546738147735596\n",
      "Epoch: 1, Train_Loss: 6.417016506195068, Test_Loss: 6.995561122894287\n",
      "Epoch: 1, Train_Loss: 6.347689628601074, Test_Loss: 6.645227432250977 *\n",
      "Epoch: 1, Train_Loss: 6.3070068359375, Test_Loss: 8.206482887268066\n",
      "Epoch: 1, Train_Loss: 6.393636703491211, Test_Loss: 7.139869689941406 *\n",
      "Epoch: 1, Train_Loss: 6.375112533569336, Test_Loss: 7.358241081237793\n",
      "Epoch: 1, Train_Loss: 6.370156288146973, Test_Loss: 6.665458679199219 *\n",
      "Epoch: 1, Train_Loss: 6.413164138793945, Test_Loss: 6.339369773864746 *\n",
      "Epoch: 1, Train_Loss: 6.441831588745117, Test_Loss: 6.546167373657227\n",
      "Epoch: 1, Train_Loss: 6.398704528808594, Test_Loss: 7.578268051147461\n",
      "Epoch: 1, Train_Loss: 6.327121734619141, Test_Loss: 7.149641990661621 *\n",
      "Epoch: 1, Train_Loss: 6.303164958953857, Test_Loss: 6.457398414611816 *\n",
      "Epoch: 1, Train_Loss: 6.291196823120117, Test_Loss: 6.4012064933776855 *\n",
      "Epoch: 1, Train_Loss: 6.437747478485107, Test_Loss: 6.49070930480957\n",
      "Epoch: 1, Train_Loss: 6.2983927726745605, Test_Loss: 7.070703029632568\n",
      "Epoch: 1, Train_Loss: 6.304840564727783, Test_Loss: 6.587599277496338 *\n",
      "Epoch: 1, Train_Loss: 6.350760459899902, Test_Loss: 7.374030590057373\n",
      "Epoch: 1, Train_Loss: 6.316998481750488, Test_Loss: 7.171171188354492 *\n",
      "Epoch: 1, Train_Loss: 6.362885475158691, Test_Loss: 6.582480430603027 *\n",
      "Epoch: 1, Train_Loss: 6.2978105545043945, Test_Loss: 6.318390846252441 *\n",
      "Epoch: 1, Train_Loss: 6.279098987579346, Test_Loss: 6.325568675994873\n",
      "Epoch: 1, Train_Loss: 6.258179187774658, Test_Loss: 6.420639991760254\n",
      "Epoch: 1, Train_Loss: 6.336918830871582, Test_Loss: 6.3445587158203125 *\n",
      "Epoch: 1, Train_Loss: 6.3379340171813965, Test_Loss: 6.868034362792969\n",
      "Epoch: 1, Train_Loss: 6.335653305053711, Test_Loss: 6.810577869415283 *\n",
      "Epoch: 1, Train_Loss: 6.291401386260986, Test_Loss: 6.391367435455322 *\n",
      "Epoch: 1, Train_Loss: 6.338650226593018, Test_Loss: 6.2946672439575195 *\n",
      "Epoch: 1, Train_Loss: 6.255144119262695, Test_Loss: 6.3068389892578125\n",
      "Epoch: 1, Train_Loss: 6.297270774841309, Test_Loss: 6.364414215087891\n",
      "Epoch: 1, Train_Loss: 6.330809116363525, Test_Loss: 6.529382228851318\n",
      "Epoch: 1, Train_Loss: 6.250542163848877, Test_Loss: 7.677391052246094\n",
      "Epoch: 1, Train_Loss: 6.298875331878662, Test_Loss: 7.5462446212768555 *\n",
      "Epoch: 1, Train_Loss: 6.361504554748535, Test_Loss: 6.282129287719727 *\n",
      "Epoch: 1, Train_Loss: 6.34361457824707, Test_Loss: 6.353058815002441\n",
      "Epoch: 1, Train_Loss: 6.330987453460693, Test_Loss: 6.277428150177002 *\n",
      "Epoch: 1, Train_Loss: 6.2390971183776855, Test_Loss: 6.277792453765869\n",
      "Epoch: 1, Train_Loss: 6.287665367126465, Test_Loss: 6.30354642868042\n",
      "Epoch: 1, Train_Loss: 6.324980735778809, Test_Loss: 6.460677623748779\n",
      "Epoch: 1, Train_Loss: 6.284041404724121, Test_Loss: 6.304006099700928 *\n",
      "Epoch: 1, Train_Loss: 6.367834091186523, Test_Loss: 6.302331924438477 *\n",
      "Epoch: 1, Train_Loss: 6.25960636138916, Test_Loss: 6.322795391082764\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 6.463371276855469, Test_Loss: 6.5352864265441895\n",
      "Epoch: 1, Train_Loss: 6.4400153160095215, Test_Loss: 6.683210372924805\n",
      "Epoch: 1, Train_Loss: 6.464473247528076, Test_Loss: 6.666417598724365 *\n",
      "Epoch: 1, Train_Loss: 6.2790021896362305, Test_Loss: 6.390900135040283 *\n",
      "Epoch: 1, Train_Loss: 6.423586845397949, Test_Loss: 6.330938816070557 *\n",
      "Epoch: 1, Train_Loss: 6.303089618682861, Test_Loss: 6.32280969619751 *\n",
      "Epoch: 1, Train_Loss: 6.268754482269287, Test_Loss: 6.4359354972839355\n",
      "Epoch: 1, Train_Loss: 6.3100152015686035, Test_Loss: 6.399371147155762 *\n",
      "Epoch: 1, Train_Loss: 6.240715026855469, Test_Loss: 6.783639430999756\n",
      "Epoch: 1, Train_Loss: 6.342398643493652, Test_Loss: 11.528697967529297\n",
      "Epoch: 1, Train_Loss: 6.404347896575928, Test_Loss: 6.371955394744873 *\n",
      "Epoch: 1, Train_Loss: 6.286901950836182, Test_Loss: 6.309325695037842 *\n",
      "Epoch: 1, Train_Loss: 6.403759479522705, Test_Loss: 6.311282157897949\n",
      "Epoch: 1, Train_Loss: 6.251596450805664, Test_Loss: 6.318565845489502\n",
      "Epoch: 1, Train_Loss: 6.259254455566406, Test_Loss: 6.240429878234863 *\n",
      "Epoch: 1, Train_Loss: 6.410393714904785, Test_Loss: 6.337926387786865\n",
      "Epoch: 1, Train_Loss: 6.459535598754883, Test_Loss: 6.27597188949585 *\n",
      "Epoch: 1, Train_Loss: 6.2511515617370605, Test_Loss: 6.323929309844971\n",
      "Epoch: 1, Train_Loss: 6.24574089050293, Test_Loss: 6.284217834472656 *\n",
      "Epoch: 1, Train_Loss: 6.245974063873291, Test_Loss: 6.270122051239014 *\n",
      "Epoch: 1, Train_Loss: 6.297305107116699, Test_Loss: 6.290276527404785\n",
      "Epoch: 1, Train_Loss: 6.299977779388428, Test_Loss: 6.252792835235596 *\n",
      "Epoch: 1, Train_Loss: 6.277894973754883, Test_Loss: 6.332609176635742\n",
      "Epoch: 1, Train_Loss: 6.303672790527344, Test_Loss: 6.2913618087768555 *\n",
      "Epoch: 1, Train_Loss: 6.303586959838867, Test_Loss: 6.2794904708862305 *\n",
      "Epoch: 1, Train_Loss: 6.295053482055664, Test_Loss: 6.248140335083008 *\n",
      "Epoch: 1, Train_Loss: 6.27846622467041, Test_Loss: 6.24449348449707 *\n",
      "Epoch: 1, Train_Loss: 6.256823539733887, Test_Loss: 6.305212497711182\n",
      "Epoch: 1, Train_Loss: 6.276963710784912, Test_Loss: 6.2649383544921875 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 6.271998405456543, Test_Loss: 6.281365394592285\n",
      "Epoch: 1, Train_Loss: 6.244236469268799, Test_Loss: 6.216499328613281 *\n",
      "Epoch: 1, Train_Loss: 6.243541717529297, Test_Loss: 6.346798896789551\n",
      "Epoch: 1, Train_Loss: 6.257387161254883, Test_Loss: 6.3042988777160645 *\n",
      "Epoch: 1, Train_Loss: 6.26304817199707, Test_Loss: 6.330956935882568\n",
      "Epoch: 1, Train_Loss: 6.262423992156982, Test_Loss: 6.307503700256348 *\n",
      "Epoch: 1, Train_Loss: 6.390876770019531, Test_Loss: 6.40657377243042\n",
      "Epoch: 1, Train_Loss: 6.334863662719727, Test_Loss: 6.290474891662598 *\n",
      "Epoch: 1, Train_Loss: 6.354247570037842, Test_Loss: 6.232412815093994 *\n",
      "Epoch: 1, Train_Loss: 6.216482162475586, Test_Loss: 6.3060479164123535\n",
      "Epoch: 1, Train_Loss: 6.2347187995910645, Test_Loss: 6.362957000732422\n",
      "Epoch: 1, Train_Loss: 6.255314826965332, Test_Loss: 9.11413860321045\n",
      "Epoch: 1, Train_Loss: 6.265202045440674, Test_Loss: 9.340883255004883\n",
      "Epoch: 1, Train_Loss: 6.262577056884766, Test_Loss: 6.2402472496032715 *\n",
      "Epoch: 1, Train_Loss: 6.286448955535889, Test_Loss: 6.227755069732666 *\n",
      "Epoch: 1, Train_Loss: 7.646296501159668, Test_Loss: 6.263183116912842\n",
      "Epoch: 1, Train_Loss: 10.080662727355957, Test_Loss: 6.222102165222168 *\n",
      "Epoch: 1, Train_Loss: 6.467273712158203, Test_Loss: 6.257775783538818\n",
      "Epoch: 1, Train_Loss: 6.2389092445373535, Test_Loss: 6.2896246910095215\n",
      "Epoch: 1, Train_Loss: 6.256300926208496, Test_Loss: 6.442464351654053\n",
      "Epoch: 1, Train_Loss: 6.422215461730957, Test_Loss: 6.2514567375183105 *\n",
      "Epoch: 1, Train_Loss: 6.321419715881348, Test_Loss: 6.242581367492676 *\n",
      "Epoch: 1, Train_Loss: 6.2931809425354, Test_Loss: 6.231220722198486 *\n",
      "Epoch: 1, Train_Loss: 6.234846115112305, Test_Loss: 6.221328258514404 *\n",
      "Epoch: 1, Train_Loss: 6.352402210235596, Test_Loss: 6.235986709594727\n",
      "Epoch: 1, Train_Loss: 6.300968647003174, Test_Loss: 6.31953239440918\n",
      "Epoch: 1, Train_Loss: 6.232889652252197, Test_Loss: 6.296877384185791 *\n",
      "Epoch: 1, Train_Loss: 6.655585289001465, Test_Loss: 6.33949613571167\n",
      "Epoch: 1, Train_Loss: 7.824862003326416, Test_Loss: 6.374507427215576\n",
      "Epoch: 1, Train_Loss: 7.524722099304199, Test_Loss: 6.19628381729126 *\n",
      "Epoch: 1, Train_Loss: 6.417013645172119, Test_Loss: 6.227343559265137\n",
      "Epoch: 1, Train_Loss: 6.487820148468018, Test_Loss: 6.257626533508301\n",
      "Epoch: 1, Train_Loss: 8.465791702270508, Test_Loss: 6.264712333679199\n",
      "Epoch: 1, Train_Loss: 6.989011764526367, Test_Loss: 6.236087799072266 *\n",
      "Epoch: 1, Train_Loss: 6.249926567077637, Test_Loss: 6.207757472991943 *\n",
      "Epoch: 1, Train_Loss: 6.18352746963501, Test_Loss: 6.211588382720947\n",
      "Epoch: 1, Train_Loss: 7.216499328613281, Test_Loss: 6.207388877868652 *\n",
      "Epoch: 1, Train_Loss: 8.03707504272461, Test_Loss: 6.20640230178833 *\n",
      "Epoch: 1, Train_Loss: 6.940669059753418, Test_Loss: 6.2858710289001465\n",
      "Epoch: 1, Train_Loss: 6.265299320220947, Test_Loss: 6.230133533477783 *\n",
      "Epoch: 1, Train_Loss: 6.226943016052246, Test_Loss: 6.240992546081543\n",
      "Epoch: 1, Train_Loss: 6.704770565032959, Test_Loss: 6.1973490715026855 *\n",
      "Epoch: 1, Train_Loss: 6.502196788787842, Test_Loss: 6.311091423034668\n",
      "Epoch: 1, Train_Loss: 6.3214640617370605, Test_Loss: 6.6158223152160645\n",
      "Epoch: 1, Train_Loss: 6.256566047668457, Test_Loss: 6.272908687591553 *\n",
      "Epoch: 1, Train_Loss: 6.400211811065674, Test_Loss: 6.254303455352783 *\n",
      "Epoch: 1, Train_Loss: 6.3196516036987305, Test_Loss: 6.349721908569336\n",
      "Epoch: 1, Train_Loss: 6.29002571105957, Test_Loss: 6.412441253662109\n",
      "Epoch: 1, Train_Loss: 6.6836347579956055, Test_Loss: 6.4113616943359375 *\n",
      "Epoch: 1, Train_Loss: 6.358455181121826, Test_Loss: 6.28740119934082 *\n",
      "Epoch: 1, Train_Loss: 6.243622303009033, Test_Loss: 6.430706977844238\n",
      "Epoch: 1, Train_Loss: 6.408757209777832, Test_Loss: 6.631115436553955\n",
      "Epoch: 1, Train_Loss: 6.459132671356201, Test_Loss: 6.245802879333496 *\n",
      "Epoch: 1, Train_Loss: 6.571451663970947, Test_Loss: 6.292045593261719\n",
      "Epoch: 1, Train_Loss: 6.4545183181762695, Test_Loss: 6.204012870788574 *\n",
      "Epoch: 1, Train_Loss: 6.288435935974121, Test_Loss: 6.340933322906494\n",
      "Epoch: 1, Train_Loss: 6.470026969909668, Test_Loss: 6.273501396179199 *\n",
      "Epoch: 1, Train_Loss: 6.236152648925781, Test_Loss: 7.129872798919678\n",
      "Epoch: 1, Train_Loss: 6.247164726257324, Test_Loss: 6.399932861328125 *\n",
      "Epoch: 1, Train_Loss: 6.198440074920654, Test_Loss: 7.188310623168945\n",
      "Epoch: 1, Train_Loss: 6.254798412322998, Test_Loss: 6.908698558807373 *\n",
      "Epoch: 1, Train_Loss: 6.2411346435546875, Test_Loss: 6.454878807067871 *\n",
      "Epoch: 1, Train_Loss: 6.269987106323242, Test_Loss: 6.800819396972656\n",
      "Epoch: 1, Train_Loss: 6.251750469207764, Test_Loss: 6.314654350280762 *\n",
      "Epoch: 1, Train_Loss: 6.348734378814697, Test_Loss: 6.207699775695801 *\n",
      "Epoch: 1, Train_Loss: 6.347907543182373, Test_Loss: 6.20063591003418 *\n",
      "Epoch: 1, Train_Loss: 6.366586208343506, Test_Loss: 6.372652530670166\n",
      "Epoch: 1, Train_Loss: 6.494458198547363, Test_Loss: 6.625754356384277\n",
      "Epoch: 1, Train_Loss: 6.305656433105469, Test_Loss: 6.893900394439697\n",
      "Epoch: 1, Train_Loss: 6.219780445098877, Test_Loss: 7.38316535949707\n",
      "Epoch: 1, Train_Loss: 6.419066429138184, Test_Loss: 7.4021196365356445\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 6.687407493591309, Test_Loss: 7.220767974853516 *\n",
      "Epoch: 1, Train_Loss: 6.779436111450195, Test_Loss: 6.720507621765137 *\n",
      "Epoch: 1, Train_Loss: 6.216965675354004, Test_Loss: 6.214054584503174 *\n",
      "Epoch: 1, Train_Loss: 6.214268207550049, Test_Loss: 6.311360836029053\n",
      "Epoch: 1, Train_Loss: 6.728243350982666, Test_Loss: 7.108933448791504\n",
      "Epoch: 1, Train_Loss: 6.880944728851318, Test_Loss: 7.594202041625977\n",
      "Epoch: 1, Train_Loss: 6.33072566986084, Test_Loss: 6.2792439460754395 *\n",
      "Epoch: 1, Train_Loss: 6.232303142547607, Test_Loss: 6.32696533203125\n",
      "Epoch: 1, Train_Loss: 6.259369850158691, Test_Loss: 6.2181620597839355 *\n",
      "Epoch: 1, Train_Loss: 6.9355974197387695, Test_Loss: 6.553542613983154\n",
      "Epoch: 1, Train_Loss: 7.927650451660156, Test_Loss: 6.44874382019043 *\n",
      "Epoch: 1, Train_Loss: 6.329756736755371, Test_Loss: 7.071731090545654\n",
      "Epoch: 1, Train_Loss: 6.228403568267822, Test_Loss: 7.425929069519043\n",
      "Epoch: 1, Train_Loss: 6.233863353729248, Test_Loss: 6.5589776039123535 *\n",
      "Epoch: 1, Train_Loss: 6.170024394989014, Test_Loss: 6.1821980476379395 *\n",
      "Epoch: 1, Train_Loss: 6.564591407775879, Test_Loss: 6.239891052246094\n",
      "Epoch: 1, Train_Loss: 6.225235462188721, Test_Loss: 6.177757263183594 *\n",
      "Epoch: 1, Train_Loss: 6.273885250091553, Test_Loss: 6.29764461517334\n",
      "Epoch: 1, Train_Loss: 6.201081275939941, Test_Loss: 6.502476215362549\n",
      "Epoch: 1, Train_Loss: 6.216919898986816, Test_Loss: 6.745185375213623\n",
      "Epoch: 1, Train_Loss: 23.396196365356445, Test_Loss: 6.387932300567627 *\n",
      "Epoch: 1, Train_Loss: 6.307504653930664, Test_Loss: 6.187412261962891 *\n",
      "Epoch: 1, Train_Loss: 7.846214771270752, Test_Loss: 6.238282203674316\n",
      "Epoch: 1, Train_Loss: 8.885161399841309, Test_Loss: 6.219059944152832 *\n",
      "Epoch: 1, Train_Loss: 6.268889427185059, Test_Loss: 6.319690227508545\n",
      "Epoch: 1, Train_Loss: 6.299005031585693, Test_Loss: 7.321524143218994\n",
      "Epoch: 1, Train_Loss: 8.493009567260742, Test_Loss: 7.814870834350586\n",
      "Epoch: 1, Train_Loss: 15.945968627929688, Test_Loss: 6.275628089904785 *\n",
      "Epoch: 1, Train_Loss: 6.316856861114502, Test_Loss: 6.340639114379883\n",
      "Epoch: 1, Train_Loss: 6.193869590759277, Test_Loss: 6.230534553527832 *\n",
      "Epoch: 1, Train_Loss: 12.170494079589844, Test_Loss: 6.195908546447754 *\n",
      "Epoch: 1, Train_Loss: 6.418835639953613, Test_Loss: 6.225988864898682\n",
      "Epoch: 1, Train_Loss: 6.222714424133301, Test_Loss: 6.254541397094727\n",
      "Epoch: 1, Train_Loss: 6.2692694664001465, Test_Loss: 6.278963088989258\n",
      "Epoch: 1, Train_Loss: 6.169546604156494, Test_Loss: 6.239676475524902 *\n",
      "Epoch: 1, Train_Loss: 6.284476280212402, Test_Loss: 6.242116451263428\n",
      "Epoch: 1, Train_Loss: 6.207076072692871, Test_Loss: 6.259744644165039\n",
      "Epoch: 1, Train_Loss: 6.215634822845459, Test_Loss: 6.549056053161621\n",
      "Epoch: 1, Train_Loss: 6.1811347007751465, Test_Loss: 6.237038612365723 *\n",
      "Epoch: 1, Train_Loss: 6.22210693359375, Test_Loss: 6.309658050537109\n",
      "Epoch: 1, Train_Loss: 6.203765869140625, Test_Loss: 6.285016059875488 *\n",
      "Epoch: 1, Train_Loss: 6.177567005157471, Test_Loss: 6.250583171844482 *\n",
      "Epoch: 1, Train_Loss: 6.2374267578125, Test_Loss: 6.163539409637451 *\n",
      "Epoch: 1, Train_Loss: 6.282449722290039, Test_Loss: 6.164005756378174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 6.313840389251709, Test_Loss: 6.189540386199951\n",
      "Epoch: 1, Train_Loss: 6.306065559387207, Test_Loss: 12.014119148254395\n",
      "Epoch: 1, Train_Loss: 6.194333076477051, Test_Loss: 6.516106128692627 *\n",
      "Epoch: 1, Train_Loss: 6.169142246246338, Test_Loss: 6.2073235511779785 *\n",
      "Epoch: 1, Train_Loss: 6.185998439788818, Test_Loss: 6.195420265197754 *\n",
      "Epoch: 1, Train_Loss: 6.182398319244385, Test_Loss: 6.163614749908447 *\n",
      "Epoch: 1, Train_Loss: 6.15731143951416, Test_Loss: 6.17292594909668\n",
      "Epoch: 1, Train_Loss: 6.161816120147705, Test_Loss: 6.136012554168701 *\n",
      "Epoch: 1, Train_Loss: 6.190764904022217, Test_Loss: 6.176784992218018\n",
      "Epoch: 1, Train_Loss: 6.1480584144592285, Test_Loss: 6.157794952392578 *\n",
      "Epoch: 1, Train_Loss: 6.217182636260986, Test_Loss: 6.182671070098877\n",
      "Epoch: 1, Train_Loss: 6.169025421142578, Test_Loss: 6.183043479919434\n",
      "Epoch: 1, Train_Loss: 6.1443023681640625, Test_Loss: 6.1955718994140625\n",
      "Epoch: 1, Train_Loss: 6.133215427398682, Test_Loss: 6.151391983032227 *\n",
      "Epoch: 1, Train_Loss: 6.196573257446289, Test_Loss: 6.158783912658691\n",
      "Epoch: 1, Train_Loss: 6.167459487915039, Test_Loss: 6.158294677734375 *\n",
      "Epoch: 1, Train_Loss: 6.205329895019531, Test_Loss: 6.153975009918213 *\n",
      "Epoch: 1, Train_Loss: 6.575549602508545, Test_Loss: 6.152275085449219 *\n",
      "Epoch: 1, Train_Loss: 14.828012466430664, Test_Loss: 6.174442768096924\n",
      "Epoch: 1, Train_Loss: 6.214479446411133, Test_Loss: 6.191362380981445\n",
      "Epoch: 1, Train_Loss: 6.127662181854248, Test_Loss: 6.197879314422607\n",
      "Epoch: 1, Train_Loss: 6.128145694732666, Test_Loss: 6.166593551635742 *\n",
      "Epoch: 1, Train_Loss: 6.135815143585205, Test_Loss: 6.231351375579834\n",
      "Epoch: 1, Train_Loss: 6.1453633308410645, Test_Loss: 6.214573383331299 *\n",
      "Epoch: 1, Train_Loss: 6.154677867889404, Test_Loss: 6.176022529602051 *\n",
      "Epoch: 1, Train_Loss: 6.189059734344482, Test_Loss: 6.158402442932129 *\n",
      "Epoch: 1, Train_Loss: 6.31398868560791, Test_Loss: 6.171527862548828\n",
      "Epoch: 1, Train_Loss: 6.443329334259033, Test_Loss: 6.160711288452148 *\n",
      "Epoch: 1, Train_Loss: 6.317831039428711, Test_Loss: 6.170662879943848\n",
      "Epoch: 1, Train_Loss: 6.152517318725586, Test_Loss: 6.128941059112549 *\n",
      "Epoch: 1, Train_Loss: 6.268535137176514, Test_Loss: 6.197514533996582\n",
      "Epoch: 1, Train_Loss: 6.279396057128906, Test_Loss: 6.254180908203125\n",
      "Epoch: 1, Train_Loss: 6.314414024353027, Test_Loss: 7.55831241607666\n",
      "Epoch: 1, Train_Loss: 6.301783561706543, Test_Loss: 11.16034984588623\n",
      "Epoch: 1, Train_Loss: 6.280816078186035, Test_Loss: 6.1315507888793945 *\n",
      "Epoch: 1, Train_Loss: 6.176722049713135, Test_Loss: 6.151843070983887\n",
      "Epoch: 1, Train_Loss: 6.1961212158203125, Test_Loss: 6.151106357574463 *\n",
      "Epoch: 1, Train_Loss: 6.287826061248779, Test_Loss: 6.164804935455322\n",
      "Epoch: 1, Train_Loss: 6.166577339172363, Test_Loss: 6.127676010131836 *\n",
      "Epoch: 1, Train_Loss: 6.1833696365356445, Test_Loss: 6.15181303024292\n",
      "Epoch: 1, Train_Loss: 6.126476287841797, Test_Loss: 6.325131416320801\n",
      "Epoch: 1, Train_Loss: 6.153867244720459, Test_Loss: 6.145235538482666 *\n",
      "Epoch: 1, Train_Loss: 6.975963592529297, Test_Loss: 6.137628555297852 *\n",
      "Epoch: 1, Train_Loss: 10.857585906982422, Test_Loss: 6.159735679626465\n",
      "Epoch: 1, Train_Loss: 6.150845527648926, Test_Loss: 6.134504795074463 *\n",
      "Epoch: 1, Train_Loss: 6.149677276611328, Test_Loss: 6.157932281494141\n",
      "Epoch: 1, Train_Loss: 6.166838645935059, Test_Loss: 6.154961585998535 *\n",
      "Epoch: 1, Train_Loss: 6.133960723876953, Test_Loss: 6.18485164642334\n",
      "Epoch: 1, Train_Loss: 6.142192840576172, Test_Loss: 6.254229545593262\n",
      "Epoch: 1, Train_Loss: 6.110617160797119, Test_Loss: 6.2890706062316895\n",
      "Epoch: 1, Train_Loss: 6.127058506011963, Test_Loss: 6.141664981842041 *\n",
      "Epoch: 1, Train_Loss: 6.158792018890381, Test_Loss: 6.127374649047852 *\n",
      "Epoch: 1, Train_Loss: 6.135889530181885, Test_Loss: 6.088783264160156 *\n",
      "Epoch: 1, Train_Loss: 6.144924640655518, Test_Loss: 6.106719493865967\n",
      "Epoch: 1, Train_Loss: 6.1141767501831055, Test_Loss: 6.147574424743652\n",
      "Epoch: 1, Train_Loss: 6.112311363220215, Test_Loss: 6.152937889099121\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 6.149322509765625, Test_Loss: 6.100651264190674 *\n",
      "Epoch: 1, Train_Loss: 6.133055686950684, Test_Loss: 6.110935211181641\n",
      "Epoch: 1, Train_Loss: 6.134322643280029, Test_Loss: 6.121067523956299\n",
      "Epoch: 1, Train_Loss: 6.233710289001465, Test_Loss: 6.1312255859375\n",
      "Epoch: 1, Train_Loss: 6.206295013427734, Test_Loss: 6.115543842315674 *\n",
      "Epoch: 1, Train_Loss: 6.113603591918945, Test_Loss: 6.112375736236572 *\n",
      "Epoch: 1, Train_Loss: 6.100820064544678, Test_Loss: 6.10311222076416 *\n",
      "Epoch: 1, Train_Loss: 6.113345623016357, Test_Loss: 6.147146701812744\n",
      "Epoch: 1, Train_Loss: 6.1799235343933105, Test_Loss: 6.355957508087158\n",
      "Epoch: 1, Train_Loss: 6.193130970001221, Test_Loss: 6.362109184265137\n",
      "Epoch: 1, Train_Loss: 6.2128400802612305, Test_Loss: 6.159963607788086 *\n",
      "Epoch: 1, Train_Loss: 6.1754469871521, Test_Loss: 6.229276657104492\n",
      "Epoch: 1, Train_Loss: 6.154727935791016, Test_Loss: 6.2167582511901855 *\n",
      "Epoch: 1, Train_Loss: 6.2401323318481445, Test_Loss: 6.320990085601807\n",
      "Epoch: 1, Train_Loss: 6.220156669616699, Test_Loss: 6.13098669052124 *\n",
      "Epoch: 1, Train_Loss: 6.144944667816162, Test_Loss: 6.430622577667236\n",
      "Epoch: 1, Train_Loss: 6.285027503967285, Test_Loss: 6.649155616760254\n",
      "Epoch: 1, Train_Loss: 6.148807525634766, Test_Loss: 6.227399826049805 *\n",
      "Epoch: 1, Train_Loss: 6.084367275238037, Test_Loss: 6.244957447052002\n",
      "Epoch: 1, Train_Loss: 6.135233402252197, Test_Loss: 6.089546203613281 *\n",
      "Epoch: 1, Train_Loss: 6.158459663391113, Test_Loss: 6.2285542488098145\n",
      "Epoch: 1, Train_Loss: 6.1127753257751465, Test_Loss: 6.120678901672363 *\n",
      "Epoch: 1, Train_Loss: 6.100973606109619, Test_Loss: 6.915821552276611\n",
      "Epoch: 1, Train_Loss: 6.969876766204834, Test_Loss: 6.5033135414123535 *\n",
      "Epoch: 1, Train_Loss: 10.570323944091797, Test_Loss: 6.658896446228027\n",
      "Epoch: 1, Train_Loss: 6.093291282653809, Test_Loss: 6.9474196434021\n",
      "Epoch: 1, Train_Loss: 6.111256122589111, Test_Loss: 6.262146949768066 *\n",
      "Epoch: 1, Train_Loss: 6.131216526031494, Test_Loss: 6.856090545654297\n",
      "Epoch: 1, Train_Loss: 6.0974812507629395, Test_Loss: 6.347128868103027 *\n",
      "Epoch: 1, Train_Loss: 6.092541217803955, Test_Loss: 6.115837574005127 *\n",
      "Epoch: 1, Train_Loss: 6.1070990562438965, Test_Loss: 6.097104072570801 *\n",
      "Epoch: 1, Train_Loss: 6.095686435699463, Test_Loss: 6.202734470367432\n",
      "Epoch: 1, Train_Loss: 6.096266746520996, Test_Loss: 6.3588385581970215\n",
      "Epoch: 1, Train_Loss: 6.091453552246094, Test_Loss: 7.106200218200684\n",
      "Epoch: 1, Train_Loss: 6.221020221710205, Test_Loss: 6.708906650543213 *\n",
      "Epoch: 1, Train_Loss: 6.229422569274902, Test_Loss: 7.894237518310547\n",
      "Epoch: 1, Train_Loss: 6.211803436279297, Test_Loss: 6.818558692932129 *\n",
      "Epoch: 1, Train_Loss: 6.2010016441345215, Test_Loss: 7.021968841552734\n",
      "Epoch: 1, Train_Loss: 6.099031448364258, Test_Loss: 6.104516506195068 *\n",
      "Epoch: 1, Train_Loss: 6.2455291748046875, Test_Loss: 6.114715576171875\n",
      "Epoch: 1, Train_Loss: 6.241530418395996, Test_Loss: 6.788817882537842\n",
      "Epoch: 1, Train_Loss: 6.195383071899414, Test_Loss: 7.468109607696533\n",
      "Epoch: 1, Train_Loss: 6.194635391235352, Test_Loss: 6.21801233291626 *\n",
      "Epoch: 1, Train_Loss: 6.1127753257751465, Test_Loss: 6.2564592361450195\n",
      "Epoch: 1, Train_Loss: 6.070220470428467, Test_Loss: 6.155791282653809 *\n",
      "Epoch: 1, Train_Loss: 6.061498165130615, Test_Loss: 6.237702369689941\n",
      "Epoch: 1, Train_Loss: 6.057559490203857, Test_Loss: 6.425071716308594\n",
      "Epoch: 1, Train_Loss: 6.084895610809326, Test_Loss: 6.569662570953369\n",
      "Epoch: 1, Train_Loss: 6.0659284591674805, Test_Loss: 7.6080193519592285\n",
      "Epoch: 1, Train_Loss: 6.081620693206787, Test_Loss: 6.492773532867432 *\n",
      "Epoch: 1, Train_Loss: 6.076850891113281, Test_Loss: 6.108796119689941 *\n",
      "Epoch: 1, Train_Loss: 6.0651984214782715, Test_Loss: 6.094714164733887 *\n",
      "Epoch: 1, Train_Loss: 6.123045444488525, Test_Loss: 6.136782169342041\n",
      "Epoch: 1, Train_Loss: 6.2261061668396, Test_Loss: 6.161002159118652\n",
      "Epoch: 2, Train_Loss: 6.1907453536987305, Test_Loss: 6.333608150482178 *\n",
      "Epoch: 2, Train_Loss: 6.238938808441162, Test_Loss: 6.557607650756836\n",
      "Epoch: 2, Train_Loss: 6.18847131729126, Test_Loss: 6.329052925109863 *\n",
      "Epoch: 2, Train_Loss: 6.239923477172852, Test_Loss: 6.115542888641357 *\n",
      "Epoch: 2, Train_Loss: 6.199693202972412, Test_Loss: 6.061840534210205 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 6.290988922119141, Test_Loss: 6.133186340332031\n",
      "Epoch: 2, Train_Loss: 6.248568058013916, Test_Loss: 6.209259510040283\n",
      "Epoch: 2, Train_Loss: 6.450735092163086, Test_Loss: 6.955498695373535\n",
      "Epoch: 2, Train_Loss: 6.090745449066162, Test_Loss: 7.847283363342285\n",
      "Epoch: 2, Train_Loss: 6.07615852355957, Test_Loss: 6.417199611663818 *\n",
      "Epoch: 2, Train_Loss: 9.153335571289062, Test_Loss: 6.183498382568359 *\n",
      "Epoch: 2, Train_Loss: 6.5783772468566895, Test_Loss: 6.134675979614258 *\n",
      "Epoch: 2, Train_Loss: 6.112575531005859, Test_Loss: 6.095377445220947 *\n",
      "Epoch: 2, Train_Loss: 6.135481834411621, Test_Loss: 6.094372749328613 *\n",
      "Epoch: 2, Train_Loss: 6.0918402671813965, Test_Loss: 6.142467021942139\n",
      "Epoch: 2, Train_Loss: 6.088639736175537, Test_Loss: 6.1936798095703125\n",
      "Epoch: 2, Train_Loss: 6.069253921508789, Test_Loss: 6.118404865264893 *\n",
      "Epoch: 2, Train_Loss: 6.1465229988098145, Test_Loss: 6.072291374206543 *\n",
      "Epoch: 2, Train_Loss: 6.215056419372559, Test_Loss: 6.1476593017578125\n",
      "Epoch: 2, Train_Loss: 6.196000576019287, Test_Loss: 6.408911228179932\n",
      "Epoch: 2, Train_Loss: 6.169554233551025, Test_Loss: 6.124958038330078 *\n",
      "Epoch: 2, Train_Loss: 6.1758952140808105, Test_Loss: 6.291032314300537\n",
      "Epoch: 2, Train_Loss: 6.141533851623535, Test_Loss: 6.119716167449951 *\n",
      "Epoch: 2, Train_Loss: 6.103177070617676, Test_Loss: 6.130343437194824\n",
      "Epoch: 2, Train_Loss: 6.071821212768555, Test_Loss: 6.123260021209717 *\n",
      "Epoch: 2, Train_Loss: 6.082086086273193, Test_Loss: 6.152413845062256\n",
      "Epoch: 2, Train_Loss: 6.0499677658081055, Test_Loss: 6.127952575683594 *\n",
      "Epoch: 2, Train_Loss: 6.096075534820557, Test_Loss: 10.221714973449707\n",
      "Epoch: 2, Train_Loss: 6.0620856285095215, Test_Loss: 7.370372772216797 *\n",
      "Epoch: 2, Train_Loss: 6.151460647583008, Test_Loss: 6.071146011352539 *\n",
      "Epoch: 2, Train_Loss: 6.141781806945801, Test_Loss: 6.091139793395996\n",
      "Epoch: 2, Train_Loss: 6.051100730895996, Test_Loss: 6.058379650115967 *\n",
      "Epoch: 2, Train_Loss: 6.043673038482666, Test_Loss: 6.051247596740723 *\n",
      "Epoch: 2, Train_Loss: 6.056656360626221, Test_Loss: 6.065293312072754\n",
      "Epoch: 2, Train_Loss: 6.067676544189453, Test_Loss: 6.080410480499268\n",
      "Epoch: 2, Train_Loss: 6.076286315917969, Test_Loss: 6.041234016418457 *\n",
      "Epoch: 2, Train_Loss: 6.065627098083496, Test_Loss: 6.059834003448486\n",
      "Epoch: 2, Train_Loss: 6.092589378356934, Test_Loss: 6.06876277923584\n",
      "Epoch: 2, Train_Loss: 6.0764546394348145, Test_Loss: 6.059977054595947 *\n",
      "Epoch: 2, Train_Loss: 6.062482833862305, Test_Loss: 6.062036514282227\n",
      "Epoch: 2, Train_Loss: 6.072380542755127, Test_Loss: 6.097996234893799\n",
      "Epoch: 2, Train_Loss: 6.071002960205078, Test_Loss: 6.067789077758789 *\n",
      "Epoch: 2, Train_Loss: 6.057124614715576, Test_Loss: 6.125220775604248\n",
      "Epoch: 2, Train_Loss: 6.038693904876709, Test_Loss: 6.05441427230835 *\n",
      "Epoch: 2, Train_Loss: 6.064647674560547, Test_Loss: 6.061598777770996\n",
      "Epoch: 2, Train_Loss: 6.07029390335083, Test_Loss: 6.116555213928223\n",
      "Epoch: 2, Train_Loss: 6.081707954406738, Test_Loss: 6.066677570343018 *\n",
      "Epoch: 2, Train_Loss: 6.072535514831543, Test_Loss: 6.059513568878174 *\n",
      "Epoch: 2, Train_Loss: 6.051947116851807, Test_Loss: 6.052245616912842 *\n",
      "Epoch: 2, Train_Loss: 6.082878112792969, Test_Loss: 6.075409889221191\n",
      "Epoch: 2, Train_Loss: 6.067232608795166, Test_Loss: 6.071890354156494 *\n",
      "Epoch: 2, Train_Loss: 6.08433723449707, Test_Loss: 6.072422027587891\n",
      "Epoch: 2, Train_Loss: 6.079031944274902, Test_Loss: 6.0637054443359375 *\n",
      "Epoch: 2, Train_Loss: 6.056613922119141, Test_Loss: 6.054839611053467 *\n",
      "Epoch: 2, Train_Loss: 6.13602352142334, Test_Loss: 6.043930530548096 *\n",
      "Epoch: 2, Train_Loss: 6.110642910003662, Test_Loss: 6.0808424949646\n",
      "Epoch: 2, Train_Loss: 6.078478813171387, Test_Loss: 6.067584037780762 *\n",
      "Epoch: 2, Train_Loss: 6.0888671875, Test_Loss: 6.122098445892334\n",
      "Epoch: 2, Train_Loss: 6.0720953941345215, Test_Loss: 6.163907051086426\n",
      "Epoch: 2, Train_Loss: 6.130716800689697, Test_Loss: 11.70213794708252\n",
      "Epoch: 2, Train_Loss: 6.070519924163818, Test_Loss: 6.096843719482422 *\n",
      "Epoch: 2, Train_Loss: 6.0897536277771, Test_Loss: 6.064398288726807 *\n",
      "Epoch: 2, Train_Loss: 6.081783294677734, Test_Loss: 6.0902791023254395\n",
      "Epoch: 2, Train_Loss: 6.064544200897217, Test_Loss: 6.1301984786987305\n",
      "Epoch: 2, Train_Loss: 6.2425408363342285, Test_Loss: 6.080319404602051 *\n",
      "Epoch: 2, Train_Loss: 6.184659957885742, Test_Loss: 6.055452346801758 *\n",
      "Epoch: 2, Train_Loss: 6.065613269805908, Test_Loss: 6.190536975860596\n",
      "Epoch: 2, Train_Loss: 6.09580135345459, Test_Loss: 6.12887716293335 *\n",
      "Epoch: 2, Train_Loss: 6.093061923980713, Test_Loss: 6.02566385269165 *\n",
      "Epoch: 2, Train_Loss: 6.082759380340576, Test_Loss: 6.0767316818237305\n",
      "Epoch: 2, Train_Loss: 6.049524307250977, Test_Loss: 6.0401291847229 *\n",
      "Epoch: 2, Train_Loss: 6.045689105987549, Test_Loss: 6.049513816833496\n",
      "Epoch: 2, Train_Loss: 6.0539116859436035, Test_Loss: 6.06572961807251\n",
      "Epoch: 2, Train_Loss: 6.142876625061035, Test_Loss: 6.139744758605957\n",
      "Epoch: 2, Train_Loss: 6.121880054473877, Test_Loss: 6.08173942565918 *\n",
      "Epoch: 2, Train_Loss: 6.079576015472412, Test_Loss: 6.189699172973633\n",
      "Epoch: 2, Train_Loss: 6.119704723358154, Test_Loss: 6.127742290496826 *\n",
      "Epoch: 2, Train_Loss: 6.072811126708984, Test_Loss: 6.051022529602051 *\n",
      "Epoch: 2, Train_Loss: 6.052282333374023, Test_Loss: 6.019736289978027 *\n",
      "Epoch: 2, Train_Loss: 6.170782089233398, Test_Loss: 6.064074993133545\n",
      "Epoch: 2, Train_Loss: 6.287550449371338, Test_Loss: 6.0220746994018555 *\n",
      "Epoch: 2, Train_Loss: 6.069170951843262, Test_Loss: 6.032534122467041\n",
      "Epoch: 2, Train_Loss: 6.028603553771973, Test_Loss: 6.032886505126953\n",
      "Epoch: 2, Train_Loss: 6.022706508636475, Test_Loss: 6.028992652893066 *\n",
      "Epoch: 2, Train_Loss: 6.0400567054748535, Test_Loss: 6.043164253234863\n",
      "Epoch: 2, Train_Loss: 6.049440860748291, Test_Loss: 6.029997825622559 *\n",
      "Epoch: 2, Train_Loss: 6.019635200500488, Test_Loss: 6.045290946960449\n",
      "Epoch: 2, Train_Loss: 6.051233291625977, Test_Loss: 6.030330657958984 *\n",
      "Epoch: 2, Train_Loss: 6.050032138824463, Test_Loss: 6.050329208374023\n",
      "Epoch: 2, Train_Loss: 6.063010215759277, Test_Loss: 6.052912712097168\n",
      "Epoch: 2, Train_Loss: 6.072166442871094, Test_Loss: 6.110230445861816\n",
      "Epoch: 2, Train_Loss: 6.058653354644775, Test_Loss: 6.381735801696777\n",
      "Epoch: 2, Train_Loss: 6.033384799957275, Test_Loss: 6.088678359985352 *\n",
      "Epoch: 2, Train_Loss: 6.023672103881836, Test_Loss: 6.116286754608154\n",
      "Epoch: 2, Train_Loss: 6.021762847900391, Test_Loss: 6.13874626159668\n",
      "Epoch: 2, Train_Loss: 6.067178249359131, Test_Loss: 6.33324670791626\n",
      "Epoch: 2, Train_Loss: 6.039190769195557, Test_Loss: 6.093112468719482 *\n",
      "Epoch: 2, Train_Loss: 6.014540195465088, Test_Loss: 6.222191333770752\n",
      "Epoch: 2, Train_Loss: 6.042091369628906, Test_Loss: 6.392219066619873\n",
      "Epoch: 2, Train_Loss: 6.113152027130127, Test_Loss: 6.212841987609863 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 6.112855434417725, Test_Loss: 6.092154026031494 *\n",
      "Epoch: 2, Train_Loss: 6.052928924560547, Test_Loss: 6.050816059112549 *\n",
      "Epoch: 2, Train_Loss: 6.010316371917725, Test_Loss: 6.051940441131592\n",
      "Epoch: 2, Train_Loss: 6.015034198760986, Test_Loss: 6.0686492919921875\n",
      "Epoch: 2, Train_Loss: 6.029841423034668, Test_Loss: 6.48451042175293\n",
      "Epoch: 2, Train_Loss: 6.049537181854248, Test_Loss: 6.624473571777344\n",
      "Epoch: 2, Train_Loss: 6.032644748687744, Test_Loss: 6.445152759552002 *\n",
      "Epoch: 2, Train_Loss: 6.072366714477539, Test_Loss: 6.925514221191406\n",
      "Epoch: 2, Train_Loss: 8.122644424438477, Test_Loss: 6.183887958526611 *\n",
      "Epoch: 2, Train_Loss: 9.414271354675293, Test_Loss: 6.609556198120117\n",
      "Epoch: 2, Train_Loss: 6.045688152313232, Test_Loss: 6.331503868103027 *\n",
      "Epoch: 2, Train_Loss: 6.041797637939453, Test_Loss: 6.025425910949707 *\n",
      "Epoch: 2, Train_Loss: 6.043227672576904, Test_Loss: 6.011101722717285 *\n",
      "Epoch: 2, Train_Loss: 6.260837078094482, Test_Loss: 6.088120460510254\n",
      "Epoch: 2, Train_Loss: 6.08725118637085, Test_Loss: 6.1633806228637695\n",
      "Epoch: 2, Train_Loss: 6.0116658210754395, Test_Loss: 7.044098854064941\n",
      "Epoch: 2, Train_Loss: 6.017246246337891, Test_Loss: 6.293834686279297 *\n",
      "Epoch: 2, Train_Loss: 6.122531890869141, Test_Loss: 8.066242218017578\n",
      "Epoch: 2, Train_Loss: 6.007243633270264, Test_Loss: 6.5462188720703125 *\n",
      "Epoch: 2, Train_Loss: 6.033247947692871, Test_Loss: 7.082731246948242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 6.832450866699219, Test_Loss: 6.060942649841309 *\n",
      "Epoch: 2, Train_Loss: 7.539290428161621, Test_Loss: 6.032349586486816 *\n",
      "Epoch: 2, Train_Loss: 6.888787269592285, Test_Loss: 6.319018363952637\n",
      "Epoch: 2, Train_Loss: 6.171042442321777, Test_Loss: 7.399642467498779\n",
      "Epoch: 2, Train_Loss: 6.508482933044434, Test_Loss: 6.408514022827148 *\n",
      "Epoch: 2, Train_Loss: 8.391817092895508, Test_Loss: 6.210813045501709 *\n",
      "Epoch: 2, Train_Loss: 6.584255695343018, Test_Loss: 6.051183700561523 *\n",
      "Epoch: 2, Train_Loss: 6.016429424285889, Test_Loss: 6.184615135192871\n",
      "Epoch: 2, Train_Loss: 6.011021137237549, Test_Loss: 6.347448348999023\n",
      "Epoch: 2, Train_Loss: 7.342227935791016, Test_Loss: 6.375436782836914\n",
      "Epoch: 2, Train_Loss: 7.694479942321777, Test_Loss: 7.350779056549072\n",
      "Epoch: 2, Train_Loss: 6.397271633148193, Test_Loss: 6.608878135681152 *\n",
      "Epoch: 2, Train_Loss: 6.066294193267822, Test_Loss: 6.01864767074585 *\n",
      "Epoch: 2, Train_Loss: 6.024923801422119, Test_Loss: 6.0222578048706055\n",
      "Epoch: 2, Train_Loss: 6.597201347351074, Test_Loss: 6.029852390289307\n",
      "Epoch: 2, Train_Loss: 6.175400257110596, Test_Loss: 6.05304479598999\n",
      "Epoch: 2, Train_Loss: 6.038052558898926, Test_Loss: 6.116042137145996\n",
      "Epoch: 2, Train_Loss: 6.081557273864746, Test_Loss: 6.602144241333008\n",
      "Epoch: 2, Train_Loss: 6.123939037322998, Test_Loss: 6.451767921447754 *\n",
      "Epoch: 2, Train_Loss: 6.164538860321045, Test_Loss: 6.094440937042236 *\n",
      "Epoch: 2, Train_Loss: 6.112500190734863, Test_Loss: 6.015595436096191 *\n",
      "Epoch: 2, Train_Loss: 6.464962482452393, Test_Loss: 5.997636318206787 *\n",
      "Epoch: 2, Train_Loss: 6.191703796386719, Test_Loss: 6.07006311416626\n",
      "Epoch: 2, Train_Loss: 6.072976589202881, Test_Loss: 6.553218841552734\n",
      "Epoch: 2, Train_Loss: 6.163044452667236, Test_Loss: 7.497903823852539\n",
      "Epoch: 2, Train_Loss: 6.310495376586914, Test_Loss: 6.636568546295166 *\n",
      "Epoch: 2, Train_Loss: 6.454367160797119, Test_Loss: 6.0999555587768555 *\n",
      "Epoch: 2, Train_Loss: 6.0809173583984375, Test_Loss: 6.003632545471191 *\n",
      "Epoch: 2, Train_Loss: 6.174342155456543, Test_Loss: 6.04728889465332\n",
      "Epoch: 2, Train_Loss: 6.170955657958984, Test_Loss: 6.013764381408691 *\n",
      "Epoch: 2, Train_Loss: 6.047172546386719, Test_Loss: 6.000059604644775 *\n",
      "Epoch: 2, Train_Loss: 6.034108638763428, Test_Loss: 6.046863555908203\n",
      "Epoch: 2, Train_Loss: 6.001507759094238, Test_Loss: 6.065720081329346\n",
      "Epoch: 2, Train_Loss: 5.980546474456787, Test_Loss: 5.9816765785217285 *\n",
      "Epoch: 2, Train_Loss: 6.022409439086914, Test_Loss: 6.100298881530762\n",
      "Epoch: 2, Train_Loss: 5.98182487487793, Test_Loss: 6.170337677001953\n",
      "Epoch: 2, Train_Loss: 6.09171724319458, Test_Loss: 6.2733917236328125\n",
      "Epoch: 2, Train_Loss: 6.104490756988525, Test_Loss: 6.191335201263428 *\n",
      "Epoch: 2, Train_Loss: 6.103673934936523, Test_Loss: 6.017932415008545 *\n",
      "Epoch: 2, Train_Loss: 6.093605041503906, Test_Loss: 6.0027079582214355 *\n",
      "Epoch: 2, Train_Loss: 6.571846008300781, Test_Loss: 6.049783706665039\n",
      "Epoch: 2, Train_Loss: 5.999866485595703, Test_Loss: 6.012448787689209 *\n",
      "Epoch: 2, Train_Loss: 6.015895366668701, Test_Loss: 6.022406578063965\n",
      "Epoch: 2, Train_Loss: 6.177717208862305, Test_Loss: 8.643899917602539\n",
      "Epoch: 2, Train_Loss: 6.561192035675049, Test_Loss: 9.099725723266602\n",
      "Epoch: 2, Train_Loss: 6.505168914794922, Test_Loss: 5.999752998352051 *\n",
      "Epoch: 2, Train_Loss: 5.970950126647949, Test_Loss: 6.029575824737549\n",
      "Epoch: 2, Train_Loss: 6.020194053649902, Test_Loss: 6.047142505645752\n",
      "Epoch: 2, Train_Loss: 6.699831962585449, Test_Loss: 5.98122501373291 *\n",
      "Epoch: 2, Train_Loss: 6.823827743530273, Test_Loss: 5.958316326141357 *\n",
      "Epoch: 2, Train_Loss: 6.11988639831543, Test_Loss: 5.9761223793029785\n",
      "Epoch: 2, Train_Loss: 6.019683837890625, Test_Loss: 6.0005693435668945\n",
      "Epoch: 2, Train_Loss: 5.999237537384033, Test_Loss: 5.967105388641357 *\n",
      "Epoch: 2, Train_Loss: 7.054829120635986, Test_Loss: 5.986981391906738\n",
      "Epoch: 2, Train_Loss: 7.4587554931640625, Test_Loss: 5.972287654876709 *\n",
      "Epoch: 2, Train_Loss: 6.015990734100342, Test_Loss: 5.9914984703063965\n",
      "Epoch: 2, Train_Loss: 6.036696434020996, Test_Loss: 5.987681865692139 *\n",
      "Epoch: 2, Train_Loss: 5.969660758972168, Test_Loss: 5.990316867828369\n",
      "Epoch: 2, Train_Loss: 5.984130382537842, Test_Loss: 5.977381706237793 *\n",
      "Epoch: 2, Train_Loss: 6.506251335144043, Test_Loss: 5.981841564178467\n",
      "Epoch: 2, Train_Loss: 6.035787582397461, Test_Loss: 5.983706474304199\n",
      "Epoch: 2, Train_Loss: 6.0558762550354, Test_Loss: 5.999654769897461\n",
      "Epoch: 2, Train_Loss: 6.006575584411621, Test_Loss: 5.9630513191223145 *\n",
      "Epoch: 2, Train_Loss: 5.991136074066162, Test_Loss: 6.0003981590271\n",
      "Epoch: 2, Train_Loss: 23.405162811279297, Test_Loss: 5.972565174102783 *\n",
      "Epoch: 2, Train_Loss: 5.999126434326172, Test_Loss: 5.979068279266357\n",
      "Epoch: 2, Train_Loss: 8.11278247833252, Test_Loss: 5.997295379638672\n",
      "Epoch: 2, Train_Loss: 7.956509590148926, Test_Loss: 5.98907470703125 *\n",
      "Epoch: 2, Train_Loss: 5.982997894287109, Test_Loss: 5.984893321990967 *\n",
      "Epoch: 2, Train_Loss: 6.051440715789795, Test_Loss: 5.951044082641602 *\n",
      "Epoch: 2, Train_Loss: 10.132211685180664, Test_Loss: 5.989582538604736\n",
      "Epoch: 2, Train_Loss: 14.507156372070312, Test_Loss: 5.971517086029053 *\n",
      "Epoch: 2, Train_Loss: 6.0197882652282715, Test_Loss: 5.97868537902832\n",
      "Epoch: 2, Train_Loss: 5.99977445602417, Test_Loss: 6.00347375869751\n",
      "Epoch: 2, Train_Loss: 11.99777603149414, Test_Loss: 6.05100154876709\n",
      "Epoch: 2, Train_Loss: 6.03415584564209, Test_Loss: 11.05905532836914\n",
      "Epoch: 2, Train_Loss: 5.997146129608154, Test_Loss: 6.801048278808594 *\n",
      "Epoch: 2, Train_Loss: 6.006652355194092, Test_Loss: 5.969887733459473 *\n",
      "Epoch: 2, Train_Loss: 5.977951526641846, Test_Loss: 5.983453273773193\n",
      "Epoch: 2, Train_Loss: 5.990500450134277, Test_Loss: 6.033053398132324\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 5.969789505004883, Test_Loss: 5.990090847015381 *\n",
      "Epoch: 2, Train_Loss: 5.972968101501465, Test_Loss: 5.964858055114746 *\n",
      "Epoch: 2, Train_Loss: 5.9788737297058105, Test_Loss: 6.1182332038879395\n",
      "Epoch: 2, Train_Loss: 6.004720687866211, Test_Loss: 6.079537391662598 *\n",
      "Epoch: 2, Train_Loss: 5.98344087600708, Test_Loss: 5.943508625030518 *\n",
      "Epoch: 2, Train_Loss: 5.99497652053833, Test_Loss: 6.042089462280273\n",
      "Epoch: 2, Train_Loss: 6.0074687004089355, Test_Loss: 5.974274635314941 *\n",
      "Epoch: 2, Train_Loss: 6.023284912109375, Test_Loss: 5.985713481903076\n",
      "Epoch: 2, Train_Loss: 6.061515808105469, Test_Loss: 5.9421586990356445 *\n",
      "Epoch: 2, Train_Loss: 6.0034284591674805, Test_Loss: 6.0290021896362305\n",
      "Epoch: 2, Train_Loss: 5.962994575500488, Test_Loss: 6.006331443786621 *\n",
      "Epoch: 2, Train_Loss: 5.972721099853516, Test_Loss: 6.1533074378967285\n",
      "Epoch: 2, Train_Loss: 5.9877028465271, Test_Loss: 6.080040454864502 *\n",
      "Epoch: 2, Train_Loss: 5.960654258728027, Test_Loss: 5.964716911315918 *\n",
      "Epoch: 2, Train_Loss: 5.96038293838501, Test_Loss: 5.961431503295898 *\n",
      "Epoch: 2, Train_Loss: 5.948363304138184, Test_Loss: 5.960867404937744 *\n",
      "Epoch: 2, Train_Loss: 5.9662275314331055, Test_Loss: 5.995828628540039\n",
      "Epoch: 2, Train_Loss: 5.96282958984375, Test_Loss: 5.938599586486816 *\n",
      "Epoch: 2, Train_Loss: 5.961023330688477, Test_Loss: 5.95981502532959\n",
      "Epoch: 2, Train_Loss: 5.938904285430908, Test_Loss: 5.935825824737549 *\n",
      "Epoch: 2, Train_Loss: 5.955996513366699, Test_Loss: 5.949889183044434\n",
      "Epoch: 2, Train_Loss: 5.963685512542725, Test_Loss: 5.941860675811768 *\n",
      "Epoch: 2, Train_Loss: 5.955611228942871, Test_Loss: 5.940874099731445 *\n",
      "Epoch: 2, Train_Loss: 5.968960762023926, Test_Loss: 5.931798458099365 *\n",
      "Epoch: 2, Train_Loss: 5.971221446990967, Test_Loss: 5.946609973907471\n",
      "Epoch: 2, Train_Loss: 7.293855667114258, Test_Loss: 5.975120544433594\n",
      "Epoch: 2, Train_Loss: 14.045225143432617, Test_Loss: 5.955338001251221 *\n",
      "Epoch: 2, Train_Loss: 5.9919047355651855, Test_Loss: 6.383790969848633\n",
      "Epoch: 2, Train_Loss: 5.94121789932251, Test_Loss: 5.963554382324219 *\n",
      "Epoch: 2, Train_Loss: 5.972383975982666, Test_Loss: 5.984426021575928\n",
      "Epoch: 2, Train_Loss: 5.969729423522949, Test_Loss: 5.9911885261535645\n",
      "Epoch: 2, Train_Loss: 5.964084148406982, Test_Loss: 6.237292289733887\n",
      "Epoch: 2, Train_Loss: 5.9586334228515625, Test_Loss: 5.976917266845703 *\n",
      "Epoch: 2, Train_Loss: 5.965545654296875, Test_Loss: 6.1012959480285645\n",
      "Epoch: 2, Train_Loss: 6.191859722137451, Test_Loss: 6.317035675048828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 6.177387237548828, Test_Loss: 6.301609516143799 *\n",
      "Epoch: 2, Train_Loss: 6.07053279876709, Test_Loss: 5.999285697937012 *\n",
      "Epoch: 2, Train_Loss: 6.005167484283447, Test_Loss: 6.001798629760742\n",
      "Epoch: 2, Train_Loss: 6.0707597732543945, Test_Loss: 5.94979190826416 *\n",
      "Epoch: 2, Train_Loss: 6.04550838470459, Test_Loss: 5.953701972961426\n",
      "Epoch: 2, Train_Loss: 6.078276634216309, Test_Loss: 6.123903751373291\n",
      "Epoch: 2, Train_Loss: 6.071348667144775, Test_Loss: 6.873183727264404\n",
      "Epoch: 2, Train_Loss: 6.073304176330566, Test_Loss: 6.137047290802002 *\n",
      "Epoch: 2, Train_Loss: 5.953073024749756, Test_Loss: 6.738984107971191\n",
      "Epoch: 2, Train_Loss: 5.942909240722656, Test_Loss: 6.28618049621582 *\n",
      "Epoch: 2, Train_Loss: 5.9974846839904785, Test_Loss: 6.505739212036133\n",
      "Epoch: 2, Train_Loss: 5.956895351409912, Test_Loss: 6.432782173156738 *\n",
      "Epoch: 2, Train_Loss: 5.933566570281982, Test_Loss: 5.9724907875061035 *\n",
      "Epoch: 2, Train_Loss: 5.945140361785889, Test_Loss: 5.955210208892822 *\n",
      "Epoch: 2, Train_Loss: 5.923186779022217, Test_Loss: 5.96317720413208\n",
      "Epoch: 2, Train_Loss: 7.58254861831665, Test_Loss: 6.022162437438965\n",
      "Epoch: 2, Train_Loss: 9.905889511108398, Test_Loss: 6.898558616638184\n",
      "Epoch: 2, Train_Loss: 5.9488749504089355, Test_Loss: 6.187140464782715 *\n",
      "Epoch: 2, Train_Loss: 5.960684299468994, Test_Loss: 7.66425085067749\n",
      "Epoch: 2, Train_Loss: 5.956815242767334, Test_Loss: 6.51192569732666 *\n",
      "Epoch: 2, Train_Loss: 5.943944454193115, Test_Loss: 7.193134307861328\n",
      "Epoch: 2, Train_Loss: 5.935125827789307, Test_Loss: 6.134757995605469 *\n",
      "Epoch: 2, Train_Loss: 5.976704120635986, Test_Loss: 5.9373321533203125 *\n",
      "Epoch: 2, Train_Loss: 5.943745136260986, Test_Loss: 6.040517330169678\n",
      "Epoch: 2, Train_Loss: 5.94891881942749, Test_Loss: 7.124712944030762\n",
      "Epoch: 2, Train_Loss: 5.933810710906982, Test_Loss: 6.579288005828857 *\n",
      "Epoch: 2, Train_Loss: 5.927855968475342, Test_Loss: 6.058814525604248 *\n",
      "Epoch: 2, Train_Loss: 5.935957431793213, Test_Loss: 5.975932598114014 *\n",
      "Epoch: 2, Train_Loss: 5.899800777435303, Test_Loss: 6.007976055145264\n",
      "Epoch: 2, Train_Loss: 5.923286437988281, Test_Loss: 6.221255302429199\n",
      "Epoch: 2, Train_Loss: 5.94743013381958, Test_Loss: 6.171456813812256 *\n",
      "Epoch: 2, Train_Loss: 5.930017948150635, Test_Loss: 7.1499786376953125\n",
      "Epoch: 2, Train_Loss: 6.015114784240723, Test_Loss: 6.853338718414307 *\n",
      "Epoch: 2, Train_Loss: 5.983950138092041, Test_Loss: 6.039726734161377 *\n",
      "Epoch: 2, Train_Loss: 5.954819202423096, Test_Loss: 5.936339378356934 *\n",
      "Epoch: 2, Train_Loss: 5.922619342803955, Test_Loss: 5.968949317932129\n",
      "Epoch: 2, Train_Loss: 5.919727802276611, Test_Loss: 5.9897260665893555\n",
      "Epoch: 2, Train_Loss: 6.023655891418457, Test_Loss: 5.987040042877197 *\n",
      "Epoch: 2, Train_Loss: 5.953974723815918, Test_Loss: 6.358857154846191\n",
      "Epoch: 2, Train_Loss: 5.9855828285217285, Test_Loss: 6.424365520477295\n",
      "Epoch: 2, Train_Loss: 5.947710037231445, Test_Loss: 6.0389790534973145 *\n",
      "Epoch: 2, Train_Loss: 5.998332500457764, Test_Loss: 5.929152965545654 *\n",
      "Epoch: 2, Train_Loss: 5.986886501312256, Test_Loss: 5.9491682052612305\n",
      "Epoch: 2, Train_Loss: 5.98723840713501, Test_Loss: 5.966454982757568\n",
      "Epoch: 2, Train_Loss: 5.917613983154297, Test_Loss: 6.330439567565918\n",
      "Epoch: 2, Train_Loss: 6.0449724197387695, Test_Loss: 7.52658224105835\n",
      "Epoch: 2, Train_Loss: 5.955840110778809, Test_Loss: 6.824760437011719 *\n",
      "Epoch: 2, Train_Loss: 5.921506881713867, Test_Loss: 5.965788841247559 *\n",
      "Epoch: 2, Train_Loss: 5.930751800537109, Test_Loss: 5.927622318267822 *\n",
      "Epoch: 2, Train_Loss: 5.917369365692139, Test_Loss: 5.936849594116211\n",
      "Epoch: 2, Train_Loss: 5.915627479553223, Test_Loss: 5.915075302124023 *\n",
      "Epoch: 2, Train_Loss: 5.895599365234375, Test_Loss: 5.90002965927124 *\n",
      "Epoch: 2, Train_Loss: 7.798715591430664, Test_Loss: 5.935520648956299\n",
      "Epoch: 2, Train_Loss: 9.117713928222656, Test_Loss: 6.0002360343933105\n",
      "Epoch: 2, Train_Loss: 5.885415554046631, Test_Loss: 5.8948235511779785 *\n",
      "Epoch: 2, Train_Loss: 5.931727409362793, Test_Loss: 5.963166236877441\n",
      "Epoch: 2, Train_Loss: 5.941373348236084, Test_Loss: 6.0374436378479\n",
      "Epoch: 2, Train_Loss: 5.91182804107666, Test_Loss: 6.263791084289551\n",
      "Epoch: 2, Train_Loss: 5.915086269378662, Test_Loss: 6.077821731567383 *\n",
      "Epoch: 2, Train_Loss: 5.89124870300293, Test_Loss: 5.931527614593506 *\n",
      "Epoch: 2, Train_Loss: 5.8830976486206055, Test_Loss: 5.924842834472656 *\n",
      "Epoch: 2, Train_Loss: 5.897637367248535, Test_Loss: 5.93952751159668\n",
      "Epoch: 2, Train_Loss: 5.893948554992676, Test_Loss: 5.906991958618164 *\n",
      "Epoch: 2, Train_Loss: 5.98317813873291, Test_Loss: 5.905665397644043 *\n",
      "Epoch: 2, Train_Loss: 5.995917797088623, Test_Loss: 6.904082298278809\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 6.012227535247803, Test_Loss: 10.425273895263672\n",
      "Epoch: 2, Train_Loss: 5.981858730316162, Test_Loss: 5.91579008102417 *\n",
      "Epoch: 2, Train_Loss: 5.904832363128662, Test_Loss: 5.923940181732178\n",
      "Epoch: 2, Train_Loss: 6.063965320587158, Test_Loss: 5.918057918548584 *\n",
      "Epoch: 2, Train_Loss: 6.083567142486572, Test_Loss: 5.865743637084961 *\n",
      "Epoch: 2, Train_Loss: 6.037480354309082, Test_Loss: 5.86855936050415\n",
      "Epoch: 2, Train_Loss: 5.991538047790527, Test_Loss: 5.876552104949951\n",
      "Epoch: 2, Train_Loss: 5.902026653289795, Test_Loss: 5.882152557373047\n",
      "Epoch: 2, Train_Loss: 5.882521629333496, Test_Loss: 5.890570640563965\n",
      "Epoch: 2, Train_Loss: 5.885427951812744, Test_Loss: 5.881378650665283 *\n",
      "Epoch: 2, Train_Loss: 5.88568115234375, Test_Loss: 5.89607572555542\n",
      "Epoch: 2, Train_Loss: 5.89588737487793, Test_Loss: 5.914368152618408\n",
      "Epoch: 2, Train_Loss: 5.8761701583862305, Test_Loss: 5.9192399978637695\n",
      "Epoch: 2, Train_Loss: 5.901309013366699, Test_Loss: 5.899293422698975 *\n",
      "Epoch: 2, Train_Loss: 5.867059230804443, Test_Loss: 5.895920753479004 *\n",
      "Epoch: 2, Train_Loss: 5.904190540313721, Test_Loss: 5.90772819519043\n",
      "Epoch: 2, Train_Loss: 5.9294753074646, Test_Loss: 5.888586521148682 *\n",
      "Epoch: 2, Train_Loss: 6.063712120056152, Test_Loss: 5.903451919555664\n",
      "Epoch: 2, Train_Loss: 5.994147300720215, Test_Loss: 5.885624408721924 *\n",
      "Epoch: 2, Train_Loss: 5.987933158874512, Test_Loss: 5.875547885894775 *\n",
      "Epoch: 2, Train_Loss: 5.998605728149414, Test_Loss: 5.893094062805176\n",
      "Epoch: 2, Train_Loss: 6.102595806121826, Test_Loss: 5.897739410400391\n",
      "Epoch: 2, Train_Loss: 5.936169624328613, Test_Loss: 5.891489028930664 *\n",
      "Epoch: 2, Train_Loss: 6.0453200340271, Test_Loss: 5.920925140380859\n",
      "Epoch: 2, Train_Loss: 5.991143703460693, Test_Loss: 5.880616188049316 *\n",
      "Epoch: 2, Train_Loss: 6.164198398590088, Test_Loss: 5.870625019073486 *\n",
      "Epoch: 2, Train_Loss: 5.901235580444336, Test_Loss: 5.889227390289307\n",
      "Epoch: 2, Train_Loss: 5.950133323669434, Test_Loss: 5.892822742462158\n",
      "Epoch: 2, Train_Loss: 9.131606101989746, Test_Loss: 5.893998622894287\n",
      "Epoch: 2, Train_Loss: 6.097670555114746, Test_Loss: 5.885733604431152 *\n",
      "Epoch: 2, Train_Loss: 5.912240982055664, Test_Loss: 5.9549994468688965\n",
      "Epoch: 2, Train_Loss: 5.958255767822266, Test_Loss: 9.144424438476562\n",
      "Epoch: 2, Train_Loss: 5.925032138824463, Test_Loss: 8.103307723999023 *\n",
      "Epoch: 2, Train_Loss: 5.874414443969727, Test_Loss: 5.868110656738281 *\n",
      "Epoch: 2, Train_Loss: 5.870701313018799, Test_Loss: 5.874046802520752\n",
      "Epoch: 2, Train_Loss: 5.959541320800781, Test_Loss: 5.877702713012695\n",
      "Epoch: 2, Train_Loss: 6.045744895935059, Test_Loss: 5.898370742797852\n",
      "Epoch: 2, Train_Loss: 5.991201400756836, Test_Loss: 5.877157211303711 *\n",
      "Epoch: 2, Train_Loss: 5.948861598968506, Test_Loss: 5.94625997543335\n",
      "Epoch: 2, Train_Loss: 5.9822096824646, Test_Loss: 5.987509250640869\n",
      "Epoch: 2, Train_Loss: 5.889735221862793, Test_Loss: 5.855621337890625 *\n",
      "Epoch: 2, Train_Loss: 5.899497032165527, Test_Loss: 5.890435218811035\n",
      "Epoch: 2, Train_Loss: 5.874085903167725, Test_Loss: 5.879446029663086 *\n",
      "Epoch: 2, Train_Loss: 5.896036148071289, Test_Loss: 5.883546352386475\n",
      "Epoch: 2, Train_Loss: 5.862663269042969, Test_Loss: 5.880150318145752 *\n",
      "Epoch: 2, Train_Loss: 5.838769912719727, Test_Loss: 5.962585926055908\n",
      "Epoch: 2, Train_Loss: 5.8933186531066895, Test_Loss: 5.9155120849609375 *\n",
      "Epoch: 2, Train_Loss: 5.91937780380249, Test_Loss: 5.989602565765381\n",
      "Epoch: 2, Train_Loss: 5.894919395446777, Test_Loss: 5.963286876678467 *\n",
      "Epoch: 2, Train_Loss: 5.860750675201416, Test_Loss: 5.889912128448486 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 5.855111598968506, Test_Loss: 5.885117053985596 *\n",
      "Epoch: 2, Train_Loss: 5.852243900299072, Test_Loss: 5.867414474487305 *\n",
      "Epoch: 2, Train_Loss: 5.858054161071777, Test_Loss: 5.863393783569336 *\n",
      "Epoch: 2, Train_Loss: 5.865026473999023, Test_Loss: 5.85720682144165 *\n",
      "Epoch: 2, Train_Loss: 5.857365131378174, Test_Loss: 5.863661289215088\n",
      "Epoch: 2, Train_Loss: 5.888033866882324, Test_Loss: 5.86355447769165 *\n",
      "Epoch: 2, Train_Loss: 5.853796005249023, Test_Loss: 5.86084508895874 *\n",
      "Epoch: 2, Train_Loss: 5.853811264038086, Test_Loss: 5.86527681350708\n",
      "Epoch: 2, Train_Loss: 5.841110706329346, Test_Loss: 5.861731052398682 *\n",
      "Epoch: 2, Train_Loss: 5.875982761383057, Test_Loss: 5.8780694007873535\n",
      "Epoch: 2, Train_Loss: 5.852963924407959, Test_Loss: 5.847076416015625 *\n",
      "Epoch: 2, Train_Loss: 5.866925239562988, Test_Loss: 5.884790897369385\n",
      "Epoch: 2, Train_Loss: 5.857140064239502, Test_Loss: 5.942756175994873\n",
      "Epoch: 2, Train_Loss: 5.879056930541992, Test_Loss: 6.177288055419922\n",
      "Epoch: 2, Train_Loss: 5.8664422035217285, Test_Loss: 5.903275012969971 *\n",
      "Epoch: 2, Train_Loss: 5.84970760345459, Test_Loss: 5.896854877471924 *\n",
      "Epoch: 2, Train_Loss: 5.841032981872559, Test_Loss: 5.917638301849365\n",
      "Epoch: 2, Train_Loss: 5.842379093170166, Test_Loss: 6.174929141998291\n",
      "Epoch: 2, Train_Loss: 5.838326454162598, Test_Loss: 5.963263511657715 *\n",
      "Epoch: 2, Train_Loss: 5.870214939117432, Test_Loss: 5.890644550323486 *\n",
      "Epoch: 2, Train_Loss: 5.860272407531738, Test_Loss: 6.114916801452637\n",
      "Epoch: 2, Train_Loss: 5.846568584442139, Test_Loss: 6.164062023162842\n",
      "Epoch: 2, Train_Loss: 5.931515216827393, Test_Loss: 5.857743740081787 *\n",
      "Epoch: 2, Train_Loss: 5.881585121154785, Test_Loss: 5.916609764099121\n",
      "Epoch: 2, Train_Loss: 5.899300575256348, Test_Loss: 5.857133388519287 *\n",
      "Epoch: 2, Train_Loss: 5.8447136878967285, Test_Loss: 5.870182991027832\n",
      "Epoch: 2, Train_Loss: 5.8392791748046875, Test_Loss: 5.893489360809326\n",
      "Epoch: 2, Train_Loss: 5.894167423248291, Test_Loss: 6.773496627807617\n",
      "Epoch: 2, Train_Loss: 5.862168312072754, Test_Loss: 6.0102458000183105 *\n",
      "Epoch: 2, Train_Loss: 5.8574628829956055, Test_Loss: 6.646572589874268\n",
      "Epoch: 2, Train_Loss: 5.889347553253174, Test_Loss: 6.453705310821533 *\n",
      "Epoch: 2, Train_Loss: 5.887796878814697, Test_Loss: 6.206010341644287 *\n",
      "Epoch: 2, Train_Loss: 6.004727840423584, Test_Loss: 6.3485589027404785\n",
      "Epoch: 2, Train_Loss: 5.965656280517578, Test_Loss: 5.8972344398498535 *\n",
      "Epoch: 2, Train_Loss: 5.904894828796387, Test_Loss: 5.840933322906494 *\n",
      "Epoch: 2, Train_Loss: 5.857089996337891, Test_Loss: 5.869269847869873\n",
      "Epoch: 2, Train_Loss: 5.88210391998291, Test_Loss: 6.022067070007324\n",
      "Epoch: 2, Train_Loss: 5.83515739440918, Test_Loss: 6.486304759979248\n",
      "Epoch: 2, Train_Loss: 5.857063293457031, Test_Loss: 6.3529205322265625 *\n",
      "Epoch: 2, Train_Loss: 5.851991176605225, Test_Loss: 7.333507061004639\n",
      "Epoch: 2, Train_Loss: 5.831441402435303, Test_Loss: 6.7473368644714355 *\n",
      "Epoch: 2, Train_Loss: 5.866598129272461, Test_Loss: 6.94085693359375\n",
      "Epoch: 2, Train_Loss: 5.92141056060791, Test_Loss: 6.214631080627441 *\n",
      "Epoch: 2, Train_Loss: 5.895852565765381, Test_Loss: 5.848611831665039 *\n",
      "Epoch: 2, Train_Loss: 5.890903949737549, Test_Loss: 5.8957200050354\n",
      "Epoch: 2, Train_Loss: 5.862491607666016, Test_Loss: 6.768967151641846\n",
      "Epoch: 2, Train_Loss: 5.821205139160156, Test_Loss: 6.98198127746582\n",
      "Epoch: 2, Train_Loss: 5.9996442794799805, Test_Loss: 5.909289836883545 *\n",
      "Epoch: 2, Train_Loss: 6.007272243499756, Test_Loss: 5.938299179077148\n",
      "Epoch: 2, Train_Loss: 5.836159706115723, Test_Loss: 5.856159210205078 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 5.833578586578369, Test_Loss: 6.180383682250977\n",
      "Epoch: 2, Train_Loss: 5.824678897857666, Test_Loss: 6.0022382736206055 *\n",
      "Epoch: 2, Train_Loss: 5.838247776031494, Test_Loss: 6.778103351593018\n",
      "Epoch: 2, Train_Loss: 5.820930480957031, Test_Loss: 6.821081161499023\n",
      "Epoch: 2, Train_Loss: 5.835629463195801, Test_Loss: 6.054316520690918 *\n",
      "Epoch: 2, Train_Loss: 5.837540149688721, Test_Loss: 5.827019214630127 *\n",
      "Epoch: 2, Train_Loss: 5.861555576324463, Test_Loss: 5.865142822265625\n",
      "Epoch: 2, Train_Loss: 5.873340129852295, Test_Loss: 5.856982707977295 *\n",
      "Epoch: 2, Train_Loss: 5.881717681884766, Test_Loss: 5.90334939956665\n",
      "Epoch: 2, Train_Loss: 5.856037139892578, Test_Loss: 6.151671886444092\n",
      "Epoch: 2, Train_Loss: 5.8215107917785645, Test_Loss: 6.401383399963379\n",
      "Epoch: 2, Train_Loss: 5.835206985473633, Test_Loss: 5.991812705993652 *\n",
      "Epoch: 2, Train_Loss: 5.827970504760742, Test_Loss: 5.854454040527344 *\n",
      "Epoch: 2, Train_Loss: 5.827040195465088, Test_Loss: 5.846287727355957 *\n",
      "Epoch: 2, Train_Loss: 5.834232807159424, Test_Loss: 5.837522506713867 *\n",
      "Epoch: 2, Train_Loss: 5.826846122741699, Test_Loss: 5.9996562004089355\n",
      "Epoch: 2, Train_Loss: 5.837538719177246, Test_Loss: 6.970303535461426\n",
      "Epoch: 2, Train_Loss: 5.884438514709473, Test_Loss: 7.096390724182129\n",
      "Epoch: 2, Train_Loss: 5.881966590881348, Test_Loss: 5.871748924255371 *\n",
      "Epoch: 2, Train_Loss: 5.840296745300293, Test_Loss: 5.904592037200928\n",
      "Epoch: 2, Train_Loss: 5.8205132484436035, Test_Loss: 5.809800148010254 *\n",
      "Epoch: 2, Train_Loss: 5.856932640075684, Test_Loss: 5.811348915100098\n",
      "Epoch: 2, Train_Loss: 5.82318639755249, Test_Loss: 5.828015327453613\n",
      "Epoch: 2, Train_Loss: 5.828164100646973, Test_Loss: 5.840091705322266\n",
      "Epoch: 2, Train_Loss: 5.831045150756836, Test_Loss: 5.901494026184082\n",
      "Epoch: 2, Train_Loss: 5.859920501708984, Test_Loss: 5.810117244720459 *\n",
      "Epoch: 2, Train_Loss: 8.204315185546875, Test_Loss: 5.825224876403809\n",
      "Epoch: 2, Train_Loss: 8.932594299316406, Test_Loss: 5.925595283508301\n",
      "Epoch: 2, Train_Loss: 5.828556537628174, Test_Loss: 6.1885576248168945\n",
      "Epoch: 2, Train_Loss: 5.815245628356934, Test_Loss: 6.017804145812988 *\n",
      "Epoch: 2, Train_Loss: 5.875263214111328, Test_Loss: 5.84881067276001 *\n",
      "Epoch: 2, Train_Loss: 6.036540985107422, Test_Loss: 5.834934711456299 *\n",
      "Epoch: 2, Train_Loss: 5.8576741218566895, Test_Loss: 5.81291389465332 *\n",
      "Epoch: 2, Train_Loss: 5.818734169006348, Test_Loss: 5.814953327178955\n",
      "Epoch: 2, Train_Loss: 5.806702136993408, Test_Loss: 5.8293352127075195\n",
      "Epoch: 2, Train_Loss: 5.907314300537109, Test_Loss: 6.013875961303711\n",
      "Epoch: 2, Train_Loss: 5.816213130950928, Test_Loss: 11.015705108642578\n",
      "Epoch: 2, Train_Loss: 5.811516284942627, Test_Loss: 5.909532070159912 *\n",
      "Epoch: 2, Train_Loss: 6.7222208976745605, Test_Loss: 5.826696395874023 *\n",
      "Epoch: 2, Train_Loss: 7.208115100860596, Test_Loss: 5.839456558227539\n",
      "Epoch: 2, Train_Loss: 6.372067928314209, Test_Loss: 5.841887950897217\n",
      "Epoch: 2, Train_Loss: 5.947762966156006, Test_Loss: 5.813156604766846 *\n",
      "Epoch: 2, Train_Loss: 6.7200727462768555, Test_Loss: 5.82490873336792\n",
      "Epoch: 2, Train_Loss: 8.251253128051758, Test_Loss: 5.80877685546875 *\n",
      "Epoch: 2, Train_Loss: 6.192624092102051, Test_Loss: 5.793961048126221 *\n",
      "Epoch: 2, Train_Loss: 5.844939231872559, Test_Loss: 5.838624000549316\n",
      "Epoch: 2, Train_Loss: 5.831942081451416, Test_Loss: 5.809483051300049 *\n",
      "Epoch: 2, Train_Loss: 7.402487754821777, Test_Loss: 5.813724517822266\n",
      "Epoch: 2, Train_Loss: 7.325787544250488, Test_Loss: 5.86727237701416\n",
      "Epoch: 2, Train_Loss: 5.9091691970825195, Test_Loss: 5.829704284667969 *\n",
      "Epoch: 2, Train_Loss: 5.81487512588501, Test_Loss: 5.813878059387207 *\n",
      "Epoch: 2, Train_Loss: 5.78842306137085, Test_Loss: 5.8095269203186035 *\n",
      "Epoch: 2, Train_Loss: 6.487314701080322, Test_Loss: 5.815749645233154\n",
      "Epoch: 2, Train_Loss: 5.905145645141602, Test_Loss: 5.800059795379639 *\n",
      "Epoch: 3, Train_Loss: 5.833393573760986, Test_Loss: 5.79454231262207 *\n",
      "Epoch: 3, Train_Loss: 5.815524101257324, Test_Loss: 5.811715602874756\n",
      "Epoch: 3, Train_Loss: 5.919778823852539, Test_Loss: 5.807494163513184 *\n",
      "Epoch: 3, Train_Loss: 5.93938684463501, Test_Loss: 5.803088665008545 *\n",
      "Epoch: 3, Train_Loss: 5.965786933898926, Test_Loss: 5.787796974182129 *\n",
      "Epoch: 3, Train_Loss: 6.166788578033447, Test_Loss: 5.818197250366211\n",
      "Epoch: 3, Train_Loss: 5.911815166473389, Test_Loss: 5.776017665863037 *\n",
      "Epoch: 3, Train_Loss: 5.878997802734375, Test_Loss: 5.805893421173096\n",
      "Epoch: 3, Train_Loss: 6.022100448608398, Test_Loss: 5.794778347015381 *\n",
      "Epoch: 3, Train_Loss: 6.211972713470459, Test_Loss: 5.799220085144043\n",
      "Epoch: 3, Train_Loss: 6.214917182922363, Test_Loss: 5.787601470947266 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 5.813235282897949, Test_Loss: 5.801356792449951\n",
      "Epoch: 3, Train_Loss: 5.947491645812988, Test_Loss: 5.857940196990967\n",
      "Epoch: 3, Train_Loss: 5.982810974121094, Test_Loss: 7.860772132873535\n",
      "Epoch: 3, Train_Loss: 5.83400297164917, Test_Loss: 9.3826904296875\n",
      "Epoch: 3, Train_Loss: 5.843520164489746, Test_Loss: 5.778071403503418 *\n",
      "Epoch: 3, Train_Loss: 5.773409366607666, Test_Loss: 5.769880294799805 *\n",
      "Epoch: 3, Train_Loss: 5.797427177429199, Test_Loss: 5.812211036682129\n",
      "Epoch: 3, Train_Loss: 5.796470642089844, Test_Loss: 5.811290740966797 *\n",
      "Epoch: 3, Train_Loss: 5.8289995193481445, Test_Loss: 5.863809108734131\n",
      "Epoch: 3, Train_Loss: 5.875144958496094, Test_Loss: 5.817836284637451 *\n",
      "Epoch: 3, Train_Loss: 5.876907825469971, Test_Loss: 5.960031986236572\n",
      "Epoch: 3, Train_Loss: 5.91864013671875, Test_Loss: 5.784150123596191 *\n",
      "Epoch: 3, Train_Loss: 5.878253936767578, Test_Loss: 5.805252552032471\n",
      "Epoch: 3, Train_Loss: 6.255850791931152, Test_Loss: 5.8027262687683105 *\n",
      "Epoch: 3, Train_Loss: 5.77156400680542, Test_Loss: 5.778561115264893 *\n",
      "Epoch: 3, Train_Loss: 5.7951531410217285, Test_Loss: 5.7890625\n",
      "Epoch: 3, Train_Loss: 5.978938102722168, Test_Loss: 5.857424259185791\n",
      "Epoch: 3, Train_Loss: 6.373558521270752, Test_Loss: 5.859762191772461\n",
      "Epoch: 3, Train_Loss: 6.1153483390808105, Test_Loss: 5.894898891448975\n",
      "Epoch: 3, Train_Loss: 5.774211883544922, Test_Loss: 5.9174652099609375\n",
      "Epoch: 3, Train_Loss: 5.900566577911377, Test_Loss: 5.79114294052124 *\n",
      "Epoch: 3, Train_Loss: 6.411297798156738, Test_Loss: 5.788726806640625 *\n",
      "Epoch: 3, Train_Loss: 6.402327537536621, Test_Loss: 5.788199424743652 *\n",
      "Epoch: 3, Train_Loss: 5.838371276855469, Test_Loss: 5.77589750289917 *\n",
      "Epoch: 3, Train_Loss: 5.7883501052856445, Test_Loss: 5.776151657104492\n",
      "Epoch: 3, Train_Loss: 5.796229362487793, Test_Loss: 5.764580249786377 *\n",
      "Epoch: 3, Train_Loss: 6.983809471130371, Test_Loss: 5.771844863891602\n",
      "Epoch: 3, Train_Loss: 6.997552871704102, Test_Loss: 5.766892433166504 *\n",
      "Epoch: 3, Train_Loss: 5.780208587646484, Test_Loss: 5.777981758117676\n",
      "Epoch: 3, Train_Loss: 5.82318115234375, Test_Loss: 5.777475357055664 *\n",
      "Epoch: 3, Train_Loss: 5.757994651794434, Test_Loss: 5.785097122192383\n",
      "Epoch: 3, Train_Loss: 5.789848327636719, Test_Loss: 5.759227275848389 *\n",
      "Epoch: 3, Train_Loss: 6.142865180969238, Test_Loss: 5.794764518737793\n",
      "Epoch: 3, Train_Loss: 5.805028915405273, Test_Loss: 5.841065406799316\n",
      "Epoch: 3, Train_Loss: 5.825087070465088, Test_Loss: 6.030215740203857\n",
      "Epoch: 3, Train_Loss: 5.767201900482178, Test_Loss: 5.869083881378174 *\n",
      "Epoch: 3, Train_Loss: 5.77681303024292, Test_Loss: 5.808655738830566 *\n",
      "Epoch: 3, Train_Loss: 23.095233917236328, Test_Loss: 5.843775272369385\n",
      "Epoch: 3, Train_Loss: 5.773257732391357, Test_Loss: 6.015612602233887\n",
      "Epoch: 3, Train_Loss: 8.437728881835938, Test_Loss: 5.999316215515137 *\n",
      "Epoch: 3, Train_Loss: 7.458343982696533, Test_Loss: 5.767108917236328 *\n",
      "Epoch: 3, Train_Loss: 5.792155742645264, Test_Loss: 5.990769386291504\n",
      "Epoch: 3, Train_Loss: 5.827112674713135, Test_Loss: 6.13550329208374\n",
      "Epoch: 3, Train_Loss: 11.892999649047852, Test_Loss: 5.799992084503174 *\n",
      "Epoch: 3, Train_Loss: 12.33380126953125, Test_Loss: 5.8103837966918945\n",
      "Epoch: 3, Train_Loss: 5.77604866027832, Test_Loss: 5.755561828613281 *\n",
      "Epoch: 3, Train_Loss: 5.858920574188232, Test_Loss: 5.764441967010498\n",
      "Epoch: 3, Train_Loss: 11.992128372192383, Test_Loss: 5.772550582885742\n",
      "Epoch: 3, Train_Loss: 5.807343482971191, Test_Loss: 6.628751277923584\n",
      "Epoch: 3, Train_Loss: 5.8033599853515625, Test_Loss: 6.026279449462891 *\n",
      "Epoch: 3, Train_Loss: 5.7880377769470215, Test_Loss: 6.312340259552002\n",
      "Epoch: 3, Train_Loss: 5.7946624755859375, Test_Loss: 6.505473613739014\n",
      "Epoch: 3, Train_Loss: 5.7960405349731445, Test_Loss: 5.982418537139893 *\n",
      "Epoch: 3, Train_Loss: 5.774588108062744, Test_Loss: 6.493229866027832\n",
      "Epoch: 3, Train_Loss: 5.778097152709961, Test_Loss: 5.917670726776123 *\n",
      "Epoch: 3, Train_Loss: 5.773580074310303, Test_Loss: 5.750540733337402 *\n",
      "Epoch: 3, Train_Loss: 5.807844638824463, Test_Loss: 5.764782428741455\n",
      "Epoch: 3, Train_Loss: 5.808113098144531, Test_Loss: 5.856069087982178\n",
      "Epoch: 3, Train_Loss: 5.764522075653076, Test_Loss: 6.096334457397461\n",
      "Epoch: 3, Train_Loss: 5.791571617126465, Test_Loss: 6.764625549316406\n",
      "Epoch: 3, Train_Loss: 5.844605922698975, Test_Loss: 6.551465034484863 *\n",
      "Epoch: 3, Train_Loss: 5.838860511779785, Test_Loss: 7.155786037445068\n",
      "Epoch: 3, Train_Loss: 5.79351282119751, Test_Loss: 6.756936073303223 *\n",
      "Epoch: 3, Train_Loss: 5.767814636230469, Test_Loss: 6.4515533447265625 *\n",
      "Epoch: 3, Train_Loss: 5.7730865478515625, Test_Loss: 5.77594518661499 *\n",
      "Epoch: 3, Train_Loss: 5.753968238830566, Test_Loss: 5.758923053741455 *\n",
      "Epoch: 3, Train_Loss: 5.749105453491211, Test_Loss: 6.335615158081055\n",
      "Epoch: 3, Train_Loss: 5.7676897048950195, Test_Loss: 6.904460906982422\n",
      "Epoch: 3, Train_Loss: 5.746770858764648, Test_Loss: 5.841725826263428 *\n",
      "Epoch: 3, Train_Loss: 5.772051811218262, Test_Loss: 5.885916709899902\n",
      "Epoch: 3, Train_Loss: 5.749063968658447, Test_Loss: 5.760637283325195 *\n",
      "Epoch: 3, Train_Loss: 5.748191833496094, Test_Loss: 5.93796968460083\n",
      "Epoch: 3, Train_Loss: 5.73709774017334, Test_Loss: 6.000330924987793\n",
      "Epoch: 3, Train_Loss: 5.733459949493408, Test_Loss: 6.500446796417236\n",
      "Epoch: 3, Train_Loss: 5.772965908050537, Test_Loss: 7.16969633102417\n",
      "Epoch: 3, Train_Loss: 5.79310941696167, Test_Loss: 6.139730930328369 *\n",
      "Epoch: 3, Train_Loss: 5.761187553405762, Test_Loss: 5.7500152587890625 *\n",
      "Epoch: 3, Train_Loss: 5.761574745178223, Test_Loss: 5.78166389465332\n",
      "Epoch: 3, Train_Loss: 8.768564224243164, Test_Loss: 5.800229072570801\n",
      "Epoch: 3, Train_Loss: 11.931636810302734, Test_Loss: 5.831424236297607\n",
      "Epoch: 3, Train_Loss: 5.774318218231201, Test_Loss: 5.958237171173096\n",
      "Epoch: 3, Train_Loss: 5.725253105163574, Test_Loss: 6.311879634857178\n",
      "Epoch: 3, Train_Loss: 5.753464221954346, Test_Loss: 5.948643684387207 *\n",
      "Epoch: 3, Train_Loss: 5.747107028961182, Test_Loss: 5.784855365753174 *\n",
      "Epoch: 3, Train_Loss: 5.747954845428467, Test_Loss: 5.744153022766113 *\n",
      "Epoch: 3, Train_Loss: 5.754321575164795, Test_Loss: 5.766034126281738\n",
      "Epoch: 3, Train_Loss: 5.7769083976745605, Test_Loss: 5.907817363739014\n",
      "Epoch: 3, Train_Loss: 6.019522666931152, Test_Loss: 6.73085880279541\n",
      "Epoch: 3, Train_Loss: 5.930397987365723, Test_Loss: 7.369429588317871\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 5.827608108520508, Test_Loss: 5.951061248779297 *\n",
      "Epoch: 3, Train_Loss: 5.7817277908325195, Test_Loss: 5.781052112579346 *\n",
      "Epoch: 3, Train_Loss: 5.8462677001953125, Test_Loss: 5.725035667419434 *\n",
      "Epoch: 3, Train_Loss: 5.822206020355225, Test_Loss: 5.766495227813721\n",
      "Epoch: 3, Train_Loss: 5.889535903930664, Test_Loss: 5.753097057342529 *\n",
      "Epoch: 3, Train_Loss: 5.823108673095703, Test_Loss: 5.737975120544434 *\n",
      "Epoch: 3, Train_Loss: 5.815705299377441, Test_Loss: 5.800644397735596\n",
      "Epoch: 3, Train_Loss: 5.718618869781494, Test_Loss: 5.792418003082275 *\n",
      "Epoch: 3, Train_Loss: 5.76059103012085, Test_Loss: 5.737964630126953 *\n",
      "Epoch: 3, Train_Loss: 5.786316871643066, Test_Loss: 5.812331199645996\n",
      "Epoch: 3, Train_Loss: 5.72451114654541, Test_Loss: 6.108401775360107\n",
      "Epoch: 3, Train_Loss: 5.725561618804932, Test_Loss: 5.7864532470703125 *\n",
      "Epoch: 3, Train_Loss: 5.7114410400390625, Test_Loss: 5.846039772033691\n",
      "Epoch: 3, Train_Loss: 5.7398481369018555, Test_Loss: 5.728054523468018 *\n",
      "Epoch: 3, Train_Loss: 8.821603775024414, Test_Loss: 5.721850395202637 *\n",
      "Epoch: 3, Train_Loss: 8.463634490966797, Test_Loss: 5.700098514556885 *\n",
      "Epoch: 3, Train_Loss: 5.731340408325195, Test_Loss: 5.711287975311279\n",
      "Epoch: 3, Train_Loss: 5.731142520904541, Test_Loss: 5.719327449798584\n",
      "Epoch: 3, Train_Loss: 5.748241901397705, Test_Loss: 10.999855041503906\n",
      "Epoch: 3, Train_Loss: 5.7133708000183105, Test_Loss: 6.3447723388671875 *\n",
      "Epoch: 3, Train_Loss: 5.726007461547852, Test_Loss: 5.736754417419434 *\n",
      "Epoch: 3, Train_Loss: 5.722871780395508, Test_Loss: 5.7161054611206055 *\n",
      "Epoch: 3, Train_Loss: 5.709604740142822, Test_Loss: 5.710692882537842 *\n",
      "Epoch: 3, Train_Loss: 5.7258524894714355, Test_Loss: 5.701493740081787 *\n",
      "Epoch: 3, Train_Loss: 5.707122802734375, Test_Loss: 5.688793182373047 *\n",
      "Epoch: 3, Train_Loss: 5.715029239654541, Test_Loss: 5.707263946533203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 5.707451343536377, Test_Loss: 5.712120056152344\n",
      "Epoch: 3, Train_Loss: 5.717487335205078, Test_Loss: 5.721506595611572\n",
      "Epoch: 3, Train_Loss: 5.7069501876831055, Test_Loss: 5.704729080200195 *\n",
      "Epoch: 3, Train_Loss: 5.714382648468018, Test_Loss: 5.711966037750244\n",
      "Epoch: 3, Train_Loss: 5.7049407958984375, Test_Loss: 5.700541973114014 *\n",
      "Epoch: 3, Train_Loss: 5.79640531539917, Test_Loss: 5.71502685546875\n",
      "Epoch: 3, Train_Loss: 5.7628560066223145, Test_Loss: 5.693342685699463 *\n",
      "Epoch: 3, Train_Loss: 5.703558444976807, Test_Loss: 5.7171854972839355\n",
      "Epoch: 3, Train_Loss: 5.705373764038086, Test_Loss: 5.699061393737793 *\n",
      "Epoch: 3, Train_Loss: 5.7450737953186035, Test_Loss: 5.7025957107543945\n",
      "Epoch: 3, Train_Loss: 5.818176746368408, Test_Loss: 5.697493553161621 *\n",
      "Epoch: 3, Train_Loss: 5.75746488571167, Test_Loss: 5.710368633270264\n",
      "Epoch: 3, Train_Loss: 5.765193939208984, Test_Loss: 5.7243332862854\n",
      "Epoch: 3, Train_Loss: 5.732234477996826, Test_Loss: 5.702846050262451 *\n",
      "Epoch: 3, Train_Loss: 5.767457008361816, Test_Loss: 5.718466758728027\n",
      "Epoch: 3, Train_Loss: 5.771357536315918, Test_Loss: 5.705721855163574 *\n",
      "Epoch: 3, Train_Loss: 5.749913692474365, Test_Loss: 5.707577705383301\n",
      "Epoch: 3, Train_Loss: 5.747529983520508, Test_Loss: 5.720092296600342\n",
      "Epoch: 3, Train_Loss: 5.763425350189209, Test_Loss: 5.695971488952637 *\n",
      "Epoch: 3, Train_Loss: 5.711482524871826, Test_Loss: 5.692515850067139 *\n",
      "Epoch: 3, Train_Loss: 5.6971940994262695, Test_Loss: 5.737720966339111\n",
      "Epoch: 3, Train_Loss: 5.706329345703125, Test_Loss: 5.69465446472168 *\n",
      "Epoch: 3, Train_Loss: 5.683926582336426, Test_Loss: 5.752946853637695\n",
      "Epoch: 3, Train_Loss: 5.699580669403076, Test_Loss: 6.322540283203125\n",
      "Epoch: 3, Train_Loss: 5.710123062133789, Test_Loss: 10.844560623168945\n",
      "Epoch: 3, Train_Loss: 8.700788497924805, Test_Loss: 5.69508171081543 *\n",
      "Epoch: 3, Train_Loss: 7.643030643463135, Test_Loss: 5.678885459899902 *\n",
      "Epoch: 3, Train_Loss: 5.681849479675293, Test_Loss: 5.7342705726623535\n",
      "Epoch: 3, Train_Loss: 5.6990180015563965, Test_Loss: 5.715317726135254 *\n",
      "Epoch: 3, Train_Loss: 5.689304828643799, Test_Loss: 5.7055253982543945 *\n",
      "Epoch: 3, Train_Loss: 5.691720962524414, Test_Loss: 5.675954341888428 *\n",
      "Epoch: 3, Train_Loss: 5.677558422088623, Test_Loss: 5.836298942565918\n",
      "Epoch: 3, Train_Loss: 5.667052745819092, Test_Loss: 5.7158989906311035 *\n",
      "Epoch: 3, Train_Loss: 5.679524898529053, Test_Loss: 5.681222915649414 *\n",
      "Epoch: 3, Train_Loss: 5.672422885894775, Test_Loss: 5.728938102722168\n",
      "Epoch: 3, Train_Loss: 5.698970794677734, Test_Loss: 5.696340084075928 *\n",
      "Epoch: 3, Train_Loss: 5.775259017944336, Test_Loss: 5.68093729019165 *\n",
      "Epoch: 3, Train_Loss: 5.789871692657471, Test_Loss: 5.736867427825928\n",
      "Epoch: 3, Train_Loss: 5.759763240814209, Test_Loss: 5.821410179138184\n",
      "Epoch: 3, Train_Loss: 5.706747055053711, Test_Loss: 5.726257801055908 *\n",
      "Epoch: 3, Train_Loss: 5.693585395812988, Test_Loss: 5.797000408172607\n",
      "Epoch: 3, Train_Loss: 5.8508219718933105, Test_Loss: 5.7233757972717285 *\n",
      "Epoch: 3, Train_Loss: 5.879086494445801, Test_Loss: 5.716908931732178 *\n",
      "Epoch: 3, Train_Loss: 5.882776737213135, Test_Loss: 5.687287330627441 *\n",
      "Epoch: 3, Train_Loss: 5.745681285858154, Test_Loss: 5.681285858154297 *\n",
      "Epoch: 3, Train_Loss: 5.659401893615723, Test_Loss: 5.688321590423584\n",
      "Epoch: 3, Train_Loss: 5.680873394012451, Test_Loss: 5.66786527633667 *\n",
      "Epoch: 3, Train_Loss: 5.659689903259277, Test_Loss: 5.686468124389648\n",
      "Epoch: 3, Train_Loss: 5.673953056335449, Test_Loss: 5.675932884216309 *\n",
      "Epoch: 3, Train_Loss: 5.677111625671387, Test_Loss: 5.681835174560547\n",
      "Epoch: 3, Train_Loss: 5.661175727844238, Test_Loss: 5.683039665222168\n",
      "Epoch: 3, Train_Loss: 5.658714294433594, Test_Loss: 5.679419994354248 *\n",
      "Epoch: 3, Train_Loss: 5.667044162750244, Test_Loss: 5.660390853881836 *\n",
      "Epoch: 3, Train_Loss: 5.657796859741211, Test_Loss: 5.672094821929932\n",
      "Epoch: 3, Train_Loss: 5.754349231719971, Test_Loss: 5.727935314178467\n",
      "Epoch: 3, Train_Loss: 5.847415447235107, Test_Loss: 5.789348602294922\n",
      "Epoch: 3, Train_Loss: 5.835778713226318, Test_Loss: 5.933988094329834\n",
      "Epoch: 3, Train_Loss: 5.768559455871582, Test_Loss: 5.694330215454102 *\n",
      "Epoch: 3, Train_Loss: 5.795413494110107, Test_Loss: 5.749533176422119\n",
      "Epoch: 3, Train_Loss: 5.849020957946777, Test_Loss: 5.769687175750732\n",
      "Epoch: 3, Train_Loss: 5.691983222961426, Test_Loss: 5.915041446685791\n",
      "Epoch: 3, Train_Loss: 5.849746227264404, Test_Loss: 5.683183670043945 *\n",
      "Epoch: 3, Train_Loss: 5.7929511070251465, Test_Loss: 5.892218589782715\n",
      "Epoch: 3, Train_Loss: 5.963176250457764, Test_Loss: 6.025210380554199\n",
      "Epoch: 3, Train_Loss: 5.663877010345459, Test_Loss: 5.750658988952637 *\n",
      "Epoch: 3, Train_Loss: 6.066225051879883, Test_Loss: 5.722537040710449 *\n",
      "Epoch: 3, Train_Loss: 8.528362274169922, Test_Loss: 5.676273822784424 *\n",
      "Epoch: 3, Train_Loss: 5.722090721130371, Test_Loss: 5.659429550170898 *\n",
      "Epoch: 3, Train_Loss: 5.689619541168213, Test_Loss: 5.6891188621521\n",
      "Epoch: 3, Train_Loss: 5.699435710906982, Test_Loss: 6.261693477630615\n",
      "Epoch: 3, Train_Loss: 5.6905927658081055, Test_Loss: 6.071893215179443 *\n",
      "Epoch: 3, Train_Loss: 5.641447067260742, Test_Loss: 6.2307257652282715\n",
      "Epoch: 3, Train_Loss: 5.650976657867432, Test_Loss: 6.418471336364746\n",
      "Epoch: 3, Train_Loss: 5.759578704833984, Test_Loss: 5.78812837600708 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 5.749730587005615, Test_Loss: 6.34641170501709\n",
      "Epoch: 3, Train_Loss: 5.754438400268555, Test_Loss: 5.860098361968994 *\n",
      "Epoch: 3, Train_Loss: 5.736105442047119, Test_Loss: 5.657736301422119 *\n",
      "Epoch: 3, Train_Loss: 5.749042510986328, Test_Loss: 5.658740043640137\n",
      "Epoch: 3, Train_Loss: 5.680893898010254, Test_Loss: 5.7994771003723145\n",
      "Epoch: 3, Train_Loss: 5.684567451477051, Test_Loss: 5.8313398361206055\n",
      "Epoch: 3, Train_Loss: 5.669400215148926, Test_Loss: 6.615616798400879\n",
      "Epoch: 3, Train_Loss: 5.686769008636475, Test_Loss: 6.12015962600708 *\n",
      "Epoch: 3, Train_Loss: 5.676262378692627, Test_Loss: 7.701439380645752\n",
      "Epoch: 3, Train_Loss: 5.639559268951416, Test_Loss: 6.127658843994141 *\n",
      "Epoch: 3, Train_Loss: 5.6725544929504395, Test_Loss: 6.410038948059082\n",
      "Epoch: 3, Train_Loss: 5.701397895812988, Test_Loss: 5.659696102142334 *\n",
      "Epoch: 3, Train_Loss: 5.66839599609375, Test_Loss: 5.683420181274414\n",
      "Epoch: 3, Train_Loss: 5.645295143127441, Test_Loss: 6.129395008087158\n",
      "Epoch: 3, Train_Loss: 5.6429123878479, Test_Loss: 7.256558418273926\n",
      "Epoch: 3, Train_Loss: 5.637419700622559, Test_Loss: 5.853977203369141 *\n",
      "Epoch: 3, Train_Loss: 5.652704238891602, Test_Loss: 5.762372016906738 *\n",
      "Epoch: 3, Train_Loss: 5.6373090744018555, Test_Loss: 5.653878688812256 *\n",
      "Epoch: 3, Train_Loss: 5.642435073852539, Test_Loss: 5.7984490394592285\n",
      "Epoch: 3, Train_Loss: 5.65162467956543, Test_Loss: 5.984169960021973\n",
      "Epoch: 3, Train_Loss: 5.651423454284668, Test_Loss: 6.046701908111572\n",
      "Epoch: 3, Train_Loss: 5.6280293464660645, Test_Loss: 6.969362258911133\n",
      "Epoch: 3, Train_Loss: 5.631293773651123, Test_Loss: 6.095859050750732 *\n",
      "Epoch: 3, Train_Loss: 5.6446099281311035, Test_Loss: 5.646650791168213 *\n",
      "Epoch: 3, Train_Loss: 5.643437385559082, Test_Loss: 5.644412517547607 *\n",
      "Epoch: 3, Train_Loss: 5.645601749420166, Test_Loss: 5.659071445465088\n",
      "Epoch: 3, Train_Loss: 5.664071083068848, Test_Loss: 5.676931858062744\n",
      "Epoch: 3, Train_Loss: 5.627951622009277, Test_Loss: 5.835327625274658\n",
      "Epoch: 3, Train_Loss: 5.639267444610596, Test_Loss: 6.184922218322754\n",
      "Epoch: 3, Train_Loss: 5.631643772125244, Test_Loss: 5.940781593322754 *\n",
      "Epoch: 3, Train_Loss: 5.624605178833008, Test_Loss: 5.729319095611572 *\n",
      "Epoch: 3, Train_Loss: 5.623331546783447, Test_Loss: 5.668839454650879 *\n",
      "Epoch: 3, Train_Loss: 5.6333842277526855, Test_Loss: 5.638689041137695 *\n",
      "Epoch: 3, Train_Loss: 5.62374210357666, Test_Loss: 5.714178562164307\n",
      "Epoch: 3, Train_Loss: 5.62734842300415, Test_Loss: 6.267220497131348\n",
      "Epoch: 3, Train_Loss: 5.638283729553223, Test_Loss: 7.0482635498046875\n",
      "Epoch: 3, Train_Loss: 5.686830520629883, Test_Loss: 6.023719787597656 *\n",
      "Epoch: 3, Train_Loss: 5.652711868286133, Test_Loss: 5.744244575500488 *\n",
      "Epoch: 3, Train_Loss: 5.669336795806885, Test_Loss: 5.630005836486816 *\n",
      "Epoch: 3, Train_Loss: 5.627260208129883, Test_Loss: 5.615142345428467 *\n",
      "Epoch: 3, Train_Loss: 5.628495693206787, Test_Loss: 5.622951507568359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 5.65755558013916, Test_Loss: 5.62257719039917 *\n",
      "Epoch: 3, Train_Loss: 5.618563652038574, Test_Loss: 5.663482666015625\n",
      "Epoch: 3, Train_Loss: 5.637491226196289, Test_Loss: 5.674173355102539\n",
      "Epoch: 3, Train_Loss: 5.65666389465332, Test_Loss: 5.628614902496338 *\n",
      "Epoch: 3, Train_Loss: 5.688617706298828, Test_Loss: 5.72125768661499\n",
      "Epoch: 3, Train_Loss: 5.7382354736328125, Test_Loss: 5.933916091918945\n",
      "Epoch: 3, Train_Loss: 5.69854736328125, Test_Loss: 5.760343074798584 *\n",
      "Epoch: 3, Train_Loss: 5.659176826477051, Test_Loss: 5.7821221351623535\n",
      "Epoch: 3, Train_Loss: 5.636117458343506, Test_Loss: 5.638437271118164 *\n",
      "Epoch: 3, Train_Loss: 5.636740207672119, Test_Loss: 5.625409126281738 *\n",
      "Epoch: 3, Train_Loss: 5.615949630737305, Test_Loss: 5.636715888977051\n",
      "Epoch: 3, Train_Loss: 5.614536285400391, Test_Loss: 5.628879547119141 *\n",
      "Epoch: 3, Train_Loss: 5.640582084655762, Test_Loss: 5.655832290649414\n",
      "Epoch: 3, Train_Loss: 5.630587100982666, Test_Loss: 9.043304443359375\n",
      "Epoch: 3, Train_Loss: 5.693195819854736, Test_Loss: 7.7151336669921875 *\n",
      "Epoch: 3, Train_Loss: 5.63361120223999, Test_Loss: 5.6185503005981445 *\n",
      "Epoch: 3, Train_Loss: 5.68873929977417, Test_Loss: 5.61106538772583 *\n",
      "Epoch: 3, Train_Loss: 5.654138565063477, Test_Loss: 5.60629940032959 *\n",
      "Epoch: 3, Train_Loss: 5.626789093017578, Test_Loss: 5.613908767700195\n",
      "Epoch: 3, Train_Loss: 5.61746072769165, Test_Loss: 5.612539768218994 *\n",
      "Epoch: 3, Train_Loss: 5.833517074584961, Test_Loss: 5.623966217041016\n",
      "Epoch: 3, Train_Loss: 5.68082332611084, Test_Loss: 5.609972953796387 *\n",
      "Epoch: 3, Train_Loss: 5.607386589050293, Test_Loss: 5.609246730804443 *\n",
      "Epoch: 3, Train_Loss: 5.6279377937316895, Test_Loss: 5.590217590332031 *\n",
      "Epoch: 3, Train_Loss: 5.58653450012207, Test_Loss: 5.588972091674805 *\n",
      "Epoch: 3, Train_Loss: 5.634270668029785, Test_Loss: 5.603382110595703\n",
      "Epoch: 3, Train_Loss: 5.60188627243042, Test_Loss: 5.632918834686279\n",
      "Epoch: 3, Train_Loss: 5.612117767333984, Test_Loss: 5.60958194732666 *\n",
      "Epoch: 3, Train_Loss: 5.628843307495117, Test_Loss: 5.604773044586182 *\n",
      "Epoch: 3, Train_Loss: 5.638247013092041, Test_Loss: 5.598289489746094 *\n",
      "Epoch: 3, Train_Loss: 5.629586696624756, Test_Loss: 5.6252970695495605\n",
      "Epoch: 3, Train_Loss: 5.619094371795654, Test_Loss: 5.59102201461792 *\n",
      "Epoch: 3, Train_Loss: 5.623725414276123, Test_Loss: 5.603449821472168\n",
      "Epoch: 3, Train_Loss: 5.594791412353516, Test_Loss: 5.592874526977539 *\n",
      "Epoch: 3, Train_Loss: 5.603382587432861, Test_Loss: 5.604272842407227\n",
      "Epoch: 3, Train_Loss: 5.594893455505371, Test_Loss: 5.604090213775635 *\n",
      "Epoch: 3, Train_Loss: 5.620092391967773, Test_Loss: 5.59782600402832 *\n",
      "Epoch: 3, Train_Loss: 5.631089687347412, Test_Loss: 5.599514484405518\n",
      "Epoch: 3, Train_Loss: 5.5930609703063965, Test_Loss: 5.591032981872559 *\n",
      "Epoch: 3, Train_Loss: 5.626418113708496, Test_Loss: 5.594326019287109\n",
      "Epoch: 3, Train_Loss: 5.658202171325684, Test_Loss: 5.587263107299805 *\n",
      "Epoch: 3, Train_Loss: 5.657283306121826, Test_Loss: 5.620314121246338\n",
      "Epoch: 3, Train_Loss: 5.596959114074707, Test_Loss: 5.583641052246094 *\n",
      "Epoch: 3, Train_Loss: 5.603831768035889, Test_Loss: 5.6221489906311035\n",
      "Epoch: 3, Train_Loss: 5.585180759429932, Test_Loss: 5.64891242980957\n",
      "Epoch: 3, Train_Loss: 5.6080241203308105, Test_Loss: 11.0296630859375\n",
      "Epoch: 3, Train_Loss: 5.581387042999268, Test_Loss: 5.787792205810547 *\n",
      "Epoch: 3, Train_Loss: 5.599579811096191, Test_Loss: 5.5810747146606445 *\n",
      "Epoch: 3, Train_Loss: 5.645016193389893, Test_Loss: 5.602083683013916\n",
      "Epoch: 3, Train_Loss: 7.979084014892578, Test_Loss: 5.643794536590576\n",
      "Epoch: 3, Train_Loss: 8.583946228027344, Test_Loss: 5.642745018005371 *\n",
      "Epoch: 3, Train_Loss: 5.60851526260376, Test_Loss: 5.590447902679443 *\n",
      "Epoch: 3, Train_Loss: 5.581860542297363, Test_Loss: 5.7077436447143555\n",
      "Epoch: 3, Train_Loss: 5.689929485321045, Test_Loss: 5.644606113433838 *\n",
      "Epoch: 3, Train_Loss: 5.762016296386719, Test_Loss: 5.572603702545166 *\n",
      "Epoch: 3, Train_Loss: 5.607291221618652, Test_Loss: 5.633730888366699\n",
      "Epoch: 3, Train_Loss: 5.586632251739502, Test_Loss: 5.588603973388672 *\n",
      "Epoch: 3, Train_Loss: 5.607273101806641, Test_Loss: 5.606069087982178\n",
      "Epoch: 3, Train_Loss: 5.641371726989746, Test_Loss: 5.59482479095459 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 5.573838233947754, Test_Loss: 5.715047836303711\n",
      "Epoch: 3, Train_Loss: 5.581861972808838, Test_Loss: 5.61484432220459 *\n",
      "Epoch: 3, Train_Loss: 6.782418251037598, Test_Loss: 5.707215309143066\n",
      "Epoch: 3, Train_Loss: 7.063004970550537, Test_Loss: 5.6354546546936035 *\n",
      "Epoch: 3, Train_Loss: 5.949780464172363, Test_Loss: 5.588338375091553 *\n",
      "Epoch: 3, Train_Loss: 5.653158187866211, Test_Loss: 5.58284854888916 *\n",
      "Epoch: 3, Train_Loss: 6.871464729309082, Test_Loss: 5.574233055114746 *\n",
      "Epoch: 3, Train_Loss: 7.692488670349121, Test_Loss: 5.595242500305176\n",
      "Epoch: 3, Train_Loss: 5.686864852905273, Test_Loss: 5.621003150939941\n",
      "Epoch: 3, Train_Loss: 5.594628810882568, Test_Loss: 5.596902847290039 *\n",
      "Epoch: 3, Train_Loss: 5.666054725646973, Test_Loss: 5.570087432861328 *\n",
      "Epoch: 3, Train_Loss: 7.37689733505249, Test_Loss: 5.584223747253418\n",
      "Epoch: 3, Train_Loss: 6.955133438110352, Test_Loss: 5.605157375335693\n",
      "Epoch: 3, Train_Loss: 5.580519676208496, Test_Loss: 5.584516525268555 *\n",
      "Epoch: 3, Train_Loss: 5.603010654449463, Test_Loss: 5.594438076019287\n",
      "Epoch: 3, Train_Loss: 5.573409080505371, Test_Loss: 5.571882247924805 *\n",
      "Epoch: 3, Train_Loss: 6.231415748596191, Test_Loss: 5.649586200714111\n",
      "Epoch: 3, Train_Loss: 5.580867767333984, Test_Loss: 5.613368034362793 *\n",
      "Epoch: 3, Train_Loss: 5.618079662322998, Test_Loss: 5.9021830558776855\n",
      "Epoch: 3, Train_Loss: 5.571117401123047, Test_Loss: 5.5633320808410645 *\n",
      "Epoch: 3, Train_Loss: 5.7419586181640625, Test_Loss: 5.6688947677612305\n",
      "Epoch: 3, Train_Loss: 5.699291706085205, Test_Loss: 5.687068462371826\n",
      "Epoch: 3, Train_Loss: 5.8471150398254395, Test_Loss: 5.965799331665039\n",
      "Epoch: 3, Train_Loss: 5.85195255279541, Test_Loss: 5.639856338500977 *\n",
      "Epoch: 3, Train_Loss: 5.650111198425293, Test_Loss: 5.682586193084717\n",
      "Epoch: 3, Train_Loss: 5.675111770629883, Test_Loss: 5.827569484710693\n",
      "Epoch: 3, Train_Loss: 5.797240734100342, Test_Loss: 5.724006652832031 *\n",
      "Epoch: 3, Train_Loss: 6.030547142028809, Test_Loss: 5.58997106552124 *\n",
      "Epoch: 3, Train_Loss: 6.004583835601807, Test_Loss: 5.565561294555664 *\n",
      "Epoch: 3, Train_Loss: 5.574667453765869, Test_Loss: 5.572679042816162\n",
      "Epoch: 3, Train_Loss: 5.680373668670654, Test_Loss: 5.590221881866455\n",
      "Epoch: 3, Train_Loss: 5.717440128326416, Test_Loss: 5.876296520233154\n",
      "Epoch: 3, Train_Loss: 5.564826965332031, Test_Loss: 6.2656779289245605\n",
      "Epoch: 3, Train_Loss: 5.5597758293151855, Test_Loss: 5.979682922363281 *\n",
      "Epoch: 3, Train_Loss: 5.554315090179443, Test_Loss: 6.4786577224731445\n",
      "Epoch: 3, Train_Loss: 5.547954559326172, Test_Loss: 5.85345458984375 *\n",
      "Epoch: 3, Train_Loss: 5.545870780944824, Test_Loss: 6.107911586761475\n",
      "Epoch: 3, Train_Loss: 5.555646896362305, Test_Loss: 5.821754455566406 *\n",
      "Epoch: 3, Train_Loss: 5.621849060058594, Test_Loss: 5.566101551055908 *\n",
      "Epoch: 3, Train_Loss: 5.638943672180176, Test_Loss: 5.585033416748047\n",
      "Epoch: 3, Train_Loss: 5.649746894836426, Test_Loss: 5.600295066833496\n",
      "Epoch: 3, Train_Loss: 5.662729740142822, Test_Loss: 5.708068370819092\n",
      "Epoch: 3, Train_Loss: 6.059690952301025, Test_Loss: 6.462282180786133\n",
      "Epoch: 3, Train_Loss: 5.540195941925049, Test_Loss: 5.738576412200928 *\n",
      "Epoch: 3, Train_Loss: 5.5988359451293945, Test_Loss: 7.708425521850586\n",
      "Epoch: 3, Train_Loss: 5.796143531799316, Test_Loss: 6.093610763549805 *\n",
      "Epoch: 3, Train_Loss: 6.101502895355225, Test_Loss: 6.549074172973633\n",
      "Epoch: 3, Train_Loss: 5.801885604858398, Test_Loss: 5.606926918029785 *\n",
      "Epoch: 3, Train_Loss: 5.539628505706787, Test_Loss: 5.543178558349609 *\n",
      "Epoch: 3, Train_Loss: 5.786924839019775, Test_Loss: 5.757403373718262\n",
      "Epoch: 3, Train_Loss: 6.157545566558838, Test_Loss: 6.834465980529785\n",
      "Epoch: 3, Train_Loss: 6.1873321533203125, Test_Loss: 6.08696985244751 *\n",
      "Epoch: 3, Train_Loss: 5.603636741638184, Test_Loss: 5.672904014587402 *\n",
      "Epoch: 3, Train_Loss: 5.554469108581543, Test_Loss: 5.5677666664123535 *\n",
      "Epoch: 3, Train_Loss: 5.582248687744141, Test_Loss: 5.598346710205078\n",
      "Epoch: 3, Train_Loss: 7.000187873840332, Test_Loss: 5.890010833740234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 6.377858638763428, Test_Loss: 5.7788872718811035 *\n",
      "Epoch: 3, Train_Loss: 5.563476085662842, Test_Loss: 6.8350019454956055\n",
      "Epoch: 3, Train_Loss: 5.5718092918396, Test_Loss: 6.238072395324707 *\n",
      "Epoch: 3, Train_Loss: 5.542724132537842, Test_Loss: 5.562152862548828 *\n",
      "Epoch: 3, Train_Loss: 5.646637916564941, Test_Loss: 5.548141956329346 *\n",
      "Epoch: 3, Train_Loss: 5.893330097198486, Test_Loss: 5.540988922119141 *\n",
      "Epoch: 3, Train_Loss: 5.569530010223389, Test_Loss: 5.569494247436523\n",
      "Epoch: 3, Train_Loss: 5.590166091918945, Test_Loss: 5.606618881225586\n",
      "Epoch: 3, Train_Loss: 5.550407886505127, Test_Loss: 6.0353617668151855\n",
      "Epoch: 3, Train_Loss: 5.684942245483398, Test_Loss: 5.988883972167969 *\n",
      "Epoch: 3, Train_Loss: 23.030765533447266, Test_Loss: 5.65180778503418 *\n",
      "Epoch: 3, Train_Loss: 5.612165927886963, Test_Loss: 5.574825763702393 *\n",
      "Epoch: 3, Train_Loss: 8.744386672973633, Test_Loss: 5.542071342468262 *\n",
      "Epoch: 3, Train_Loss: 6.5513014793396, Test_Loss: 5.59365177154541\n",
      "Epoch: 3, Train_Loss: 5.582426071166992, Test_Loss: 5.941349029541016\n",
      "Epoch: 3, Train_Loss: 5.582195281982422, Test_Loss: 7.049006462097168\n",
      "Epoch: 3, Train_Loss: 13.822868347167969, Test_Loss: 6.210515975952148 *\n",
      "Epoch: 3, Train_Loss: 9.681646347045898, Test_Loss: 5.599844455718994 *\n",
      "Epoch: 3, Train_Loss: 5.53427791595459, Test_Loss: 5.555145263671875 *\n",
      "Epoch: 3, Train_Loss: 6.195436000823975, Test_Loss: 5.523757457733154 *\n",
      "Epoch: 3, Train_Loss: 11.295186996459961, Test_Loss: 5.52573299407959\n",
      "Epoch: 3, Train_Loss: 5.541260719299316, Test_Loss: 5.530984401702881\n",
      "Epoch: 3, Train_Loss: 5.545257091522217, Test_Loss: 5.575837135314941\n",
      "Epoch: 3, Train_Loss: 5.536773681640625, Test_Loss: 5.6318488121032715\n",
      "Epoch: 3, Train_Loss: 5.542216777801514, Test_Loss: 5.531496047973633 *\n",
      "Epoch: 3, Train_Loss: 5.532989978790283, Test_Loss: 5.573464393615723\n",
      "Epoch: 3, Train_Loss: 5.549254417419434, Test_Loss: 5.64827299118042\n",
      "Epoch: 3, Train_Loss: 5.536413192749023, Test_Loss: 5.816134929656982\n",
      "Epoch: 3, Train_Loss: 5.546620845794678, Test_Loss: 5.648270130157471 *\n",
      "Epoch: 3, Train_Loss: 5.544193744659424, Test_Loss: 5.529892444610596 *\n",
      "Epoch: 3, Train_Loss: 5.519859790802002, Test_Loss: 5.513862133026123 *\n",
      "Epoch: 3, Train_Loss: 5.553744792938232, Test_Loss: 5.5276007652282715\n",
      "Epoch: 3, Train_Loss: 5.521752834320068, Test_Loss: 5.515922546386719 *\n",
      "Epoch: 3, Train_Loss: 5.586277008056641, Test_Loss: 5.513548374176025 *\n",
      "Epoch: 3, Train_Loss: 5.611357688903809, Test_Loss: 7.437230110168457\n",
      "Epoch: 3, Train_Loss: 5.525040626525879, Test_Loss: 9.422901153564453\n",
      "Epoch: 3, Train_Loss: 5.525150775909424, Test_Loss: 5.538609981536865 *\n",
      "Epoch: 3, Train_Loss: 5.510176658630371, Test_Loss: 5.513792514801025 *\n",
      "Epoch: 3, Train_Loss: 5.5088701248168945, Test_Loss: 5.5157647132873535\n",
      "Epoch: 3, Train_Loss: 5.509281635284424, Test_Loss: 5.539669513702393\n",
      "Epoch: 3, Train_Loss: 5.516515254974365, Test_Loss: 5.512845993041992 *\n",
      "Epoch: 3, Train_Loss: 5.50592565536499, Test_Loss: 5.538656234741211\n",
      "Epoch: 3, Train_Loss: 5.51500129699707, Test_Loss: 5.516648769378662 *\n",
      "Epoch: 3, Train_Loss: 5.504637718200684, Test_Loss: 5.518002986907959\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 5.50223445892334, Test_Loss: 5.507925033569336 *\n",
      "Epoch: 3, Train_Loss: 5.503660678863525, Test_Loss: 5.504944801330566 *\n",
      "Epoch: 3, Train_Loss: 5.50624418258667, Test_Loss: 5.512679100036621\n",
      "Epoch: 3, Train_Loss: 5.531579971313477, Test_Loss: 5.5021772384643555 *\n",
      "Epoch: 3, Train_Loss: 5.542263031005859, Test_Loss: 5.492775917053223 *\n",
      "Epoch: 3, Train_Loss: 5.525536060333252, Test_Loss: 5.511833667755127\n",
      "Epoch: 3, Train_Loss: 5.515691757202148, Test_Loss: 5.5139241218566895\n",
      "Epoch: 3, Train_Loss: 11.2723970413208, Test_Loss: 5.500542163848877 *\n",
      "Epoch: 3, Train_Loss: 9.019830703735352, Test_Loss: 5.50962495803833\n",
      "Epoch: 3, Train_Loss: 5.517772674560547, Test_Loss: 5.506669998168945 *\n",
      "Epoch: 3, Train_Loss: 5.499476909637451, Test_Loss: 5.511004447937012\n",
      "Epoch: 3, Train_Loss: 5.499054431915283, Test_Loss: 5.519816875457764\n",
      "Epoch: 3, Train_Loss: 5.5255866050720215, Test_Loss: 5.515090465545654 *\n",
      "Epoch: 3, Train_Loss: 5.519538402557373, Test_Loss: 5.528179168701172\n",
      "Epoch: 3, Train_Loss: 5.514215469360352, Test_Loss: 5.5345635414123535\n",
      "Epoch: 3, Train_Loss: 5.5519914627075195, Test_Loss: 5.506781578063965 *\n",
      "Epoch: 3, Train_Loss: 5.767905235290527, Test_Loss: 5.504396438598633 *\n",
      "Epoch: 3, Train_Loss: 5.731354713439941, Test_Loss: 5.494626045227051 *\n",
      "Epoch: 3, Train_Loss: 5.563886642456055, Test_Loss: 5.5284342765808105\n",
      "Epoch: 3, Train_Loss: 5.560979843139648, Test_Loss: 5.505332946777344 *\n",
      "Epoch: 3, Train_Loss: 5.6047773361206055, Test_Loss: 5.565780162811279\n",
      "Epoch: 3, Train_Loss: 5.602051734924316, Test_Loss: 5.531390190124512 *\n",
      "Epoch: 3, Train_Loss: 5.663212299346924, Test_Loss: 9.83033561706543\n",
      "Epoch: 3, Train_Loss: 5.59804105758667, Test_Loss: 7.091174602508545 *\n",
      "Epoch: 3, Train_Loss: 5.5635528564453125, Test_Loss: 5.494680404663086 *\n",
      "Epoch: 3, Train_Loss: 5.503800868988037, Test_Loss: 5.493485450744629 *\n",
      "Epoch: 3, Train_Loss: 5.568862438201904, Test_Loss: 5.515504360198975\n",
      "Epoch: 3, Train_Loss: 5.566020965576172, Test_Loss: 5.501619815826416 *\n",
      "Epoch: 3, Train_Loss: 5.512168884277344, Test_Loss: 5.479035377502441 *\n",
      "Epoch: 3, Train_Loss: 5.514618396759033, Test_Loss: 5.584168434143066\n",
      "Epoch: 3, Train_Loss: 5.482779502868652, Test_Loss: 5.605731010437012\n",
      "Epoch: 3, Train_Loss: 5.502347469329834, Test_Loss: 5.4725165367126465 *\n",
      "Epoch: 3, Train_Loss: 10.147315979003906, Test_Loss: 5.534152984619141\n",
      "Epoch: 3, Train_Loss: 6.454443454742432, Test_Loss: 5.484092712402344 *\n",
      "Epoch: 3, Train_Loss: 5.500611305236816, Test_Loss: 5.509949684143066\n",
      "Epoch: 3, Train_Loss: 5.486968040466309, Test_Loss: 5.477179527282715 *\n",
      "Epoch: 3, Train_Loss: 5.498631000518799, Test_Loss: 5.537993907928467\n",
      "Epoch: 3, Train_Loss: 5.486804008483887, Test_Loss: 5.505331516265869 *\n",
      "Epoch: 3, Train_Loss: 5.489561080932617, Test_Loss: 5.627980709075928\n",
      "Epoch: 3, Train_Loss: 5.486572742462158, Test_Loss: 5.607532024383545 *\n",
      "Epoch: 3, Train_Loss: 5.488083839416504, Test_Loss: 5.481292247772217 *\n",
      "Epoch: 3, Train_Loss: 5.514638423919678, Test_Loss: 5.477478981018066 *\n",
      "Epoch: 3, Train_Loss: 5.487652778625488, Test_Loss: 5.485232353210449\n",
      "Epoch: 3, Train_Loss: 5.484617710113525, Test_Loss: 5.470763206481934 *\n",
      "Epoch: 3, Train_Loss: 5.472391605377197, Test_Loss: 5.470170974731445 *\n",
      "Epoch: 3, Train_Loss: 5.51308536529541, Test_Loss: 5.470703601837158\n",
      "Epoch: 3, Train_Loss: 5.46097469329834, Test_Loss: 5.469551086425781 *\n",
      "Epoch: 3, Train_Loss: 5.494328022003174, Test_Loss: 5.469236373901367 *\n",
      "Epoch: 3, Train_Loss: 5.481328964233398, Test_Loss: 5.475496292114258\n",
      "Epoch: 3, Train_Loss: 5.528584003448486, Test_Loss: 5.486621856689453\n",
      "Epoch: 3, Train_Loss: 5.512585639953613, Test_Loss: 5.463440895080566 *\n",
      "Epoch: 3, Train_Loss: 5.46845817565918, Test_Loss: 5.471728324890137\n",
      "Epoch: 3, Train_Loss: 5.456293106079102, Test_Loss: 5.4909138679504395\n",
      "Epoch: 3, Train_Loss: 5.492015361785889, Test_Loss: 5.483770370483398 *\n",
      "Epoch: 4, Train_Loss: 5.576235294342041, Test_Loss: 5.8547797203063965 *\n",
      "Epoch: 4, Train_Loss: 5.536355018615723, Test_Loss: 5.495434284210205 *\n",
      "Epoch: 4, Train_Loss: 5.543896675109863, Test_Loss: 5.511146545410156\n",
      "Epoch: 4, Train_Loss: 5.496010780334473, Test_Loss: 5.537848949432373\n",
      "Epoch: 4, Train_Loss: 5.5473151206970215, Test_Loss: 5.737518310546875\n",
      "Epoch: 4, Train_Loss: 5.525404930114746, Test_Loss: 5.502606391906738 *\n",
      "Epoch: 4, Train_Loss: 5.511479377746582, Test_Loss: 5.548564434051514\n",
      "Epoch: 4, Train_Loss: 5.537778854370117, Test_Loss: 5.764970779418945\n",
      "Epoch: 4, Train_Loss: 5.500796794891357, Test_Loss: 5.787713050842285\n",
      "Epoch: 4, Train_Loss: 5.469324111938477, Test_Loss: 5.474246501922607 *\n",
      "Epoch: 4, Train_Loss: 5.471489906311035, Test_Loss: 5.536930561065674\n",
      "Epoch: 4, Train_Loss: 5.464354038238525, Test_Loss: 5.448445796966553 *\n",
      "Epoch: 4, Train_Loss: 5.455756187438965, Test_Loss: 5.464040279388428\n",
      "Epoch: 4, Train_Loss: 5.455440521240234, Test_Loss: 5.543492794036865\n",
      "Epoch: 4, Train_Loss: 5.456818580627441, Test_Loss: 6.3927412033081055\n",
      "Epoch: 4, Train_Loss: 9.423141479492188, Test_Loss: 5.594458103179932 *\n",
      "Epoch: 4, Train_Loss: 6.394851207733154, Test_Loss: 6.290443420410156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 5.44362735748291, Test_Loss: 5.991429328918457 *\n",
      "Epoch: 4, Train_Loss: 5.476569652557373, Test_Loss: 5.928603649139404 *\n",
      "Epoch: 4, Train_Loss: 5.448431968688965, Test_Loss: 5.886874675750732 *\n",
      "Epoch: 4, Train_Loss: 5.439074516296387, Test_Loss: 5.479910850524902 *\n",
      "Epoch: 4, Train_Loss: 5.436221599578857, Test_Loss: 5.449199676513672 *\n",
      "Epoch: 4, Train_Loss: 5.438514709472656, Test_Loss: 5.4771409034729\n",
      "Epoch: 4, Train_Loss: 5.447139739990234, Test_Loss: 5.593181610107422\n",
      "Epoch: 4, Train_Loss: 5.440351963043213, Test_Loss: 6.238124847412109\n",
      "Epoch: 4, Train_Loss: 5.473875522613525, Test_Loss: 5.776156902313232 *\n",
      "Epoch: 4, Train_Loss: 5.548489093780518, Test_Loss: 7.438390731811523\n",
      "Epoch: 4, Train_Loss: 5.555971145629883, Test_Loss: 6.152984619140625 *\n",
      "Epoch: 4, Train_Loss: 5.539243698120117, Test_Loss: 6.669357776641846\n",
      "Epoch: 4, Train_Loss: 5.4524922370910645, Test_Loss: 5.706344127655029 *\n",
      "Epoch: 4, Train_Loss: 5.469403266906738, Test_Loss: 5.445366382598877 *\n",
      "Epoch: 4, Train_Loss: 5.637676239013672, Test_Loss: 5.544196128845215\n",
      "Epoch: 4, Train_Loss: 5.6503472328186035, Test_Loss: 6.735401630401611\n",
      "Epoch: 4, Train_Loss: 5.690523147583008, Test_Loss: 6.289841651916504 *\n",
      "Epoch: 4, Train_Loss: 5.467285633087158, Test_Loss: 5.533711910247803 *\n",
      "Epoch: 4, Train_Loss: 5.446559429168701, Test_Loss: 5.503768444061279 *\n",
      "Epoch: 4, Train_Loss: 5.430299282073975, Test_Loss: 5.491942882537842 *\n",
      "Epoch: 4, Train_Loss: 5.439744472503662, Test_Loss: 5.824646949768066\n",
      "Epoch: 4, Train_Loss: 5.43212366104126, Test_Loss: 5.595259666442871 *\n",
      "Epoch: 4, Train_Loss: 5.438283443450928, Test_Loss: 6.444892406463623\n",
      "Epoch: 4, Train_Loss: 5.443813800811768, Test_Loss: 6.337798118591309 *\n",
      "Epoch: 4, Train_Loss: 5.419889450073242, Test_Loss: 5.597367286682129 *\n",
      "Epoch: 4, Train_Loss: 5.428988456726074, Test_Loss: 5.4369940757751465 *\n",
      "Epoch: 4, Train_Loss: 5.4414520263671875, Test_Loss: 5.437576770782471\n",
      "Epoch: 4, Train_Loss: 5.541450023651123, Test_Loss: 5.465614318847656\n",
      "Epoch: 4, Train_Loss: 5.582242012023926, Test_Loss: 5.481613636016846\n",
      "Epoch: 4, Train_Loss: 5.585460186004639, Test_Loss: 5.863168716430664\n",
      "Epoch: 4, Train_Loss: 5.496868133544922, Test_Loss: 5.9698076248168945\n",
      "Epoch: 4, Train_Loss: 5.565866947174072, Test_Loss: 5.568431377410889 *\n",
      "Epoch: 4, Train_Loss: 5.580094337463379, Test_Loss: 5.472371578216553 *\n",
      "Epoch: 4, Train_Loss: 5.433369159698486, Test_Loss: 5.443603038787842 *\n",
      "Epoch: 4, Train_Loss: 5.6039252281188965, Test_Loss: 5.452771186828613\n",
      "Epoch: 4, Train_Loss: 5.551629066467285, Test_Loss: 5.6481032371521\n",
      "Epoch: 4, Train_Loss: 5.676382541656494, Test_Loss: 6.827661991119385\n",
      "Epoch: 4, Train_Loss: 5.4315409660339355, Test_Loss: 6.461851119995117 *\n",
      "Epoch: 4, Train_Loss: 6.412686824798584, Test_Loss: 5.4642653465271 *\n",
      "Epoch: 4, Train_Loss: 7.695652484893799, Test_Loss: 5.487866401672363\n",
      "Epoch: 4, Train_Loss: 5.476562023162842, Test_Loss: 5.422793388366699 *\n",
      "Epoch: 4, Train_Loss: 5.463878154754639, Test_Loss: 5.418722152709961 *\n",
      "Epoch: 4, Train_Loss: 5.450979709625244, Test_Loss: 5.419590473175049\n",
      "Epoch: 4, Train_Loss: 5.457162380218506, Test_Loss: 5.448915958404541\n",
      "Epoch: 4, Train_Loss: 5.408787250518799, Test_Loss: 5.491375923156738\n",
      "Epoch: 4, Train_Loss: 5.416130542755127, Test_Loss: 5.418886184692383 *\n",
      "Epoch: 4, Train_Loss: 5.543740749359131, Test_Loss: 5.427664756774902\n",
      "Epoch: 4, Train_Loss: 5.514064788818359, Test_Loss: 5.528506278991699\n",
      "Epoch: 4, Train_Loss: 5.526039123535156, Test_Loss: 5.774141788482666\n",
      "Epoch: 4, Train_Loss: 5.5077433586120605, Test_Loss: 5.606449604034424 *\n",
      "Epoch: 4, Train_Loss: 5.486687660217285, Test_Loss: 5.413026809692383 *\n",
      "Epoch: 4, Train_Loss: 5.440422058105469, Test_Loss: 5.427729606628418\n",
      "Epoch: 4, Train_Loss: 5.438745975494385, Test_Loss: 5.413869380950928 *\n",
      "Epoch: 4, Train_Loss: 5.420269012451172, Test_Loss: 5.406609535217285 *\n",
      "Epoch: 4, Train_Loss: 5.423098564147949, Test_Loss: 5.4185051918029785\n",
      "Epoch: 4, Train_Loss: 5.412148952484131, Test_Loss: 5.956068515777588\n",
      "Epoch: 4, Train_Loss: 5.417084693908691, Test_Loss: 10.230096817016602\n",
      "Epoch: 4, Train_Loss: 5.437918186187744, Test_Loss: 5.449075698852539 *\n",
      "Epoch: 4, Train_Loss: 5.468586444854736, Test_Loss: 5.402637481689453 *\n",
      "Epoch: 4, Train_Loss: 5.4257330894470215, Test_Loss: 5.4088544845581055\n",
      "Epoch: 4, Train_Loss: 5.404420852661133, Test_Loss: 5.415530204772949\n",
      "Epoch: 4, Train_Loss: 5.4028472900390625, Test_Loss: 5.399086952209473 *\n",
      "Epoch: 4, Train_Loss: 5.411473751068115, Test_Loss: 5.403869152069092\n",
      "Epoch: 4, Train_Loss: 5.3978095054626465, Test_Loss: 5.406819820404053\n",
      "Epoch: 4, Train_Loss: 5.400873184204102, Test_Loss: 5.404999256134033 *\n",
      "Epoch: 4, Train_Loss: 5.395893096923828, Test_Loss: 5.4009108543396 *\n",
      "Epoch: 4, Train_Loss: 5.411513805389404, Test_Loss: 5.401341915130615\n",
      "Epoch: 4, Train_Loss: 5.385882377624512, Test_Loss: 5.3958234786987305 *\n",
      "Epoch: 4, Train_Loss: 5.38960075378418, Test_Loss: 5.408865928649902\n",
      "Epoch: 4, Train_Loss: 5.399191379547119, Test_Loss: 5.429319858551025\n",
      "Epoch: 4, Train_Loss: 5.398694038391113, Test_Loss: 5.416171073913574 *\n",
      "Epoch: 4, Train_Loss: 5.395737171173096, Test_Loss: 5.401376724243164 *\n",
      "Epoch: 4, Train_Loss: 5.387012958526611, Test_Loss: 5.391764163970947 *\n",
      "Epoch: 4, Train_Loss: 5.4091877937316895, Test_Loss: 5.387233734130859 *\n",
      "Epoch: 4, Train_Loss: 5.406363010406494, Test_Loss: 5.393773555755615\n",
      "Epoch: 4, Train_Loss: 5.392077445983887, Test_Loss: 5.383169651031494 *\n",
      "Epoch: 4, Train_Loss: 5.391829013824463, Test_Loss: 5.389838218688965\n",
      "Epoch: 4, Train_Loss: 5.385009288787842, Test_Loss: 5.383306980133057 *\n",
      "Epoch: 4, Train_Loss: 5.404085159301758, Test_Loss: 5.393497943878174\n",
      "Epoch: 4, Train_Loss: 5.3971638679504395, Test_Loss: 5.393446445465088 *\n",
      "Epoch: 4, Train_Loss: 5.392159461975098, Test_Loss: 5.385888576507568 *\n",
      "Epoch: 4, Train_Loss: 5.376331329345703, Test_Loss: 5.3879923820495605\n",
      "Epoch: 4, Train_Loss: 5.389923095703125, Test_Loss: 5.397037982940674\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 5.48961877822876, Test_Loss: 5.3851799964904785 *\n",
      "Epoch: 4, Train_Loss: 5.397567272186279, Test_Loss: 5.377531051635742 *\n",
      "Epoch: 4, Train_Loss: 5.433599472045898, Test_Loss: 5.392337322235107\n",
      "Epoch: 4, Train_Loss: 5.381227970123291, Test_Loss: 5.442319393157959\n",
      "Epoch: 4, Train_Loss: 5.409073829650879, Test_Loss: 8.219011306762695\n",
      "Epoch: 4, Train_Loss: 5.409470081329346, Test_Loss: 8.114105224609375 *\n",
      "Epoch: 4, Train_Loss: 5.382227897644043, Test_Loss: 5.378362655639648 *\n",
      "Epoch: 4, Train_Loss: 5.39348840713501, Test_Loss: 5.383051872253418\n",
      "Epoch: 4, Train_Loss: 5.4040913581848145, Test_Loss: 5.41914701461792\n",
      "Epoch: 4, Train_Loss: 5.490077018737793, Test_Loss: 5.427356243133545\n",
      "Epoch: 4, Train_Loss: 5.512260437011719, Test_Loss: 5.426586627960205 *\n",
      "Epoch: 4, Train_Loss: 5.453906536102295, Test_Loss: 5.417945861816406 *\n",
      "Epoch: 4, Train_Loss: 5.429657459259033, Test_Loss: 5.505193710327148\n",
      "Epoch: 4, Train_Loss: 5.384043216705322, Test_Loss: 5.375854015350342 *\n",
      "Epoch: 4, Train_Loss: 5.399044036865234, Test_Loss: 5.401000022888184\n",
      "Epoch: 4, Train_Loss: 5.372941017150879, Test_Loss: 5.403008937835693\n",
      "Epoch: 4, Train_Loss: 5.380629062652588, Test_Loss: 5.373712062835693 *\n",
      "Epoch: 4, Train_Loss: 5.384437561035156, Test_Loss: 5.379553318023682\n",
      "Epoch: 4, Train_Loss: 5.385769367218018, Test_Loss: 5.445869445800781\n",
      "Epoch: 4, Train_Loss: 5.487910747528076, Test_Loss: 5.452621936798096\n",
      "Epoch: 4, Train_Loss: 5.376091480255127, Test_Loss: 5.469453811645508\n",
      "Epoch: 4, Train_Loss: 5.44828462600708, Test_Loss: 5.47534704208374\n",
      "Epoch: 4, Train_Loss: 5.369518280029297, Test_Loss: 5.387889385223389 *\n",
      "Epoch: 4, Train_Loss: 5.391995906829834, Test_Loss: 5.383490085601807 *\n",
      "Epoch: 4, Train_Loss: 5.371297836303711, Test_Loss: 5.3607096672058105 *\n",
      "Epoch: 4, Train_Loss: 5.656223773956299, Test_Loss: 5.366141319274902\n",
      "Epoch: 4, Train_Loss: 5.395997047424316, Test_Loss: 5.370694637298584\n",
      "Epoch: 4, Train_Loss: 5.395571708679199, Test_Loss: 5.376677513122559\n",
      "Epoch: 4, Train_Loss: 5.360917568206787, Test_Loss: 5.364388942718506 *\n",
      "Epoch: 4, Train_Loss: 5.358434677124023, Test_Loss: 5.360424518585205 *\n",
      "Epoch: 4, Train_Loss: 5.3634467124938965, Test_Loss: 5.392308235168457\n",
      "Epoch: 4, Train_Loss: 5.368148326873779, Test_Loss: 5.383000373840332 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 5.377259731292725, Test_Loss: 5.385810852050781\n",
      "Epoch: 4, Train_Loss: 5.375462532043457, Test_Loss: 5.353220462799072 *\n",
      "Epoch: 4, Train_Loss: 5.3923115730285645, Test_Loss: 5.384441375732422\n",
      "Epoch: 4, Train_Loss: 5.390208721160889, Test_Loss: 5.431136131286621\n",
      "Epoch: 4, Train_Loss: 5.371600151062012, Test_Loss: 5.642944812774658\n",
      "Epoch: 4, Train_Loss: 5.378621578216553, Test_Loss: 5.417984962463379 *\n",
      "Epoch: 4, Train_Loss: 5.36809778213501, Test_Loss: 5.3941545486450195 *\n",
      "Epoch: 4, Train_Loss: 5.3529276847839355, Test_Loss: 5.449713230133057\n",
      "Epoch: 4, Train_Loss: 5.366344928741455, Test_Loss: 5.601577281951904\n",
      "Epoch: 4, Train_Loss: 5.371774196624756, Test_Loss: 5.523138523101807 *\n",
      "Epoch: 4, Train_Loss: 5.374826431274414, Test_Loss: 5.385644435882568 *\n",
      "Epoch: 4, Train_Loss: 5.3520731925964355, Test_Loss: 5.610822677612305\n",
      "Epoch: 4, Train_Loss: 5.390651702880859, Test_Loss: 5.6725993156433105\n",
      "Epoch: 4, Train_Loss: 5.401289939880371, Test_Loss: 5.373836517333984 *\n",
      "Epoch: 4, Train_Loss: 5.407297134399414, Test_Loss: 5.406193256378174\n",
      "Epoch: 4, Train_Loss: 5.343558311462402, Test_Loss: 5.342151641845703 *\n",
      "Epoch: 4, Train_Loss: 5.369948863983154, Test_Loss: 5.381980895996094\n",
      "Epoch: 4, Train_Loss: 5.3596367835998535, Test_Loss: 5.371872901916504 *\n",
      "Epoch: 4, Train_Loss: 5.362955570220947, Test_Loss: 6.126212120056152\n",
      "Epoch: 4, Train_Loss: 5.337172985076904, Test_Loss: 5.532710552215576 *\n",
      "Epoch: 4, Train_Loss: 5.3652544021606445, Test_Loss: 6.11647891998291\n",
      "Epoch: 4, Train_Loss: 5.397161483764648, Test_Loss: 6.045935153961182 *\n",
      "Epoch: 4, Train_Loss: 7.9804182052612305, Test_Loss: 5.590422630310059 *\n",
      "Epoch: 4, Train_Loss: 8.026679039001465, Test_Loss: 5.863588809967041\n",
      "Epoch: 4, Train_Loss: 5.360144138336182, Test_Loss: 5.422745704650879 *\n",
      "Epoch: 4, Train_Loss: 5.333357810974121, Test_Loss: 5.3492913246154785 *\n",
      "Epoch: 4, Train_Loss: 5.50053596496582, Test_Loss: 5.347275733947754 *\n",
      "Epoch: 4, Train_Loss: 5.484683036804199, Test_Loss: 5.544437408447266\n",
      "Epoch: 4, Train_Loss: 5.3600029945373535, Test_Loss: 5.781434059143066\n",
      "Epoch: 4, Train_Loss: 5.3384108543396, Test_Loss: 5.994028091430664\n",
      "Epoch: 4, Train_Loss: 5.391168594360352, Test_Loss: 6.629238128662109\n",
      "Epoch: 4, Train_Loss: 5.389544486999512, Test_Loss: 6.561465263366699 *\n",
      "Epoch: 4, Train_Loss: 5.346598148345947, Test_Loss: 6.243409156799316 *\n",
      "Epoch: 4, Train_Loss: 5.404863357543945, Test_Loss: 5.8036956787109375 *\n",
      "Epoch: 4, Train_Loss: 6.569912910461426, Test_Loss: 5.329611778259277 *\n",
      "Epoch: 4, Train_Loss: 6.861830234527588, Test_Loss: 5.361941337585449\n",
      "Epoch: 4, Train_Loss: 5.493690490722656, Test_Loss: 6.246324062347412\n",
      "Epoch: 4, Train_Loss: 5.393061637878418, Test_Loss: 6.674736976623535\n",
      "Epoch: 4, Train_Loss: 7.117828369140625, Test_Loss: 5.401314735412598 *\n",
      "Epoch: 4, Train_Loss: 7.095597743988037, Test_Loss: 5.410374641418457\n",
      "Epoch: 4, Train_Loss: 5.376067161560059, Test_Loss: 5.331656455993652 *\n",
      "Epoch: 4, Train_Loss: 5.372322082519531, Test_Loss: 5.623514175415039\n",
      "Epoch: 4, Train_Loss: 5.59505558013916, Test_Loss: 5.564050674438477 *\n",
      "Epoch: 4, Train_Loss: 7.064675331115723, Test_Loss: 6.121788501739502\n",
      "Epoch: 4, Train_Loss: 6.595061302185059, Test_Loss: 6.387796878814697\n",
      "Epoch: 4, Train_Loss: 5.336130619049072, Test_Loss: 5.582025051116943 *\n",
      "Epoch: 4, Train_Loss: 5.346883296966553, Test_Loss: 5.328582286834717 *\n",
      "Epoch: 4, Train_Loss: 5.404754161834717, Test_Loss: 5.333251476287842\n",
      "Epoch: 4, Train_Loss: 5.990474700927734, Test_Loss: 5.333144187927246 *\n",
      "Epoch: 4, Train_Loss: 5.33477258682251, Test_Loss: 5.351226806640625\n",
      "Epoch: 4, Train_Loss: 5.370835304260254, Test_Loss: 5.697484016418457\n",
      "Epoch: 4, Train_Loss: 5.363327980041504, Test_Loss: 5.902252674102783\n",
      "Epoch: 4, Train_Loss: 5.491188049316406, Test_Loss: 5.57178258895874 *\n",
      "Epoch: 4, Train_Loss: 5.456636905670166, Test_Loss: 5.3957109451293945 *\n",
      "Epoch: 4, Train_Loss: 5.626726150512695, Test_Loss: 5.348583698272705 *\n",
      "Epoch: 4, Train_Loss: 5.558192253112793, Test_Loss: 5.31815767288208 *\n",
      "Epoch: 4, Train_Loss: 5.395968914031982, Test_Loss: 5.453136444091797\n",
      "Epoch: 4, Train_Loss: 5.531422138214111, Test_Loss: 6.324445724487305\n",
      "Epoch: 4, Train_Loss: 5.57822847366333, Test_Loss: 6.611824989318848\n",
      "Epoch: 4, Train_Loss: 5.85345983505249, Test_Loss: 5.388248443603516 *\n",
      "Epoch: 4, Train_Loss: 5.620080471038818, Test_Loss: 5.413681983947754\n",
      "Epoch: 4, Train_Loss: 5.36094856262207, Test_Loss: 5.323003768920898 *\n",
      "Epoch: 4, Train_Loss: 5.442809104919434, Test_Loss: 5.326937675476074\n",
      "Epoch: 4, Train_Loss: 5.4248247146606445, Test_Loss: 5.3190789222717285 *\n",
      "Epoch: 4, Train_Loss: 5.325255870819092, Test_Loss: 5.347827911376953\n",
      "Epoch: 4, Train_Loss: 5.321935653686523, Test_Loss: 5.377441883087158\n",
      "Epoch: 4, Train_Loss: 5.31678581237793, Test_Loss: 5.3488664627075195 *\n",
      "Epoch: 4, Train_Loss: 5.320712089538574, Test_Loss: 5.321641445159912 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 5.3142266273498535, Test_Loss: 5.396981239318848\n",
      "Epoch: 4, Train_Loss: 5.314358234405518, Test_Loss: 5.694694519042969\n",
      "Epoch: 4, Train_Loss: 5.4157304763793945, Test_Loss: 5.431711196899414 *\n",
      "Epoch: 4, Train_Loss: 5.369636535644531, Test_Loss: 5.409470081329346 *\n",
      "Epoch: 4, Train_Loss: 5.418928623199463, Test_Loss: 5.305622100830078 *\n",
      "Epoch: 4, Train_Loss: 5.442302703857422, Test_Loss: 5.3106842041015625\n",
      "Epoch: 4, Train_Loss: 5.691798210144043, Test_Loss: 5.306467056274414 *\n",
      "Epoch: 4, Train_Loss: 5.314956188201904, Test_Loss: 5.302476406097412 *\n",
      "Epoch: 4, Train_Loss: 5.353618144989014, Test_Loss: 5.355144023895264\n",
      "Epoch: 4, Train_Loss: 5.567355632781982, Test_Loss: 10.716079711914062\n",
      "Epoch: 4, Train_Loss: 5.869783401489258, Test_Loss: 5.54090690612793 *\n",
      "Epoch: 4, Train_Loss: 5.486276149749756, Test_Loss: 5.299668788909912 *\n",
      "Epoch: 4, Train_Loss: 5.30571985244751, Test_Loss: 5.316580295562744\n",
      "Epoch: 4, Train_Loss: 5.641885757446289, Test_Loss: 5.309643745422363 *\n",
      "Epoch: 4, Train_Loss: 5.924757957458496, Test_Loss: 5.312869071960449\n",
      "Epoch: 4, Train_Loss: 5.733865737915039, Test_Loss: 5.295732498168945 *\n",
      "Epoch: 4, Train_Loss: 5.324665069580078, Test_Loss: 5.293839931488037 *\n",
      "Epoch: 4, Train_Loss: 5.306501388549805, Test_Loss: 5.2949957847595215\n",
      "Epoch: 4, Train_Loss: 5.442073345184326, Test_Loss: 5.29994535446167\n",
      "Epoch: 4, Train_Loss: 6.974822044372559, Test_Loss: 5.284292221069336 *\n",
      "Epoch: 4, Train_Loss: 6.0079426765441895, Test_Loss: 5.2978105545043945\n",
      "Epoch: 4, Train_Loss: 5.298150062561035, Test_Loss: 5.312150955200195\n",
      "Epoch: 4, Train_Loss: 5.320569038391113, Test_Loss: 5.30781888961792 *\n",
      "Epoch: 4, Train_Loss: 5.2828168869018555, Test_Loss: 5.302433490753174 *\n",
      "Epoch: 4, Train_Loss: 5.506647109985352, Test_Loss: 5.286011219024658 *\n",
      "Epoch: 4, Train_Loss: 5.497420310974121, Test_Loss: 5.280823707580566 *\n",
      "Epoch: 4, Train_Loss: 5.3370041847229, Test_Loss: 5.2846879959106445\n",
      "Epoch: 4, Train_Loss: 5.332249641418457, Test_Loss: 5.300009727478027\n",
      "Epoch: 4, Train_Loss: 5.301805019378662, Test_Loss: 5.291686534881592 *\n",
      "Epoch: 4, Train_Loss: 10.532541275024414, Test_Loss: 5.2949748039245605\n",
      "Epoch: 4, Train_Loss: 17.583406448364258, Test_Loss: 5.269238471984863 *\n",
      "Epoch: 4, Train_Loss: 5.803581237792969, Test_Loss: 5.286045551300049\n",
      "Epoch: 4, Train_Loss: 8.62024974822998, Test_Loss: 5.2983269691467285\n",
      "Epoch: 4, Train_Loss: 5.761005401611328, Test_Loss: 5.27682638168335 *\n",
      "Epoch: 4, Train_Loss: 5.352049350738525, Test_Loss: 5.283885955810547\n",
      "Epoch: 4, Train_Loss: 5.394111633300781, Test_Loss: 5.279576778411865 *\n",
      "Epoch: 4, Train_Loss: 15.542341232299805, Test_Loss: 5.274759292602539 *\n",
      "Epoch: 4, Train_Loss: 7.733323097229004, Test_Loss: 5.272814750671387 *\n",
      "Epoch: 4, Train_Loss: 5.284793376922607, Test_Loss: 5.277722358703613\n",
      "Epoch: 4, Train_Loss: 7.714714050292969, Test_Loss: 5.334866046905518\n",
      "Epoch: 4, Train_Loss: 9.173112869262695, Test_Loss: 6.732091903686523\n",
      "Epoch: 4, Train_Loss: 5.311320781707764, Test_Loss: 9.632164001464844\n",
      "Epoch: 4, Train_Loss: 5.2850494384765625, Test_Loss: 5.297839164733887 *\n",
      "Epoch: 4, Train_Loss: 5.281257152557373, Test_Loss: 5.288052558898926 *\n",
      "Epoch: 4, Train_Loss: 5.308093547821045, Test_Loss: 5.2921342849731445\n",
      "Epoch: 4, Train_Loss: 5.302302837371826, Test_Loss: 5.300300121307373\n",
      "Epoch: 4, Train_Loss: 5.302702903747559, Test_Loss: 5.308934211730957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 5.300251007080078, Test_Loss: 5.305884838104248 *\n",
      "Epoch: 4, Train_Loss: 5.2884697914123535, Test_Loss: 5.509003162384033\n",
      "Epoch: 4, Train_Loss: 5.296237468719482, Test_Loss: 5.301829814910889 *\n",
      "Epoch: 4, Train_Loss: 5.293102741241455, Test_Loss: 5.28442907333374 *\n",
      "Epoch: 4, Train_Loss: 5.281721591949463, Test_Loss: 5.346651554107666\n",
      "Epoch: 4, Train_Loss: 5.294586658477783, Test_Loss: 5.274513244628906 *\n",
      "Epoch: 4, Train_Loss: 5.380329608917236, Test_Loss: 5.3032307624816895\n",
      "Epoch: 4, Train_Loss: 5.322298049926758, Test_Loss: 5.299979209899902 *\n",
      "Epoch: 4, Train_Loss: 5.305431365966797, Test_Loss: 5.331559181213379\n",
      "Epoch: 4, Train_Loss: 5.286515712738037, Test_Loss: 5.388098239898682\n",
      "Epoch: 4, Train_Loss: 5.271673679351807, Test_Loss: 5.460386276245117\n",
      "Epoch: 4, Train_Loss: 5.286605358123779, Test_Loss: 5.305850505828857 *\n",
      "Epoch: 4, Train_Loss: 5.2714524269104, Test_Loss: 5.263789176940918 *\n",
      "Epoch: 4, Train_Loss: 5.269282817840576, Test_Loss: 5.2623443603515625 *\n",
      "Epoch: 4, Train_Loss: 5.26564884185791, Test_Loss: 5.275102138519287\n",
      "Epoch: 4, Train_Loss: 5.265264987945557, Test_Loss: 5.254047393798828 *\n",
      "Epoch: 4, Train_Loss: 5.269482612609863, Test_Loss: 5.261528491973877\n",
      "Epoch: 4, Train_Loss: 5.263092517852783, Test_Loss: 5.262561798095703\n",
      "Epoch: 4, Train_Loss: 5.262775897979736, Test_Loss: 5.266276836395264\n",
      "Epoch: 4, Train_Loss: 5.250243663787842, Test_Loss: 5.271450519561768\n",
      "Epoch: 4, Train_Loss: 5.268568515777588, Test_Loss: 5.248964309692383 *\n",
      "Epoch: 4, Train_Loss: 5.272700309753418, Test_Loss: 5.26180362701416\n",
      "Epoch: 4, Train_Loss: 5.274411201477051, Test_Loss: 5.25625467300415 *\n",
      "Epoch: 4, Train_Loss: 5.260211944580078, Test_Loss: 5.255610942840576 *\n",
      "Epoch: 4, Train_Loss: 13.522632598876953, Test_Loss: 5.282815456390381\n",
      "Epoch: 4, Train_Loss: 6.429297924041748, Test_Loss: 5.4745192527771\n",
      "Epoch: 4, Train_Loss: 5.258608341217041, Test_Loss: 5.451162338256836 *\n",
      "Epoch: 4, Train_Loss: 5.262571334838867, Test_Loss: 5.275432109832764 *\n",
      "Epoch: 4, Train_Loss: 5.255975723266602, Test_Loss: 5.313541889190674\n",
      "Epoch: 4, Train_Loss: 5.2704691886901855, Test_Loss: 5.374454021453857\n",
      "Epoch: 4, Train_Loss: 5.2743377685546875, Test_Loss: 5.471172332763672\n",
      "Epoch: 4, Train_Loss: 5.2573442459106445, Test_Loss: 5.266289234161377 *\n",
      "Epoch: 4, Train_Loss: 5.351042747497559, Test_Loss: 5.586393356323242\n",
      "Epoch: 4, Train_Loss: 5.505277633666992, Test_Loss: 5.665822505950928\n",
      "Epoch: 4, Train_Loss: 5.4499311447143555, Test_Loss: 5.314655303955078 *\n",
      "Epoch: 4, Train_Loss: 5.275289535522461, Test_Loss: 5.332844257354736\n",
      "Epoch: 4, Train_Loss: 5.342367172241211, Test_Loss: 5.242249011993408 *\n",
      "Epoch: 4, Train_Loss: 5.368126392364502, Test_Loss: 5.252121448516846\n",
      "Epoch: 4, Train_Loss: 5.374817371368408, Test_Loss: 5.250540733337402 *\n",
      "Epoch: 4, Train_Loss: 5.368524074554443, Test_Loss: 6.0575714111328125\n",
      "Epoch: 4, Train_Loss: 5.335637092590332, Test_Loss: 5.595515251159668 *\n",
      "Epoch: 4, Train_Loss: 5.306014537811279, Test_Loss: 5.808236598968506\n",
      "Epoch: 4, Train_Loss: 5.250328540802002, Test_Loss: 5.957025051116943\n",
      "Epoch: 4, Train_Loss: 5.31349515914917, Test_Loss: 5.3816819190979 *\n",
      "Epoch: 4, Train_Loss: 5.267043590545654, Test_Loss: 5.919250965118408\n",
      "Epoch: 4, Train_Loss: 5.238456726074219, Test_Loss: 5.422171115875244 *\n",
      "Epoch: 4, Train_Loss: 5.231787204742432, Test_Loss: 5.232658863067627 *\n",
      "Epoch: 4, Train_Loss: 5.230860710144043, Test_Loss: 5.2498087882995605\n",
      "Epoch: 4, Train_Loss: 5.276984691619873, Test_Loss: 5.315389156341553\n",
      "Epoch: 4, Train_Loss: 11.022026062011719, Test_Loss: 5.464461326599121\n",
      "Epoch: 4, Train_Loss: 5.3130106925964355, Test_Loss: 6.260663986206055\n",
      "Epoch: 4, Train_Loss: 5.238923072814941, Test_Loss: 5.890857696533203 *\n",
      "Epoch: 4, Train_Loss: 5.250110149383545, Test_Loss: 6.969165325164795\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 5.244990825653076, Test_Loss: 6.052336692810059 *\n",
      "Epoch: 4, Train_Loss: 5.248564720153809, Test_Loss: 5.971227169036865 *\n",
      "Epoch: 4, Train_Loss: 5.2354865074157715, Test_Loss: 5.243869781494141 *\n",
      "Epoch: 4, Train_Loss: 5.227330684661865, Test_Loss: 5.2296342849731445 *\n",
      "Epoch: 4, Train_Loss: 5.253082275390625, Test_Loss: 5.8105854988098145\n",
      "Epoch: 4, Train_Loss: 5.235620498657227, Test_Loss: 6.580782413482666\n",
      "Epoch: 4, Train_Loss: 5.227843284606934, Test_Loss: 5.30812406539917 *\n",
      "Epoch: 4, Train_Loss: 5.222522258758545, Test_Loss: 5.354743480682373\n",
      "Epoch: 4, Train_Loss: 5.2301344871521, Test_Loss: 5.226954460144043 *\n",
      "Epoch: 4, Train_Loss: 5.241460800170898, Test_Loss: 5.3362627029418945\n",
      "Epoch: 4, Train_Loss: 5.211430549621582, Test_Loss: 5.484155654907227\n",
      "Epoch: 4, Train_Loss: 5.225488662719727, Test_Loss: 5.780106544494629\n",
      "Epoch: 4, Train_Loss: 5.247519493103027, Test_Loss: 6.642086505889893\n",
      "Epoch: 4, Train_Loss: 5.272555828094482, Test_Loss: 5.626729965209961 *\n",
      "Epoch: 4, Train_Loss: 5.243724346160889, Test_Loss: 5.223349571228027 *\n",
      "Epoch: 4, Train_Loss: 5.2082343101501465, Test_Loss: 5.239234447479248\n",
      "Epoch: 4, Train_Loss: 5.218071937561035, Test_Loss: 5.256162166595459\n",
      "Epoch: 4, Train_Loss: 5.265183925628662, Test_Loss: 5.271020412445068\n",
      "Epoch: 4, Train_Loss: 5.298844337463379, Test_Loss: 5.435023307800293\n",
      "Epoch: 4, Train_Loss: 5.2824578285217285, Test_Loss: 5.74644660949707\n",
      "Epoch: 4, Train_Loss: 5.255210876464844, Test_Loss: 5.4476470947265625 *\n",
      "Epoch: 4, Train_Loss: 5.2347331047058105, Test_Loss: 5.265746116638184 *\n",
      "Epoch: 4, Train_Loss: 5.278069972991943, Test_Loss: 5.232829570770264 *\n",
      "Epoch: 4, Train_Loss: 5.2792582511901855, Test_Loss: 5.215437412261963 *\n",
      "Epoch: 4, Train_Loss: 5.2487688064575195, Test_Loss: 5.328230857849121\n",
      "Epoch: 4, Train_Loss: 5.296861171722412, Test_Loss: 6.06411075592041\n",
      "Epoch: 4, Train_Loss: 5.223709583282471, Test_Loss: 6.7143473625183105\n",
      "Epoch: 4, Train_Loss: 5.218903064727783, Test_Loss: 5.499815464019775 *\n",
      "Epoch: 4, Train_Loss: 5.21457052230835, Test_Loss: 5.274763584136963 *\n",
      "Epoch: 4, Train_Loss: 5.212595462799072, Test_Loss: 5.209849834442139 *\n",
      "Epoch: 4, Train_Loss: 5.217076778411865, Test_Loss: 5.2098307609558105 *\n",
      "Epoch: 4, Train_Loss: 5.195625305175781, Test_Loss: 5.201635360717773 *\n",
      "Epoch: 4, Train_Loss: 5.213542461395264, Test_Loss: 5.226476669311523\n",
      "Epoch: 4, Train_Loss: 9.803267478942871, Test_Loss: 5.250565528869629\n",
      "Epoch: 4, Train_Loss: 5.5795183181762695, Test_Loss: 5.262829303741455\n",
      "Epoch: 4, Train_Loss: 5.19787073135376, Test_Loss: 5.202922821044922 *\n",
      "Epoch: 4, Train_Loss: 5.212044715881348, Test_Loss: 5.306824207305908\n",
      "Epoch: 4, Train_Loss: 5.193060398101807, Test_Loss: 5.570445537567139\n",
      "Epoch: 4, Train_Loss: 5.188373565673828, Test_Loss: 5.26548433303833 *\n",
      "Epoch: 4, Train_Loss: 5.197565078735352, Test_Loss: 5.352987766265869\n",
      "Epoch: 4, Train_Loss: 5.18258810043335, Test_Loss: 5.193690299987793 *\n",
      "Epoch: 4, Train_Loss: 5.191451072692871, Test_Loss: 5.189059257507324 *\n",
      "Epoch: 4, Train_Loss: 5.188385963439941, Test_Loss: 5.195397853851318\n",
      "Epoch: 4, Train_Loss: 5.255290508270264, Test_Loss: 5.19333553314209 *\n",
      "Epoch: 4, Train_Loss: 5.275934219360352, Test_Loss: 5.233407020568848\n",
      "Epoch: 4, Train_Loss: 5.315268039703369, Test_Loss: 9.574752807617188\n",
      "Epoch: 4, Train_Loss: 5.2810444831848145, Test_Loss: 6.318439960479736 *\n",
      "Epoch: 4, Train_Loss: 5.191020965576172, Test_Loss: 5.191891193389893 *\n",
      "Epoch: 4, Train_Loss: 5.25847053527832, Test_Loss: 5.1957879066467285\n",
      "Epoch: 4, Train_Loss: 5.40218448638916, Test_Loss: 5.196318626403809\n",
      "Epoch: 4, Train_Loss: 5.357791900634766, Test_Loss: 5.217818260192871\n",
      "Epoch: 4, Train_Loss: 5.410142421722412, Test_Loss: 5.188602924346924 *\n",
      "Epoch: 4, Train_Loss: 5.181964874267578, Test_Loss: 5.184841156005859 *\n",
      "Epoch: 4, Train_Loss: 5.180862903594971, Test_Loss: 5.182163238525391 *\n",
      "Epoch: 4, Train_Loss: 5.181908130645752, Test_Loss: 5.182097434997559 *\n",
      "Epoch: 4, Train_Loss: 5.173828601837158, Test_Loss: 5.1781768798828125 *\n",
      "Epoch: 4, Train_Loss: 5.182173252105713, Test_Loss: 5.18564510345459\n",
      "Epoch: 4, Train_Loss: 5.181342124938965, Test_Loss: 5.188521385192871\n",
      "Epoch: 4, Train_Loss: 5.172124862670898, Test_Loss: 5.210201263427734\n",
      "Epoch: 4, Train_Loss: 5.174826622009277, Test_Loss: 5.186334609985352 *\n",
      "Epoch: 4, Train_Loss: 5.172923564910889, Test_Loss: 5.1860880851745605 *\n",
      "Epoch: 4, Train_Loss: 5.18215274810791, Test_Loss: 5.172205924987793 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 5.332555770874023, Test_Loss: 5.174618244171143\n",
      "Epoch: 4, Train_Loss: 5.355935573577881, Test_Loss: 5.177011013031006\n",
      "Epoch: 4, Train_Loss: 5.353730201721191, Test_Loss: 5.171253681182861 *\n",
      "Epoch: 4, Train_Loss: 5.220922470092773, Test_Loss: 5.180408000946045\n",
      "Epoch: 4, Train_Loss: 5.319421291351318, Test_Loss: 5.171715259552002 *\n",
      "Epoch: 4, Train_Loss: 5.317157745361328, Test_Loss: 5.169471263885498 *\n",
      "Epoch: 4, Train_Loss: 5.204807281494141, Test_Loss: 5.177025318145752\n",
      "Epoch: 4, Train_Loss: 5.327065944671631, Test_Loss: 5.1727519035339355 *\n",
      "Epoch: 4, Train_Loss: 5.313413619995117, Test_Loss: 5.181560516357422\n",
      "Epoch: 4, Train_Loss: 5.378550052642822, Test_Loss: 5.17139196395874 *\n",
      "Epoch: 4, Train_Loss: 5.176187038421631, Test_Loss: 5.160569190979004 *\n",
      "Epoch: 4, Train_Loss: 6.85432243347168, Test_Loss: 5.170846462249756\n",
      "Epoch: 4, Train_Loss: 6.784214973449707, Test_Loss: 5.178569793701172\n",
      "Epoch: 4, Train_Loss: 5.201709270477295, Test_Loss: 5.223484992980957\n",
      "Epoch: 4, Train_Loss: 5.21491003036499, Test_Loss: 5.31404972076416\n",
      "Epoch: 4, Train_Loss: 5.195897102355957, Test_Loss: 10.687826156616211\n",
      "Epoch: 4, Train_Loss: 5.20117712020874, Test_Loss: 5.173781871795654 *\n",
      "Epoch: 4, Train_Loss: 5.162310600280762, Test_Loss: 5.158687591552734 *\n",
      "Epoch: 4, Train_Loss: 5.1630706787109375, Test_Loss: 5.199628829956055\n",
      "Epoch: 4, Train_Loss: 5.302657604217529, Test_Loss: 5.205831527709961\n",
      "Epoch: 4, Train_Loss: 5.249759674072266, Test_Loss: 5.208644866943359\n",
      "Epoch: 4, Train_Loss: 5.250362396240234, Test_Loss: 5.155166149139404 *\n",
      "Epoch: 4, Train_Loss: 5.237305164337158, Test_Loss: 5.289088726043701\n",
      "Epoch: 4, Train_Loss: 5.194469451904297, Test_Loss: 5.208248615264893 *\n",
      "Epoch: 4, Train_Loss: 5.190440654754639, Test_Loss: 5.144803524017334 *\n",
      "Epoch: 4, Train_Loss: 5.163771629333496, Test_Loss: 5.203851222991943\n",
      "Epoch: 4, Train_Loss: 5.178706169128418, Test_Loss: 5.16637659072876 *\n",
      "Epoch: 4, Train_Loss: 5.192752838134766, Test_Loss: 5.160007476806641 *\n",
      "Epoch: 4, Train_Loss: 5.152679443359375, Test_Loss: 5.1902079582214355\n",
      "Epoch: 4, Train_Loss: 5.147730827331543, Test_Loss: 5.284669399261475\n",
      "Epoch: 4, Train_Loss: 5.191807270050049, Test_Loss: 5.205893516540527 *\n",
      "Epoch: 4, Train_Loss: 5.207539081573486, Test_Loss: 5.264062404632568\n",
      "Epoch: 4, Train_Loss: 5.160714626312256, Test_Loss: 5.196811676025391 *\n",
      "Epoch: 4, Train_Loss: 5.139545917510986, Test_Loss: 5.187627792358398 *\n",
      "Epoch: 4, Train_Loss: 5.153717041015625, Test_Loss: 5.161688327789307 *\n",
      "Epoch: 4, Train_Loss: 5.151007652282715, Test_Loss: 5.159524917602539 *\n",
      "Epoch: 4, Train_Loss: 5.149804592132568, Test_Loss: 5.176408767700195\n",
      "Epoch: 4, Train_Loss: 5.150068759918213, Test_Loss: 5.173658847808838 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 5.136751174926758, Test_Loss: 5.155704498291016 *\n",
      "Epoch: 4, Train_Loss: 5.1407084465026855, Test_Loss: 5.157125473022461\n",
      "Epoch: 4, Train_Loss: 5.141118049621582, Test_Loss: 5.1623687744140625\n",
      "Epoch: 4, Train_Loss: 5.142084121704102, Test_Loss: 5.164050102233887\n",
      "Epoch: 4, Train_Loss: 5.13555908203125, Test_Loss: 5.180656909942627\n",
      "Epoch: 4, Train_Loss: 5.138324737548828, Test_Loss: 5.144712448120117 *\n",
      "Epoch: 4, Train_Loss: 5.156041145324707, Test_Loss: 5.159355640411377\n",
      "Epoch: 4, Train_Loss: 5.1470818519592285, Test_Loss: 5.205118656158447\n",
      "Epoch: 4, Train_Loss: 5.144935131072998, Test_Loss: 5.213507652282715\n",
      "Epoch: 4, Train_Loss: 5.131244659423828, Test_Loss: 5.411107063293457\n",
      "Epoch: 4, Train_Loss: 5.143246173858643, Test_Loss: 5.14450740814209 *\n",
      "Epoch: 4, Train_Loss: 5.136915683746338, Test_Loss: 5.26838493347168\n",
      "Epoch: 4, Train_Loss: 5.132424354553223, Test_Loss: 5.20985221862793 *\n",
      "Epoch: 4, Train_Loss: 5.1566643714904785, Test_Loss: 5.508997917175293\n",
      "Epoch: 4, Train_Loss: 5.137120723724365, Test_Loss: 5.154305934906006 *\n",
      "Epoch: 4, Train_Loss: 5.12939977645874, Test_Loss: 5.280951499938965\n",
      "Epoch: 4, Train_Loss: 5.128019332885742, Test_Loss: 5.402614116668701\n",
      "Epoch: 4, Train_Loss: 5.148303985595703, Test_Loss: 5.254220008850098 *\n",
      "Epoch: 4, Train_Loss: 5.194337368011475, Test_Loss: 5.189310550689697 *\n",
      "Epoch: 4, Train_Loss: 5.166409969329834, Test_Loss: 5.1289849281311035 *\n",
      "Epoch: 4, Train_Loss: 5.145209312438965, Test_Loss: 5.138018608093262\n",
      "Epoch: 4, Train_Loss: 5.123955249786377, Test_Loss: 5.167623043060303\n",
      "Epoch: 4, Train_Loss: 5.1580705642700195, Test_Loss: 5.591238975524902\n",
      "Epoch: 4, Train_Loss: 5.136552810668945, Test_Loss: 5.624380588531494\n",
      "Epoch: 4, Train_Loss: 5.119213104248047, Test_Loss: 5.626775741577148\n",
      "Epoch: 4, Train_Loss: 5.158527374267578, Test_Loss: 5.992882251739502\n",
      "Epoch: 4, Train_Loss: 5.150647163391113, Test_Loss: 5.300478935241699 *\n",
      "Epoch: 4, Train_Loss: 5.237015724182129, Test_Loss: 5.742778778076172\n",
      "Epoch: 4, Train_Loss: 5.21365213394165, Test_Loss: 5.373861789703369 *\n",
      "Epoch: 4, Train_Loss: 5.1676716804504395, Test_Loss: 5.128570556640625 *\n",
      "Epoch: 4, Train_Loss: 5.138227462768555, Test_Loss: 5.14655065536499\n",
      "Epoch: 4, Train_Loss: 5.130618572235107, Test_Loss: 5.20050048828125\n",
      "Epoch: 4, Train_Loss: 5.149094104766846, Test_Loss: 5.298498153686523\n",
      "Epoch: 4, Train_Loss: 5.1127214431762695, Test_Loss: 6.1546101570129395\n",
      "Epoch: 4, Train_Loss: 5.120195388793945, Test_Loss: 5.408466339111328 *\n",
      "Epoch: 4, Train_Loss: 5.136316299438477, Test_Loss: 7.201038837432861\n",
      "Epoch: 4, Train_Loss: 5.1244988441467285, Test_Loss: 5.649558067321777 *\n",
      "Epoch: 4, Train_Loss: 5.208315849304199, Test_Loss: 6.050988674163818\n",
      "Epoch: 4, Train_Loss: 5.11094331741333, Test_Loss: 5.131152629852295 *\n",
      "Epoch: 4, Train_Loss: 5.208864212036133, Test_Loss: 5.1351189613342285\n",
      "Epoch: 4, Train_Loss: 5.119970798492432, Test_Loss: 5.464813232421875\n",
      "Epoch: 4, Train_Loss: 5.13144063949585, Test_Loss: 6.51444673538208\n",
      "Epoch: 4, Train_Loss: 5.147700786590576, Test_Loss: 5.44317102432251 *\n",
      "Epoch: 4, Train_Loss: 5.378651142120361, Test_Loss: 5.227582931518555 *\n",
      "Epoch: 4, Train_Loss: 5.105007648468018, Test_Loss: 5.109189033508301 *\n",
      "Epoch: 4, Train_Loss: 5.142609596252441, Test_Loss: 5.25796365737915\n",
      "Epoch: 4, Train_Loss: 5.0985565185546875, Test_Loss: 5.472970008850098\n",
      "Epoch: 4, Train_Loss: 5.110361099243164, Test_Loss: 5.416025638580322 *\n",
      "Epoch: 4, Train_Loss: 5.100529193878174, Test_Loss: 6.3620710372924805\n",
      "Epoch: 4, Train_Loss: 5.101754665374756, Test_Loss: 5.613803863525391 *\n",
      "Epoch: 4, Train_Loss: 5.111347675323486, Test_Loss: 5.110341548919678 *\n",
      "Epoch: 4, Train_Loss: 5.126708030700684, Test_Loss: 5.12116003036499\n",
      "Epoch: 4, Train_Loss: 5.126285076141357, Test_Loss: 5.122150897979736\n",
      "Epoch: 4, Train_Loss: 5.1325364112854, Test_Loss: 5.1408185958862305\n",
      "Epoch: 5, Train_Loss: 5.12548828125, Test_Loss: 5.243031978607178 *\n",
      "Epoch: 5, Train_Loss: 5.112444877624512, Test_Loss: 5.624597072601318\n",
      "Epoch: 5, Train_Loss: 5.095403671264648, Test_Loss: 5.4927873611450195 *\n",
      "Epoch: 5, Train_Loss: 5.088913440704346, Test_Loss: 5.2121758460998535 *\n",
      "Epoch: 5, Train_Loss: 5.107767105102539, Test_Loss: 5.125403881072998 *\n",
      "Epoch: 5, Train_Loss: 5.1054911613464355, Test_Loss: 5.107193946838379 *\n",
      "Epoch: 5, Train_Loss: 5.113962173461914, Test_Loss: 5.160759925842285\n",
      "Epoch: 5, Train_Loss: 5.090276718139648, Test_Loss: 5.627040863037109\n",
      "Epoch: 5, Train_Loss: 5.144003868103027, Test_Loss: 6.601341247558594\n",
      "Epoch: 5, Train_Loss: 5.151620388031006, Test_Loss: 5.599757194519043 *\n",
      "Epoch: 5, Train_Loss: 5.142282962799072, Test_Loss: 5.185516357421875 *\n",
      "Epoch: 5, Train_Loss: 5.08948278427124, Test_Loss: 5.092846393585205 *\n",
      "Epoch: 5, Train_Loss: 5.122241020202637, Test_Loss: 5.090609550476074 *\n",
      "Epoch: 5, Train_Loss: 5.090771198272705, Test_Loss: 5.088089942932129 *\n",
      "Epoch: 5, Train_Loss: 5.103388786315918, Test_Loss: 5.096540451049805\n",
      "Epoch: 5, Train_Loss: 5.083041667938232, Test_Loss: 5.121590614318848\n",
      "Epoch: 5, Train_Loss: 5.124192714691162, Test_Loss: 5.123841285705566\n",
      "Epoch: 5, Train_Loss: 5.296607494354248, Test_Loss: 5.079333782196045 *\n",
      "Epoch: 5, Train_Loss: 8.77492904663086, Test_Loss: 5.163974285125732\n",
      "Epoch: 5, Train_Loss: 6.6441802978515625, Test_Loss: 5.298269748687744\n",
      "Epoch: 5, Train_Loss: 5.095647811889648, Test_Loss: 5.325833320617676\n",
      "Epoch: 5, Train_Loss: 5.0753068923950195, Test_Loss: 5.280305862426758 *\n",
      "Epoch: 5, Train_Loss: 5.259139060974121, Test_Loss: 5.077193737030029 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 5.180104732513428, Test_Loss: 5.07034158706665 *\n",
      "Epoch: 5, Train_Loss: 5.089709758758545, Test_Loss: 5.087883949279785\n",
      "Epoch: 5, Train_Loss: 5.074028491973877, Test_Loss: 5.080405235290527 *\n",
      "Epoch: 5, Train_Loss: 5.143705368041992, Test_Loss: 5.087649345397949\n",
      "Epoch: 5, Train_Loss: 5.100989818572998, Test_Loss: 7.741870880126953\n",
      "Epoch: 5, Train_Loss: 5.098290920257568, Test_Loss: 7.926150321960449\n",
      "Epoch: 5, Train_Loss: 5.256211757659912, Test_Loss: 5.081383228302002 *\n",
      "Epoch: 5, Train_Loss: 6.444034576416016, Test_Loss: 5.0876851081848145\n",
      "Epoch: 5, Train_Loss: 6.3999714851379395, Test_Loss: 5.080661773681641 *\n",
      "Epoch: 5, Train_Loss: 5.202394962310791, Test_Loss: 5.075021266937256 *\n",
      "Epoch: 5, Train_Loss: 5.148631572723389, Test_Loss: 5.073455333709717 *\n",
      "Epoch: 5, Train_Loss: 7.179933547973633, Test_Loss: 5.070465087890625 *\n",
      "Epoch: 5, Train_Loss: 6.466259479522705, Test_Loss: 5.07749605178833\n",
      "Epoch: 5, Train_Loss: 5.108933448791504, Test_Loss: 5.0621209144592285 *\n",
      "Epoch: 5, Train_Loss: 5.11134147644043, Test_Loss: 5.06901216506958\n",
      "Epoch: 5, Train_Loss: 5.544073104858398, Test_Loss: 5.071969509124756\n",
      "Epoch: 5, Train_Loss: 6.740971565246582, Test_Loss: 5.075685024261475\n",
      "Epoch: 5, Train_Loss: 6.081442832946777, Test_Loss: 5.111954689025879\n",
      "Epoch: 5, Train_Loss: 5.08248233795166, Test_Loss: 5.109124183654785 *\n",
      "Epoch: 5, Train_Loss: 5.091711521148682, Test_Loss: 5.062585830688477 *\n",
      "Epoch: 5, Train_Loss: 5.281631946563721, Test_Loss: 5.069094181060791\n",
      "Epoch: 5, Train_Loss: 5.619950771331787, Test_Loss: 5.074698448181152\n",
      "Epoch: 5, Train_Loss: 5.0661091804504395, Test_Loss: 5.071397304534912 *\n",
      "Epoch: 5, Train_Loss: 5.108752727508545, Test_Loss: 5.065674781799316 *\n",
      "Epoch: 5, Train_Loss: 5.1488728523254395, Test_Loss: 5.060929298400879 *\n",
      "Epoch: 5, Train_Loss: 5.222435474395752, Test_Loss: 5.065847873687744\n",
      "Epoch: 5, Train_Loss: 5.165884971618652, Test_Loss: 5.057566165924072 *\n",
      "Epoch: 5, Train_Loss: 5.435471057891846, Test_Loss: 5.061910629272461\n",
      "Epoch: 5, Train_Loss: 5.2666497230529785, Test_Loss: 5.066007614135742\n",
      "Epoch: 5, Train_Loss: 5.107717037200928, Test_Loss: 5.064609050750732 *\n",
      "Epoch: 5, Train_Loss: 5.3121161460876465, Test_Loss: 5.051910400390625 *\n",
      "Epoch: 5, Train_Loss: 5.347796440124512, Test_Loss: 5.051605224609375 *\n",
      "Epoch: 5, Train_Loss: 5.536797523498535, Test_Loss: 5.057474136352539\n",
      "Epoch: 5, Train_Loss: 5.310762405395508, Test_Loss: 5.056981563568115 *\n",
      "Epoch: 5, Train_Loss: 5.100833892822266, Test_Loss: 5.090366840362549\n",
      "Epoch: 5, Train_Loss: 5.193034648895264, Test_Loss: 5.093194484710693\n",
      "Epoch: 5, Train_Loss: 5.127005100250244, Test_Loss: 9.755000114440918\n",
      "Epoch: 5, Train_Loss: 5.0617570877075195, Test_Loss: 5.676090240478516 *\n",
      "Epoch: 5, Train_Loss: 5.044439792633057, Test_Loss: 5.050393104553223 *\n",
      "Epoch: 5, Train_Loss: 5.045032501220703, Test_Loss: 5.0584821701049805\n",
      "Epoch: 5, Train_Loss: 5.060790061950684, Test_Loss: 5.086926460266113\n",
      "Epoch: 5, Train_Loss: 5.05034875869751, Test_Loss: 5.102512836456299\n",
      "Epoch: 5, Train_Loss: 5.061089992523193, Test_Loss: 5.048913955688477 *\n",
      "Epoch: 5, Train_Loss: 5.1188740730285645, Test_Loss: 5.137520790100098\n",
      "Epoch: 5, Train_Loss: 5.132693290710449, Test_Loss: 5.12299919128418 *\n",
      "Epoch: 5, Train_Loss: 5.174497604370117, Test_Loss: 5.041701793670654 *\n",
      "Epoch: 5, Train_Loss: 5.241833686828613, Test_Loss: 5.090144157409668\n",
      "Epoch: 5, Train_Loss: 5.391292572021484, Test_Loss: 5.064177989959717 *\n",
      "Epoch: 5, Train_Loss: 5.056394100189209, Test_Loss: 5.051738262176514 *\n",
      "Epoch: 5, Train_Loss: 5.120298385620117, Test_Loss: 5.036551475524902 *\n",
      "Epoch: 5, Train_Loss: 5.408647060394287, Test_Loss: 5.157901287078857\n",
      "Epoch: 5, Train_Loss: 5.637918472290039, Test_Loss: 5.066723346710205 *\n",
      "Epoch: 5, Train_Loss: 5.085962772369385, Test_Loss: 5.1665754318237305\n",
      "Epoch: 5, Train_Loss: 5.039653778076172, Test_Loss: 5.111387729644775 *\n",
      "Epoch: 5, Train_Loss: 5.526865005493164, Test_Loss: 5.071248531341553 *\n",
      "Epoch: 5, Train_Loss: 5.8471550941467285, Test_Loss: 5.045207500457764 *\n",
      "Epoch: 5, Train_Loss: 5.335168838500977, Test_Loss: 5.041666507720947 *\n",
      "Epoch: 5, Train_Loss: 5.053348064422607, Test_Loss: 5.04181432723999\n",
      "Epoch: 5, Train_Loss: 5.057915210723877, Test_Loss: 5.044045925140381\n",
      "Epoch: 5, Train_Loss: 5.387083053588867, Test_Loss: 5.027806282043457 *\n",
      "Epoch: 5, Train_Loss: 6.812493324279785, Test_Loss: 5.03085994720459\n",
      "Epoch: 5, Train_Loss: 5.432553768157959, Test_Loss: 5.020979881286621 *\n",
      "Epoch: 5, Train_Loss: 5.049639701843262, Test_Loss: 5.042442798614502\n",
      "Epoch: 5, Train_Loss: 5.033723831176758, Test_Loss: 5.0543437004089355\n",
      "Epoch: 5, Train_Loss: 5.026561260223389, Test_Loss: 5.036646842956543 *\n",
      "Epoch: 5, Train_Loss: 5.384370803833008, Test_Loss: 5.0381269454956055\n",
      "Epoch: 5, Train_Loss: 5.120394706726074, Test_Loss: 5.076702117919922\n",
      "Epoch: 5, Train_Loss: 5.082972049713135, Test_Loss: 5.043497085571289 *\n",
      "Epoch: 5, Train_Loss: 5.042383193969727, Test_Loss: 5.352330684661865\n",
      "Epoch: 5, Train_Loss: 5.039858818054199, Test_Loss: 5.030128002166748 *\n",
      "Epoch: 5, Train_Loss: 19.28364372253418, Test_Loss: 5.095704078674316\n",
      "Epoch: 5, Train_Loss: 8.154452323913574, Test_Loss: 5.080131530761719 *\n",
      "Epoch: 5, Train_Loss: 6.063030242919922, Test_Loss: 5.378818988800049\n",
      "Epoch: 5, Train_Loss: 8.096230506896973, Test_Loss: 5.053483009338379 *\n",
      "Epoch: 5, Train_Loss: 5.1241607666015625, Test_Loss: 5.1361613273620605\n",
      "Epoch: 5, Train_Loss: 5.101247310638428, Test_Loss: 5.318245887756348\n",
      "Epoch: 5, Train_Loss: 5.856545448303223, Test_Loss: 5.2598185539245605 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 15.709821701049805, Test_Loss: 5.047347545623779 *\n",
      "Epoch: 5, Train_Loss: 5.965052127838135, Test_Loss: 5.046331405639648 *\n",
      "Epoch: 5, Train_Loss: 5.025137424468994, Test_Loss: 5.017702102661133 *\n",
      "Epoch: 5, Train_Loss: 9.561202049255371, Test_Loss: 5.031039714813232\n",
      "Epoch: 5, Train_Loss: 6.765755653381348, Test_Loss: 5.181974411010742\n",
      "Epoch: 5, Train_Loss: 5.029947280883789, Test_Loss: 5.874709606170654\n",
      "Epoch: 5, Train_Loss: 5.0139479637146, Test_Loss: 5.225901126861572 *\n",
      "Epoch: 5, Train_Loss: 5.010126113891602, Test_Loss: 5.839076519012451\n",
      "Epoch: 5, Train_Loss: 5.013835906982422, Test_Loss: 5.356834888458252 *\n",
      "Epoch: 5, Train_Loss: 5.010871410369873, Test_Loss: 5.575993537902832\n",
      "Epoch: 5, Train_Loss: 5.013220310211182, Test_Loss: 5.406815052032471 *\n",
      "Epoch: 5, Train_Loss: 5.011741638183594, Test_Loss: 5.037843227386475 *\n",
      "Epoch: 5, Train_Loss: 5.015531063079834, Test_Loss: 5.010837554931641 *\n",
      "Epoch: 5, Train_Loss: 5.018632888793945, Test_Loss: 5.05011510848999\n",
      "Epoch: 5, Train_Loss: 5.0117268562316895, Test_Loss: 5.117572784423828\n",
      "Epoch: 5, Train_Loss: 5.001760005950928, Test_Loss: 5.985748291015625\n",
      "Epoch: 5, Train_Loss: 5.025729656219482, Test_Loss: 5.25806188583374 *\n",
      "Epoch: 5, Train_Loss: 5.088536262512207, Test_Loss: 6.90830135345459\n",
      "Epoch: 5, Train_Loss: 5.064720630645752, Test_Loss: 5.559186935424805 *\n",
      "Epoch: 5, Train_Loss: 5.005496501922607, Test_Loss: 6.299756050109863\n",
      "Epoch: 5, Train_Loss: 5.006044864654541, Test_Loss: 5.162631034851074 *\n",
      "Epoch: 5, Train_Loss: 4.99766206741333, Test_Loss: 5.012436389923096 *\n",
      "Epoch: 5, Train_Loss: 5.004690647125244, Test_Loss: 5.121047019958496\n",
      "Epoch: 5, Train_Loss: 4.994367599487305, Test_Loss: 6.155717849731445\n",
      "Epoch: 5, Train_Loss: 4.9962382316589355, Test_Loss: 5.622776985168457 *\n",
      "Epoch: 5, Train_Loss: 5.006567478179932, Test_Loss: 5.14320707321167 *\n",
      "Epoch: 5, Train_Loss: 4.994143486022949, Test_Loss: 5.040678024291992 *\n",
      "Epoch: 5, Train_Loss: 4.9888916015625, Test_Loss: 5.066977024078369\n",
      "Epoch: 5, Train_Loss: 4.991921424865723, Test_Loss: 5.3065009117126465\n",
      "Epoch: 5, Train_Loss: 5.003111362457275, Test_Loss: 5.212828159332275 *\n",
      "Epoch: 5, Train_Loss: 4.993060111999512, Test_Loss: 6.2709479331970215\n",
      "Epoch: 5, Train_Loss: 5.027895927429199, Test_Loss: 5.8050150871276855 *\n",
      "Epoch: 5, Train_Loss: 5.002387523651123, Test_Loss: 5.065817832946777 *\n",
      "Epoch: 5, Train_Loss: 4.999923229217529, Test_Loss: 5.007267951965332 *\n",
      "Epoch: 5, Train_Loss: 4.983465671539307, Test_Loss: 5.0140700340271\n",
      "Epoch: 5, Train_Loss: 14.130583763122559, Test_Loss: 5.017800807952881\n",
      "Epoch: 5, Train_Loss: 5.298196792602539, Test_Loss: 5.0385894775390625\n",
      "Epoch: 5, Train_Loss: 4.990273475646973, Test_Loss: 5.439561367034912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 4.9842848777771, Test_Loss: 5.429544448852539 *\n",
      "Epoch: 5, Train_Loss: 4.990001201629639, Test_Loss: 5.0779900550842285 *\n",
      "Epoch: 5, Train_Loss: 5.0241475105285645, Test_Loss: 4.993663787841797 *\n",
      "Epoch: 5, Train_Loss: 5.010489463806152, Test_Loss: 4.988054275512695 *\n",
      "Epoch: 5, Train_Loss: 4.99308443069458, Test_Loss: 5.02276611328125\n",
      "Epoch: 5, Train_Loss: 5.118466854095459, Test_Loss: 5.318133354187012\n",
      "Epoch: 5, Train_Loss: 5.179135322570801, Test_Loss: 6.573881149291992\n",
      "Epoch: 5, Train_Loss: 5.143850803375244, Test_Loss: 5.881023406982422 *\n",
      "Epoch: 5, Train_Loss: 4.98188591003418, Test_Loss: 5.032606601715088 *\n",
      "Epoch: 5, Train_Loss: 5.062317848205566, Test_Loss: 5.001553058624268 *\n",
      "Epoch: 5, Train_Loss: 5.070789813995361, Test_Loss: 4.993688106536865 *\n",
      "Epoch: 5, Train_Loss: 5.100543022155762, Test_Loss: 4.990458011627197 *\n",
      "Epoch: 5, Train_Loss: 5.089314937591553, Test_Loss: 4.994510173797607\n",
      "Epoch: 5, Train_Loss: 5.061243057250977, Test_Loss: 5.019862174987793\n",
      "Epoch: 5, Train_Loss: 5.001465797424316, Test_Loss: 5.060981273651123\n",
      "Epoch: 5, Train_Loss: 4.979842662811279, Test_Loss: 4.966676712036133 *\n",
      "Epoch: 5, Train_Loss: 5.041465759277344, Test_Loss: 5.002835750579834\n",
      "Epoch: 5, Train_Loss: 4.9872846603393555, Test_Loss: 5.097152233123779\n",
      "Epoch: 5, Train_Loss: 4.969717502593994, Test_Loss: 5.279262542724609\n",
      "Epoch: 5, Train_Loss: 4.972444534301758, Test_Loss: 5.172786712646484 *\n",
      "Epoch: 5, Train_Loss: 4.96966028213501, Test_Loss: 4.970757484436035 *\n",
      "Epoch: 5, Train_Loss: 5.077442169189453, Test_Loss: 4.96897029876709 *\n",
      "Epoch: 5, Train_Loss: 10.535431861877441, Test_Loss: 4.965444087982178 *\n",
      "Epoch: 5, Train_Loss: 4.974311351776123, Test_Loss: 4.961940288543701 *\n",
      "Epoch: 5, Train_Loss: 4.970736503601074, Test_Loss: 4.977541446685791\n",
      "Epoch: 5, Train_Loss: 4.978310585021973, Test_Loss: 6.1577229499816895\n",
      "Epoch: 5, Train_Loss: 4.99099588394165, Test_Loss: 9.471831321716309\n",
      "Epoch: 5, Train_Loss: 4.9723801612854, Test_Loss: 4.9757795333862305 *\n",
      "Epoch: 5, Train_Loss: 4.971492767333984, Test_Loss: 4.962212562561035 *\n",
      "Epoch: 5, Train_Loss: 4.971844673156738, Test_Loss: 4.957301616668701 *\n",
      "Epoch: 5, Train_Loss: 4.985732555389404, Test_Loss: 4.964809417724609\n",
      "Epoch: 5, Train_Loss: 4.9749226570129395, Test_Loss: 4.959904193878174 *\n",
      "Epoch: 5, Train_Loss: 4.9654459953308105, Test_Loss: 4.962300777435303\n",
      "Epoch: 5, Train_Loss: 4.950364112854004, Test_Loss: 4.965468406677246\n",
      "Epoch: 5, Train_Loss: 4.953860759735107, Test_Loss: 4.971663951873779\n",
      "Epoch: 5, Train_Loss: 4.975160598754883, Test_Loss: 4.964505672454834 *\n",
      "Epoch: 5, Train_Loss: 4.953152179718018, Test_Loss: 4.95963716506958 *\n",
      "Epoch: 5, Train_Loss: 4.956916332244873, Test_Loss: 4.966249942779541\n",
      "Epoch: 5, Train_Loss: 4.993005752563477, Test_Loss: 4.959937572479248 *\n",
      "Epoch: 5, Train_Loss: 5.0306501388549805, Test_Loss: 4.951696872711182 *\n",
      "Epoch: 5, Train_Loss: 4.980245590209961, Test_Loss: 4.96048641204834\n",
      "Epoch: 5, Train_Loss: 4.945814609527588, Test_Loss: 4.95238733291626 *\n",
      "Epoch: 5, Train_Loss: 4.956841468811035, Test_Loss: 4.949084758758545 *\n",
      "Epoch: 5, Train_Loss: 5.006547451019287, Test_Loss: 4.952512741088867\n",
      "Epoch: 5, Train_Loss: 5.017085552215576, Test_Loss: 4.94848108291626 *\n",
      "Epoch: 5, Train_Loss: 5.035734176635742, Test_Loss: 4.947605609893799 *\n",
      "Epoch: 5, Train_Loss: 5.014125823974609, Test_Loss: 4.9503254890441895\n",
      "Epoch: 5, Train_Loss: 4.965489864349365, Test_Loss: 4.945248126983643 *\n",
      "Epoch: 5, Train_Loss: 5.034536838531494, Test_Loss: 4.966257095336914\n",
      "Epoch: 5, Train_Loss: 5.019404411315918, Test_Loss: 4.946875095367432 *\n",
      "Epoch: 5, Train_Loss: 4.95965051651001, Test_Loss: 4.9441423416137695 *\n",
      "Epoch: 5, Train_Loss: 5.044018745422363, Test_Loss: 4.9390482902526855 *\n",
      "Epoch: 5, Train_Loss: 4.959232807159424, Test_Loss: 4.955904483795166\n",
      "Epoch: 5, Train_Loss: 4.949018955230713, Test_Loss: 4.940182209014893 *\n",
      "Epoch: 5, Train_Loss: 4.937716960906982, Test_Loss: 4.939204216003418 *\n",
      "Epoch: 5, Train_Loss: 4.937442779541016, Test_Loss: 4.964019298553467\n",
      "Epoch: 5, Train_Loss: 4.930782794952393, Test_Loss: 4.98124885559082\n",
      "Epoch: 5, Train_Loss: 4.929769039154053, Test_Loss: 8.502450942993164\n",
      "Epoch: 5, Train_Loss: 5.0142436027526855, Test_Loss: 7.119860649108887 *\n",
      "Epoch: 5, Train_Loss: 9.827898025512695, Test_Loss: 4.925375461578369 *\n",
      "Epoch: 5, Train_Loss: 5.025553226470947, Test_Loss: 4.934552192687988\n",
      "Epoch: 5, Train_Loss: 4.931548118591309, Test_Loss: 4.961905479431152\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 4.958540916442871, Test_Loss: 4.981994152069092\n",
      "Epoch: 5, Train_Loss: 4.93125057220459, Test_Loss: 4.949337959289551 *\n",
      "Epoch: 5, Train_Loss: 4.937525749206543, Test_Loss: 5.004000663757324\n",
      "Epoch: 5, Train_Loss: 4.92361307144165, Test_Loss: 5.028048992156982\n",
      "Epoch: 5, Train_Loss: 4.924839019775391, Test_Loss: 4.924778938293457 *\n",
      "Epoch: 5, Train_Loss: 4.922597408294678, Test_Loss: 4.961035251617432\n",
      "Epoch: 5, Train_Loss: 4.919206142425537, Test_Loss: 4.935765743255615 *\n",
      "Epoch: 5, Train_Loss: 4.99309778213501, Test_Loss: 4.939939975738525\n",
      "Epoch: 5, Train_Loss: 5.0199408531188965, Test_Loss: 4.918759346008301 *\n",
      "Epoch: 5, Train_Loss: 5.00039005279541, Test_Loss: 5.032325267791748\n",
      "Epoch: 5, Train_Loss: 4.9995574951171875, Test_Loss: 4.977343559265137 *\n",
      "Epoch: 5, Train_Loss: 4.916308403015137, Test_Loss: 5.022585391998291\n",
      "Epoch: 5, Train_Loss: 5.021464824676514, Test_Loss: 5.010278224945068 *\n",
      "Epoch: 5, Train_Loss: 5.096010208129883, Test_Loss: 4.951849937438965 *\n",
      "Epoch: 5, Train_Loss: 5.099526882171631, Test_Loss: 4.932628154754639 *\n",
      "Epoch: 5, Train_Loss: 5.101691246032715, Test_Loss: 4.928144454956055 *\n",
      "Epoch: 5, Train_Loss: 4.916476726531982, Test_Loss: 4.929657459259033\n",
      "Epoch: 5, Train_Loss: 4.91262149810791, Test_Loss: 4.913211345672607 *\n",
      "Epoch: 5, Train_Loss: 4.906103134155273, Test_Loss: 4.926480293273926\n",
      "Epoch: 5, Train_Loss: 4.916563034057617, Test_Loss: 4.930956840515137\n",
      "Epoch: 5, Train_Loss: 4.914569854736328, Test_Loss: 4.908157825469971 *\n",
      "Epoch: 5, Train_Loss: 4.914999961853027, Test_Loss: 4.9339399337768555\n",
      "Epoch: 5, Train_Loss: 4.904428482055664, Test_Loss: 4.918099880218506 *\n",
      "Epoch: 5, Train_Loss: 4.8975934982299805, Test_Loss: 4.915923118591309 *\n",
      "Epoch: 5, Train_Loss: 4.906228542327881, Test_Loss: 4.914943695068359 *\n",
      "Epoch: 5, Train_Loss: 4.918455600738525, Test_Loss: 4.940483093261719\n",
      "Epoch: 5, Train_Loss: 5.05216121673584, Test_Loss: 4.9426188468933105\n",
      "Epoch: 5, Train_Loss: 5.053102493286133, Test_Loss: 5.230464935302734\n",
      "Epoch: 5, Train_Loss: 5.115814208984375, Test_Loss: 4.928869247436523 *\n",
      "Epoch: 5, Train_Loss: 4.951533794403076, Test_Loss: 4.932648658752441\n",
      "Epoch: 5, Train_Loss: 5.071383953094482, Test_Loss: 4.9826531410217285\n",
      "Epoch: 5, Train_Loss: 5.0205841064453125, Test_Loss: 5.200969219207764\n",
      "Epoch: 5, Train_Loss: 4.97634744644165, Test_Loss: 5.025028705596924 *\n",
      "Epoch: 5, Train_Loss: 5.030019760131836, Test_Loss: 4.949500560760498 *\n",
      "Epoch: 5, Train_Loss: 5.154337406158447, Test_Loss: 5.181602478027344\n",
      "Epoch: 5, Train_Loss: 4.96942138671875, Test_Loss: 5.226852893829346\n",
      "Epoch: 5, Train_Loss: 4.903540134429932, Test_Loss: 4.910346031188965 *\n",
      "Epoch: 5, Train_Loss: 7.174071311950684, Test_Loss: 4.954561233520508\n",
      "Epoch: 5, Train_Loss: 5.825626373291016, Test_Loss: 4.893688201904297 *\n",
      "Epoch: 5, Train_Loss: 4.924432277679443, Test_Loss: 4.9492878913879395\n",
      "Epoch: 5, Train_Loss: 4.951292514801025, Test_Loss: 4.9556732177734375\n",
      "Epoch: 5, Train_Loss: 4.965942859649658, Test_Loss: 5.822178363800049\n",
      "Epoch: 5, Train_Loss: 4.930538177490234, Test_Loss: 5.051589488983154 *\n",
      "Epoch: 5, Train_Loss: 4.892265319824219, Test_Loss: 5.701514720916748\n",
      "Epoch: 5, Train_Loss: 4.921596050262451, Test_Loss: 5.5293121337890625 *\n",
      "Epoch: 5, Train_Loss: 5.020955562591553, Test_Loss: 5.240190029144287 *\n",
      "Epoch: 5, Train_Loss: 4.984749794006348, Test_Loss: 5.38739538192749\n",
      "Epoch: 5, Train_Loss: 4.980930805206299, Test_Loss: 4.928607940673828 *\n",
      "Epoch: 5, Train_Loss: 4.977952003479004, Test_Loss: 4.898245334625244 *\n",
      "Epoch: 5, Train_Loss: 4.949287414550781, Test_Loss: 4.915065765380859\n",
      "Epoch: 5, Train_Loss: 4.907724380493164, Test_Loss: 5.070441722869873\n",
      "Epoch: 5, Train_Loss: 4.907529830932617, Test_Loss: 5.488102912902832\n",
      "Epoch: 5, Train_Loss: 4.917694568634033, Test_Loss: 5.357949733734131 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 4.89810848236084, Test_Loss: 6.518072128295898\n",
      "Epoch: 5, Train_Loss: 4.891193389892578, Test_Loss: 5.79545259475708 *\n",
      "Epoch: 5, Train_Loss: 4.894660949707031, Test_Loss: 5.9528961181640625\n",
      "Epoch: 5, Train_Loss: 4.935218334197998, Test_Loss: 5.218398571014404 *\n",
      "Epoch: 5, Train_Loss: 4.936245441436768, Test_Loss: 4.881855010986328 *\n",
      "Epoch: 5, Train_Loss: 4.888218879699707, Test_Loss: 4.944355487823486\n",
      "Epoch: 5, Train_Loss: 4.885697364807129, Test_Loss: 5.952488899230957\n",
      "Epoch: 5, Train_Loss: 4.870267868041992, Test_Loss: 5.855628967285156 *\n",
      "Epoch: 5, Train_Loss: 4.877905368804932, Test_Loss: 4.947212219238281 *\n",
      "Epoch: 5, Train_Loss: 4.88145112991333, Test_Loss: 4.944268226623535 *\n",
      "Epoch: 5, Train_Loss: 4.875012397766113, Test_Loss: 4.892081260681152 *\n",
      "Epoch: 5, Train_Loss: 4.869734287261963, Test_Loss: 5.243713855743408\n",
      "Epoch: 5, Train_Loss: 4.8725972175598145, Test_Loss: 5.0427374839782715 *\n",
      "Epoch: 5, Train_Loss: 4.872310638427734, Test_Loss: 5.792875289916992\n",
      "Epoch: 5, Train_Loss: 4.873039245605469, Test_Loss: 5.823524475097656\n",
      "Epoch: 5, Train_Loss: 4.861934661865234, Test_Loss: 5.097560405731201 *\n",
      "Epoch: 5, Train_Loss: 4.875286102294922, Test_Loss: 4.872334003448486 *\n",
      "Epoch: 5, Train_Loss: 4.8739752769470215, Test_Loss: 4.886417388916016\n",
      "Epoch: 5, Train_Loss: 4.876416206359863, Test_Loss: 4.8884196281433105\n",
      "Epoch: 5, Train_Loss: 4.884543418884277, Test_Loss: 4.909815311431885\n",
      "Epoch: 5, Train_Loss: 4.866154193878174, Test_Loss: 5.3198466300964355\n",
      "Epoch: 5, Train_Loss: 4.858498573303223, Test_Loss: 5.449169635772705\n",
      "Epoch: 5, Train_Loss: 4.85521936416626, Test_Loss: 5.052693843841553 *\n",
      "Epoch: 5, Train_Loss: 4.857909202575684, Test_Loss: 4.885991096496582 *\n",
      "Epoch: 5, Train_Loss: 4.870239734649658, Test_Loss: 4.892884731292725\n",
      "Epoch: 5, Train_Loss: 4.852527141571045, Test_Loss: 4.880822658538818 *\n",
      "Epoch: 5, Train_Loss: 4.860584259033203, Test_Loss: 5.000204086303711\n",
      "Epoch: 5, Train_Loss: 4.863709926605225, Test_Loss: 6.0525689125061035\n",
      "Epoch: 5, Train_Loss: 4.894365310668945, Test_Loss: 6.118129730224609\n",
      "Epoch: 5, Train_Loss: 4.927631855010986, Test_Loss: 4.916721820831299 *\n",
      "Epoch: 5, Train_Loss: 4.902017116546631, Test_Loss: 4.9442853927612305\n",
      "Epoch: 5, Train_Loss: 4.871413230895996, Test_Loss: 4.85332727432251 *\n",
      "Epoch: 5, Train_Loss: 4.857340335845947, Test_Loss: 4.866530895233154\n",
      "Epoch: 5, Train_Loss: 4.8981194496154785, Test_Loss: 4.856328964233398 *\n",
      "Epoch: 5, Train_Loss: 4.864314079284668, Test_Loss: 4.878222942352295\n",
      "Epoch: 5, Train_Loss: 4.856561660766602, Test_Loss: 4.918206214904785\n",
      "Epoch: 5, Train_Loss: 4.884896755218506, Test_Loss: 4.871712684631348 *\n",
      "Epoch: 5, Train_Loss: 4.873427867889404, Test_Loss: 4.866339683532715 *\n",
      "Epoch: 5, Train_Loss: 4.988975524902344, Test_Loss: 4.9618072509765625\n",
      "Epoch: 5, Train_Loss: 4.963020324707031, Test_Loss: 5.240139961242676\n",
      "Epoch: 5, Train_Loss: 4.9080729484558105, Test_Loss: 5.029796600341797 *\n",
      "Epoch: 5, Train_Loss: 4.858959197998047, Test_Loss: 4.869330883026123 *\n",
      "Epoch: 5, Train_Loss: 4.861935615539551, Test_Loss: 4.8434224128723145 *\n",
      "Epoch: 5, Train_Loss: 4.874573230743408, Test_Loss: 4.845930576324463\n",
      "Epoch: 5, Train_Loss: 4.8437275886535645, Test_Loss: 4.852974891662598\n",
      "Epoch: 5, Train_Loss: 4.850189208984375, Test_Loss: 4.84385347366333 *\n",
      "Epoch: 5, Train_Loss: 4.8482441902160645, Test_Loss: 5.073772430419922\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 4.860151290893555, Test_Loss: 10.032299041748047\n",
      "Epoch: 5, Train_Loss: 4.924472332000732, Test_Loss: 4.92723274230957 *\n",
      "Epoch: 5, Train_Loss: 4.850223541259766, Test_Loss: 4.850008010864258 *\n",
      "Epoch: 5, Train_Loss: 4.911622047424316, Test_Loss: 4.840244770050049 *\n",
      "Epoch: 5, Train_Loss: 4.851984024047852, Test_Loss: 4.855021953582764\n",
      "Epoch: 5, Train_Loss: 4.851968765258789, Test_Loss: 4.839653491973877 *\n",
      "Epoch: 5, Train_Loss: 4.931429862976074, Test_Loss: 4.8424482345581055\n",
      "Epoch: 5, Train_Loss: 5.052371501922607, Test_Loss: 4.827248573303223 *\n",
      "Epoch: 5, Train_Loss: 4.835441589355469, Test_Loss: 4.837746620178223\n",
      "Epoch: 5, Train_Loss: 4.864071369171143, Test_Loss: 4.834620475769043 *\n",
      "Epoch: 5, Train_Loss: 4.8263654708862305, Test_Loss: 4.833266258239746 *\n",
      "Epoch: 5, Train_Loss: 4.829488754272461, Test_Loss: 4.833581447601318\n",
      "Epoch: 5, Train_Loss: 4.826305389404297, Test_Loss: 4.845027923583984\n",
      "Epoch: 5, Train_Loss: 4.836837291717529, Test_Loss: 4.845022678375244 *\n",
      "Epoch: 5, Train_Loss: 4.84501838684082, Test_Loss: 4.849774360656738\n",
      "Epoch: 5, Train_Loss: 4.841944217681885, Test_Loss: 4.826606273651123 *\n",
      "Epoch: 5, Train_Loss: 4.843592166900635, Test_Loss: 4.825260162353516 *\n",
      "Epoch: 5, Train_Loss: 4.858171463012695, Test_Loss: 4.822147846221924 *\n",
      "Epoch: 5, Train_Loss: 4.8502421379089355, Test_Loss: 4.822751998901367\n",
      "Epoch: 5, Train_Loss: 4.834227085113525, Test_Loss: 4.825031280517578\n",
      "Epoch: 5, Train_Loss: 4.8278398513793945, Test_Loss: 4.825101375579834\n",
      "Epoch: 5, Train_Loss: 4.824464797973633, Test_Loss: 4.8163580894470215 *\n",
      "Epoch: 5, Train_Loss: 4.834643840789795, Test_Loss: 4.82256555557251\n",
      "Epoch: 5, Train_Loss: 4.845748424530029, Test_Loss: 4.8170485496521 *\n",
      "Epoch: 5, Train_Loss: 4.839466094970703, Test_Loss: 4.817875862121582\n",
      "Epoch: 5, Train_Loss: 4.8134260177612305, Test_Loss: 4.812950134277344 *\n",
      "Epoch: 5, Train_Loss: 4.881336688995361, Test_Loss: 4.813976287841797\n",
      "Epoch: 5, Train_Loss: 4.874887466430664, Test_Loss: 4.81318998336792 *\n",
      "Epoch: 5, Train_Loss: 4.847304821014404, Test_Loss: 4.818097114562988\n",
      "Epoch: 5, Train_Loss: 4.822419166564941, Test_Loss: 4.820647716522217\n",
      "Epoch: 5, Train_Loss: 4.840980529785156, Test_Loss: 4.872919082641602\n",
      "Epoch: 5, Train_Loss: 4.821711540222168, Test_Loss: 6.939268112182617\n",
      "Epoch: 5, Train_Loss: 4.840803146362305, Test_Loss: 8.122702598571777\n",
      "Epoch: 5, Train_Loss: 4.813694477081299, Test_Loss: 4.8124308586120605 *\n",
      "Epoch: 5, Train_Loss: 4.82964563369751, Test_Loss: 4.807962417602539 *\n",
      "Epoch: 5, Train_Loss: 5.898499965667725, Test_Loss: 4.851759910583496\n",
      "Epoch: 5, Train_Loss: 8.86001968383789, Test_Loss: 4.844857692718506 *\n",
      "Epoch: 5, Train_Loss: 5.160184383392334, Test_Loss: 4.859304904937744\n",
      "Epoch: 5, Train_Loss: 4.823380470275879, Test_Loss: 4.847610950469971 *\n",
      "Epoch: 5, Train_Loss: 4.818451404571533, Test_Loss: 4.94516134262085\n",
      "Epoch: 5, Train_Loss: 4.989943504333496, Test_Loss: 4.804242134094238 *\n",
      "Epoch: 5, Train_Loss: 4.875077247619629, Test_Loss: 4.828487396240234\n",
      "Epoch: 5, Train_Loss: 4.813221454620361, Test_Loss: 4.833108901977539\n",
      "Epoch: 5, Train_Loss: 4.800171852111816, Test_Loss: 4.810122013092041 *\n",
      "Epoch: 5, Train_Loss: 4.864658832550049, Test_Loss: 4.805310249328613 *\n",
      "Epoch: 5, Train_Loss: 4.824193954467773, Test_Loss: 4.88022518157959\n",
      "Epoch: 5, Train_Loss: 4.819185733795166, Test_Loss: 4.892187595367432\n",
      "Epoch: 5, Train_Loss: 5.12437105178833, Test_Loss: 4.87778377532959 *\n",
      "Epoch: 5, Train_Loss: 6.288753032684326, Test_Loss: 4.906307220458984\n",
      "Epoch: 5, Train_Loss: 5.927708625793457, Test_Loss: 4.817968368530273 *\n",
      "Epoch: 5, Train_Loss: 4.9529194831848145, Test_Loss: 4.830850124359131\n",
      "Epoch: 5, Train_Loss: 4.969283580780029, Test_Loss: 4.811033725738525 *\n",
      "Epoch: 5, Train_Loss: 7.134377479553223, Test_Loss: 4.808000087738037 *\n",
      "Epoch: 5, Train_Loss: 5.824713230133057, Test_Loss: 4.810831069946289\n",
      "Epoch: 5, Train_Loss: 4.84837532043457, Test_Loss: 4.823266506195068\n",
      "Epoch: 5, Train_Loss: 4.824110507965088, Test_Loss: 4.8104963302612305 *\n",
      "Epoch: 5, Train_Loss: 5.560408115386963, Test_Loss: 4.798580646514893 *\n",
      "Epoch: 5, Train_Loss: 6.5396318435668945, Test_Loss: 4.826406955718994\n",
      "Epoch: 5, Train_Loss: 5.566129207611084, Test_Loss: 4.805583953857422 *\n",
      "Epoch: 5, Train_Loss: 4.811593055725098, Test_Loss: 4.8208112716674805\n",
      "Epoch: 5, Train_Loss: 4.804318904876709, Test_Loss: 4.804641246795654 *\n",
      "Epoch: 5, Train_Loss: 5.15862512588501, Test_Loss: 4.838067531585693\n",
      "Epoch: 5, Train_Loss: 5.174075126647949, Test_Loss: 4.852108478546143\n",
      "Epoch: 5, Train_Loss: 4.8075032234191895, Test_Loss: 5.027764320373535\n",
      "Epoch: 5, Train_Loss: 4.830968856811523, Test_Loss: 4.868168354034424 *\n",
      "Epoch: 5, Train_Loss: 4.896763324737549, Test_Loss: 4.821444988250732 *\n",
      "Epoch: 5, Train_Loss: 4.942827224731445, Test_Loss: 4.889729022979736\n",
      "Epoch: 5, Train_Loss: 4.850018501281738, Test_Loss: 5.010886192321777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 5.202514171600342, Test_Loss: 5.03497838973999\n",
      "Epoch: 5, Train_Loss: 4.918210029602051, Test_Loss: 4.799620628356934 *\n",
      "Epoch: 5, Train_Loss: 4.8528594970703125, Test_Loss: 4.986820220947266\n",
      "Epoch: 5, Train_Loss: 5.052521228790283, Test_Loss: 5.092576026916504\n",
      "Epoch: 5, Train_Loss: 5.130043983459473, Test_Loss: 4.805248260498047 *\n",
      "Epoch: 5, Train_Loss: 5.249281406402588, Test_Loss: 4.832003116607666\n",
      "Epoch: 5, Train_Loss: 5.0071001052856445, Test_Loss: 4.780198574066162 *\n",
      "Epoch: 5, Train_Loss: 4.8687543869018555, Test_Loss: 4.844488620758057\n",
      "Epoch: 5, Train_Loss: 4.906231880187988, Test_Loss: 4.781372547149658 *\n",
      "Epoch: 5, Train_Loss: 4.842004776000977, Test_Loss: 5.551287651062012\n",
      "Epoch: 5, Train_Loss: 4.7949371337890625, Test_Loss: 5.026989936828613 *\n",
      "Epoch: 5, Train_Loss: 4.7670793533325195, Test_Loss: 5.495424270629883\n",
      "Epoch: 5, Train_Loss: 4.77011251449585, Test_Loss: 5.530415058135986\n",
      "Epoch: 5, Train_Loss: 4.774781703948975, Test_Loss: 4.97114896774292 *\n",
      "Epoch: 5, Train_Loss: 4.764326572418213, Test_Loss: 5.3864898681640625\n",
      "Epoch: 5, Train_Loss: 4.804565906524658, Test_Loss: 4.862185001373291 *\n",
      "Epoch: 5, Train_Loss: 4.859555244445801, Test_Loss: 4.770758152008057 *\n",
      "Epoch: 5, Train_Loss: 4.845340251922607, Test_Loss: 4.791675567626953\n",
      "Epoch: 5, Train_Loss: 4.851003170013428, Test_Loss: 4.911861419677734\n",
      "Epoch: 5, Train_Loss: 5.146921157836914, Test_Loss: 5.093277454376221\n",
      "Epoch: 5, Train_Loss: 4.939136028289795, Test_Loss: 5.554236888885498\n",
      "Epoch: 5, Train_Loss: 4.777707576751709, Test_Loss: 5.708504676818848\n",
      "Epoch: 5, Train_Loss: 4.872293472290039, Test_Loss: 6.23976993560791\n",
      "Epoch: 5, Train_Loss: 5.188543796539307, Test_Loss: 5.625061511993408 *\n",
      "Epoch: 5, Train_Loss: 5.3044023513793945, Test_Loss: 5.328167915344238 *\n",
      "Epoch: 5, Train_Loss: 4.770082473754883, Test_Loss: 4.769914627075195 *\n",
      "Epoch: 5, Train_Loss: 4.76743745803833, Test_Loss: 4.768470764160156 *\n",
      "Epoch: 5, Train_Loss: 5.351630687713623, Test_Loss: 5.54488468170166\n",
      "Epoch: 5, Train_Loss: 5.508052825927734, Test_Loss: 6.117751598358154\n",
      "Epoch: 5, Train_Loss: 4.9233198165893555, Test_Loss: 4.8154730796813965 *\n",
      "Epoch: 5, Train_Loss: 4.780630111694336, Test_Loss: 4.858304023742676\n",
      "Epoch: 5, Train_Loss: 4.766603946685791, Test_Loss: 4.765684127807617 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 5.364551544189453, Test_Loss: 4.947342872619629\n",
      "Epoch: 5, Train_Loss: 6.397561550140381, Test_Loss: 5.002138614654541\n",
      "Epoch: 5, Train_Loss: 4.880630970001221, Test_Loss: 5.386497497558594\n",
      "Epoch: 5, Train_Loss: 4.780665874481201, Test_Loss: 5.894652843475342\n",
      "Epoch: 5, Train_Loss: 4.757593154907227, Test_Loss: 5.064679145812988 *\n",
      "Epoch: 5, Train_Loss: 4.748819828033447, Test_Loss: 4.755682945251465 *\n",
      "Epoch: 5, Train_Loss: 5.221194267272949, Test_Loss: 4.760370254516602\n",
      "Epoch: 5, Train_Loss: 4.781379699707031, Test_Loss: 4.782131671905518\n",
      "Epoch: 5, Train_Loss: 4.813889026641846, Test_Loss: 4.777823448181152 *\n",
      "Epoch: 5, Train_Loss: 4.749500751495361, Test_Loss: 5.028720378875732\n",
      "Epoch: 5, Train_Loss: 4.762706756591797, Test_Loss: 5.341704845428467\n",
      "Epoch: 5, Train_Loss: 22.0285587310791, Test_Loss: 4.975683212280273 *\n",
      "Epoch: 5, Train_Loss: 4.908714294433594, Test_Loss: 4.815457820892334 *\n",
      "Epoch: 5, Train_Loss: 6.273233413696289, Test_Loss: 4.766901969909668 *\n",
      "Epoch: 5, Train_Loss: 7.445917129516602, Test_Loss: 4.748950481414795 *\n",
      "Epoch: 5, Train_Loss: 4.758480072021484, Test_Loss: 4.865454196929932\n",
      "Epoch: 5, Train_Loss: 4.798722743988037, Test_Loss: 5.646092414855957\n",
      "Epoch: 5, Train_Loss: 6.84153413772583, Test_Loss: 6.164714336395264\n",
      "Epoch: 5, Train_Loss: 15.041491508483887, Test_Loss: 4.8781232833862305 *\n",
      "Epoch: 5, Train_Loss: 5.0128326416015625, Test_Loss: 4.8321685791015625 *\n",
      "Epoch: 5, Train_Loss: 4.764265537261963, Test_Loss: 4.738082408905029 *\n",
      "Epoch: 5, Train_Loss: 10.838090896606445, Test_Loss: 4.74618673324585\n",
      "Epoch: 5, Train_Loss: 5.124661922454834, Test_Loss: 4.740818023681641 *\n",
      "Epoch: 5, Train_Loss: 4.756738185882568, Test_Loss: 4.760573863983154\n",
      "Epoch: 5, Train_Loss: 4.744764804840088, Test_Loss: 4.806504726409912\n",
      "Epoch: 5, Train_Loss: 4.74105978012085, Test_Loss: 4.797159194946289 *\n",
      "Epoch: 5, Train_Loss: 4.7379536628723145, Test_Loss: 4.749942779541016 *\n",
      "Epoch: 5, Train_Loss: 4.741822719573975, Test_Loss: 4.820403099060059\n",
      "Epoch: 5, Train_Loss: 4.739449501037598, Test_Loss: 5.112549781799316\n",
      "Epoch: 5, Train_Loss: 4.741439342498779, Test_Loss: 4.7833967208862305 *\n",
      "Epoch: 5, Train_Loss: 4.745120048522949, Test_Loss: 4.856849193572998\n",
      "Epoch: 5, Train_Loss: 4.749181270599365, Test_Loss: 4.740411758422852 *\n",
      "Epoch: 5, Train_Loss: 4.759479999542236, Test_Loss: 4.738060474395752 *\n",
      "Epoch: 5, Train_Loss: 4.737249374389648, Test_Loss: 4.722362518310547 *\n",
      "Epoch: 5, Train_Loss: 4.766902446746826, Test_Loss: 4.728017330169678\n",
      "Epoch: 5, Train_Loss: 4.779559135437012, Test_Loss: 4.737941265106201\n",
      "Epoch: 5, Train_Loss: 4.752933025360107, Test_Loss: 10.216819763183594\n",
      "Epoch: 5, Train_Loss: 4.73184871673584, Test_Loss: 5.223992824554443 *\n",
      "Epoch: 5, Train_Loss: 4.736520767211914, Test_Loss: 4.743907928466797 *\n",
      "Epoch: 5, Train_Loss: 4.732343673706055, Test_Loss: 4.727016448974609 *\n",
      "Epoch: 5, Train_Loss: 4.719054222106934, Test_Loss: 4.729813575744629\n",
      "Epoch: 5, Train_Loss: 4.723344326019287, Test_Loss: 4.724188804626465 *\n",
      "Epoch: 5, Train_Loss: 4.71948766708374, Test_Loss: 4.72959566116333\n",
      "Epoch: 5, Train_Loss: 4.73520040512085, Test_Loss: 4.736746311187744\n",
      "Epoch: 5, Train_Loss: 4.72285795211792, Test_Loss: 4.727505207061768 *\n",
      "Epoch: 5, Train_Loss: 4.726503849029541, Test_Loss: 4.7294464111328125\n",
      "Epoch: 5, Train_Loss: 4.724832057952881, Test_Loss: 4.730422019958496\n",
      "Epoch: 5, Train_Loss: 4.720849990844727, Test_Loss: 4.736664772033691\n",
      "Epoch: 5, Train_Loss: 4.71491813659668, Test_Loss: 4.71321964263916 *\n",
      "Epoch: 5, Train_Loss: 4.7273712158203125, Test_Loss: 4.727027416229248\n",
      "Epoch: 5, Train_Loss: 4.718761444091797, Test_Loss: 4.731738090515137\n",
      "Epoch: 5, Train_Loss: 4.734301567077637, Test_Loss: 4.722264766693115 *\n",
      "Epoch: 5, Train_Loss: 4.823677062988281, Test_Loss: 4.708473205566406 *\n",
      "Epoch: 5, Train_Loss: 13.798145294189453, Test_Loss: 4.719695568084717\n",
      "Epoch: 6, Train_Loss: 4.825859069824219, Test_Loss: 4.709475994110107 *\n",
      "Epoch: 6, Train_Loss: 4.713461875915527, Test_Loss: 4.7185378074646\n",
      "Epoch: 6, Train_Loss: 4.71907901763916, Test_Loss: 4.733102321624756\n",
      "Epoch: 6, Train_Loss: 4.726572036743164, Test_Loss: 4.722079277038574 *\n",
      "Epoch: 6, Train_Loss: 4.737565517425537, Test_Loss: 4.7202582359313965 *\n",
      "Epoch: 6, Train_Loss: 4.734340667724609, Test_Loss: 4.7248735427856445\n",
      "Epoch: 6, Train_Loss: 4.715825080871582, Test_Loss: 4.720102787017822 *\n",
      "Epoch: 6, Train_Loss: 4.873866558074951, Test_Loss: 4.710345268249512 *\n",
      "Epoch: 6, Train_Loss: 4.887348651885986, Test_Loss: 4.706017017364502 *\n",
      "Epoch: 6, Train_Loss: 4.86481237411499, Test_Loss: 4.7129669189453125\n",
      "Epoch: 6, Train_Loss: 4.710205078125, Test_Loss: 4.7136712074279785\n",
      "Epoch: 6, Train_Loss: 4.831836700439453, Test_Loss: 4.726315975189209\n",
      "Epoch: 6, Train_Loss: 4.834141731262207, Test_Loss: 4.767965793609619\n",
      "Epoch: 6, Train_Loss: 4.83990478515625, Test_Loss: 5.465477466583252\n",
      "Epoch: 6, Train_Loss: 4.822760581970215, Test_Loss: 9.80409049987793\n",
      "Epoch: 6, Train_Loss: 4.801153659820557, Test_Loss: 4.724793910980225 *\n",
      "Epoch: 6, Train_Loss: 4.727261066436768, Test_Loss: 4.685030460357666 *\n",
      "Epoch: 6, Train_Loss: 4.719376087188721, Test_Loss: 4.707947254180908\n",
      "Epoch: 6, Train_Loss: 4.790978908538818, Test_Loss: 4.722748756408691\n",
      "Epoch: 6, Train_Loss: 4.711052417755127, Test_Loss: 4.7246575355529785\n",
      "Epoch: 6, Train_Loss: 4.6912994384765625, Test_Loss: 4.698455333709717 *\n",
      "Epoch: 6, Train_Loss: 4.698487281799316, Test_Loss: 4.886470317840576\n",
      "Epoch: 6, Train_Loss: 4.697418689727783, Test_Loss: 4.736035346984863 *\n",
      "Epoch: 6, Train_Loss: 5.245187282562256, Test_Loss: 4.688529014587402 *\n",
      "Epoch: 6, Train_Loss: 9.738984107971191, Test_Loss: 4.74972677230835\n",
      "Epoch: 6, Train_Loss: 4.693514823913574, Test_Loss: 4.681793689727783 *\n",
      "Epoch: 6, Train_Loss: 4.69482421875, Test_Loss: 4.708954811096191\n",
      "Epoch: 6, Train_Loss: 4.702442646026611, Test_Loss: 4.7145233154296875\n",
      "Epoch: 6, Train_Loss: 4.714447975158691, Test_Loss: 4.745185852050781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 4.699789524078369, Test_Loss: 4.77821159362793\n",
      "Epoch: 6, Train_Loss: 4.6881279945373535, Test_Loss: 4.852578639984131\n",
      "Epoch: 6, Train_Loss: 4.685596466064453, Test_Loss: 4.732324600219727 *\n",
      "Epoch: 6, Train_Loss: 4.702597618103027, Test_Loss: 4.68853235244751 *\n",
      "Epoch: 6, Train_Loss: 4.692424297332764, Test_Loss: 4.671319961547852 *\n",
      "Epoch: 6, Train_Loss: 4.684610366821289, Test_Loss: 4.678948402404785\n",
      "Epoch: 6, Train_Loss: 4.692440986633301, Test_Loss: 4.67156457901001 *\n",
      "Epoch: 6, Train_Loss: 4.671879291534424, Test_Loss: 4.679581165313721\n",
      "Epoch: 6, Train_Loss: 4.696005344390869, Test_Loss: 4.672067165374756 *\n",
      "Epoch: 6, Train_Loss: 4.680108547210693, Test_Loss: 4.673789978027344\n",
      "Epoch: 6, Train_Loss: 4.669698715209961, Test_Loss: 4.672695159912109 *\n",
      "Epoch: 6, Train_Loss: 4.718262195587158, Test_Loss: 4.67234468460083 *\n",
      "Epoch: 6, Train_Loss: 4.735073089599609, Test_Loss: 4.6808857917785645\n",
      "Epoch: 6, Train_Loss: 4.679112911224365, Test_Loss: 4.671478748321533 *\n",
      "Epoch: 6, Train_Loss: 4.677400588989258, Test_Loss: 4.673907279968262\n",
      "Epoch: 6, Train_Loss: 4.674943447113037, Test_Loss: 4.711131572723389\n",
      "Epoch: 6, Train_Loss: 4.76633358001709, Test_Loss: 4.8121466636657715\n",
      "Epoch: 6, Train_Loss: 4.763225555419922, Test_Loss: 4.893743515014648\n",
      "Epoch: 6, Train_Loss: 4.742950439453125, Test_Loss: 4.688586711883545 *\n",
      "Epoch: 6, Train_Loss: 4.70721435546875, Test_Loss: 4.723328113555908\n",
      "Epoch: 6, Train_Loss: 4.70805549621582, Test_Loss: 4.757139205932617\n",
      "Epoch: 6, Train_Loss: 4.740776062011719, Test_Loss: 4.958336353302002\n",
      "Epoch: 6, Train_Loss: 4.739816188812256, Test_Loss: 4.666635036468506 *\n",
      "Epoch: 6, Train_Loss: 4.670675277709961, Test_Loss: 4.9245123863220215\n",
      "Epoch: 6, Train_Loss: 4.76608419418335, Test_Loss: 5.084624290466309\n",
      "Epoch: 6, Train_Loss: 4.67294979095459, Test_Loss: 4.74967098236084 *\n",
      "Epoch: 6, Train_Loss: 4.660074710845947, Test_Loss: 4.732292175292969 *\n",
      "Epoch: 6, Train_Loss: 4.657544136047363, Test_Loss: 4.66099214553833 *\n",
      "Epoch: 6, Train_Loss: 4.658379554748535, Test_Loss: 4.664087772369385\n",
      "Epoch: 6, Train_Loss: 4.653019905090332, Test_Loss: 4.664591312408447\n",
      "Epoch: 6, Train_Loss: 4.653467178344727, Test_Loss: 5.333551406860352\n",
      "Epoch: 6, Train_Loss: 5.194161891937256, Test_Loss: 5.064433574676514 *\n",
      "Epoch: 6, Train_Loss: 9.005403518676758, Test_Loss: 5.193605422973633\n",
      "Epoch: 6, Train_Loss: 4.659274101257324, Test_Loss: 5.436676979064941\n",
      "Epoch: 6, Train_Loss: 4.664120674133301, Test_Loss: 4.773552417755127 *\n",
      "Epoch: 6, Train_Loss: 4.66801118850708, Test_Loss: 5.314935684204102\n",
      "Epoch: 6, Train_Loss: 4.646466255187988, Test_Loss: 4.840231895446777 *\n",
      "Epoch: 6, Train_Loss: 4.640481948852539, Test_Loss: 4.645787239074707 *\n",
      "Epoch: 6, Train_Loss: 4.650321960449219, Test_Loss: 4.655983924865723\n",
      "Epoch: 6, Train_Loss: 4.643985271453857, Test_Loss: 4.73512601852417\n",
      "Epoch: 6, Train_Loss: 4.6388936042785645, Test_Loss: 4.829118251800537\n",
      "Epoch: 6, Train_Loss: 4.639218330383301, Test_Loss: 5.582787990570068\n",
      "Epoch: 6, Train_Loss: 4.742503643035889, Test_Loss: 5.114528656005859 *\n",
      "Epoch: 6, Train_Loss: 4.7162957191467285, Test_Loss: 6.621763229370117\n",
      "Epoch: 6, Train_Loss: 4.741275310516357, Test_Loss: 5.242705821990967 *\n",
      "Epoch: 6, Train_Loss: 4.707395553588867, Test_Loss: 5.48112678527832\n",
      "Epoch: 6, Train_Loss: 4.638372421264648, Test_Loss: 4.64531135559082 *\n",
      "Epoch: 6, Train_Loss: 4.777187347412109, Test_Loss: 4.635659217834473 *\n",
      "Epoch: 6, Train_Loss: 4.825748920440674, Test_Loss: 5.180245876312256\n",
      "Epoch: 6, Train_Loss: 4.830095291137695, Test_Loss: 6.063626289367676\n",
      "Epoch: 6, Train_Loss: 4.777209281921387, Test_Loss: 4.8163275718688965 *\n",
      "Epoch: 6, Train_Loss: 4.633489608764648, Test_Loss: 4.758618354797363 *\n",
      "Epoch: 6, Train_Loss: 4.630340099334717, Test_Loss: 4.633282661437988 *\n",
      "Epoch: 6, Train_Loss: 4.627683162689209, Test_Loss: 4.767290115356445\n",
      "Epoch: 6, Train_Loss: 4.6352858543396, Test_Loss: 4.959957122802734\n",
      "Epoch: 6, Train_Loss: 4.63517427444458, Test_Loss: 4.995553970336914\n",
      "Epoch: 6, Train_Loss: 4.630581378936768, Test_Loss: 5.992569446563721\n",
      "Epoch: 6, Train_Loss: 4.628065586090088, Test_Loss: 5.0477190017700195 *\n",
      "Epoch: 6, Train_Loss: 4.624430179595947, Test_Loss: 4.624963760375977 *\n",
      "Epoch: 6, Train_Loss: 4.629805088043213, Test_Loss: 4.639430046081543\n",
      "Epoch: 6, Train_Loss: 4.633594036102295, Test_Loss: 4.66029167175293\n",
      "Epoch: 6, Train_Loss: 4.800942897796631, Test_Loss: 4.669246673583984\n",
      "Epoch: 6, Train_Loss: 4.817021369934082, Test_Loss: 4.845854759216309\n",
      "Epoch: 6, Train_Loss: 4.801415920257568, Test_Loss: 5.132549285888672\n",
      "Epoch: 6, Train_Loss: 4.683287143707275, Test_Loss: 4.91049861907959 *\n",
      "Epoch: 6, Train_Loss: 4.778858184814453, Test_Loss: 4.6962738037109375 *\n",
      "Epoch: 6, Train_Loss: 4.713530540466309, Test_Loss: 4.652324676513672 *\n",
      "Epoch: 6, Train_Loss: 4.726747035980225, Test_Loss: 4.631595611572266 *\n",
      "Epoch: 6, Train_Loss: 4.736321449279785, Test_Loss: 4.739921569824219\n",
      "Epoch: 6, Train_Loss: 4.926991939544678, Test_Loss: 5.301103115081787\n",
      "Epoch: 6, Train_Loss: 4.6250739097595215, Test_Loss: 6.13800048828125\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 4.636972904205322, Test_Loss: 4.992617607116699 *\n",
      "Epoch: 6, Train_Loss: 7.411586761474609, Test_Loss: 4.739965915679932 *\n",
      "Epoch: 6, Train_Loss: 5.137746810913086, Test_Loss: 4.618617057800293 *\n",
      "Epoch: 6, Train_Loss: 4.65632963180542, Test_Loss: 4.611464977264404 *\n",
      "Epoch: 6, Train_Loss: 4.6760663986206055, Test_Loss: 4.616941928863525\n",
      "Epoch: 6, Train_Loss: 4.657766342163086, Test_Loss: 4.621332168579102\n",
      "Epoch: 6, Train_Loss: 4.629600524902344, Test_Loss: 4.645285129547119\n",
      "Epoch: 6, Train_Loss: 4.613471031188965, Test_Loss: 4.6592488288879395\n",
      "Epoch: 6, Train_Loss: 4.660039901733398, Test_Loss: 4.613208293914795 *\n",
      "Epoch: 6, Train_Loss: 4.742457389831543, Test_Loss: 4.7140116691589355\n",
      "Epoch: 6, Train_Loss: 4.704473495483398, Test_Loss: 4.951939582824707\n",
      "Epoch: 6, Train_Loss: 4.67751407623291, Test_Loss: 4.75399923324585 *\n",
      "Epoch: 6, Train_Loss: 4.695659160614014, Test_Loss: 4.795848846435547\n",
      "Epoch: 6, Train_Loss: 4.643033027648926, Test_Loss: 4.603503227233887 *\n",
      "Epoch: 6, Train_Loss: 4.633576393127441, Test_Loss: 4.614909648895264\n",
      "Epoch: 6, Train_Loss: 4.607704162597656, Test_Loss: 4.622497081756592\n",
      "Epoch: 6, Train_Loss: 4.6442413330078125, Test_Loss: 4.613508701324463 *\n",
      "Epoch: 6, Train_Loss: 4.625824451446533, Test_Loss: 4.615962505340576\n",
      "Epoch: 6, Train_Loss: 4.605124473571777, Test_Loss: 8.15195369720459\n",
      "Epoch: 6, Train_Loss: 4.611893653869629, Test_Loss: 6.438112735748291 *\n",
      "Epoch: 6, Train_Loss: 4.647143840789795, Test_Loss: 4.606008052825928 *\n",
      "Epoch: 6, Train_Loss: 4.640035629272461, Test_Loss: 4.597410202026367 *\n",
      "Epoch: 6, Train_Loss: 4.597043037414551, Test_Loss: 4.596949100494385 *\n",
      "Epoch: 6, Train_Loss: 4.591940402984619, Test_Loss: 4.613446235656738\n",
      "Epoch: 6, Train_Loss: 4.596628665924072, Test_Loss: 4.598216533660889 *\n",
      "Epoch: 6, Train_Loss: 4.59210205078125, Test_Loss: 4.594858169555664 *\n",
      "Epoch: 6, Train_Loss: 4.5901875495910645, Test_Loss: 4.5918288230896 *\n",
      "Epoch: 6, Train_Loss: 4.587533473968506, Test_Loss: 4.59333610534668\n",
      "Epoch: 6, Train_Loss: 4.590240478515625, Test_Loss: 4.59412145614624\n",
      "Epoch: 6, Train_Loss: 4.603971004486084, Test_Loss: 4.593358039855957 *\n",
      "Epoch: 6, Train_Loss: 4.591489315032959, Test_Loss: 4.59583044052124\n",
      "Epoch: 6, Train_Loss: 4.585504055023193, Test_Loss: 4.628464221954346\n",
      "Epoch: 6, Train_Loss: 4.588450908660889, Test_Loss: 4.611029624938965 *\n",
      "Epoch: 6, Train_Loss: 4.596508026123047, Test_Loss: 4.582785129547119 *\n",
      "Epoch: 6, Train_Loss: 4.5814008712768555, Test_Loss: 4.578756809234619 *\n",
      "Epoch: 6, Train_Loss: 4.5884833335876465, Test_Loss: 4.591079235076904\n",
      "Epoch: 6, Train_Loss: 4.59627628326416, Test_Loss: 4.585045337677002 *\n",
      "Epoch: 6, Train_Loss: 4.582518100738525, Test_Loss: 4.584463596343994 *\n",
      "Epoch: 6, Train_Loss: 4.585666179656982, Test_Loss: 4.57961368560791 *\n",
      "Epoch: 6, Train_Loss: 4.578115463256836, Test_Loss: 4.579480171203613 *\n",
      "Epoch: 6, Train_Loss: 4.585995674133301, Test_Loss: 4.5780863761901855 *\n",
      "Epoch: 6, Train_Loss: 4.603314399719238, Test_Loss: 4.577872276306152 *\n",
      "Epoch: 6, Train_Loss: 4.571741104125977, Test_Loss: 4.579357624053955\n",
      "Epoch: 6, Train_Loss: 4.571619510650635, Test_Loss: 4.583224296569824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 4.583279132843018, Test_Loss: 4.574187755584717 *\n",
      "Epoch: 6, Train_Loss: 4.61607551574707, Test_Loss: 4.578459739685059\n",
      "Epoch: 6, Train_Loss: 4.632176876068115, Test_Loss: 4.575313091278076 *\n",
      "Epoch: 6, Train_Loss: 4.615194320678711, Test_Loss: 4.578417778015137\n",
      "Epoch: 6, Train_Loss: 4.585323333740234, Test_Loss: 4.613834857940674\n",
      "Epoch: 6, Train_Loss: 4.576152324676514, Test_Loss: 4.605689525604248 *\n",
      "Epoch: 6, Train_Loss: 4.617262840270996, Test_Loss: 9.78558349609375\n",
      "Epoch: 6, Train_Loss: 4.572534084320068, Test_Loss: 4.71718692779541 *\n",
      "Epoch: 6, Train_Loss: 4.582984924316406, Test_Loss: 4.566426753997803 *\n",
      "Epoch: 6, Train_Loss: 4.6052680015563965, Test_Loss: 4.585190773010254\n",
      "Epoch: 6, Train_Loss: 4.576604843139648, Test_Loss: 4.618974685668945\n",
      "Epoch: 6, Train_Loss: 4.73484468460083, Test_Loss: 4.6230058670043945\n",
      "Epoch: 6, Train_Loss: 4.658150672912598, Test_Loss: 4.569977760314941 *\n",
      "Epoch: 6, Train_Loss: 4.615096092224121, Test_Loss: 4.677428722381592\n",
      "Epoch: 6, Train_Loss: 4.575101375579834, Test_Loss: 4.6181559562683105 *\n",
      "Epoch: 6, Train_Loss: 4.578189373016357, Test_Loss: 4.552608966827393 *\n",
      "Epoch: 6, Train_Loss: 4.57180643081665, Test_Loss: 4.614705562591553\n",
      "Epoch: 6, Train_Loss: 4.563377857208252, Test_Loss: 4.569590091705322 *\n",
      "Epoch: 6, Train_Loss: 4.563316822052002, Test_Loss: 4.566537380218506 *\n",
      "Epoch: 6, Train_Loss: 4.566146373748779, Test_Loss: 4.5770087242126465\n",
      "Epoch: 6, Train_Loss: 4.587520122528076, Test_Loss: 4.6688008308410645\n",
      "Epoch: 6, Train_Loss: 4.642124652862549, Test_Loss: 4.587545394897461 *\n",
      "Epoch: 6, Train_Loss: 4.573980331420898, Test_Loss: 4.681972026824951\n",
      "Epoch: 6, Train_Loss: 4.618320465087891, Test_Loss: 4.6238179206848145 *\n",
      "Epoch: 6, Train_Loss: 4.5766730308532715, Test_Loss: 4.574418067932129 *\n",
      "Epoch: 6, Train_Loss: 4.566616058349609, Test_Loss: 4.5605549812316895 *\n",
      "Epoch: 6, Train_Loss: 4.657681465148926, Test_Loss: 4.561413288116455\n",
      "Epoch: 6, Train_Loss: 4.77565336227417, Test_Loss: 4.560022354125977 *\n",
      "Epoch: 6, Train_Loss: 4.555118083953857, Test_Loss: 4.560256004333496\n",
      "Epoch: 6, Train_Loss: 4.570955276489258, Test_Loss: 4.566633224487305\n",
      "Epoch: 6, Train_Loss: 4.549808979034424, Test_Loss: 4.559274673461914 *\n",
      "Epoch: 6, Train_Loss: 4.5474653244018555, Test_Loss: 4.551387310028076 *\n",
      "Epoch: 6, Train_Loss: 4.54982328414917, Test_Loss: 4.565440654754639\n",
      "Epoch: 6, Train_Loss: 4.541562080383301, Test_Loss: 4.562246799468994 *\n",
      "Epoch: 6, Train_Loss: 4.557413578033447, Test_Loss: 4.541476249694824 *\n",
      "Epoch: 6, Train_Loss: 4.570547103881836, Test_Loss: 4.550405979156494\n",
      "Epoch: 6, Train_Loss: 4.5580220222473145, Test_Loss: 4.590865612030029\n",
      "Epoch: 6, Train_Loss: 4.571358680725098, Test_Loss: 4.57177209854126 *\n",
      "Epoch: 6, Train_Loss: 4.571408271789551, Test_Loss: 4.864472389221191\n",
      "Epoch: 6, Train_Loss: 4.547900676727295, Test_Loss: 4.548852920532227 *\n",
      "Epoch: 6, Train_Loss: 4.536891460418701, Test_Loss: 4.609314918518066\n",
      "Epoch: 6, Train_Loss: 4.537591934204102, Test_Loss: 4.615863800048828\n",
      "Epoch: 6, Train_Loss: 4.5628156661987305, Test_Loss: 4.883918762207031\n",
      "Epoch: 6, Train_Loss: 4.560690402984619, Test_Loss: 4.574774742126465 *\n",
      "Epoch: 6, Train_Loss: 4.546799182891846, Test_Loss: 4.674431324005127\n",
      "Epoch: 6, Train_Loss: 4.539732933044434, Test_Loss: 4.82867956161499\n",
      "Epoch: 6, Train_Loss: 4.601019382476807, Test_Loss: 4.71225118637085 *\n",
      "Epoch: 6, Train_Loss: 4.591034889221191, Test_Loss: 4.572299003601074 *\n",
      "Epoch: 6, Train_Loss: 4.559947490692139, Test_Loss: 4.550635814666748 *\n",
      "Epoch: 6, Train_Loss: 4.537485599517822, Test_Loss: 4.538908958435059 *\n",
      "Epoch: 6, Train_Loss: 4.5448079109191895, Test_Loss: 4.554808139801025\n",
      "Epoch: 6, Train_Loss: 4.5295867919921875, Test_Loss: 4.849830627441406\n",
      "Epoch: 6, Train_Loss: 4.5399346351623535, Test_Loss: 5.186963081359863\n",
      "Epoch: 6, Train_Loss: 4.535496711730957, Test_Loss: 4.903685569763184 *\n",
      "Epoch: 6, Train_Loss: 4.553001403808594, Test_Loss: 5.448331356048584\n",
      "Epoch: 6, Train_Loss: 6.478513240814209, Test_Loss: 4.772091865539551 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 8.093392372131348, Test_Loss: 5.077867031097412\n",
      "Epoch: 6, Train_Loss: 4.537890911102295, Test_Loss: 4.802452564239502 *\n",
      "Epoch: 6, Train_Loss: 4.537768363952637, Test_Loss: 4.534505367279053 *\n",
      "Epoch: 6, Train_Loss: 4.554894924163818, Test_Loss: 4.522543907165527 *\n",
      "Epoch: 6, Train_Loss: 4.7374420166015625, Test_Loss: 4.572391986846924\n",
      "Epoch: 6, Train_Loss: 4.5619330406188965, Test_Loss: 4.668275356292725\n",
      "Epoch: 6, Train_Loss: 4.530444145202637, Test_Loss: 5.472323894500732\n",
      "Epoch: 6, Train_Loss: 4.512686252593994, Test_Loss: 4.712587833404541 *\n",
      "Epoch: 6, Train_Loss: 4.602826118469238, Test_Loss: 6.600107192993164\n",
      "Epoch: 6, Train_Loss: 4.5283708572387695, Test_Loss: 5.053718566894531 *\n",
      "Epoch: 6, Train_Loss: 4.526432991027832, Test_Loss: 5.569565773010254\n",
      "Epoch: 6, Train_Loss: 5.151856422424316, Test_Loss: 4.588339805603027 *\n",
      "Epoch: 6, Train_Loss: 5.957149982452393, Test_Loss: 4.519989490509033 *\n",
      "Epoch: 6, Train_Loss: 5.375420570373535, Test_Loss: 4.763724327087402\n",
      "Epoch: 6, Train_Loss: 4.648979187011719, Test_Loss: 5.997655868530273\n",
      "Epoch: 6, Train_Loss: 4.953130722045898, Test_Loss: 5.035400867462158 *\n",
      "Epoch: 6, Train_Loss: 6.874093532562256, Test_Loss: 4.615639686584473 *\n",
      "Epoch: 6, Train_Loss: 5.201018810272217, Test_Loss: 4.524463653564453 *\n",
      "Epoch: 6, Train_Loss: 4.545132637023926, Test_Loss: 4.601970195770264\n",
      "Epoch: 6, Train_Loss: 4.537286758422852, Test_Loss: 4.895918369293213\n",
      "Epoch: 6, Train_Loss: 5.730708122253418, Test_Loss: 4.750586032867432 *\n",
      "Epoch: 6, Train_Loss: 6.149686813354492, Test_Loss: 5.732892036437988\n",
      "Epoch: 6, Train_Loss: 4.966267108917236, Test_Loss: 5.117885589599609 *\n",
      "Epoch: 6, Train_Loss: 4.521921634674072, Test_Loss: 4.5253682136535645 *\n",
      "Epoch: 6, Train_Loss: 4.504849433898926, Test_Loss: 4.507125377655029 *\n",
      "Epoch: 6, Train_Loss: 5.005224227905273, Test_Loss: 4.518370151519775\n",
      "Epoch: 6, Train_Loss: 4.7327961921691895, Test_Loss: 4.514717102050781 *\n",
      "Epoch: 6, Train_Loss: 4.5197434425354, Test_Loss: 4.590068817138672\n",
      "Epoch: 6, Train_Loss: 4.534339904785156, Test_Loss: 5.091268539428711\n",
      "Epoch: 6, Train_Loss: 4.642364501953125, Test_Loss: 4.980307102203369 *\n",
      "Epoch: 6, Train_Loss: 4.686482906341553, Test_Loss: 4.638742923736572 *\n",
      "Epoch: 6, Train_Loss: 4.595736503601074, Test_Loss: 4.539102554321289 *\n",
      "Epoch: 6, Train_Loss: 4.9526591300964355, Test_Loss: 4.513235092163086 *\n",
      "Epoch: 6, Train_Loss: 4.600632667541504, Test_Loss: 4.5365190505981445\n",
      "Epoch: 6, Train_Loss: 4.561410427093506, Test_Loss: 4.877987861633301\n",
      "Epoch: 6, Train_Loss: 4.752608299255371, Test_Loss: 5.890758991241455\n",
      "Epoch: 6, Train_Loss: 4.880119323730469, Test_Loss: 5.126766204833984 *\n",
      "Epoch: 6, Train_Loss: 4.938704013824463, Test_Loss: 4.580655097961426 *\n",
      "Epoch: 6, Train_Loss: 4.635941028594971, Test_Loss: 4.512999534606934 *\n",
      "Epoch: 6, Train_Loss: 4.603347301483154, Test_Loss: 4.48825216293335 *\n",
      "Epoch: 6, Train_Loss: 4.626584053039551, Test_Loss: 4.488084316253662 *\n",
      "Epoch: 6, Train_Loss: 4.540537357330322, Test_Loss: 4.489934921264648\n",
      "Epoch: 6, Train_Loss: 4.511745929718018, Test_Loss: 4.5125203132629395\n",
      "Epoch: 6, Train_Loss: 4.488324165344238, Test_Loss: 4.541518211364746\n",
      "Epoch: 6, Train_Loss: 4.4853739738464355, Test_Loss: 4.482550621032715 *\n",
      "Epoch: 6, Train_Loss: 4.485210418701172, Test_Loss: 4.554061412811279\n",
      "Epoch: 6, Train_Loss: 4.484666347503662, Test_Loss: 4.610318660736084\n",
      "Epoch: 6, Train_Loss: 4.524322032928467, Test_Loss: 4.841689586639404\n",
      "Epoch: 6, Train_Loss: 4.5563554763793945, Test_Loss: 4.6842756271362305 *\n",
      "Epoch: 6, Train_Loss: 4.5736002922058105, Test_Loss: 4.481451988220215 *\n",
      "Epoch: 6, Train_Loss: 4.5701212882995605, Test_Loss: 4.47938346862793 *\n",
      "Epoch: 6, Train_Loss: 5.01543664932251, Test_Loss: 4.488385200500488\n",
      "Epoch: 6, Train_Loss: 4.51197624206543, Test_Loss: 4.478632926940918 *\n",
      "Epoch: 6, Train_Loss: 4.4944071769714355, Test_Loss: 4.484667778015137\n",
      "Epoch: 6, Train_Loss: 4.616753101348877, Test_Loss: 6.424274444580078\n",
      "Epoch: 6, Train_Loss: 4.9516777992248535, Test_Loss: 8.184842109680176\n",
      "Epoch: 6, Train_Loss: 4.968362331390381, Test_Loss: 4.482954502105713 *\n",
      "Epoch: 6, Train_Loss: 4.472790718078613, Test_Loss: 4.4720845222473145 *\n",
      "Epoch: 6, Train_Loss: 4.482102394104004, Test_Loss: 4.47256326675415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 5.0724382400512695, Test_Loss: 4.474135398864746\n",
      "Epoch: 6, Train_Loss: 5.14850378036499, Test_Loss: 4.471505641937256 *\n",
      "Epoch: 6, Train_Loss: 4.5963029861450195, Test_Loss: 4.4633636474609375 *\n",
      "Epoch: 6, Train_Loss: 4.505516052246094, Test_Loss: 4.465492248535156\n",
      "Epoch: 6, Train_Loss: 4.47498083114624, Test_Loss: 4.469446659088135\n",
      "Epoch: 6, Train_Loss: 5.35772705078125, Test_Loss: 4.4657368659973145 *\n",
      "Epoch: 6, Train_Loss: 5.93358850479126, Test_Loss: 4.462953567504883 *\n",
      "Epoch: 6, Train_Loss: 4.525784969329834, Test_Loss: 4.468410491943359\n",
      "Epoch: 6, Train_Loss: 4.493467330932617, Test_Loss: 4.4886274337768555\n",
      "Epoch: 6, Train_Loss: 4.467623233795166, Test_Loss: 4.487227439880371 *\n",
      "Epoch: 6, Train_Loss: 4.461301803588867, Test_Loss: 4.462968349456787 *\n",
      "Epoch: 6, Train_Loss: 4.910207748413086, Test_Loss: 4.463961601257324\n",
      "Epoch: 6, Train_Loss: 4.497471332550049, Test_Loss: 4.46164083480835 *\n",
      "Epoch: 6, Train_Loss: 4.508474826812744, Test_Loss: 4.455763339996338 *\n",
      "Epoch: 6, Train_Loss: 4.459428310394287, Test_Loss: 4.465020179748535\n",
      "Epoch: 6, Train_Loss: 4.479528427124023, Test_Loss: 4.458300590515137 *\n",
      "Epoch: 6, Train_Loss: 22.02023696899414, Test_Loss: 4.4643940925598145\n",
      "Epoch: 6, Train_Loss: 4.457984924316406, Test_Loss: 4.4531378746032715 *\n",
      "Epoch: 6, Train_Loss: 6.458404541015625, Test_Loss: 4.450789928436279 *\n",
      "Epoch: 6, Train_Loss: 6.674270153045654, Test_Loss: 4.458812236785889\n",
      "Epoch: 6, Train_Loss: 4.461675643920898, Test_Loss: 4.457286834716797 *\n",
      "Epoch: 6, Train_Loss: 4.522644519805908, Test_Loss: 4.4547858238220215 *\n",
      "Epoch: 6, Train_Loss: 8.232961654663086, Test_Loss: 4.453004837036133 *\n",
      "Epoch: 6, Train_Loss: 13.33018970489502, Test_Loss: 4.451718330383301 *\n",
      "Epoch: 6, Train_Loss: 4.533515930175781, Test_Loss: 4.4487223625183105 *\n",
      "Epoch: 6, Train_Loss: 4.466315269470215, Test_Loss: 4.49928617477417\n",
      "Epoch: 6, Train_Loss: 10.812122344970703, Test_Loss: 4.481025695800781 *\n",
      "Epoch: 6, Train_Loss: 4.489702224731445, Test_Loss: 8.812051773071289\n",
      "Epoch: 6, Train_Loss: 4.483701705932617, Test_Loss: 5.830589294433594 *\n",
      "Epoch: 6, Train_Loss: 4.448751449584961, Test_Loss: 4.445174217224121 *\n",
      "Epoch: 6, Train_Loss: 4.445451736450195, Test_Loss: 4.462770938873291\n",
      "Epoch: 6, Train_Loss: 4.446603298187256, Test_Loss: 4.488659858703613\n",
      "Epoch: 6, Train_Loss: 4.451837539672852, Test_Loss: 4.492273807525635\n",
      "Epoch: 6, Train_Loss: 4.443872928619385, Test_Loss: 4.450809001922607 *\n",
      "Epoch: 6, Train_Loss: 4.444559097290039, Test_Loss: 4.546345233917236\n",
      "Epoch: 6, Train_Loss: 4.441151142120361, Test_Loss: 4.559403419494629\n",
      "Epoch: 6, Train_Loss: 4.45182466506958, Test_Loss: 4.435542106628418 *\n",
      "Epoch: 6, Train_Loss: 4.445339202880859, Test_Loss: 4.498432159423828\n",
      "Epoch: 6, Train_Loss: 4.448178768157959, Test_Loss: 4.448975086212158 *\n",
      "Epoch: 6, Train_Loss: 4.480712890625, Test_Loss: 4.456904888153076\n",
      "Epoch: 6, Train_Loss: 4.503385543823242, Test_Loss: 4.4503607749938965 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 4.454646110534668, Test_Loss: 4.532355785369873\n",
      "Epoch: 6, Train_Loss: 4.433660507202148, Test_Loss: 4.470982551574707 *\n",
      "Epoch: 6, Train_Loss: 4.434852600097656, Test_Loss: 4.591213226318359\n",
      "Epoch: 6, Train_Loss: 4.4328155517578125, Test_Loss: 4.561548709869385 *\n",
      "Epoch: 6, Train_Loss: 4.428816795349121, Test_Loss: 4.442636013031006 *\n",
      "Epoch: 6, Train_Loss: 4.428795337677002, Test_Loss: 4.432593822479248 *\n",
      "Epoch: 6, Train_Loss: 4.42547607421875, Test_Loss: 4.432284832000732 *\n",
      "Epoch: 6, Train_Loss: 4.4263200759887695, Test_Loss: 4.4375\n",
      "Epoch: 6, Train_Loss: 4.426029205322266, Test_Loss: 4.426618576049805 *\n",
      "Epoch: 6, Train_Loss: 4.417973518371582, Test_Loss: 4.430296897888184\n",
      "Epoch: 6, Train_Loss: 4.419478416442871, Test_Loss: 4.4255218505859375 *\n",
      "Epoch: 6, Train_Loss: 4.422740936279297, Test_Loss: 4.419435024261475 *\n",
      "Epoch: 6, Train_Loss: 4.435728549957275, Test_Loss: 4.431939125061035\n",
      "Epoch: 6, Train_Loss: 4.44019079208374, Test_Loss: 4.4258856773376465 *\n",
      "Epoch: 6, Train_Loss: 4.4267048835754395, Test_Loss: 4.425694942474365 *\n",
      "Epoch: 6, Train_Loss: 4.4315290451049805, Test_Loss: 4.434372425079346\n",
      "Epoch: 6, Train_Loss: 5.3927903175354, Test_Loss: 4.462501049041748\n",
      "Epoch: 6, Train_Loss: 12.66633415222168, Test_Loss: 4.460250377655029 *\n",
      "Epoch: 6, Train_Loss: 4.478970527648926, Test_Loss: 4.79973840713501\n",
      "Epoch: 6, Train_Loss: 4.419591426849365, Test_Loss: 4.447491645812988 *\n",
      "Epoch: 6, Train_Loss: 4.420166969299316, Test_Loss: 4.452637672424316\n",
      "Epoch: 6, Train_Loss: 4.434074878692627, Test_Loss: 4.470848560333252\n",
      "Epoch: 6, Train_Loss: 4.458673000335693, Test_Loss: 4.703812599182129\n",
      "Epoch: 6, Train_Loss: 4.42352819442749, Test_Loss: 4.467336654663086 *\n",
      "Epoch: 6, Train_Loss: 4.434284687042236, Test_Loss: 4.507680416107178\n",
      "Epoch: 6, Train_Loss: 4.607905864715576, Test_Loss: 4.722097396850586\n",
      "Epoch: 6, Train_Loss: 4.593599796295166, Test_Loss: 4.727328300476074\n",
      "Epoch: 6, Train_Loss: 4.5471320152282715, Test_Loss: 4.433811187744141 *\n",
      "Epoch: 6, Train_Loss: 4.437744140625, Test_Loss: 4.480339050292969\n",
      "Epoch: 6, Train_Loss: 4.509072303771973, Test_Loss: 4.405825614929199 *\n",
      "Epoch: 6, Train_Loss: 4.505731105804443, Test_Loss: 4.435812473297119\n",
      "Epoch: 6, Train_Loss: 4.546751022338867, Test_Loss: 4.5272908210754395\n",
      "Epoch: 6, Train_Loss: 4.517587661743164, Test_Loss: 5.342663288116455\n",
      "Epoch: 6, Train_Loss: 4.511800289154053, Test_Loss: 4.560973644256592 *\n",
      "Epoch: 6, Train_Loss: 4.419963836669922, Test_Loss: 5.228714466094971\n",
      "Epoch: 6, Train_Loss: 4.437119007110596, Test_Loss: 4.8513078689575195 *\n",
      "Epoch: 6, Train_Loss: 4.472421646118164, Test_Loss: 4.862183570861816\n",
      "Epoch: 6, Train_Loss: 4.420652866363525, Test_Loss: 4.824042320251465 *\n",
      "Epoch: 6, Train_Loss: 4.398428440093994, Test_Loss: 4.435964107513428 *\n",
      "Epoch: 6, Train_Loss: 4.4025983810424805, Test_Loss: 4.400782585144043 *\n",
      "Epoch: 6, Train_Loss: 4.394620418548584, Test_Loss: 4.417273044586182\n",
      "Epoch: 6, Train_Loss: 5.8881988525390625, Test_Loss: 4.517616271972656\n",
      "Epoch: 6, Train_Loss: 8.693035125732422, Test_Loss: 5.256970405578613\n",
      "Epoch: 6, Train_Loss: 4.402528762817383, Test_Loss: 4.712552547454834 *\n",
      "Epoch: 6, Train_Loss: 4.405551433563232, Test_Loss: 6.188363552093506\n",
      "Epoch: 6, Train_Loss: 4.406674385070801, Test_Loss: 5.039422988891602 *\n",
      "Epoch: 6, Train_Loss: 4.413449764251709, Test_Loss: 5.586335182189941\n",
      "Epoch: 6, Train_Loss: 4.40489387512207, Test_Loss: 4.6378655433654785 *\n",
      "Epoch: 6, Train_Loss: 4.392248630523682, Test_Loss: 4.393512725830078 *\n",
      "Epoch: 6, Train_Loss: 4.393980979919434, Test_Loss: 4.4882330894470215\n",
      "Epoch: 6, Train_Loss: 4.424497127532959, Test_Loss: 5.461799144744873\n",
      "Epoch: 6, Train_Loss: 4.403141498565674, Test_Loss: 5.115259170532227 *\n",
      "Epoch: 6, Train_Loss: 4.384076118469238, Test_Loss: 4.4969282150268555 *\n",
      "Epoch: 6, Train_Loss: 4.394282817840576, Test_Loss: 4.45153284072876 *\n",
      "Epoch: 6, Train_Loss: 4.393008708953857, Test_Loss: 4.432651996612549 *\n",
      "Epoch: 6, Train_Loss: 4.399285793304443, Test_Loss: 4.734726428985596\n",
      "Epoch: 6, Train_Loss: 4.385530471801758, Test_Loss: 4.582163333892822 *\n",
      "Epoch: 6, Train_Loss: 4.38941764831543, Test_Loss: 5.486790180206299\n",
      "Epoch: 6, Train_Loss: 4.433379173278809, Test_Loss: 5.277054786682129 *\n",
      "Epoch: 6, Train_Loss: 4.445010662078857, Test_Loss: 4.535704612731934 *\n",
      "Epoch: 6, Train_Loss: 4.389383792877197, Test_Loss: 4.393935680389404 *\n",
      "Epoch: 6, Train_Loss: 4.369515419006348, Test_Loss: 4.405468463897705\n",
      "Epoch: 6, Train_Loss: 4.386882781982422, Test_Loss: 4.409189701080322\n",
      "Epoch: 6, Train_Loss: 4.482415199279785, Test_Loss: 4.426955223083496\n",
      "Epoch: 6, Train_Loss: 4.454800128936768, Test_Loss: 4.787825584411621\n",
      "Epoch: 6, Train_Loss: 4.481887340545654, Test_Loss: 4.836122989654541\n",
      "Epoch: 6, Train_Loss: 4.4178643226623535, Test_Loss: 4.494406223297119 *\n",
      "Epoch: 6, Train_Loss: 4.426363468170166, Test_Loss: 4.409113883972168 *\n",
      "Epoch: 6, Train_Loss: 4.437413215637207, Test_Loss: 4.381438255310059 *\n",
      "Epoch: 6, Train_Loss: 4.4355926513671875, Test_Loss: 4.402496814727783\n",
      "Epoch: 6, Train_Loss: 4.370621204376221, Test_Loss: 4.6218390464782715\n",
      "Epoch: 6, Train_Loss: 4.491033554077148, Test_Loss: 5.774875640869141\n",
      "Epoch: 6, Train_Loss: 4.3888397216796875, Test_Loss: 5.4555983543396 *\n",
      "Epoch: 6, Train_Loss: 4.36388635635376, Test_Loss: 4.412689685821533 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 4.3706464767456055, Test_Loss: 4.411523342132568 *\n",
      "Epoch: 6, Train_Loss: 4.374912738800049, Test_Loss: 4.367328643798828 *\n",
      "Epoch: 6, Train_Loss: 4.364513397216797, Test_Loss: 4.374216556549072\n",
      "Epoch: 6, Train_Loss: 4.362868309020996, Test_Loss: 4.365808486938477 *\n",
      "Epoch: 6, Train_Loss: 5.914506912231445, Test_Loss: 4.391841888427734\n",
      "Epoch: 6, Train_Loss: 7.764961242675781, Test_Loss: 4.430544376373291\n",
      "Epoch: 6, Train_Loss: 4.36191463470459, Test_Loss: 4.3634934425354 *\n",
      "Epoch: 6, Train_Loss: 4.375998020172119, Test_Loss: 4.379968643188477\n",
      "Epoch: 6, Train_Loss: 4.365640163421631, Test_Loss: 4.47299337387085\n",
      "Epoch: 6, Train_Loss: 4.355289459228516, Test_Loss: 4.749207973480225\n",
      "Epoch: 6, Train_Loss: 4.351815223693848, Test_Loss: 4.548366069793701 *\n",
      "Epoch: 6, Train_Loss: 4.346790313720703, Test_Loss: 4.362276554107666 *\n",
      "Epoch: 6, Train_Loss: 4.355397701263428, Test_Loss: 4.360781192779541 *\n",
      "Epoch: 6, Train_Loss: 4.352845191955566, Test_Loss: 4.356756210327148 *\n",
      "Epoch: 6, Train_Loss: 4.356642723083496, Test_Loss: 4.355237007141113 *\n",
      "Epoch: 6, Train_Loss: 4.450996398925781, Test_Loss: 4.359854221343994\n",
      "Epoch: 6, Train_Loss: 4.433856010437012, Test_Loss: 4.949616432189941\n",
      "Epoch: 6, Train_Loss: 4.4696197509765625, Test_Loss: 9.226171493530273\n",
      "Epoch: 6, Train_Loss: 4.392764568328857, Test_Loss: 4.3824567794799805 *\n",
      "Epoch: 6, Train_Loss: 4.360532760620117, Test_Loss: 4.354083061218262 *\n",
      "Epoch: 6, Train_Loss: 4.5167741775512695, Test_Loss: 4.36393404006958\n",
      "Epoch: 6, Train_Loss: 4.568729400634766, Test_Loss: 4.349761009216309 *\n",
      "Epoch: 6, Train_Loss: 4.555276870727539, Test_Loss: 4.342841148376465 *\n",
      "Epoch: 6, Train_Loss: 4.462178707122803, Test_Loss: 4.3459367752075195\n",
      "Epoch: 6, Train_Loss: 4.336042881011963, Test_Loss: 4.344357013702393 *\n",
      "Epoch: 6, Train_Loss: 4.3400115966796875, Test_Loss: 4.348564624786377\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 4.340197563171387, Test_Loss: 4.342468738555908 *\n",
      "Epoch: 6, Train_Loss: 4.345700740814209, Test_Loss: 4.3369855880737305 *\n",
      "Epoch: 6, Train_Loss: 4.340200901031494, Test_Loss: 4.341963291168213\n",
      "Epoch: 6, Train_Loss: 4.336422443389893, Test_Loss: 4.352712154388428\n",
      "Epoch: 6, Train_Loss: 4.33352518081665, Test_Loss: 4.351693630218506 *\n",
      "Epoch: 6, Train_Loss: 4.335717678070068, Test_Loss: 4.35458517074585\n",
      "Epoch: 6, Train_Loss: 4.336945533752441, Test_Loss: 4.335640907287598 *\n",
      "Epoch: 6, Train_Loss: 4.38865327835083, Test_Loss: 4.331695556640625 *\n",
      "Epoch: 6, Train_Loss: 4.530993461608887, Test_Loss: 4.3304009437561035 *\n",
      "Epoch: 6, Train_Loss: 4.494246959686279, Test_Loss: 4.331963539123535\n",
      "Epoch: 6, Train_Loss: 4.459621429443359, Test_Loss: 4.332340240478516\n",
      "Epoch: 6, Train_Loss: 4.425436973571777, Test_Loss: 4.333353519439697\n",
      "Epoch: 6, Train_Loss: 4.499746799468994, Test_Loss: 4.332123756408691 *\n",
      "Epoch: 6, Train_Loss: 4.384685039520264, Test_Loss: 4.335474967956543\n",
      "Epoch: 6, Train_Loss: 4.457935333251953, Test_Loss: 4.336624622344971\n",
      "Epoch: 6, Train_Loss: 4.453641414642334, Test_Loss: 4.327895164489746 *\n",
      "Epoch: 6, Train_Loss: 4.639423847198486, Test_Loss: 4.330929279327393\n",
      "Epoch: 6, Train_Loss: 4.333799839019775, Test_Loss: 4.3283586502075195 *\n",
      "Epoch: 6, Train_Loss: 4.356987953186035, Test_Loss: 4.320116996765137 *\n",
      "Epoch: 6, Train_Loss: 7.321237564086914, Test_Loss: 4.320588111877441\n",
      "Epoch: 6, Train_Loss: 4.572474479675293, Test_Loss: 4.338030815124512\n",
      "Epoch: 6, Train_Loss: 4.384116172790527, Test_Loss: 4.3813886642456055\n",
      "Epoch: 6, Train_Loss: 4.392136096954346, Test_Loss: 7.21275520324707\n",
      "Epoch: 6, Train_Loss: 4.380886077880859, Test_Loss: 6.950131416320801 *\n",
      "Epoch: 6, Train_Loss: 4.327758312225342, Test_Loss: 4.317080020904541 *\n",
      "Epoch: 6, Train_Loss: 4.323180675506592, Test_Loss: 4.318382740020752\n",
      "Epoch: 6, Train_Loss: 4.377452373504639, Test_Loss: 4.369207859039307\n",
      "Epoch: 6, Train_Loss: 4.416820526123047, Test_Loss: 4.373465061187744\n",
      "Epoch: 6, Train_Loss: 4.402397632598877, Test_Loss: 4.370925426483154 *\n",
      "Epoch: 6, Train_Loss: 4.379607677459717, Test_Loss: 4.364189147949219 *\n",
      "Epoch: 6, Train_Loss: 4.403195381164551, Test_Loss: 4.430383205413818\n",
      "Epoch: 6, Train_Loss: 4.3462934494018555, Test_Loss: 4.307474136352539 *\n",
      "Epoch: 6, Train_Loss: 4.343983173370361, Test_Loss: 4.334532737731934\n",
      "Epoch: 6, Train_Loss: 4.322232246398926, Test_Loss: 4.338624477386475\n",
      "Epoch: 6, Train_Loss: 4.344607830047607, Test_Loss: 4.328305244445801 *\n",
      "Epoch: 6, Train_Loss: 4.331789016723633, Test_Loss: 4.317359924316406 *\n",
      "Epoch: 6, Train_Loss: 4.310486793518066, Test_Loss: 4.412051200866699\n",
      "Epoch: 6, Train_Loss: 4.320420265197754, Test_Loss: 4.392990589141846 *\n",
      "Epoch: 6, Train_Loss: 4.361247539520264, Test_Loss: 4.397321701049805\n",
      "Epoch: 6, Train_Loss: 4.338797569274902, Test_Loss: 4.402927875518799\n",
      "Epoch: 6, Train_Loss: 4.301545143127441, Test_Loss: 4.338219165802002 *\n",
      "Epoch: 6, Train_Loss: 4.303799629211426, Test_Loss: 4.348734378814697\n",
      "Epoch: 6, Train_Loss: 4.301313877105713, Test_Loss: 4.3139519691467285 *\n",
      "Epoch: 6, Train_Loss: 4.3067731857299805, Test_Loss: 4.322874069213867\n",
      "Epoch: 6, Train_Loss: 4.296459197998047, Test_Loss: 4.319408416748047 *\n",
      "Epoch: 6, Train_Loss: 4.301040172576904, Test_Loss: 4.3117594718933105 *\n",
      "Epoch: 6, Train_Loss: 4.303944110870361, Test_Loss: 4.320770740509033\n",
      "Epoch: 6, Train_Loss: 4.302292346954346, Test_Loss: 4.313726425170898 *\n",
      "Epoch: 6, Train_Loss: 4.296630859375, Test_Loss: 4.323652267456055\n",
      "Epoch: 6, Train_Loss: 4.3000359535217285, Test_Loss: 4.325606822967529\n",
      "Epoch: 6, Train_Loss: 4.300085067749023, Test_Loss: 4.31321382522583 *\n",
      "Epoch: 6, Train_Loss: 4.294127941131592, Test_Loss: 4.29464864730835 *\n",
      "Epoch: 6, Train_Loss: 4.303426265716553, Test_Loss: 4.330899238586426\n",
      "Epoch: 6, Train_Loss: 4.312285900115967, Test_Loss: 4.3612470626831055\n",
      "Epoch: 7, Train_Loss: 4.304141521453857, Test_Loss: 4.570040702819824 *\n",
      "Epoch: 7, Train_Loss: 4.296480655670166, Test_Loss: 4.3257646560668945 *\n",
      "Epoch: 7, Train_Loss: 4.287511348724365, Test_Loss: 4.337608337402344\n",
      "Epoch: 7, Train_Loss: 4.290943145751953, Test_Loss: 4.385441780090332\n",
      "Epoch: 7, Train_Loss: 4.292120456695557, Test_Loss: 4.58414888381958\n",
      "Epoch: 7, Train_Loss: 4.3031206130981445, Test_Loss: 4.464958667755127 *\n",
      "Epoch: 7, Train_Loss: 4.287187576293945, Test_Loss: 4.319882869720459 *\n",
      "Epoch: 7, Train_Loss: 4.278761863708496, Test_Loss: 4.544394016265869\n",
      "Epoch: 7, Train_Loss: 4.285167217254639, Test_Loss: 4.595417022705078\n",
      "Epoch: 7, Train_Loss: 4.349291801452637, Test_Loss: 4.30211877822876 *\n",
      "Epoch: 7, Train_Loss: 4.3236002922058105, Test_Loss: 4.335093021392822\n",
      "Epoch: 7, Train_Loss: 4.309716701507568, Test_Loss: 4.285363674163818 *\n",
      "Epoch: 7, Train_Loss: 4.288974285125732, Test_Loss: 4.309841632843018\n",
      "Epoch: 7, Train_Loss: 4.277061939239502, Test_Loss: 4.301904201507568 *\n",
      "Epoch: 7, Train_Loss: 4.321073532104492, Test_Loss: 5.090537071228027\n",
      "Epoch: 7, Train_Loss: 4.278186798095703, Test_Loss: 4.460141181945801 *\n",
      "Epoch: 7, Train_Loss: 4.299188613891602, Test_Loss: 5.065251350402832\n",
      "Epoch: 7, Train_Loss: 4.308622360229492, Test_Loss: 4.963627815246582 *\n",
      "Epoch: 7, Train_Loss: 4.294782638549805, Test_Loss: 4.54754114151001 *\n",
      "Epoch: 7, Train_Loss: 4.433905601501465, Test_Loss: 4.796817779541016\n",
      "Epoch: 7, Train_Loss: 4.351808071136475, Test_Loss: 4.336732387542725 *\n",
      "Epoch: 7, Train_Loss: 4.309727191925049, Test_Loss: 4.286319732666016 *\n",
      "Epoch: 7, Train_Loss: 4.274309158325195, Test_Loss: 4.314902305603027\n",
      "Epoch: 7, Train_Loss: 4.293600082397461, Test_Loss: 4.451927661895752\n",
      "Epoch: 7, Train_Loss: 4.266268730163574, Test_Loss: 4.732769012451172\n",
      "Epoch: 7, Train_Loss: 4.272618770599365, Test_Loss: 4.903274059295654\n",
      "Epoch: 7, Train_Loss: 4.267688274383545, Test_Loss: 5.600632667541504\n",
      "Epoch: 7, Train_Loss: 4.2835469245910645, Test_Loss: 5.436168670654297 *\n",
      "Epoch: 7, Train_Loss: 4.319401741027832, Test_Loss: 5.235761642456055 *\n",
      "Epoch: 7, Train_Loss: 4.322646617889404, Test_Loss: 4.6633195877075195 *\n",
      "Epoch: 7, Train_Loss: 4.298160552978516, Test_Loss: 4.266653537750244 *\n",
      "Epoch: 7, Train_Loss: 4.309200286865234, Test_Loss: 4.300105571746826\n",
      "Epoch: 7, Train_Loss: 4.2837748527526855, Test_Loss: 5.266560077667236\n",
      "Epoch: 7, Train_Loss: 4.275774955749512, Test_Loss: 5.409389495849609\n",
      "Epoch: 7, Train_Loss: 4.3769121170043945, Test_Loss: 4.324430465698242 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 4.459679126739502, Test_Loss: 4.341425895690918\n",
      "Epoch: 7, Train_Loss: 4.254855632781982, Test_Loss: 4.267576217651367 *\n",
      "Epoch: 7, Train_Loss: 4.281840801239014, Test_Loss: 4.561757564544678\n",
      "Epoch: 7, Train_Loss: 4.254631042480469, Test_Loss: 4.473118782043457 *\n",
      "Epoch: 7, Train_Loss: 4.2456746101379395, Test_Loss: 5.056018352508545\n",
      "Epoch: 7, Train_Loss: 4.255307674407959, Test_Loss: 5.296547889709473\n",
      "Epoch: 7, Train_Loss: 4.253472805023193, Test_Loss: 4.513260841369629 *\n",
      "Epoch: 7, Train_Loss: 4.275190830230713, Test_Loss: 4.250472545623779 *\n",
      "Epoch: 7, Train_Loss: 4.2797064781188965, Test_Loss: 4.265806198120117\n",
      "Epoch: 7, Train_Loss: 4.269300937652588, Test_Loss: 4.280402660369873\n",
      "Epoch: 7, Train_Loss: 4.272029876708984, Test_Loss: 4.293257713317871\n",
      "Epoch: 7, Train_Loss: 4.279209613800049, Test_Loss: 4.6023640632629395\n",
      "Epoch: 7, Train_Loss: 4.250372886657715, Test_Loss: 4.844233512878418\n",
      "Epoch: 7, Train_Loss: 4.249743938446045, Test_Loss: 4.446575164794922 *\n",
      "Epoch: 7, Train_Loss: 4.246225833892822, Test_Loss: 4.286722183227539 *\n",
      "Epoch: 7, Train_Loss: 4.27695369720459, Test_Loss: 4.283044815063477 *\n",
      "Epoch: 7, Train_Loss: 4.269574165344238, Test_Loss: 4.255140781402588 *\n",
      "Epoch: 7, Train_Loss: 4.250916481018066, Test_Loss: 4.368920803070068\n",
      "Epoch: 7, Train_Loss: 4.249533653259277, Test_Loss: 5.266547679901123\n",
      "Epoch: 7, Train_Loss: 4.299086093902588, Test_Loss: 5.530449867248535\n",
      "Epoch: 7, Train_Loss: 4.302882194519043, Test_Loss: 4.313117027282715 *\n",
      "Epoch: 7, Train_Loss: 4.255521297454834, Test_Loss: 4.323068141937256\n",
      "Epoch: 7, Train_Loss: 4.2527546882629395, Test_Loss: 4.234742641448975 *\n",
      "Epoch: 7, Train_Loss: 4.257779598236084, Test_Loss: 4.245419979095459\n",
      "Epoch: 7, Train_Loss: 4.2460761070251465, Test_Loss: 4.23902702331543 *\n",
      "Epoch: 7, Train_Loss: 4.245214462280273, Test_Loss: 4.252178192138672\n",
      "Epoch: 7, Train_Loss: 4.253777027130127, Test_Loss: 4.289305210113525\n",
      "Epoch: 7, Train_Loss: 4.279333591461182, Test_Loss: 4.261070728302002 *\n",
      "Epoch: 7, Train_Loss: 6.479758262634277, Test_Loss: 4.238840579986572 *\n",
      "Epoch: 7, Train_Loss: 7.40941047668457, Test_Loss: 4.340408802032471\n",
      "Epoch: 7, Train_Loss: 4.244626045227051, Test_Loss: 4.629144668579102\n",
      "Epoch: 7, Train_Loss: 4.234978199005127, Test_Loss: 4.345839023590088 *\n",
      "Epoch: 7, Train_Loss: 4.289299011230469, Test_Loss: 4.3282012939453125 *\n",
      "Epoch: 7, Train_Loss: 4.4116315841674805, Test_Loss: 4.2326765060424805 *\n",
      "Epoch: 7, Train_Loss: 4.263646602630615, Test_Loss: 4.227823257446289 *\n",
      "Epoch: 7, Train_Loss: 4.2344441413879395, Test_Loss: 4.225817680358887 *\n",
      "Epoch: 7, Train_Loss: 4.238365173339844, Test_Loss: 4.225175857543945 *\n",
      "Epoch: 7, Train_Loss: 4.298761367797852, Test_Loss: 4.293304920196533\n",
      "Epoch: 7, Train_Loss: 4.222785472869873, Test_Loss: 9.38314151763916\n",
      "Epoch: 7, Train_Loss: 4.229030609130859, Test_Loss: 4.396400451660156 *\n",
      "Epoch: 7, Train_Loss: 5.0969367027282715, Test_Loss: 4.228507995605469 *\n",
      "Epoch: 7, Train_Loss: 5.689784049987793, Test_Loss: 4.2227559089660645 *\n",
      "Epoch: 7, Train_Loss: 4.865051746368408, Test_Loss: 4.223998069763184\n",
      "Epoch: 7, Train_Loss: 4.3393330574035645, Test_Loss: 4.231256484985352\n",
      "Epoch: 7, Train_Loss: 5.044216632843018, Test_Loss: 4.221076488494873 *\n",
      "Epoch: 7, Train_Loss: 6.559964179992676, Test_Loss: 4.2147536277771 *\n",
      "Epoch: 7, Train_Loss: 4.580504894256592, Test_Loss: 4.21199893951416 *\n",
      "Epoch: 7, Train_Loss: 4.256577014923096, Test_Loss: 4.209346294403076 *\n",
      "Epoch: 7, Train_Loss: 4.231600761413574, Test_Loss: 4.213498115539551\n",
      "Epoch: 7, Train_Loss: 5.731400012969971, Test_Loss: 4.210267066955566 *\n",
      "Epoch: 7, Train_Loss: 5.813230514526367, Test_Loss: 4.23608922958374\n",
      "Epoch: 7, Train_Loss: 4.360507965087891, Test_Loss: 4.231048107147217 *\n",
      "Epoch: 7, Train_Loss: 4.236902713775635, Test_Loss: 4.226410388946533 *\n",
      "Epoch: 7, Train_Loss: 4.212815761566162, Test_Loss: 4.20794153213501 *\n",
      "Epoch: 7, Train_Loss: 4.868271827697754, Test_Loss: 4.210141181945801\n",
      "Epoch: 7, Train_Loss: 4.312358379364014, Test_Loss: 4.203392505645752 *\n",
      "Epoch: 7, Train_Loss: 4.240437030792236, Test_Loss: 4.205068111419678\n",
      "Epoch: 7, Train_Loss: 4.230540752410889, Test_Loss: 4.204456329345703 *\n",
      "Epoch: 7, Train_Loss: 4.362675666809082, Test_Loss: 4.206950664520264\n",
      "Epoch: 7, Train_Loss: 4.37975549697876, Test_Loss: 4.201904773712158 *\n",
      "Epoch: 7, Train_Loss: 4.365752220153809, Test_Loss: 4.208499908447266\n",
      "Epoch: 7, Train_Loss: 4.585233211517334, Test_Loss: 4.208361625671387 *\n",
      "Epoch: 7, Train_Loss: 4.282862186431885, Test_Loss: 4.203247547149658 *\n",
      "Epoch: 7, Train_Loss: 4.2861738204956055, Test_Loss: 4.203397750854492\n",
      "Epoch: 7, Train_Loss: 4.4796319007873535, Test_Loss: 4.199930191040039 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 4.654537200927734, Test_Loss: 4.1999359130859375\n",
      "Epoch: 7, Train_Loss: 4.675824165344238, Test_Loss: 4.196302890777588 *\n",
      "Epoch: 7, Train_Loss: 4.254082679748535, Test_Loss: 4.20160436630249\n",
      "Epoch: 7, Train_Loss: 4.3043951988220215, Test_Loss: 4.260359287261963\n",
      "Epoch: 7, Train_Loss: 4.327808856964111, Test_Loss: 5.73045539855957\n",
      "Epoch: 7, Train_Loss: 4.2282514572143555, Test_Loss: 8.204668045043945\n",
      "Epoch: 7, Train_Loss: 4.212326526641846, Test_Loss: 4.193298816680908 *\n",
      "Epoch: 7, Train_Loss: 4.193049430847168, Test_Loss: 4.183396816253662 *\n",
      "Epoch: 7, Train_Loss: 4.1953816413879395, Test_Loss: 4.229055404663086\n",
      "Epoch: 7, Train_Loss: 4.194812297821045, Test_Loss: 4.2436017990112305\n",
      "Epoch: 7, Train_Loss: 4.19223165512085, Test_Loss: 4.251952648162842\n",
      "Epoch: 7, Train_Loss: 4.2529497146606445, Test_Loss: 4.21113395690918 *\n",
      "Epoch: 7, Train_Loss: 4.251805782318115, Test_Loss: 4.312201499938965\n",
      "Epoch: 7, Train_Loss: 4.276129245758057, Test_Loss: 4.204922676086426 *\n",
      "Epoch: 7, Train_Loss: 4.2700700759887695, Test_Loss: 4.190871715545654 *\n",
      "Epoch: 7, Train_Loss: 4.696027755737305, Test_Loss: 4.2232666015625\n",
      "Epoch: 7, Train_Loss: 4.1872687339782715, Test_Loss: 4.190853118896484 *\n",
      "Epoch: 7, Train_Loss: 4.202670574188232, Test_Loss: 4.193665504455566\n",
      "Epoch: 7, Train_Loss: 4.365812301635742, Test_Loss: 4.248427867889404\n",
      "Epoch: 7, Train_Loss: 4.71421480178833, Test_Loss: 4.285683631896973\n",
      "Epoch: 7, Train_Loss: 4.559178829193115, Test_Loss: 4.254033088684082 *\n",
      "Epoch: 7, Train_Loss: 4.180336952209473, Test_Loss: 4.28842306137085\n",
      "Epoch: 7, Train_Loss: 4.265128135681152, Test_Loss: 4.202731609344482 *\n",
      "Epoch: 7, Train_Loss: 4.816012859344482, Test_Loss: 4.201157569885254 *\n",
      "Epoch: 7, Train_Loss: 4.799197196960449, Test_Loss: 4.187527656555176 *\n",
      "Epoch: 7, Train_Loss: 4.234301567077637, Test_Loss: 4.176753520965576 *\n",
      "Epoch: 7, Train_Loss: 4.192841529846191, Test_Loss: 4.1873064041137695\n",
      "Epoch: 7, Train_Loss: 4.177353858947754, Test_Loss: 4.177262306213379 *\n",
      "Epoch: 7, Train_Loss: 5.28019905090332, Test_Loss: 4.172586441040039 *\n",
      "Epoch: 7, Train_Loss: 5.372461318969727, Test_Loss: 4.183231353759766\n",
      "Epoch: 7, Train_Loss: 4.1847052574157715, Test_Loss: 4.179688453674316 *\n",
      "Epoch: 7, Train_Loss: 4.192887306213379, Test_Loss: 4.187155246734619\n",
      "Epoch: 7, Train_Loss: 4.164497375488281, Test_Loss: 4.1955108642578125\n",
      "Epoch: 7, Train_Loss: 4.180098056793213, Test_Loss: 4.174441814422607 *\n",
      "Epoch: 7, Train_Loss: 4.593813896179199, Test_Loss: 4.183225631713867\n",
      "Epoch: 7, Train_Loss: 4.211931228637695, Test_Loss: 4.240604877471924\n",
      "Epoch: 7, Train_Loss: 4.205549240112305, Test_Loss: 4.359098434448242\n",
      "Epoch: 7, Train_Loss: 4.171687126159668, Test_Loss: 4.283674716949463 *\n",
      "Epoch: 7, Train_Loss: 4.175034046173096, Test_Loss: 4.191896915435791 *\n",
      "Epoch: 7, Train_Loss: 21.720455169677734, Test_Loss: 4.246150970458984\n",
      "Epoch: 7, Train_Loss: 4.1650614738464355, Test_Loss: 4.332561492919922\n",
      "Epoch: 7, Train_Loss: 6.596767425537109, Test_Loss: 4.424643039703369\n",
      "Epoch: 7, Train_Loss: 5.843311786651611, Test_Loss: 4.161383152008057 *\n",
      "Epoch: 7, Train_Loss: 4.158810138702393, Test_Loss: 4.358682155609131\n",
      "Epoch: 7, Train_Loss: 4.210261344909668, Test_Loss: 4.480153560638428\n",
      "Epoch: 7, Train_Loss: 9.66038990020752, Test_Loss: 4.193587303161621 *\n",
      "Epoch: 7, Train_Loss: 11.199697494506836, Test_Loss: 4.2055535316467285\n",
      "Epoch: 7, Train_Loss: 4.192224502563477, Test_Loss: 4.1500773429870605 *\n",
      "Epoch: 7, Train_Loss: 4.193732738494873, Test_Loss: 4.185038089752197\n",
      "Epoch: 7, Train_Loss: 10.648941040039062, Test_Loss: 4.159607887268066 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 4.164320468902588, Test_Loss: 4.964266300201416\n",
      "Epoch: 7, Train_Loss: 4.172835826873779, Test_Loss: 4.463010787963867 *\n",
      "Epoch: 7, Train_Loss: 4.15435791015625, Test_Loss: 4.739719867706299\n",
      "Epoch: 7, Train_Loss: 4.153911590576172, Test_Loss: 4.901747703552246\n",
      "Epoch: 7, Train_Loss: 4.151271343231201, Test_Loss: 4.306543827056885 *\n",
      "Epoch: 7, Train_Loss: 4.149978160858154, Test_Loss: 4.85892915725708\n",
      "Epoch: 7, Train_Loss: 4.145124435424805, Test_Loss: 4.3403096199035645 *\n",
      "Epoch: 7, Train_Loss: 4.151889324188232, Test_Loss: 4.14824104309082 *\n",
      "Epoch: 7, Train_Loss: 4.1566548347473145, Test_Loss: 4.154334545135498\n",
      "Epoch: 7, Train_Loss: 4.157175064086914, Test_Loss: 4.245182514190674\n",
      "Epoch: 7, Train_Loss: 4.155218124389648, Test_Loss: 4.413793563842773\n",
      "Epoch: 7, Train_Loss: 4.150152683258057, Test_Loss: 5.12513542175293\n",
      "Epoch: 7, Train_Loss: 4.182939052581787, Test_Loss: 4.824282169342041 *\n",
      "Epoch: 7, Train_Loss: 4.208552360534668, Test_Loss: 5.711669445037842\n",
      "Epoch: 7, Train_Loss: 4.151113033294678, Test_Loss: 4.94565486907959 *\n",
      "Epoch: 7, Train_Loss: 4.142534255981445, Test_Loss: 4.8616437911987305 *\n",
      "Epoch: 7, Train_Loss: 4.138190746307373, Test_Loss: 4.151826858520508 *\n",
      "Epoch: 7, Train_Loss: 4.134458065032959, Test_Loss: 4.140183925628662 *\n",
      "Epoch: 7, Train_Loss: 4.1319966316223145, Test_Loss: 4.733869552612305\n",
      "Epoch: 7, Train_Loss: 4.142166614532471, Test_Loss: 5.405208587646484\n",
      "Epoch: 7, Train_Loss: 4.131206512451172, Test_Loss: 4.211644172668457 *\n",
      "Epoch: 7, Train_Loss: 4.140537261962891, Test_Loss: 4.263101577758789\n",
      "Epoch: 7, Train_Loss: 4.131008625030518, Test_Loss: 4.134888648986816 *\n",
      "Epoch: 7, Train_Loss: 4.133008003234863, Test_Loss: 4.262725353240967\n",
      "Epoch: 7, Train_Loss: 4.12837553024292, Test_Loss: 4.416537761688232\n",
      "Epoch: 7, Train_Loss: 4.12655782699585, Test_Loss: 4.701286792755127\n",
      "Epoch: 7, Train_Loss: 4.1298322677612305, Test_Loss: 5.537450790405273\n",
      "Epoch: 7, Train_Loss: 4.139463901519775, Test_Loss: 4.530985355377197 *\n",
      "Epoch: 7, Train_Loss: 4.129045009613037, Test_Loss: 4.132312774658203 *\n",
      "Epoch: 7, Train_Loss: 4.132350444793701, Test_Loss: 4.148884296417236\n",
      "Epoch: 7, Train_Loss: 6.676163196563721, Test_Loss: 4.17242431640625\n",
      "Epoch: 7, Train_Loss: 10.804306983947754, Test_Loss: 4.210962295532227\n",
      "Epoch: 7, Train_Loss: 4.147304058074951, Test_Loss: 4.346442699432373\n",
      "Epoch: 7, Train_Loss: 4.120448112487793, Test_Loss: 4.645236968994141\n",
      "Epoch: 7, Train_Loss: 4.130433559417725, Test_Loss: 4.329265117645264 *\n",
      "Epoch: 7, Train_Loss: 4.136358737945557, Test_Loss: 4.184258460998535 *\n",
      "Epoch: 7, Train_Loss: 4.164673328399658, Test_Loss: 4.138752460479736 *\n",
      "Epoch: 7, Train_Loss: 4.127274513244629, Test_Loss: 4.1328654289245605 *\n",
      "Epoch: 7, Train_Loss: 4.156306266784668, Test_Loss: 4.254295825958252\n",
      "Epoch: 7, Train_Loss: 4.3327860832214355, Test_Loss: 4.983554840087891\n",
      "Epoch: 7, Train_Loss: 4.314569473266602, Test_Loss: 5.790960788726807\n",
      "Epoch: 7, Train_Loss: 4.206552982330322, Test_Loss: 4.3864521980285645 *\n",
      "Epoch: 7, Train_Loss: 4.174805164337158, Test_Loss: 4.18511962890625 *\n",
      "Epoch: 7, Train_Loss: 4.2312235832214355, Test_Loss: 4.117547988891602 *\n",
      "Epoch: 7, Train_Loss: 4.20416784286499, Test_Loss: 4.1213202476501465\n",
      "Epoch: 7, Train_Loss: 4.2583327293396, Test_Loss: 4.1230788230896\n",
      "Epoch: 7, Train_Loss: 4.1921892166137695, Test_Loss: 4.135365962982178\n",
      "Epoch: 7, Train_Loss: 4.199809551239014, Test_Loss: 4.164453983306885\n",
      "Epoch: 7, Train_Loss: 4.1051225662231445, Test_Loss: 4.164930820465088\n",
      "Epoch: 7, Train_Loss: 4.171263694763184, Test_Loss: 4.114020824432373 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 4.162238121032715, Test_Loss: 4.188680171966553\n",
      "Epoch: 7, Train_Loss: 4.114070892333984, Test_Loss: 4.45914363861084\n",
      "Epoch: 7, Train_Loss: 4.109402656555176, Test_Loss: 4.16707181930542 *\n",
      "Epoch: 7, Train_Loss: 4.103072166442871, Test_Loss: 4.249886989593506\n",
      "Epoch: 7, Train_Loss: 4.100466728210449, Test_Loss: 4.099184036254883 *\n",
      "Epoch: 7, Train_Loss: 6.830514907836914, Test_Loss: 4.1026997566223145\n",
      "Epoch: 7, Train_Loss: 7.267062187194824, Test_Loss: 4.099234104156494 *\n",
      "Epoch: 7, Train_Loss: 4.100673675537109, Test_Loss: 4.0962233543396 *\n",
      "Epoch: 7, Train_Loss: 4.1208038330078125, Test_Loss: 4.100994110107422\n",
      "Epoch: 7, Train_Loss: 4.110226631164551, Test_Loss: 8.662761688232422\n",
      "Epoch: 7, Train_Loss: 4.105792999267578, Test_Loss: 5.15295934677124 *\n",
      "Epoch: 7, Train_Loss: 4.100934982299805, Test_Loss: 4.099599361419678 *\n",
      "Epoch: 7, Train_Loss: 4.101052761077881, Test_Loss: 4.094186305999756 *\n",
      "Epoch: 7, Train_Loss: 4.105653762817383, Test_Loss: 4.093944549560547 *\n",
      "Epoch: 7, Train_Loss: 4.1147027015686035, Test_Loss: 4.094425201416016\n",
      "Epoch: 7, Train_Loss: 4.102746963500977, Test_Loss: 4.089423656463623 *\n",
      "Epoch: 7, Train_Loss: 4.100931644439697, Test_Loss: 4.104804992675781\n",
      "Epoch: 7, Train_Loss: 4.096872329711914, Test_Loss: 4.094947814941406 *\n",
      "Epoch: 7, Train_Loss: 4.100470542907715, Test_Loss: 4.105442523956299\n",
      "Epoch: 7, Train_Loss: 4.1025519371032715, Test_Loss: 4.101839065551758 *\n",
      "Epoch: 7, Train_Loss: 4.092650890350342, Test_Loss: 4.098342418670654 *\n",
      "Epoch: 7, Train_Loss: 4.094630241394043, Test_Loss: 4.092746734619141 *\n",
      "Epoch: 7, Train_Loss: 4.142910480499268, Test_Loss: 4.094027519226074\n",
      "Epoch: 7, Train_Loss: 4.140505313873291, Test_Loss: 4.087669849395752 *\n",
      "Epoch: 7, Train_Loss: 4.0895915031433105, Test_Loss: 4.090554237365723\n",
      "Epoch: 7, Train_Loss: 4.078423023223877, Test_Loss: 4.082903861999512 *\n",
      "Epoch: 7, Train_Loss: 4.108298301696777, Test_Loss: 4.089963912963867\n",
      "Epoch: 7, Train_Loss: 4.196669578552246, Test_Loss: 4.083755970001221 *\n",
      "Epoch: 7, Train_Loss: 4.150493621826172, Test_Loss: 4.088434219360352\n",
      "Epoch: 7, Train_Loss: 4.143222808837891, Test_Loss: 4.10394811630249\n",
      "Epoch: 7, Train_Loss: 4.113211154937744, Test_Loss: 4.076991558074951 *\n",
      "Epoch: 7, Train_Loss: 4.133205413818359, Test_Loss: 4.086731910705566\n",
      "Epoch: 7, Train_Loss: 4.146255970001221, Test_Loss: 4.080173492431641 *\n",
      "Epoch: 7, Train_Loss: 4.114908218383789, Test_Loss: 4.080196857452393\n",
      "Epoch: 7, Train_Loss: 4.102935314178467, Test_Loss: 4.071666240692139 *\n",
      "Epoch: 7, Train_Loss: 4.151554584503174, Test_Loss: 4.077029705047607\n",
      "Epoch: 7, Train_Loss: 4.084229946136475, Test_Loss: 4.073915481567383 *\n",
      "Epoch: 7, Train_Loss: 4.067882537841797, Test_Loss: 4.07883358001709\n",
      "Epoch: 7, Train_Loss: 4.067016124725342, Test_Loss: 4.082208156585693\n",
      "Epoch: 7, Train_Loss: 4.0667500495910645, Test_Loss: 4.132318496704102\n",
      "Epoch: 7, Train_Loss: 4.06654167175293, Test_Loss: 4.288865089416504\n",
      "Epoch: 7, Train_Loss: 4.061637878417969, Test_Loss: 9.698709487915039\n",
      "Epoch: 7, Train_Loss: 6.865935325622559, Test_Loss: 4.088994979858398 *\n",
      "Epoch: 7, Train_Loss: 6.312963485717773, Test_Loss: 4.059082508087158 *\n",
      "Epoch: 7, Train_Loss: 4.057892322540283, Test_Loss: 4.086825847625732\n",
      "Epoch: 7, Train_Loss: 4.074127197265625, Test_Loss: 4.097431659698486\n",
      "Epoch: 7, Train_Loss: 4.069036960601807, Test_Loss: 4.102054595947266\n",
      "Epoch: 7, Train_Loss: 4.0554518699646, Test_Loss: 4.064713954925537 *\n",
      "Epoch: 7, Train_Loss: 4.059596538543701, Test_Loss: 4.201715469360352\n",
      "Epoch: 7, Train_Loss: 4.053751468658447, Test_Loss: 4.1044464111328125 *\n",
      "Epoch: 7, Train_Loss: 4.055363178253174, Test_Loss: 4.056716442108154 *\n",
      "Epoch: 7, Train_Loss: 4.054210662841797, Test_Loss: 4.103686332702637\n",
      "Epoch: 7, Train_Loss: 4.071323871612549, Test_Loss: 4.057491302490234 *\n",
      "Epoch: 7, Train_Loss: 4.141035079956055, Test_Loss: 4.058874130249023\n",
      "Epoch: 7, Train_Loss: 4.142247200012207, Test_Loss: 4.084707736968994\n",
      "Epoch: 7, Train_Loss: 4.14909029006958, Test_Loss: 4.162582874298096\n",
      "Epoch: 7, Train_Loss: 4.08512020111084, Test_Loss: 4.1021728515625 *\n",
      "Epoch: 7, Train_Loss: 4.0544352531433105, Test_Loss: 4.184103012084961\n",
      "Epoch: 7, Train_Loss: 4.23622465133667, Test_Loss: 4.090387344360352 *\n",
      "Epoch: 7, Train_Loss: 4.244420051574707, Test_Loss: 4.077547073364258 *\n",
      "Epoch: 7, Train_Loss: 4.273542881011963, Test_Loss: 4.055639266967773 *\n",
      "Epoch: 7, Train_Loss: 4.115793228149414, Test_Loss: 4.050233840942383 *\n",
      "Epoch: 7, Train_Loss: 4.0398478507995605, Test_Loss: 4.053420543670654\n",
      "Epoch: 7, Train_Loss: 4.039652347564697, Test_Loss: 4.055563926696777\n",
      "Epoch: 7, Train_Loss: 4.043725490570068, Test_Loss: 4.054828643798828 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 4.048798561096191, Test_Loss: 4.046374320983887 *\n",
      "Epoch: 7, Train_Loss: 4.042230606079102, Test_Loss: 4.053744792938232\n",
      "Epoch: 7, Train_Loss: 4.049643039703369, Test_Loss: 4.051741600036621 *\n",
      "Epoch: 7, Train_Loss: 4.033187389373779, Test_Loss: 4.069046497344971\n",
      "Epoch: 7, Train_Loss: 4.034354209899902, Test_Loss: 4.042578220367432 *\n",
      "Epoch: 7, Train_Loss: 4.04926061630249, Test_Loss: 4.050168991088867\n",
      "Epoch: 7, Train_Loss: 4.1020917892456055, Test_Loss: 4.107125759124756\n",
      "Epoch: 7, Train_Loss: 4.239235877990723, Test_Loss: 4.117866516113281\n",
      "Epoch: 7, Train_Loss: 4.2010908126831055, Test_Loss: 4.3238749504089355\n",
      "Epoch: 7, Train_Loss: 4.141333103179932, Test_Loss: 4.0431928634643555 *\n",
      "Epoch: 7, Train_Loss: 4.151388168334961, Test_Loss: 4.112151145935059\n",
      "Epoch: 7, Train_Loss: 4.200831890106201, Test_Loss: 4.126367092132568\n",
      "Epoch: 7, Train_Loss: 4.054582118988037, Test_Loss: 4.351995468139648\n",
      "Epoch: 7, Train_Loss: 4.177072048187256, Test_Loss: 4.059561729431152 *\n",
      "Epoch: 7, Train_Loss: 4.151057720184326, Test_Loss: 4.228016376495361\n",
      "Epoch: 7, Train_Loss: 4.302340507507324, Test_Loss: 4.356836795806885\n",
      "Epoch: 7, Train_Loss: 4.033483982086182, Test_Loss: 4.166853904724121 *\n",
      "Epoch: 7, Train_Loss: 4.303441047668457, Test_Loss: 4.0810770988464355 *\n",
      "Epoch: 7, Train_Loss: 6.88037109375, Test_Loss: 4.0293169021606445 *\n",
      "Epoch: 7, Train_Loss: 4.118582725524902, Test_Loss: 4.0311279296875\n",
      "Epoch: 7, Train_Loss: 4.0717291831970215, Test_Loss: 4.044755458831787\n",
      "Epoch: 7, Train_Loss: 4.07794713973999, Test_Loss: 4.525867938995361\n",
      "Epoch: 7, Train_Loss: 4.090241432189941, Test_Loss: 4.483996391296387 *\n",
      "Epoch: 7, Train_Loss: 4.021308422088623, Test_Loss: 4.498960018157959\n",
      "Epoch: 7, Train_Loss: 4.022082328796387, Test_Loss: 4.8852739334106445\n",
      "Epoch: 7, Train_Loss: 4.117435455322266, Test_Loss: 4.181877136230469 *\n",
      "Epoch: 7, Train_Loss: 4.123311519622803, Test_Loss: 4.65055513381958\n",
      "Epoch: 7, Train_Loss: 4.111713886260986, Test_Loss: 4.22773551940918 *\n",
      "Epoch: 7, Train_Loss: 4.091358661651611, Test_Loss: 4.020431995391846 *\n",
      "Epoch: 7, Train_Loss: 4.109773635864258, Test_Loss: 4.0279765129089355\n",
      "Epoch: 7, Train_Loss: 4.034986972808838, Test_Loss: 4.091045379638672\n",
      "Epoch: 7, Train_Loss: 4.042104244232178, Test_Loss: 4.164725303649902\n",
      "Epoch: 7, Train_Loss: 4.029234886169434, Test_Loss: 5.001532554626465\n",
      "Epoch: 7, Train_Loss: 4.0407795906066895, Test_Loss: 4.330637454986572 *\n",
      "Epoch: 7, Train_Loss: 4.030207633972168, Test_Loss: 6.245349884033203\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 4.0054612159729, Test_Loss: 4.525862693786621 *\n",
      "Epoch: 7, Train_Loss: 4.038637638092041, Test_Loss: 4.882477283477783\n",
      "Epoch: 7, Train_Loss: 4.0462188720703125, Test_Loss: 4.036594390869141 *\n",
      "Epoch: 7, Train_Loss: 4.051544666290283, Test_Loss: 4.006893634796143 *\n",
      "Epoch: 7, Train_Loss: 3.9994521141052246, Test_Loss: 4.40604305267334\n",
      "Epoch: 7, Train_Loss: 4.005757808685303, Test_Loss: 5.5816755294799805\n",
      "Epoch: 7, Train_Loss: 4.001695156097412, Test_Loss: 4.32026481628418 *\n",
      "Epoch: 7, Train_Loss: 4.001237392425537, Test_Loss: 4.123917102813721 *\n",
      "Epoch: 7, Train_Loss: 4.0007429122924805, Test_Loss: 4.005112171173096 *\n",
      "Epoch: 7, Train_Loss: 3.998835563659668, Test_Loss: 4.131850242614746\n",
      "Epoch: 7, Train_Loss: 4.003294944763184, Test_Loss: 4.347843170166016\n",
      "Epoch: 7, Train_Loss: 3.9969985485076904, Test_Loss: 4.296105861663818 *\n",
      "Epoch: 7, Train_Loss: 4.000942230224609, Test_Loss: 5.347632884979248\n",
      "Epoch: 7, Train_Loss: 4.001880645751953, Test_Loss: 4.451109886169434 *\n",
      "Epoch: 7, Train_Loss: 3.996474266052246, Test_Loss: 4.002935409545898 *\n",
      "Epoch: 7, Train_Loss: 4.000584125518799, Test_Loss: 4.004208087921143\n",
      "Epoch: 7, Train_Loss: 4.006027698516846, Test_Loss: 4.007949352264404\n",
      "Epoch: 7, Train_Loss: 4.0154242515563965, Test_Loss: 4.020507335662842\n",
      "Epoch: 7, Train_Loss: 3.9924328327178955, Test_Loss: 4.153683185577393\n",
      "Epoch: 7, Train_Loss: 3.9894590377807617, Test_Loss: 4.605302810668945\n",
      "Epoch: 7, Train_Loss: 3.9872448444366455, Test_Loss: 4.392734050750732 *\n",
      "Epoch: 7, Train_Loss: 3.988018035888672, Test_Loss: 4.120219707489014 *\n",
      "Epoch: 7, Train_Loss: 3.998389959335327, Test_Loss: 4.022572040557861 *\n",
      "Epoch: 7, Train_Loss: 3.9930989742279053, Test_Loss: 4.00390100479126 *\n",
      "Epoch: 7, Train_Loss: 3.9892425537109375, Test_Loss: 4.058191776275635\n",
      "Epoch: 7, Train_Loss: 3.986233711242676, Test_Loss: 4.508269786834717\n",
      "Epoch: 7, Train_Loss: 3.984438180923462, Test_Loss: 5.467109203338623\n",
      "Epoch: 7, Train_Loss: 4.057209014892578, Test_Loss: 4.461345195770264 *\n",
      "Epoch: 7, Train_Loss: 4.01345157623291, Test_Loss: 4.084326267242432 *\n",
      "Epoch: 7, Train_Loss: 4.027644157409668, Test_Loss: 3.9861068725585938 *\n",
      "Epoch: 7, Train_Loss: 3.9820683002471924, Test_Loss: 3.9870073795318604\n",
      "Epoch: 7, Train_Loss: 3.9848644733428955, Test_Loss: 3.974776029586792 *\n",
      "Epoch: 7, Train_Loss: 4.0357136726379395, Test_Loss: 3.9873757362365723\n",
      "Epoch: 7, Train_Loss: 3.9767203330993652, Test_Loss: 4.013799667358398\n",
      "Epoch: 7, Train_Loss: 3.99099063873291, Test_Loss: 4.028151035308838\n",
      "Epoch: 7, Train_Loss: 4.01234245300293, Test_Loss: 3.9778950214385986 *\n",
      "Epoch: 7, Train_Loss: 4.030770778656006, Test_Loss: 4.057831764221191\n",
      "Epoch: 7, Train_Loss: 4.073419570922852, Test_Loss: 4.205172061920166\n",
      "Epoch: 7, Train_Loss: 4.0435099601745605, Test_Loss: 4.196889877319336 *\n",
      "Epoch: 7, Train_Loss: 3.9990339279174805, Test_Loss: 4.160305023193359 *\n",
      "Epoch: 7, Train_Loss: 3.973475933074951, Test_Loss: 3.9713521003723145 *\n",
      "Epoch: 7, Train_Loss: 3.997157573699951, Test_Loss: 3.969616651535034 *\n",
      "Epoch: 7, Train_Loss: 3.9688754081726074, Test_Loss: 3.9692928791046143 *\n",
      "Epoch: 7, Train_Loss: 3.969578504562378, Test_Loss: 3.9792168140411377\n",
      "Epoch: 7, Train_Loss: 3.9718141555786133, Test_Loss: 3.983208179473877\n",
      "Epoch: 7, Train_Loss: 3.977440595626831, Test_Loss: 6.781991481781006\n",
      "Epoch: 7, Train_Loss: 4.036334037780762, Test_Loss: 6.624424457550049 *\n",
      "Epoch: 7, Train_Loss: 3.993685483932495, Test_Loss: 3.971184492111206 *\n",
      "Epoch: 7, Train_Loss: 4.020027160644531, Test_Loss: 3.961869955062866 *\n",
      "Epoch: 7, Train_Loss: 3.9888198375701904, Test_Loss: 3.9600000381469727 *\n",
      "Epoch: 7, Train_Loss: 3.982313394546509, Test_Loss: 3.9692869186401367\n",
      "Epoch: 7, Train_Loss: 3.969433069229126, Test_Loss: 3.956786632537842 *\n",
      "Epoch: 7, Train_Loss: 4.157264709472656, Test_Loss: 3.9592607021331787\n",
      "Epoch: 7, Train_Loss: 4.074112415313721, Test_Loss: 3.962642192840576\n",
      "Epoch: 7, Train_Loss: 3.957589626312256, Test_Loss: 3.9602866172790527 *\n",
      "Epoch: 7, Train_Loss: 3.983976125717163, Test_Loss: 3.9579756259918213 *\n",
      "Epoch: 7, Train_Loss: 3.9515302181243896, Test_Loss: 3.9554755687713623 *\n",
      "Epoch: 7, Train_Loss: 3.948854684829712, Test_Loss: 3.963045597076416\n",
      "Epoch: 7, Train_Loss: 3.9524197578430176, Test_Loss: 3.9830422401428223\n",
      "Epoch: 7, Train_Loss: 3.9569764137268066, Test_Loss: 3.9680697917938232 *\n",
      "Epoch: 7, Train_Loss: 3.9628169536590576, Test_Loss: 3.952383279800415 *\n",
      "Epoch: 7, Train_Loss: 3.9737582206726074, Test_Loss: 3.950169086456299 *\n",
      "Epoch: 7, Train_Loss: 3.965651273727417, Test_Loss: 3.949305295944214 *\n",
      "Epoch: 7, Train_Loss: 3.966184377670288, Test_Loss: 3.944283962249756 *\n",
      "Epoch: 7, Train_Loss: 3.9752919673919678, Test_Loss: 3.94511079788208\n",
      "Epoch: 7, Train_Loss: 3.943563461303711, Test_Loss: 3.9519331455230713\n",
      "Epoch: 7, Train_Loss: 3.9471096992492676, Test_Loss: 3.943580389022827 *\n",
      "Epoch: 7, Train_Loss: 3.939682960510254, Test_Loss: 3.949767589569092\n",
      "Epoch: 7, Train_Loss: 3.9692022800445557, Test_Loss: 3.956638813018799\n",
      "Epoch: 7, Train_Loss: 3.971381664276123, Test_Loss: 3.943201780319214 *\n",
      "Epoch: 7, Train_Loss: 3.9433531761169434, Test_Loss: 3.94665789604187\n",
      "Epoch: 7, Train_Loss: 3.963101387023926, Test_Loss: 3.9420084953308105 *\n",
      "Epoch: 7, Train_Loss: 4.009010314941406, Test_Loss: 3.9420087337493896\n",
      "Epoch: 7, Train_Loss: 4.000055313110352, Test_Loss: 3.937453031539917 *\n",
      "Epoch: 7, Train_Loss: 3.9412643909454346, Test_Loss: 3.9361727237701416 *\n",
      "Epoch: 7, Train_Loss: 3.953186511993408, Test_Loss: 3.976855754852295\n",
      "Epoch: 7, Train_Loss: 3.9457476139068604, Test_Loss: 3.9646358489990234 *\n",
      "Epoch: 7, Train_Loss: 3.946195125579834, Test_Loss: 8.836427688598633\n",
      "Epoch: 7, Train_Loss: 3.9349429607391357, Test_Loss: 4.447625160217285 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 3.9618077278137207, Test_Loss: 3.9301226139068604 *\n",
      "Epoch: 7, Train_Loss: 3.9806926250457764, Test_Loss: 3.9600486755371094\n",
      "Epoch: 7, Train_Loss: 6.206787109375, Test_Loss: 3.993069648742676\n",
      "Epoch: 7, Train_Loss: 7.000945091247559, Test_Loss: 3.994149684906006\n",
      "Epoch: 7, Train_Loss: 3.9516570568084717, Test_Loss: 3.935652732849121 *\n",
      "Epoch: 7, Train_Loss: 3.9333224296569824, Test_Loss: 4.032690525054932\n",
      "Epoch: 7, Train_Loss: 4.018951892852783, Test_Loss: 4.001063346862793 *\n",
      "Epoch: 7, Train_Loss: 4.110385417938232, Test_Loss: 3.9249658584594727 *\n",
      "Epoch: 7, Train_Loss: 3.953347682952881, Test_Loss: 3.971266984939575\n",
      "Epoch: 7, Train_Loss: 3.9277172088623047, Test_Loss: 3.9392895698547363 *\n",
      "Epoch: 7, Train_Loss: 3.940131425857544, Test_Loss: 3.938059091567993 *\n",
      "Epoch: 7, Train_Loss: 3.9870247840881348, Test_Loss: 3.9265174865722656 *\n",
      "Epoch: 7, Train_Loss: 3.9284417629241943, Test_Loss: 4.065701961517334\n",
      "Epoch: 7, Train_Loss: 3.9269821643829346, Test_Loss: 3.953869342803955 *\n",
      "Epoch: 7, Train_Loss: 4.9806904792785645, Test_Loss: 4.047563552856445\n",
      "Epoch: 7, Train_Loss: 5.386929988861084, Test_Loss: 3.9928736686706543 *\n",
      "Epoch: 7, Train_Loss: 4.334651470184326, Test_Loss: 3.958967924118042 *\n",
      "Epoch: 7, Train_Loss: 4.0088958740234375, Test_Loss: 3.9335358142852783 *\n",
      "Epoch: 7, Train_Loss: 5.1258463859558105, Test_Loss: 3.940770149230957\n",
      "Epoch: 7, Train_Loss: 6.04789924621582, Test_Loss: 3.9244344234466553 *\n",
      "Epoch: 7, Train_Loss: 4.058140277862549, Test_Loss: 3.928107738494873\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 3.9455788135528564, Test_Loss: 3.9239110946655273 *\n",
      "Epoch: 7, Train_Loss: 3.9882633686065674, Test_Loss: 3.931826591491699\n",
      "Epoch: 7, Train_Loss: 5.588072299957275, Test_Loss: 3.9219913482666016 *\n",
      "Epoch: 7, Train_Loss: 5.412363052368164, Test_Loss: 3.9405517578125\n",
      "Epoch: 7, Train_Loss: 3.9363584518432617, Test_Loss: 3.9492313861846924\n",
      "Epoch: 7, Train_Loss: 3.9380815029144287, Test_Loss: 3.9157235622406006 *\n",
      "Epoch: 7, Train_Loss: 3.918524980545044, Test_Loss: 3.918539047241211\n",
      "Epoch: 7, Train_Loss: 4.58515739440918, Test_Loss: 3.9839725494384766\n",
      "Epoch: 7, Train_Loss: 3.959456205368042, Test_Loss: 3.939488172531128 *\n",
      "Epoch: 7, Train_Loss: 3.955744981765747, Test_Loss: 4.213665962219238\n",
      "Epoch: 7, Train_Loss: 3.9119536876678467, Test_Loss: 3.9157536029815674 *\n",
      "Epoch: 7, Train_Loss: 4.096665382385254, Test_Loss: 3.9547481536865234\n",
      "Epoch: 7, Train_Loss: 4.07301664352417, Test_Loss: 3.9886677265167236\n",
      "Epoch: 7, Train_Loss: 4.132111549377441, Test_Loss: 4.310912609100342\n",
      "Epoch: 7, Train_Loss: 4.213125228881836, Test_Loss: 3.9635677337646484 *\n",
      "Epoch: 7, Train_Loss: 3.9721415042877197, Test_Loss: 4.016556262969971\n",
      "Epoch: 7, Train_Loss: 4.0341081619262695, Test_Loss: 4.144802570343018\n",
      "Epoch: 7, Train_Loss: 4.173311710357666, Test_Loss: 4.113785743713379 *\n",
      "Epoch: 7, Train_Loss: 4.383406639099121, Test_Loss: 3.9254631996154785 *\n",
      "Epoch: 7, Train_Loss: 4.283794403076172, Test_Loss: 3.9165658950805664 *\n",
      "Epoch: 7, Train_Loss: 3.918856620788574, Test_Loss: 3.8967034816741943 *\n",
      "Epoch: 7, Train_Loss: 4.0251994132995605, Test_Loss: 3.937084674835205\n",
      "Epoch: 7, Train_Loss: 4.040834903717041, Test_Loss: 4.075070858001709\n",
      "Epoch: 7, Train_Loss: 3.908134698867798, Test_Loss: 4.65392541885376\n",
      "Epoch: 7, Train_Loss: 3.9073033332824707, Test_Loss: 4.152016639709473 *\n",
      "Epoch: 7, Train_Loss: 3.890521287918091, Test_Loss: 4.72285795211792\n",
      "Epoch: 7, Train_Loss: 3.8884103298187256, Test_Loss: 4.2255449295043945 *\n",
      "Epoch: 7, Train_Loss: 3.9029150009155273, Test_Loss: 4.370485305786133\n",
      "Epoch: 7, Train_Loss: 3.8900976181030273, Test_Loss: 4.213475704193115 *\n",
      "Epoch: 7, Train_Loss: 3.9813549518585205, Test_Loss: 3.9093992710113525 *\n",
      "Epoch: 7, Train_Loss: 3.945028066635132, Test_Loss: 3.8994698524475098 *\n",
      "Epoch: 7, Train_Loss: 4.003249168395996, Test_Loss: 3.9243547916412354\n",
      "Epoch: 7, Train_Loss: 3.991140127182007, Test_Loss: 4.0017805099487305\n",
      "Epoch: 7, Train_Loss: 4.365115165710449, Test_Loss: 4.87540864944458\n",
      "Epoch: 7, Train_Loss: 3.8829052448272705, Test_Loss: 4.093321800231934 *\n",
      "Epoch: 7, Train_Loss: 3.9118058681488037, Test_Loss: 5.882216453552246\n",
      "Epoch: 7, Train_Loss: 4.105399131774902, Test_Loss: 4.444822311401367 *\n",
      "Epoch: 7, Train_Loss: 4.392081260681152, Test_Loss: 4.9541778564453125\n",
      "Epoch: 7, Train_Loss: 4.162590980529785, Test_Loss: 4.001698017120361 *\n",
      "Epoch: 7, Train_Loss: 3.8869338035583496, Test_Loss: 3.8851046562194824 *\n",
      "Epoch: 7, Train_Loss: 4.097136974334717, Test_Loss: 4.0519890785217285\n",
      "Epoch: 7, Train_Loss: 4.49347448348999, Test_Loss: 5.19681453704834\n",
      "Epoch: 7, Train_Loss: 4.4231486320495605, Test_Loss: 4.558812618255615 *\n",
      "Epoch: 7, Train_Loss: 3.920787811279297, Test_Loss: 3.9777190685272217 *\n",
      "Epoch: 7, Train_Loss: 3.8861453533172607, Test_Loss: 3.8991448879241943 *\n",
      "Epoch: 7, Train_Loss: 3.892987012863159, Test_Loss: 3.94482421875\n",
      "Epoch: 7, Train_Loss: 5.232881546020508, Test_Loss: 4.243602752685547\n",
      "Epoch: 7, Train_Loss: 4.820956707000732, Test_Loss: 4.093161582946777 *\n",
      "Epoch: 7, Train_Loss: 3.871103048324585, Test_Loss: 5.04683256149292\n",
      "Epoch: 7, Train_Loss: 3.896632671356201, Test_Loss: 4.572116374969482 *\n",
      "Epoch: 7, Train_Loss: 3.8639872074127197, Test_Loss: 3.926478862762451 *\n",
      "Epoch: 7, Train_Loss: 3.957977771759033, Test_Loss: 3.883254051208496 *\n",
      "Epoch: 7, Train_Loss: 4.238531589508057, Test_Loss: 3.8834807872772217\n",
      "Epoch: 7, Train_Loss: 3.900400400161743, Test_Loss: 3.8842432498931885\n",
      "Epoch: 8, Train_Loss: 3.8949968814849854, Test_Loss: 3.925062894821167 *\n",
      "Epoch: 8, Train_Loss: 3.8867764472961426, Test_Loss: 4.386673927307129\n",
      "Epoch: 8, Train_Loss: 3.8759913444519043, Test_Loss: 4.328071594238281 *\n",
      "Epoch: 8, Train_Loss: 21.260009765625, Test_Loss: 4.017455577850342 *\n",
      "Epoch: 8, Train_Loss: 3.8831608295440674, Test_Loss: 3.8805127143859863 *\n",
      "Epoch: 8, Train_Loss: 6.935871601104736, Test_Loss: 3.879848003387451 *\n",
      "Epoch: 8, Train_Loss: 5.0159807205200195, Test_Loss: 3.8948020935058594\n",
      "Epoch: 8, Train_Loss: 3.869588851928711, Test_Loss: 4.188653469085693\n",
      "Epoch: 8, Train_Loss: 3.913315534591675, Test_Loss: 5.3005900382995605\n",
      "Epoch: 8, Train_Loss: 11.638072967529297, Test_Loss: 4.63646125793457 *\n",
      "Epoch: 8, Train_Loss: 8.64341926574707, Test_Loss: 3.9153778553009033 *\n",
      "Epoch: 8, Train_Loss: 3.871411085128784, Test_Loss: 3.882683515548706 *\n",
      "Epoch: 8, Train_Loss: 4.263724327087402, Test_Loss: 3.8575477600097656 *\n",
      "Epoch: 8, Train_Loss: 9.747228622436523, Test_Loss: 3.869621753692627\n",
      "Epoch: 8, Train_Loss: 3.8752193450927734, Test_Loss: 3.868372917175293 *\n",
      "Epoch: 8, Train_Loss: 3.8634278774261475, Test_Loss: 3.9104814529418945\n",
      "Epoch: 8, Train_Loss: 3.8573977947235107, Test_Loss: 3.959430694580078\n",
      "Epoch: 8, Train_Loss: 3.855318784713745, Test_Loss: 3.8648953437805176 *\n",
      "Epoch: 8, Train_Loss: 3.861807346343994, Test_Loss: 3.8962552547454834\n",
      "Epoch: 8, Train_Loss: 3.8607358932495117, Test_Loss: 3.979206085205078\n",
      "Epoch: 8, Train_Loss: 3.86965274810791, Test_Loss: 4.1701741218566895\n",
      "Epoch: 8, Train_Loss: 3.8602569103240967, Test_Loss: 3.998997449874878 *\n",
      "Epoch: 8, Train_Loss: 3.8692121505737305, Test_Loss: 3.849505662918091 *\n",
      "Epoch: 8, Train_Loss: 3.8559017181396484, Test_Loss: 3.856468677520752\n",
      "Epoch: 8, Train_Loss: 3.8626441955566406, Test_Loss: 3.857551097869873\n",
      "Epoch: 8, Train_Loss: 3.856736660003662, Test_Loss: 3.8595597743988037\n",
      "Epoch: 8, Train_Loss: 3.923832416534424, Test_Loss: 3.8407537937164307 *\n",
      "Epoch: 8, Train_Loss: 3.9348034858703613, Test_Loss: 5.262849807739258\n",
      "Epoch: 8, Train_Loss: 3.8586466312408447, Test_Loss: 8.372912406921387\n",
      "Epoch: 8, Train_Loss: 3.8566462993621826, Test_Loss: 3.8596742153167725 *\n",
      "Epoch: 8, Train_Loss: 3.837348222732544, Test_Loss: 3.846587896347046 *\n",
      "Epoch: 8, Train_Loss: 3.846111297607422, Test_Loss: 3.8582139015197754\n",
      "Epoch: 8, Train_Loss: 3.8358118534088135, Test_Loss: 3.8320796489715576 *\n",
      "Epoch: 8, Train_Loss: 3.8506741523742676, Test_Loss: 3.853835344314575\n",
      "Epoch: 8, Train_Loss: 3.835763692855835, Test_Loss: 3.845266819000244 *\n",
      "Epoch: 8, Train_Loss: 3.8380753993988037, Test_Loss: 3.873427152633667\n",
      "Epoch: 8, Train_Loss: 3.835157632827759, Test_Loss: 3.8532769680023193 *\n",
      "Epoch: 8, Train_Loss: 3.8432135581970215, Test_Loss: 3.86724591255188\n",
      "Epoch: 8, Train_Loss: 3.8303585052490234, Test_Loss: 3.8480207920074463 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 3.83024525642395, Test_Loss: 3.8517305850982666\n",
      "Epoch: 8, Train_Loss: 3.8380286693573, Test_Loss: 3.8433096408843994 *\n",
      "Epoch: 8, Train_Loss: 3.848161220550537, Test_Loss: 3.8467612266540527\n",
      "Epoch: 8, Train_Loss: 3.8390285968780518, Test_Loss: 3.858549118041992\n",
      "Epoch: 8, Train_Loss: 3.8351662158966064, Test_Loss: 3.8408689498901367 *\n",
      "Epoch: 8, Train_Loss: 8.848811149597168, Test_Loss: 3.834686517715454 *\n",
      "Epoch: 8, Train_Loss: 8.051024436950684, Test_Loss: 3.8262009620666504 *\n",
      "Epoch: 8, Train_Loss: 3.83652663230896, Test_Loss: 3.84613299369812\n",
      "Epoch: 8, Train_Loss: 3.8348538875579834, Test_Loss: 3.834712028503418 *\n",
      "Epoch: 8, Train_Loss: 3.8406436443328857, Test_Loss: 3.8383476734161377\n",
      "Epoch: 8, Train_Loss: 3.8427858352661133, Test_Loss: 3.8445329666137695\n",
      "Epoch: 8, Train_Loss: 3.84098482131958, Test_Loss: 3.849087953567505\n",
      "Epoch: 8, Train_Loss: 3.8333396911621094, Test_Loss: 3.8412044048309326 *\n",
      "Epoch: 8, Train_Loss: 3.863666296005249, Test_Loss: 3.8449513912200928\n",
      "Epoch: 8, Train_Loss: 4.0894455909729, Test_Loss: 3.8274455070495605 *\n",
      "Epoch: 8, Train_Loss: 3.9911162853240967, Test_Loss: 3.839630126953125\n",
      "Epoch: 8, Train_Loss: 3.885221004486084, Test_Loss: 3.8324759006500244 *\n",
      "Epoch: 8, Train_Loss: 3.896890640258789, Test_Loss: 3.8351166248321533\n",
      "Epoch: 8, Train_Loss: 3.920375108718872, Test_Loss: 3.8765640258789062\n",
      "Epoch: 8, Train_Loss: 3.947312355041504, Test_Loss: 3.851658821105957 *\n",
      "Epoch: 8, Train_Loss: 3.965770959854126, Test_Loss: 7.5964860916137695\n",
      "Epoch: 8, Train_Loss: 3.9191136360168457, Test_Loss: 5.948568344116211 *\n",
      "Epoch: 8, Train_Loss: 3.8904731273651123, Test_Loss: 3.813211679458618 *\n",
      "Epoch: 8, Train_Loss: 3.8080179691314697, Test_Loss: 3.8123786449432373 *\n",
      "Epoch: 8, Train_Loss: 3.870591640472412, Test_Loss: 3.8365492820739746\n",
      "Epoch: 8, Train_Loss: 3.839165687561035, Test_Loss: 3.8452823162078857\n",
      "Epoch: 8, Train_Loss: 3.8093442916870117, Test_Loss: 3.820718765258789 *\n",
      "Epoch: 8, Train_Loss: 3.807776927947998, Test_Loss: 3.8887014389038086\n",
      "Epoch: 8, Train_Loss: 3.805706739425659, Test_Loss: 3.9101691246032715\n",
      "Epoch: 8, Train_Loss: 3.814091444015503, Test_Loss: 3.7996785640716553 *\n",
      "Epoch: 8, Train_Loss: 8.220551490783691, Test_Loss: 3.8426730632781982\n",
      "Epoch: 8, Train_Loss: 5.2214789390563965, Test_Loss: 3.810462236404419 *\n",
      "Epoch: 8, Train_Loss: 3.805553674697876, Test_Loss: 3.811753511428833\n",
      "Epoch: 8, Train_Loss: 3.8151707649230957, Test_Loss: 3.7982852458953857 *\n",
      "Epoch: 8, Train_Loss: 3.829014301300049, Test_Loss: 3.8699705600738525\n",
      "Epoch: 8, Train_Loss: 3.816340446472168, Test_Loss: 3.834648847579956 *\n",
      "Epoch: 8, Train_Loss: 3.8017635345458984, Test_Loss: 3.9215033054351807\n",
      "Epoch: 8, Train_Loss: 3.810218334197998, Test_Loss: 3.9145944118499756 *\n",
      "Epoch: 8, Train_Loss: 3.817909002304077, Test_Loss: 3.7982568740844727 *\n",
      "Epoch: 8, Train_Loss: 3.8163909912109375, Test_Loss: 3.792088508605957 *\n",
      "Epoch: 8, Train_Loss: 3.81246018409729, Test_Loss: 3.7926480770111084\n",
      "Epoch: 8, Train_Loss: 3.799781560897827, Test_Loss: 3.7903106212615967 *\n",
      "Epoch: 8, Train_Loss: 3.7881762981414795, Test_Loss: 3.787336826324463 *\n",
      "Epoch: 8, Train_Loss: 3.807995080947876, Test_Loss: 3.7847959995269775 *\n",
      "Epoch: 8, Train_Loss: 3.791443109512329, Test_Loss: 3.788214921951294\n",
      "Epoch: 8, Train_Loss: 3.786703586578369, Test_Loss: 3.7860376834869385 *\n",
      "Epoch: 8, Train_Loss: 3.7916667461395264, Test_Loss: 3.7945477962493896\n",
      "Epoch: 8, Train_Loss: 3.844022512435913, Test_Loss: 3.789874792098999 *\n",
      "Epoch: 8, Train_Loss: 3.82806658744812, Test_Loss: 3.7887234687805176 *\n",
      "Epoch: 8, Train_Loss: 3.7815394401550293, Test_Loss: 3.789229154586792\n",
      "Epoch: 8, Train_Loss: 3.787229061126709, Test_Loss: 3.8090479373931885\n",
      "Epoch: 8, Train_Loss: 3.815572738647461, Test_Loss: 3.813764810562134\n",
      "Epoch: 8, Train_Loss: 3.8861448764801025, Test_Loss: 4.11970329284668\n",
      "Epoch: 8, Train_Loss: 3.8506569862365723, Test_Loss: 3.8061282634735107 *\n",
      "Epoch: 8, Train_Loss: 3.840247869491577, Test_Loss: 3.8022258281707764 *\n",
      "Epoch: 8, Train_Loss: 3.808788299560547, Test_Loss: 3.8458263874053955\n",
      "Epoch: 8, Train_Loss: 3.836650848388672, Test_Loss: 4.042115211486816\n",
      "Epoch: 8, Train_Loss: 3.8242084980010986, Test_Loss: 3.8678903579711914 *\n",
      "Epoch: 8, Train_Loss: 3.8134355545043945, Test_Loss: 3.828282117843628 *\n",
      "Epoch: 8, Train_Loss: 3.8678808212280273, Test_Loss: 4.04396915435791\n",
      "Epoch: 8, Train_Loss: 3.81086802482605, Test_Loss: 4.076955318450928\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 3.7796452045440674, Test_Loss: 3.7872774600982666 *\n",
      "Epoch: 8, Train_Loss: 3.771077871322632, Test_Loss: 3.822131872177124\n",
      "Epoch: 8, Train_Loss: 3.773423433303833, Test_Loss: 3.769425392150879 *\n",
      "Epoch: 8, Train_Loss: 3.7631614208221436, Test_Loss: 3.7930991649627686\n",
      "Epoch: 8, Train_Loss: 3.7649190425872803, Test_Loss: 3.811889410018921\n",
      "Epoch: 8, Train_Loss: 3.7698450088500977, Test_Loss: 4.625883102416992\n",
      "Epoch: 8, Train_Loss: 7.515140533447266, Test_Loss: 3.9151930809020996 *\n",
      "Epoch: 8, Train_Loss: 4.939762115478516, Test_Loss: 4.614173412322998\n",
      "Epoch: 8, Train_Loss: 3.7578980922698975, Test_Loss: 4.316629409790039 *\n",
      "Epoch: 8, Train_Loss: 3.7729384899139404, Test_Loss: 4.126547813415527 *\n",
      "Epoch: 8, Train_Loss: 3.7630672454833984, Test_Loss: 4.216689109802246\n",
      "Epoch: 8, Train_Loss: 3.7532942295074463, Test_Loss: 3.804570436477661 *\n",
      "Epoch: 8, Train_Loss: 3.7539570331573486, Test_Loss: 3.770012140274048 *\n",
      "Epoch: 8, Train_Loss: 3.7531394958496094, Test_Loss: 3.7782747745513916\n",
      "Epoch: 8, Train_Loss: 3.7452847957611084, Test_Loss: 3.9276723861694336\n",
      "Epoch: 8, Train_Loss: 3.75321364402771, Test_Loss: 4.40083646774292\n",
      "Epoch: 8, Train_Loss: 3.7920432090759277, Test_Loss: 4.18184757232666 *\n",
      "Epoch: 8, Train_Loss: 3.827077865600586, Test_Loss: 5.421670913696289\n",
      "Epoch: 8, Train_Loss: 3.841555118560791, Test_Loss: 4.637229919433594 *\n",
      "Epoch: 8, Train_Loss: 3.839402675628662, Test_Loss: 4.782299041748047\n",
      "Epoch: 8, Train_Loss: 3.75931978225708, Test_Loss: 4.020815372467041 *\n",
      "Epoch: 8, Train_Loss: 3.773409366607666, Test_Loss: 3.7480499744415283 *\n",
      "Epoch: 8, Train_Loss: 3.9555110931396484, Test_Loss: 3.8135149478912354\n",
      "Epoch: 8, Train_Loss: 3.9686803817749023, Test_Loss: 4.848937034606934\n",
      "Epoch: 8, Train_Loss: 3.942063093185425, Test_Loss: 4.720285892486572 *\n",
      "Epoch: 8, Train_Loss: 3.7830448150634766, Test_Loss: 3.8077123165130615 *\n",
      "Epoch: 8, Train_Loss: 3.740067481994629, Test_Loss: 3.809311866760254\n",
      "Epoch: 8, Train_Loss: 3.749192953109741, Test_Loss: 3.7776365280151367 *\n",
      "Epoch: 8, Train_Loss: 3.753453016281128, Test_Loss: 4.148766040802002\n",
      "Epoch: 8, Train_Loss: 3.749154567718506, Test_Loss: 3.8971478939056396 *\n",
      "Epoch: 8, Train_Loss: 3.743466854095459, Test_Loss: 4.616277694702148\n",
      "Epoch: 8, Train_Loss: 3.744490146636963, Test_Loss: 4.648265361785889\n",
      "Epoch: 8, Train_Loss: 3.73771595954895, Test_Loss: 3.9313182830810547 *\n",
      "Epoch: 8, Train_Loss: 3.7317044734954834, Test_Loss: 3.744175434112549 *\n",
      "Epoch: 8, Train_Loss: 3.738454818725586, Test_Loss: 3.7482268810272217\n",
      "Epoch: 8, Train_Loss: 3.8490071296691895, Test_Loss: 3.757413387298584\n",
      "Epoch: 8, Train_Loss: 3.927553653717041, Test_Loss: 3.7732131481170654\n",
      "Epoch: 8, Train_Loss: 3.927379846572876, Test_Loss: 4.183095455169678\n",
      "Epoch: 8, Train_Loss: 3.8064019680023193, Test_Loss: 4.2939019203186035\n",
      "Epoch: 8, Train_Loss: 3.8555145263671875, Test_Loss: 3.901277542114258 *\n",
      "Epoch: 8, Train_Loss: 3.895562171936035, Test_Loss: 3.7599658966064453 *\n",
      "Epoch: 8, Train_Loss: 3.7365238666534424, Test_Loss: 3.7589287757873535 *\n",
      "Epoch: 8, Train_Loss: 3.899339437484741, Test_Loss: 3.7474563121795654 *\n",
      "Epoch: 8, Train_Loss: 3.8269217014312744, Test_Loss: 3.883312463760376\n",
      "Epoch: 8, Train_Loss: 3.9874320030212402, Test_Loss: 4.922209739685059\n",
      "Epoch: 8, Train_Loss: 3.7299323081970215, Test_Loss: 4.914693832397461 *\n",
      "Epoch: 8, Train_Loss: 4.552370548248291, Test_Loss: 3.773265838623047 *\n",
      "Epoch: 8, Train_Loss: 6.117730140686035, Test_Loss: 3.808669328689575\n",
      "Epoch: 8, Train_Loss: 3.758802652359009, Test_Loss: 3.7233059406280518 *\n",
      "Epoch: 8, Train_Loss: 3.774462938308716, Test_Loss: 3.7271358966827393\n",
      "Epoch: 8, Train_Loss: 3.7851829528808594, Test_Loss: 3.721851348876953 *\n",
      "Epoch: 8, Train_Loss: 3.774453639984131, Test_Loss: 3.744030237197876\n",
      "Epoch: 8, Train_Loss: 3.7196288108825684, Test_Loss: 3.7789652347564697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 3.7167131900787354, Test_Loss: 3.727541923522949 *\n",
      "Epoch: 8, Train_Loss: 3.8161990642547607, Test_Loss: 3.732063055038452\n",
      "Epoch: 8, Train_Loss: 3.7995738983154297, Test_Loss: 3.8461697101593018\n",
      "Epoch: 8, Train_Loss: 3.8185856342315674, Test_Loss: 4.140058994293213\n",
      "Epoch: 8, Train_Loss: 3.7787744998931885, Test_Loss: 3.9290425777435303 *\n",
      "Epoch: 8, Train_Loss: 3.777930736541748, Test_Loss: 3.7319538593292236 *\n",
      "Epoch: 8, Train_Loss: 3.7342498302459717, Test_Loss: 3.7207725048065186 *\n",
      "Epoch: 8, Train_Loss: 3.736351251602173, Test_Loss: 3.723626136779785\n",
      "Epoch: 8, Train_Loss: 3.7288713455200195, Test_Loss: 3.720857620239258 *\n",
      "Epoch: 8, Train_Loss: 3.7338969707489014, Test_Loss: 3.723799228668213\n",
      "Epoch: 8, Train_Loss: 3.716062068939209, Test_Loss: 3.974557876586914\n",
      "Epoch: 8, Train_Loss: 3.7081775665283203, Test_Loss: 8.72260856628418\n",
      "Epoch: 8, Train_Loss: 3.744924306869507, Test_Loss: 3.7743618488311768 *\n",
      "Epoch: 8, Train_Loss: 3.7629644870758057, Test_Loss: 3.709796905517578 *\n",
      "Epoch: 8, Train_Loss: 3.732977867126465, Test_Loss: 3.7109196186065674\n",
      "Epoch: 8, Train_Loss: 3.703646659851074, Test_Loss: 3.719409704208374\n",
      "Epoch: 8, Train_Loss: 3.7051103115081787, Test_Loss: 3.7098567485809326 *\n",
      "Epoch: 8, Train_Loss: 3.7029736042022705, Test_Loss: 3.708980083465576 *\n",
      "Epoch: 8, Train_Loss: 3.7037603855133057, Test_Loss: 3.703374147415161 *\n",
      "Epoch: 8, Train_Loss: 3.7004809379577637, Test_Loss: 3.6979823112487793 *\n",
      "Epoch: 8, Train_Loss: 3.6988277435302734, Test_Loss: 3.6973235607147217 *\n",
      "Epoch: 8, Train_Loss: 3.696178913116455, Test_Loss: 3.6942296028137207 *\n",
      "Epoch: 8, Train_Loss: 3.698143243789673, Test_Loss: 3.7025835514068604\n",
      "Epoch: 8, Train_Loss: 3.6956980228424072, Test_Loss: 3.729924201965332\n",
      "Epoch: 8, Train_Loss: 3.691188335418701, Test_Loss: 3.724595546722412 *\n",
      "Epoch: 8, Train_Loss: 3.7051758766174316, Test_Loss: 3.7059054374694824 *\n",
      "Epoch: 8, Train_Loss: 3.7016074657440186, Test_Loss: 3.6964526176452637 *\n",
      "Epoch: 8, Train_Loss: 3.702665090560913, Test_Loss: 3.685011863708496 *\n",
      "Epoch: 8, Train_Loss: 3.7133312225341797, Test_Loss: 3.6881206035614014\n",
      "Epoch: 8, Train_Loss: 3.699650287628174, Test_Loss: 3.690849542617798\n",
      "Epoch: 8, Train_Loss: 3.686896800994873, Test_Loss: 3.6904098987579346 *\n",
      "Epoch: 8, Train_Loss: 3.687394380569458, Test_Loss: 3.686772346496582 *\n",
      "Epoch: 8, Train_Loss: 3.6859991550445557, Test_Loss: 3.6889917850494385\n",
      "Epoch: 8, Train_Loss: 3.694756031036377, Test_Loss: 3.6835615634918213 *\n",
      "Epoch: 8, Train_Loss: 3.6925036907196045, Test_Loss: 3.6814651489257812 *\n",
      "Epoch: 8, Train_Loss: 3.683209180831909, Test_Loss: 3.688072443008423\n",
      "Epoch: 8, Train_Loss: 3.678558588027954, Test_Loss: 3.684654951095581 *\n",
      "Epoch: 8, Train_Loss: 3.689718246459961, Test_Loss: 3.688044786453247\n",
      "Epoch: 8, Train_Loss: 3.750117540359497, Test_Loss: 3.6821846961975098 *\n",
      "Epoch: 8, Train_Loss: 3.70749568939209, Test_Loss: 3.684234380722046\n",
      "Epoch: 8, Train_Loss: 3.732384204864502, Test_Loss: 3.684070587158203 *\n",
      "Epoch: 8, Train_Loss: 3.67364239692688, Test_Loss: 3.736891031265259\n",
      "Epoch: 8, Train_Loss: 3.707336187362671, Test_Loss: 5.942879676818848\n",
      "Epoch: 8, Train_Loss: 3.7084455490112305, Test_Loss: 6.8509297370910645\n",
      "Epoch: 8, Train_Loss: 3.6737077236175537, Test_Loss: 3.682936191558838 *\n",
      "Epoch: 8, Train_Loss: 3.693168878555298, Test_Loss: 3.679218053817749 *\n",
      "Epoch: 8, Train_Loss: 3.698117733001709, Test_Loss: 3.725316047668457\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 3.7548394203186035, Test_Loss: 3.7323317527770996\n",
      "Epoch: 8, Train_Loss: 3.748194932937622, Test_Loss: 3.723320484161377 *\n",
      "Epoch: 8, Train_Loss: 3.751197576522827, Test_Loss: 3.7105414867401123 *\n",
      "Epoch: 8, Train_Loss: 3.699004650115967, Test_Loss: 3.782878875732422\n",
      "Epoch: 8, Train_Loss: 3.677171468734741, Test_Loss: 3.673692464828491 *\n",
      "Epoch: 8, Train_Loss: 3.6937062740325928, Test_Loss: 3.690793991088867\n",
      "Epoch: 8, Train_Loss: 3.661696672439575, Test_Loss: 3.6904211044311523 *\n",
      "Epoch: 8, Train_Loss: 3.675962448120117, Test_Loss: 3.6816415786743164 *\n",
      "Epoch: 8, Train_Loss: 3.676248550415039, Test_Loss: 3.6703529357910156 *\n",
      "Epoch: 8, Train_Loss: 3.6819911003112793, Test_Loss: 3.745762586593628\n",
      "Epoch: 8, Train_Loss: 3.778449535369873, Test_Loss: 3.7647812366485596\n",
      "Epoch: 8, Train_Loss: 3.6706528663635254, Test_Loss: 3.7388370037078857 *\n",
      "Epoch: 8, Train_Loss: 3.7352547645568848, Test_Loss: 3.783053398132324\n",
      "Epoch: 8, Train_Loss: 3.660249948501587, Test_Loss: 3.6795003414154053 *\n",
      "Epoch: 8, Train_Loss: 3.6945879459381104, Test_Loss: 3.6939609050750732\n",
      "Epoch: 8, Train_Loss: 3.668931245803833, Test_Loss: 3.6742210388183594 *\n",
      "Epoch: 8, Train_Loss: 3.929877281188965, Test_Loss: 3.664877414703369 *\n",
      "Epoch: 8, Train_Loss: 3.7033302783966064, Test_Loss: 3.666630983352661\n",
      "Epoch: 8, Train_Loss: 3.6831772327423096, Test_Loss: 3.6789262294769287\n",
      "Epoch: 8, Train_Loss: 3.6634891033172607, Test_Loss: 3.6639318466186523 *\n",
      "Epoch: 8, Train_Loss: 3.656827688217163, Test_Loss: 3.6747355461120605\n",
      "Epoch: 8, Train_Loss: 3.6541836261749268, Test_Loss: 3.673971652984619 *\n",
      "Epoch: 8, Train_Loss: 3.6507408618927, Test_Loss: 3.6700429916381836 *\n",
      "Epoch: 8, Train_Loss: 3.657742500305176, Test_Loss: 3.6719110012054443\n",
      "Epoch: 8, Train_Loss: 3.659346580505371, Test_Loss: 3.6471827030181885 *\n",
      "Epoch: 8, Train_Loss: 3.6727006435394287, Test_Loss: 3.677143096923828\n",
      "Epoch: 8, Train_Loss: 3.6678695678710938, Test_Loss: 3.7077677249908447\n",
      "Epoch: 8, Train_Loss: 3.66705584526062, Test_Loss: 3.9107837677001953\n",
      "Epoch: 8, Train_Loss: 3.667902708053589, Test_Loss: 3.7219979763031006 *\n",
      "Epoch: 8, Train_Loss: 3.6481142044067383, Test_Loss: 3.6773269176483154 *\n",
      "Epoch: 8, Train_Loss: 3.644705295562744, Test_Loss: 3.741219997406006\n",
      "Epoch: 8, Train_Loss: 3.646711826324463, Test_Loss: 3.8631229400634766\n",
      "Epoch: 8, Train_Loss: 3.676985263824463, Test_Loss: 3.858135461807251 *\n",
      "Epoch: 8, Train_Loss: 3.6714532375335693, Test_Loss: 3.660074234008789 *\n",
      "Epoch: 8, Train_Loss: 3.6389529705047607, Test_Loss: 3.8575668334960938\n",
      "Epoch: 8, Train_Loss: 3.6726057529449463, Test_Loss: 3.9501357078552246\n",
      "Epoch: 8, Train_Loss: 3.6902620792388916, Test_Loss: 3.665562629699707 *\n",
      "Epoch: 8, Train_Loss: 3.691315174102783, Test_Loss: 3.68265700340271\n",
      "Epoch: 8, Train_Loss: 3.6349475383758545, Test_Loss: 3.64361834526062 *\n",
      "Epoch: 8, Train_Loss: 3.6660611629486084, Test_Loss: 3.668032646179199\n",
      "Epoch: 8, Train_Loss: 3.637160062789917, Test_Loss: 3.64548397064209 *\n",
      "Epoch: 8, Train_Loss: 3.6524276733398438, Test_Loss: 4.394172668457031\n",
      "Epoch: 8, Train_Loss: 3.6334879398345947, Test_Loss: 3.862314224243164 *\n",
      "Epoch: 8, Train_Loss: 3.655116081237793, Test_Loss: 4.410141944885254\n",
      "Epoch: 8, Train_Loss: 3.7057831287384033, Test_Loss: 4.398209095001221 *\n",
      "Epoch: 8, Train_Loss: 6.122194290161133, Test_Loss: 3.845649480819702 *\n",
      "Epoch: 8, Train_Loss: 6.605916976928711, Test_Loss: 4.194420337677002\n",
      "Epoch: 8, Train_Loss: 3.6516711711883545, Test_Loss: 3.714731216430664 *\n",
      "Epoch: 8, Train_Loss: 3.621764659881592, Test_Loss: 3.6289968490600586 *\n",
      "Epoch: 8, Train_Loss: 3.7447245121002197, Test_Loss: 3.646466016769409\n",
      "Epoch: 8, Train_Loss: 3.762233018875122, Test_Loss: 3.7961177825927734\n",
      "Epoch: 8, Train_Loss: 3.6464333534240723, Test_Loss: 3.968313455581665\n",
      "Epoch: 8, Train_Loss: 3.6208460330963135, Test_Loss: 4.395645618438721\n",
      "Epoch: 8, Train_Loss: 3.6679282188415527, Test_Loss: 4.684222221374512\n",
      "Epoch: 8, Train_Loss: 3.6674890518188477, Test_Loss: 5.060473442077637\n",
      "Epoch: 8, Train_Loss: 3.631425380706787, Test_Loss: 4.478013515472412 *\n",
      "Epoch: 8, Train_Loss: 3.6685988903045654, Test_Loss: 4.147637367248535 *\n",
      "Epoch: 8, Train_Loss: 4.890646934509277, Test_Loss: 3.618574619293213 *\n",
      "Epoch: 8, Train_Loss: 5.1153459548950195, Test_Loss: 3.6394615173339844\n",
      "Epoch: 8, Train_Loss: 3.789947032928467, Test_Loss: 4.401024341583252\n",
      "Epoch: 8, Train_Loss: 3.6809451580047607, Test_Loss: 4.984476089477539\n",
      "Epoch: 8, Train_Loss: 5.245161056518555, Test_Loss: 3.663363456726074 *\n",
      "Epoch: 8, Train_Loss: 5.423746109008789, Test_Loss: 3.698838233947754\n",
      "Epoch: 8, Train_Loss: 3.6544744968414307, Test_Loss: 3.6207966804504395 *\n",
      "Epoch: 8, Train_Loss: 3.652445077896118, Test_Loss: 3.8661069869995117\n",
      "Epoch: 8, Train_Loss: 3.832076072692871, Test_Loss: 3.874018907546997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 5.333784580230713, Test_Loss: 4.245162010192871\n",
      "Epoch: 8, Train_Loss: 4.960688591003418, Test_Loss: 4.746180534362793\n",
      "Epoch: 8, Train_Loss: 3.6245346069335938, Test_Loss: 3.902949810028076 *\n",
      "Epoch: 8, Train_Loss: 3.6311659812927246, Test_Loss: 3.605578899383545 *\n",
      "Epoch: 8, Train_Loss: 3.6803410053253174, Test_Loss: 3.612464189529419\n",
      "Epoch: 8, Train_Loss: 4.29067325592041, Test_Loss: 3.625683069229126\n",
      "Epoch: 8, Train_Loss: 3.6187517642974854, Test_Loss: 3.632817268371582\n",
      "Epoch: 8, Train_Loss: 3.6554114818573, Test_Loss: 3.9234371185302734\n",
      "Epoch: 8, Train_Loss: 3.6337385177612305, Test_Loss: 4.249527931213379\n",
      "Epoch: 8, Train_Loss: 3.765064239501953, Test_Loss: 3.8517982959747314 *\n",
      "Epoch: 8, Train_Loss: 3.746312379837036, Test_Loss: 3.6846559047698975 *\n",
      "Epoch: 8, Train_Loss: 3.9237124919891357, Test_Loss: 3.6551480293273926 *\n",
      "Epoch: 8, Train_Loss: 3.8726160526275635, Test_Loss: 3.607097864151001 *\n",
      "Epoch: 8, Train_Loss: 3.6619839668273926, Test_Loss: 3.699115753173828\n",
      "Epoch: 8, Train_Loss: 3.7687010765075684, Test_Loss: 4.460589408874512\n",
      "Epoch: 8, Train_Loss: 3.847170114517212, Test_Loss: 4.949182987213135\n",
      "Epoch: 8, Train_Loss: 4.10880184173584, Test_Loss: 3.702981948852539 *\n",
      "Epoch: 8, Train_Loss: 3.936485528945923, Test_Loss: 3.699530601501465 *\n",
      "Epoch: 8, Train_Loss: 3.627715826034546, Test_Loss: 3.5940024852752686 *\n",
      "Epoch: 8, Train_Loss: 3.7132534980773926, Test_Loss: 3.600454568862915\n",
      "Epoch: 8, Train_Loss: 3.7215933799743652, Test_Loss: 3.5956532955169678 *\n",
      "Epoch: 8, Train_Loss: 3.5975728034973145, Test_Loss: 3.6042985916137695\n",
      "Epoch: 8, Train_Loss: 3.6050734519958496, Test_Loss: 3.6313133239746094\n",
      "Epoch: 8, Train_Loss: 3.5865323543548584, Test_Loss: 3.626544237136841 *\n",
      "Epoch: 8, Train_Loss: 3.5891854763031006, Test_Loss: 3.5991733074188232 *\n",
      "Epoch: 8, Train_Loss: 3.590642213821411, Test_Loss: 3.6953179836273193\n",
      "Epoch: 8, Train_Loss: 3.592778205871582, Test_Loss: 3.9890856742858887\n",
      "Epoch: 8, Train_Loss: 3.664055585861206, Test_Loss: 3.685628652572632 *\n",
      "Epoch: 8, Train_Loss: 3.6541481018066406, Test_Loss: 3.71886944770813\n",
      "Epoch: 8, Train_Loss: 3.7029430866241455, Test_Loss: 3.5838539600372314 *\n",
      "Epoch: 8, Train_Loss: 3.7087371349334717, Test_Loss: 3.585263729095459\n",
      "Epoch: 8, Train_Loss: 4.022921085357666, Test_Loss: 3.593780517578125\n",
      "Epoch: 8, Train_Loss: 3.583695888519287, Test_Loss: 3.593024730682373 *\n",
      "Epoch: 8, Train_Loss: 3.620218515396118, Test_Loss: 3.60532546043396\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 3.8621134757995605, Test_Loss: 8.648193359375\n",
      "Epoch: 8, Train_Loss: 4.1523003578186035, Test_Loss: 3.966090202331543 *\n",
      "Epoch: 8, Train_Loss: 3.763259172439575, Test_Loss: 3.5825912952423096 *\n",
      "Epoch: 8, Train_Loss: 3.5787172317504883, Test_Loss: 3.5707507133483887 *\n",
      "Epoch: 8, Train_Loss: 3.913663387298584, Test_Loss: 3.5730817317962646\n",
      "Epoch: 8, Train_Loss: 4.188043117523193, Test_Loss: 3.576542377471924\n",
      "Epoch: 8, Train_Loss: 4.046173572540283, Test_Loss: 3.5775039196014404\n",
      "Epoch: 8, Train_Loss: 3.6059818267822266, Test_Loss: 3.573282480239868 *\n",
      "Epoch: 8, Train_Loss: 3.5966715812683105, Test_Loss: 3.574631452560425\n",
      "Epoch: 8, Train_Loss: 3.670689344406128, Test_Loss: 3.5738539695739746 *\n",
      "Epoch: 8, Train_Loss: 5.148414611816406, Test_Loss: 3.577388048171997\n",
      "Epoch: 8, Train_Loss: 4.331537246704102, Test_Loss: 3.580683946609497\n",
      "Epoch: 8, Train_Loss: 3.573901653289795, Test_Loss: 3.572948694229126 *\n",
      "Epoch: 8, Train_Loss: 3.591212034225464, Test_Loss: 3.5901167392730713\n",
      "Epoch: 8, Train_Loss: 3.5677642822265625, Test_Loss: 3.578260898590088 *\n",
      "Epoch: 8, Train_Loss: 3.7665417194366455, Test_Loss: 3.56341814994812 *\n",
      "Epoch: 8, Train_Loss: 3.8295211791992188, Test_Loss: 3.5622365474700928 *\n",
      "Epoch: 8, Train_Loss: 3.607093334197998, Test_Loss: 3.560236692428589 *\n",
      "Epoch: 8, Train_Loss: 3.5957865715026855, Test_Loss: 3.5599615573883057 *\n",
      "Epoch: 8, Train_Loss: 3.578933000564575, Test_Loss: 3.5582027435302734 *\n",
      "Epoch: 8, Train_Loss: 6.310641288757324, Test_Loss: 3.5667381286621094\n",
      "Epoch: 8, Train_Loss: 18.334861755371094, Test_Loss: 3.5582635402679443 *\n",
      "Epoch: 8, Train_Loss: 3.9612507820129395, Test_Loss: 3.5630545616149902\n",
      "Epoch: 8, Train_Loss: 6.850234508514404, Test_Loss: 3.565511465072632\n",
      "Epoch: 8, Train_Loss: 4.1612548828125, Test_Loss: 3.557584285736084 *\n",
      "Epoch: 8, Train_Loss: 3.6103227138519287, Test_Loss: 3.5557639598846436 *\n",
      "Epoch: 8, Train_Loss: 3.6105575561523438, Test_Loss: 3.55558705329895 *\n",
      "Epoch: 8, Train_Loss: 13.248323440551758, Test_Loss: 3.559457778930664\n",
      "Epoch: 8, Train_Loss: 6.312638282775879, Test_Loss: 3.5594191551208496 *\n",
      "Epoch: 8, Train_Loss: 3.567676305770874, Test_Loss: 3.566863536834717\n",
      "Epoch: 8, Train_Loss: 5.443150043487549, Test_Loss: 3.616485357284546\n",
      "Epoch: 8, Train_Loss: 8.01514720916748, Test_Loss: 4.430695056915283\n",
      "Epoch: 8, Train_Loss: 3.5763230323791504, Test_Loss: 8.533329963684082\n",
      "Epoch: 8, Train_Loss: 3.5622458457946777, Test_Loss: 3.569350481033325 *\n",
      "Epoch: 8, Train_Loss: 3.5583956241607666, Test_Loss: 3.5557749271392822 *\n",
      "Epoch: 8, Train_Loss: 3.5548906326293945, Test_Loss: 3.5724570751190186\n",
      "Epoch: 8, Train_Loss: 3.5604748725891113, Test_Loss: 3.5752217769622803\n",
      "Epoch: 8, Train_Loss: 3.5534508228302, Test_Loss: 3.5804221630096436\n",
      "Epoch: 8, Train_Loss: 3.5581905841827393, Test_Loss: 3.5864017009735107\n",
      "Epoch: 8, Train_Loss: 3.564854383468628, Test_Loss: 3.752936840057373\n",
      "Epoch: 8, Train_Loss: 3.5641472339630127, Test_Loss: 3.592820644378662 *\n",
      "Epoch: 8, Train_Loss: 3.5627665519714355, Test_Loss: 3.557004928588867 *\n",
      "Epoch: 8, Train_Loss: 3.5545763969421387, Test_Loss: 3.620454788208008\n",
      "Epoch: 8, Train_Loss: 3.564216375350952, Test_Loss: 3.54274320602417 *\n",
      "Epoch: 8, Train_Loss: 3.607834577560425, Test_Loss: 3.5724854469299316\n",
      "Epoch: 8, Train_Loss: 3.5994691848754883, Test_Loss: 3.5740091800689697\n",
      "Epoch: 8, Train_Loss: 3.5537495613098145, Test_Loss: 3.5988473892211914\n",
      "Epoch: 8, Train_Loss: 3.5532305240631104, Test_Loss: 3.647067070007324\n",
      "Epoch: 8, Train_Loss: 3.5390236377716064, Test_Loss: 3.70749568939209\n",
      "Epoch: 8, Train_Loss: 3.5315206050872803, Test_Loss: 3.5815515518188477 *\n",
      "Epoch: 8, Train_Loss: 3.5375938415527344, Test_Loss: 3.5469727516174316 *\n",
      "Epoch: 8, Train_Loss: 3.534775972366333, Test_Loss: 3.540040969848633 *\n",
      "Epoch: 8, Train_Loss: 3.538475275039673, Test_Loss: 3.5536820888519287\n",
      "Epoch: 8, Train_Loss: 3.5356359481811523, Test_Loss: 3.5334630012512207 *\n",
      "Epoch: 8, Train_Loss: 3.529414176940918, Test_Loss: 3.532794237136841 *\n",
      "Epoch: 8, Train_Loss: 3.5278780460357666, Test_Loss: 3.534539222717285\n",
      "Epoch: 8, Train_Loss: 3.5307788848876953, Test_Loss: 3.535731554031372\n",
      "Epoch: 8, Train_Loss: 3.52860951423645, Test_Loss: 3.535849094390869\n",
      "Epoch: 8, Train_Loss: 3.5333425998687744, Test_Loss: 3.5301673412323 *\n",
      "Epoch: 8, Train_Loss: 3.5384328365325928, Test_Loss: 3.539940357208252\n",
      "Epoch: 8, Train_Loss: 3.5310451984405518, Test_Loss: 3.533881425857544 *\n",
      "Epoch: 8, Train_Loss: 3.5245354175567627, Test_Loss: 3.5340139865875244\n",
      "Epoch: 8, Train_Loss: 11.215877532958984, Test_Loss: 3.565171957015991\n",
      "Epoch: 8, Train_Loss: 5.094320297241211, Test_Loss: 3.69498872756958\n",
      "Epoch: 8, Train_Loss: 3.5293893814086914, Test_Loss: 3.7867534160614014\n",
      "Epoch: 8, Train_Loss: 3.531559705734253, Test_Loss: 3.535633087158203 *\n",
      "Epoch: 8, Train_Loss: 3.542576789855957, Test_Loss: 3.569753408432007\n",
      "Epoch: 8, Train_Loss: 3.5436465740203857, Test_Loss: 3.610612392425537\n",
      "Epoch: 8, Train_Loss: 3.5658435821533203, Test_Loss: 3.7436342239379883\n",
      "Epoch: 8, Train_Loss: 3.535489320755005, Test_Loss: 3.525791645050049 *\n",
      "Epoch: 8, Train_Loss: 3.6179468631744385, Test_Loss: 3.752448081970215\n",
      "Epoch: 8, Train_Loss: 3.7361204624176025, Test_Loss: 3.9442527294158936\n",
      "Epoch: 8, Train_Loss: 3.6958234310150146, Test_Loss: 3.6416616439819336 *\n",
      "Epoch: 8, Train_Loss: 3.5418879985809326, Test_Loss: 3.5842339992523193 *\n",
      "Epoch: 8, Train_Loss: 3.5850696563720703, Test_Loss: 3.5145511627197266 *\n",
      "Epoch: 8, Train_Loss: 3.6183652877807617, Test_Loss: 3.5204102993011475\n",
      "Epoch: 8, Train_Loss: 3.62896466255188, Test_Loss: 3.5225229263305664\n",
      "Epoch: 8, Train_Loss: 3.650968074798584, Test_Loss: 4.180556774139404\n",
      "Epoch: 8, Train_Loss: 3.5842227935791016, Test_Loss: 3.8777832984924316 *\n",
      "Epoch: 8, Train_Loss: 3.5538711547851562, Test_Loss: 4.052037715911865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 3.510136604309082, Test_Loss: 4.232120037078857\n",
      "Epoch: 8, Train_Loss: 3.5838749408721924, Test_Loss: 3.6338632106781006 *\n",
      "Epoch: 8, Train_Loss: 3.529110908508301, Test_Loss: 4.1114678382873535\n",
      "Epoch: 8, Train_Loss: 3.5033910274505615, Test_Loss: 3.6918582916259766 *\n",
      "Epoch: 8, Train_Loss: 3.5003175735473633, Test_Loss: 3.5065064430236816 *\n",
      "Epoch: 8, Train_Loss: 3.500516891479492, Test_Loss: 3.5129597187042236\n",
      "Epoch: 8, Train_Loss: 3.545938491821289, Test_Loss: 3.5958075523376465\n",
      "Epoch: 8, Train_Loss: 9.029703140258789, Test_Loss: 3.686507225036621\n",
      "Epoch: 8, Train_Loss: 3.669665813446045, Test_Loss: 4.492100238800049\n",
      "Epoch: 8, Train_Loss: 3.5018510818481445, Test_Loss: 3.991203784942627 *\n",
      "Epoch: 8, Train_Loss: 3.5252928733825684, Test_Loss: 5.322731971740723\n",
      "Epoch: 8, Train_Loss: 3.5115647315979004, Test_Loss: 4.092580795288086 *\n",
      "Epoch: 8, Train_Loss: 3.5063529014587402, Test_Loss: 4.343940258026123\n",
      "Epoch: 8, Train_Loss: 3.4901154041290283, Test_Loss: 3.516902446746826 *\n",
      "Epoch: 8, Train_Loss: 3.507124423980713, Test_Loss: 3.493593692779541 *\n",
      "Epoch: 8, Train_Loss: 3.515310525894165, Test_Loss: 3.9903507232666016\n",
      "Epoch: 8, Train_Loss: 3.5121874809265137, Test_Loss: 4.833762168884277\n",
      "Epoch: 8, Train_Loss: 3.5044145584106445, Test_Loss: 3.6389434337615967 *\n",
      "Epoch: 8, Train_Loss: 3.4951655864715576, Test_Loss: 3.6403801441192627\n",
      "Epoch: 8, Train_Loss: 3.484558343887329, Test_Loss: 3.488809108734131 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 3.5074551105499268, Test_Loss: 3.6123175621032715\n",
      "Epoch: 8, Train_Loss: 3.4816415309906006, Test_Loss: 3.8159143924713135\n",
      "Epoch: 8, Train_Loss: 3.4844014644622803, Test_Loss: 3.927623987197876\n",
      "Epoch: 8, Train_Loss: 3.5028698444366455, Test_Loss: 4.942986488342285\n",
      "Epoch: 8, Train_Loss: 3.5238327980041504, Test_Loss: 3.9293808937072754 *\n",
      "Epoch: 8, Train_Loss: 3.5158064365386963, Test_Loss: 3.486715078353882 *\n",
      "Epoch: 8, Train_Loss: 3.4785850048065186, Test_Loss: 3.4935193061828613\n",
      "Epoch: 8, Train_Loss: 3.481703758239746, Test_Loss: 3.5055322647094727\n",
      "Epoch: 8, Train_Loss: 3.5283164978027344, Test_Loss: 3.5159823894500732\n",
      "Epoch: 8, Train_Loss: 3.5843474864959717, Test_Loss: 3.6772897243499756\n",
      "Epoch: 8, Train_Loss: 3.551682472229004, Test_Loss: 3.9521484375\n",
      "Epoch: 8, Train_Loss: 3.531942844390869, Test_Loss: 3.7667853832244873 *\n",
      "Epoch: 8, Train_Loss: 3.4965226650238037, Test_Loss: 3.584587335586548 *\n",
      "Epoch: 8, Train_Loss: 3.544494867324829, Test_Loss: 3.4923300743103027 *\n",
      "Epoch: 8, Train_Loss: 3.51542592048645, Test_Loss: 3.4787793159484863 *\n",
      "Epoch: 8, Train_Loss: 3.498246669769287, Test_Loss: 3.5714330673217773\n",
      "Epoch: 8, Train_Loss: 3.5419931411743164, Test_Loss: 4.119166851043701\n",
      "Epoch: 8, Train_Loss: 3.5099809169769287, Test_Loss: 4.892632484436035\n",
      "Epoch: 8, Train_Loss: 3.4845752716064453, Test_Loss: 3.796025276184082 *\n",
      "Epoch: 8, Train_Loss: 3.466106414794922, Test_Loss: 3.5592548847198486 *\n",
      "Epoch: 8, Train_Loss: 3.468724012374878, Test_Loss: 3.4670231342315674 *\n",
      "Epoch: 8, Train_Loss: 3.4669809341430664, Test_Loss: 3.475947380065918\n",
      "Epoch: 8, Train_Loss: 3.4651315212249756, Test_Loss: 3.4678285121917725 *\n",
      "Epoch: 8, Train_Loss: 3.457440137863159, Test_Loss: 3.4762721061706543\n",
      "Epoch: 8, Train_Loss: 7.851497650146484, Test_Loss: 3.502248525619507\n",
      "Epoch: 8, Train_Loss: 3.9360272884368896, Test_Loss: 3.517683982849121\n",
      "Epoch: 8, Train_Loss: 3.4628772735595703, Test_Loss: 3.4610097408294678 *\n",
      "Epoch: 8, Train_Loss: 3.469085216522217, Test_Loss: 3.563977003097534\n",
      "Epoch: 8, Train_Loss: 3.459130048751831, Test_Loss: 3.812000274658203\n",
      "Epoch: 8, Train_Loss: 3.4550981521606445, Test_Loss: 3.5744125843048096 *\n",
      "Epoch: 8, Train_Loss: 3.454120635986328, Test_Loss: 3.655641794204712\n",
      "Epoch: 8, Train_Loss: 3.4517970085144043, Test_Loss: 3.4612512588500977 *\n",
      "Epoch: 8, Train_Loss: 3.451899290084839, Test_Loss: 3.4562299251556396 *\n",
      "Epoch: 8, Train_Loss: 3.449840784072876, Test_Loss: 3.461193323135376\n",
      "Epoch: 8, Train_Loss: 3.4999518394470215, Test_Loss: 3.457854986190796 *\n",
      "Epoch: 8, Train_Loss: 3.5356359481811523, Test_Loss: 3.4701766967773438\n",
      "Epoch: 8, Train_Loss: 3.5387470722198486, Test_Loss: 7.1574883460998535\n",
      "Epoch: 8, Train_Loss: 3.537353992462158, Test_Loss: 5.143967628479004 *\n",
      "Epoch: 8, Train_Loss: 3.4486083984375, Test_Loss: 3.4528844356536865 *\n",
      "Epoch: 8, Train_Loss: 3.5130128860473633, Test_Loss: 3.4553067684173584\n",
      "Epoch: 8, Train_Loss: 3.666771411895752, Test_Loss: 3.4472622871398926 *\n",
      "Epoch: 8, Train_Loss: 3.649775981903076, Test_Loss: 3.452892541885376\n",
      "Epoch: 8, Train_Loss: 3.664696216583252, Test_Loss: 3.4467339515686035 *\n",
      "Epoch: 8, Train_Loss: 3.4486935138702393, Test_Loss: 3.4473509788513184\n",
      "Epoch: 8, Train_Loss: 3.4384307861328125, Test_Loss: 3.4462618827819824 *\n",
      "Epoch: 8, Train_Loss: 3.4432764053344727, Test_Loss: 3.4431304931640625 *\n",
      "Epoch: 8, Train_Loss: 3.449092388153076, Test_Loss: 3.439544439315796 *\n",
      "Epoch: 8, Train_Loss: 3.4506168365478516, Test_Loss: 3.441098213195801\n",
      "Epoch: 8, Train_Loss: 3.4430532455444336, Test_Loss: 3.452373504638672\n",
      "Epoch: 8, Train_Loss: 3.4383535385131836, Test_Loss: 3.4699954986572266\n",
      "Epoch: 8, Train_Loss: 3.4350638389587402, Test_Loss: 3.454277753829956 *\n",
      "Epoch: 8, Train_Loss: 3.4325790405273438, Test_Loss: 3.4367895126342773 *\n",
      "Epoch: 8, Train_Loss: 3.4458463191986084, Test_Loss: 3.43043851852417 *\n",
      "Epoch: 8, Train_Loss: 3.570812225341797, Test_Loss: 3.436479330062866\n",
      "Epoch: 9, Train_Loss: 3.614166498184204, Test_Loss: 3.4284636974334717 *\n",
      "Epoch: 9, Train_Loss: 3.641491651535034, Test_Loss: 3.4334347248077393\n",
      "Epoch: 9, Train_Loss: 3.4896130561828613, Test_Loss: 3.431213855743408 *\n",
      "Epoch: 9, Train_Loss: 3.559093713760376, Test_Loss: 3.4282641410827637 *\n",
      "Epoch: 9, Train_Loss: 3.5544683933258057, Test_Loss: 3.434251070022583\n",
      "Epoch: 9, Train_Loss: 3.4515511989593506, Test_Loss: 3.4305567741394043 *\n",
      "Epoch: 9, Train_Loss: 3.5860583782196045, Test_Loss: 3.4308173656463623\n",
      "Epoch: 9, Train_Loss: 3.5469629764556885, Test_Loss: 3.430464029312134 *\n",
      "Epoch: 9, Train_Loss: 3.6508803367614746, Test_Loss: 3.422327756881714 *\n",
      "Epoch: 9, Train_Loss: 3.4361557960510254, Test_Loss: 3.4316303730010986\n",
      "Epoch: 9, Train_Loss: 4.855225563049316, Test_Loss: 3.4253427982330322 *\n",
      "Epoch: 9, Train_Loss: 5.213533401489258, Test_Loss: 3.424823045730591 *\n",
      "Epoch: 9, Train_Loss: 3.4579391479492188, Test_Loss: 3.4705216884613037\n",
      "Epoch: 9, Train_Loss: 3.480457067489624, Test_Loss: 3.4673056602478027 *\n",
      "Epoch: 9, Train_Loss: 3.4805498123168945, Test_Loss: 8.871785163879395\n",
      "Epoch: 9, Train_Loss: 3.4689688682556152, Test_Loss: 3.5123798847198486 *\n",
      "Epoch: 9, Train_Loss: 3.4201462268829346, Test_Loss: 3.4172000885009766 *\n",
      "Epoch: 9, Train_Loss: 3.4253172874450684, Test_Loss: 3.4463367462158203\n",
      "Epoch: 9, Train_Loss: 3.5308969020843506, Test_Loss: 3.4865593910217285\n",
      "Epoch: 9, Train_Loss: 3.503000497817993, Test_Loss: 3.4849984645843506 *\n",
      "Epoch: 9, Train_Loss: 3.4990904331207275, Test_Loss: 3.421292543411255 *\n",
      "Epoch: 9, Train_Loss: 3.493257999420166, Test_Loss: 3.5113232135772705\n",
      "Epoch: 9, Train_Loss: 3.4802513122558594, Test_Loss: 3.468362808227539 *\n",
      "Epoch: 9, Train_Loss: 3.4345874786376953, Test_Loss: 3.412829637527466 *\n",
      "Epoch: 9, Train_Loss: 3.4343128204345703, Test_Loss: 3.448391914367676\n",
      "Epoch: 9, Train_Loss: 3.429410934448242, Test_Loss: 3.4306156635284424 *\n",
      "Epoch: 9, Train_Loss: 3.4305615425109863, Test_Loss: 3.4158616065979004 *\n",
      "Epoch: 9, Train_Loss: 3.4196386337280273, Test_Loss: 3.434758424758911\n",
      "Epoch: 9, Train_Loss: 3.4063289165496826, Test_Loss: 3.562197208404541\n",
      "Epoch: 9, Train_Loss: 3.446074962615967, Test_Loss: 3.440497636795044 *\n",
      "Epoch: 9, Train_Loss: 3.4572701454162598, Test_Loss: 3.5225491523742676\n",
      "Epoch: 9, Train_Loss: 3.41853666305542, Test_Loss: 3.4659526348114014 *\n",
      "Epoch: 9, Train_Loss: 3.401705741882324, Test_Loss: 3.4525513648986816 *\n",
      "Epoch: 9, Train_Loss: 3.3991148471832275, Test_Loss: 3.421132802963257 *\n",
      "Epoch: 9, Train_Loss: 3.399695634841919, Test_Loss: 3.412618637084961 *\n",
      "Epoch: 9, Train_Loss: 3.4076905250549316, Test_Loss: 3.416914939880371\n",
      "Epoch: 9, Train_Loss: 3.4006059169769287, Test_Loss: 3.41869854927063\n",
      "Epoch: 9, Train_Loss: 3.4012835025787354, Test_Loss: 3.4123737812042236 *\n",
      "Epoch: 9, Train_Loss: 3.4002833366394043, Test_Loss: 3.415827751159668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 3.3949484825134277, Test_Loss: 3.4071149826049805 *\n",
      "Epoch: 9, Train_Loss: 3.394155263900757, Test_Loss: 3.4220521450042725\n",
      "Epoch: 9, Train_Loss: 3.3908438682556152, Test_Loss: 3.4246935844421387\n",
      "Epoch: 9, Train_Loss: 3.4047656059265137, Test_Loss: 3.4043996334075928 *\n",
      "Epoch: 9, Train_Loss: 3.4005560874938965, Test_Loss: 3.4034600257873535 *\n",
      "Epoch: 9, Train_Loss: 3.3996970653533936, Test_Loss: 3.4631757736206055\n",
      "Epoch: 9, Train_Loss: 3.405749797821045, Test_Loss: 3.4269626140594482 *\n",
      "Epoch: 9, Train_Loss: 3.403197765350342, Test_Loss: 3.662747383117676\n",
      "Epoch: 9, Train_Loss: 3.3923592567443848, Test_Loss: 3.3994202613830566 *\n",
      "Epoch: 9, Train_Loss: 3.3890609741210938, Test_Loss: 3.469470977783203\n",
      "Epoch: 9, Train_Loss: 3.3894383907318115, Test_Loss: 3.502439022064209\n",
      "Epoch: 9, Train_Loss: 3.4226295948028564, Test_Loss: 3.765929937362671\n",
      "Epoch: 9, Train_Loss: 3.3962690830230713, Test_Loss: 3.4365732669830322 *\n",
      "Epoch: 9, Train_Loss: 3.3824779987335205, Test_Loss: 3.519536018371582\n",
      "Epoch: 9, Train_Loss: 3.3833162784576416, Test_Loss: 3.660923957824707\n",
      "Epoch: 9, Train_Loss: 3.3954355716705322, Test_Loss: 3.544463634490967 *\n",
      "Epoch: 9, Train_Loss: 3.4578094482421875, Test_Loss: 3.4259040355682373 *\n",
      "Epoch: 9, Train_Loss: 3.418121814727783, Test_Loss: 3.3863236904144287 *\n",
      "Epoch: 9, Train_Loss: 3.414754629135132, Test_Loss: 3.397265672683716\n",
      "Epoch: 9, Train_Loss: 3.3732728958129883, Test_Loss: 3.411130905151367\n",
      "Epoch: 9, Train_Loss: 3.427950382232666, Test_Loss: 3.7149643898010254\n",
      "Epoch: 9, Train_Loss: 3.398313045501709, Test_Loss: 3.995288848876953\n",
      "Epoch: 9, Train_Loss: 3.3718953132629395, Test_Loss: 3.766282558441162 *\n",
      "Epoch: 9, Train_Loss: 3.399259090423584, Test_Loss: 4.276430130004883\n",
      "Epoch: 9, Train_Loss: 3.4003000259399414, Test_Loss: 3.595015048980713 *\n",
      "Epoch: 9, Train_Loss: 3.4712343215942383, Test_Loss: 3.9365384578704834\n",
      "Epoch: 9, Train_Loss: 3.457582712173462, Test_Loss: 3.60734486579895 *\n",
      "Epoch: 9, Train_Loss: 3.4368093013763428, Test_Loss: 3.3807296752929688 *\n",
      "Epoch: 9, Train_Loss: 3.391374349594116, Test_Loss: 3.384246826171875\n",
      "Epoch: 9, Train_Loss: 3.3741066455841064, Test_Loss: 3.448126792907715\n",
      "Epoch: 9, Train_Loss: 3.3900139331817627, Test_Loss: 3.5233535766601562\n",
      "Epoch: 9, Train_Loss: 3.3609631061553955, Test_Loss: 4.356209754943848\n",
      "Epoch: 9, Train_Loss: 3.368792772293091, Test_Loss: 3.57098388671875 *\n",
      "Epoch: 9, Train_Loss: 3.3815884590148926, Test_Loss: 5.583489418029785\n",
      "Epoch: 9, Train_Loss: 3.389026403427124, Test_Loss: 3.895437717437744 *\n",
      "Epoch: 9, Train_Loss: 3.4757537841796875, Test_Loss: 4.407228469848633\n",
      "Epoch: 9, Train_Loss: 3.3673622608184814, Test_Loss: 3.42105770111084 *\n",
      "Epoch: 9, Train_Loss: 3.427172899246216, Test_Loss: 3.362151861190796 *\n",
      "Epoch: 9, Train_Loss: 3.3669214248657227, Test_Loss: 3.6442012786865234\n",
      "Epoch: 9, Train_Loss: 3.393754243850708, Test_Loss: 4.770644664764404\n",
      "Epoch: 9, Train_Loss: 3.387120485305786, Test_Loss: 3.890841007232666 *\n",
      "Epoch: 9, Train_Loss: 3.646796464920044, Test_Loss: 3.480557680130005 *\n",
      "Epoch: 9, Train_Loss: 3.3732364177703857, Test_Loss: 3.368811845779419 *\n",
      "Epoch: 9, Train_Loss: 3.395355701446533, Test_Loss: 3.4566359519958496\n",
      "Epoch: 9, Train_Loss: 3.353093385696411, Test_Loss: 3.7621114253997803\n",
      "Epoch: 9, Train_Loss: 3.350696563720703, Test_Loss: 3.604187488555908 *\n",
      "Epoch: 9, Train_Loss: 3.352081298828125, Test_Loss: 4.581043243408203\n",
      "Epoch: 9, Train_Loss: 3.352604627609253, Test_Loss: 3.945265531539917 *\n",
      "Epoch: 9, Train_Loss: 3.361933469772339, Test_Loss: 3.358926773071289 *\n",
      "Epoch: 9, Train_Loss: 3.3642525672912598, Test_Loss: 3.3612232208251953\n",
      "Epoch: 9, Train_Loss: 3.366579532623291, Test_Loss: 3.3721089363098145\n",
      "Epoch: 9, Train_Loss: 3.3636035919189453, Test_Loss: 3.372791290283203\n",
      "Epoch: 9, Train_Loss: 3.365354537963867, Test_Loss: 3.4436192512512207\n",
      "Epoch: 9, Train_Loss: 3.3647513389587402, Test_Loss: 3.9371867179870605\n",
      "Epoch: 9, Train_Loss: 3.347088575363159, Test_Loss: 3.808350086212158 *\n",
      "Epoch: 9, Train_Loss: 3.342064380645752, Test_Loss: 3.4735164642333984 *\n",
      "Epoch: 9, Train_Loss: 3.355992317199707, Test_Loss: 3.385444164276123 *\n",
      "Epoch: 9, Train_Loss: 3.363269567489624, Test_Loss: 3.3568615913391113 *\n",
      "Epoch: 9, Train_Loss: 3.3659024238586426, Test_Loss: 3.397830009460449\n",
      "Epoch: 9, Train_Loss: 3.337160110473633, Test_Loss: 3.7571640014648438\n",
      "Epoch: 9, Train_Loss: 3.3869359493255615, Test_Loss: 4.8052825927734375\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 3.3971142768859863, Test_Loss: 3.9769201278686523 *\n",
      "Epoch: 9, Train_Loss: 3.3875465393066406, Test_Loss: 3.428046703338623 *\n",
      "Epoch: 9, Train_Loss: 3.339348554611206, Test_Loss: 3.356626033782959 *\n",
      "Epoch: 9, Train_Loss: 3.3668384552001953, Test_Loss: 3.3418703079223633 *\n",
      "Epoch: 9, Train_Loss: 3.336000680923462, Test_Loss: 3.335301399230957 *\n",
      "Epoch: 9, Train_Loss: 3.3561668395996094, Test_Loss: 3.3347907066345215 *\n",
      "Epoch: 9, Train_Loss: 3.3305819034576416, Test_Loss: 3.3682479858398438\n",
      "Epoch: 9, Train_Loss: 3.3545851707458496, Test_Loss: 3.3918089866638184\n",
      "Epoch: 9, Train_Loss: 3.456151247024536, Test_Loss: 3.3293111324310303 *\n",
      "Epoch: 9, Train_Loss: 6.752276420593262, Test_Loss: 3.3960213661193848\n",
      "Epoch: 9, Train_Loss: 5.2725911140441895, Test_Loss: 3.473414182662964\n",
      "Epoch: 9, Train_Loss: 3.3526127338409424, Test_Loss: 3.6750826835632324\n",
      "Epoch: 9, Train_Loss: 3.32725191116333, Test_Loss: 3.545290470123291 *\n",
      "Epoch: 9, Train_Loss: 3.4755756855010986, Test_Loss: 3.3408539295196533 *\n",
      "Epoch: 9, Train_Loss: 3.431090831756592, Test_Loss: 3.3390791416168213 *\n",
      "Epoch: 9, Train_Loss: 3.3393826484680176, Test_Loss: 3.330439805984497 *\n",
      "Epoch: 9, Train_Loss: 3.3210842609405518, Test_Loss: 3.325146198272705 *\n",
      "Epoch: 9, Train_Loss: 3.37380313873291, Test_Loss: 3.342797040939331\n",
      "Epoch: 9, Train_Loss: 3.345923900604248, Test_Loss: 5.334775924682617\n",
      "Epoch: 9, Train_Loss: 3.3446741104125977, Test_Loss: 6.742743968963623\n",
      "Epoch: 9, Train_Loss: 3.4810791015625, Test_Loss: 3.3299217224121094 *\n",
      "Epoch: 9, Train_Loss: 4.616694927215576, Test_Loss: 3.317852735519409 *\n",
      "Epoch: 9, Train_Loss: 4.723420143127441, Test_Loss: 3.3262834548950195\n",
      "Epoch: 9, Train_Loss: 3.4513516426086426, Test_Loss: 3.324552536010742 *\n",
      "Epoch: 9, Train_Loss: 3.3820061683654785, Test_Loss: 3.316575527191162 *\n",
      "Epoch: 9, Train_Loss: 5.347550868988037, Test_Loss: 3.3189666271209717\n",
      "Epoch: 9, Train_Loss: 4.80853796005249, Test_Loss: 3.314789056777954 *\n",
      "Epoch: 9, Train_Loss: 3.3565926551818848, Test_Loss: 3.3072896003723145 *\n",
      "Epoch: 9, Train_Loss: 3.3452532291412354, Test_Loss: 3.3127341270446777\n",
      "Epoch: 9, Train_Loss: 3.7292582988739014, Test_Loss: 3.3130831718444824\n",
      "Epoch: 9, Train_Loss: 5.030387878417969, Test_Loss: 3.3160040378570557\n",
      "Epoch: 9, Train_Loss: 4.3849639892578125, Test_Loss: 3.3373429775238037\n",
      "Epoch: 9, Train_Loss: 3.321575164794922, Test_Loss: 3.3351683616638184 *\n",
      "Epoch: 9, Train_Loss: 3.3306844234466553, Test_Loss: 3.312959909439087 *\n",
      "Epoch: 9, Train_Loss: 3.497887134552002, Test_Loss: 3.3050920963287354 *\n",
      "Epoch: 9, Train_Loss: 3.8632657527923584, Test_Loss: 3.3052611351013184\n",
      "Epoch: 9, Train_Loss: 3.321985960006714, Test_Loss: 3.3007960319519043 *\n",
      "Epoch: 9, Train_Loss: 3.352874994277954, Test_Loss: 3.307238817214966\n",
      "Epoch: 9, Train_Loss: 3.3786163330078125, Test_Loss: 3.302852153778076 *\n",
      "Epoch: 9, Train_Loss: 3.453517198562622, Test_Loss: 3.3012337684631348 *\n",
      "Epoch: 9, Train_Loss: 3.412818431854248, Test_Loss: 3.302147626876831\n",
      "Epoch: 9, Train_Loss: 3.6813042163848877, Test_Loss: 3.2998104095458984 *\n",
      "Epoch: 9, Train_Loss: 3.515939712524414, Test_Loss: 3.299189567565918 *\n",
      "Epoch: 9, Train_Loss: 3.355703592300415, Test_Loss: 3.3007233142852783\n",
      "Epoch: 9, Train_Loss: 3.515542507171631, Test_Loss: 3.295081615447998 *\n",
      "Epoch: 9, Train_Loss: 3.574326753616333, Test_Loss: 3.2990150451660156\n",
      "Epoch: 9, Train_Loss: 3.7955322265625, Test_Loss: 3.2982735633850098 *\n",
      "Epoch: 9, Train_Loss: 3.582365036010742, Test_Loss: 3.2945244312286377 *\n",
      "Epoch: 9, Train_Loss: 3.343430757522583, Test_Loss: 3.3335933685302734\n",
      "Epoch: 9, Train_Loss: 3.41855788230896, Test_Loss: 3.3245012760162354 *\n",
      "Epoch: 9, Train_Loss: 3.394317865371704, Test_Loss: 7.6084394454956055\n",
      "Epoch: 9, Train_Loss: 3.300929069519043, Test_Loss: 4.459469795227051 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 3.2970526218414307, Test_Loss: 3.2901222705841064 *\n",
      "Epoch: 9, Train_Loss: 3.2899422645568848, Test_Loss: 3.305088996887207\n",
      "Epoch: 9, Train_Loss: 3.2914090156555176, Test_Loss: 3.335132122039795\n",
      "Epoch: 9, Train_Loss: 3.2915761470794678, Test_Loss: 3.3529531955718994\n",
      "Epoch: 9, Train_Loss: 3.312110424041748, Test_Loss: 3.2967464923858643 *\n",
      "Epoch: 9, Train_Loss: 3.386018991470337, Test_Loss: 3.3805084228515625\n",
      "Epoch: 9, Train_Loss: 3.3687851428985596, Test_Loss: 3.376377820968628 *\n",
      "Epoch: 9, Train_Loss: 3.405136823654175, Test_Loss: 3.288831949234009 *\n",
      "Epoch: 9, Train_Loss: 3.4503939151763916, Test_Loss: 3.322908401489258\n",
      "Epoch: 9, Train_Loss: 3.6752071380615234, Test_Loss: 3.2919235229492188 *\n",
      "Epoch: 9, Train_Loss: 3.296121835708618, Test_Loss: 3.291921615600586 *\n",
      "Epoch: 9, Train_Loss: 3.3357856273651123, Test_Loss: 3.2814395427703857 *\n",
      "Epoch: 9, Train_Loss: 3.626005172729492, Test_Loss: 3.384565830230713\n",
      "Epoch: 9, Train_Loss: 3.8637454509735107, Test_Loss: 3.317869186401367 *\n",
      "Epoch: 9, Train_Loss: 3.3499865531921387, Test_Loss: 3.383631706237793\n",
      "Epoch: 9, Train_Loss: 3.278724193572998, Test_Loss: 3.3536605834960938 *\n",
      "Epoch: 9, Train_Loss: 3.712693691253662, Test_Loss: 3.304255247116089 *\n",
      "Epoch: 9, Train_Loss: 3.9646191596984863, Test_Loss: 3.287257194519043 *\n",
      "Epoch: 9, Train_Loss: 3.5683584213256836, Test_Loss: 3.275453567504883 *\n",
      "Epoch: 9, Train_Loss: 3.2976338863372803, Test_Loss: 3.276942253112793\n",
      "Epoch: 9, Train_Loss: 3.2863128185272217, Test_Loss: 3.279189348220825\n",
      "Epoch: 9, Train_Loss: 3.541820764541626, Test_Loss: 3.2853002548217773\n",
      "Epoch: 9, Train_Loss: 4.94273567199707, Test_Loss: 3.2816388607025146 *\n",
      "Epoch: 9, Train_Loss: 3.721345901489258, Test_Loss: 3.276010036468506 *\n",
      "Epoch: 9, Train_Loss: 3.286092519760132, Test_Loss: 3.292257308959961\n",
      "Epoch: 9, Train_Loss: 3.281397819519043, Test_Loss: 3.277064561843872 *\n",
      "Epoch: 9, Train_Loss: 3.265226364135742, Test_Loss: 3.2757620811462402 *\n",
      "Epoch: 9, Train_Loss: 3.601729393005371, Test_Loss: 3.2759251594543457\n",
      "Epoch: 9, Train_Loss: 3.396740674972534, Test_Loss: 3.299428701400757\n",
      "Epoch: 9, Train_Loss: 3.3163440227508545, Test_Loss: 3.2900550365448 *\n",
      "Epoch: 9, Train_Loss: 3.282121181488037, Test_Loss: 3.6148502826690674\n",
      "Epoch: 9, Train_Loss: 3.2789065837860107, Test_Loss: 3.268275022506714 *\n",
      "Epoch: 9, Train_Loss: 15.93496322631836, Test_Loss: 3.3058316707611084\n",
      "Epoch: 9, Train_Loss: 8.168869018554688, Test_Loss: 3.3351263999938965\n",
      "Epoch: 9, Train_Loss: 4.156277656555176, Test_Loss: 3.5902271270751953\n",
      "Epoch: 9, Train_Loss: 6.376706123352051, Test_Loss: 3.3353512287139893 *\n",
      "Epoch: 9, Train_Loss: 3.4278879165649414, Test_Loss: 3.3420207500457764\n",
      "Epoch: 9, Train_Loss: 3.323779821395874, Test_Loss: 3.4858686923980713\n",
      "Epoch: 9, Train_Loss: 3.8547427654266357, Test_Loss: 3.499102830886841\n",
      "Epoch: 9, Train_Loss: 13.921688079833984, Test_Loss: 3.2724673748016357 *\n",
      "Epoch: 9, Train_Loss: 4.517460823059082, Test_Loss: 3.3034398555755615\n",
      "Epoch: 9, Train_Loss: 3.27715802192688, Test_Loss: 3.25545072555542 *\n",
      "Epoch: 9, Train_Loss: 7.19779109954834, Test_Loss: 3.2870442867279053\n",
      "Epoch: 9, Train_Loss: 5.648716926574707, Test_Loss: 3.382962942123413\n",
      "Epoch: 9, Train_Loss: 3.2707157135009766, Test_Loss: 4.152182579040527\n",
      "Epoch: 9, Train_Loss: 3.25797176361084, Test_Loss: 3.4107112884521484 *\n",
      "Epoch: 9, Train_Loss: 3.252176284790039, Test_Loss: 3.9638607501983643\n",
      "Epoch: 9, Train_Loss: 3.2510292530059814, Test_Loss: 3.655456781387329 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 3.265793561935425, Test_Loss: 3.77543306350708\n",
      "Epoch: 9, Train_Loss: 3.261218547821045, Test_Loss: 3.7556850910186768 *\n",
      "Epoch: 9, Train_Loss: 3.2568020820617676, Test_Loss: 3.298300266265869 *\n",
      "Epoch: 9, Train_Loss: 3.2599422931671143, Test_Loss: 3.266908645629883 *\n",
      "Epoch: 9, Train_Loss: 3.2725579738616943, Test_Loss: 3.2656524181365967 *\n",
      "Epoch: 9, Train_Loss: 3.2615346908569336, Test_Loss: 3.3435821533203125\n",
      "Epoch: 9, Train_Loss: 3.2501680850982666, Test_Loss: 4.225015640258789\n",
      "Epoch: 9, Train_Loss: 3.286294937133789, Test_Loss: 3.601275682449341 *\n",
      "Epoch: 9, Train_Loss: 3.3206419944763184, Test_Loss: 4.929166793823242\n",
      "Epoch: 9, Train_Loss: 3.291002035140991, Test_Loss: 3.839974880218506 *\n",
      "Epoch: 9, Train_Loss: 3.257486581802368, Test_Loss: 4.4958672523498535\n",
      "Epoch: 9, Train_Loss: 3.2410125732421875, Test_Loss: 3.461561679840088 *\n",
      "Epoch: 9, Train_Loss: 3.2441046237945557, Test_Loss: 3.254389524459839 *\n",
      "Epoch: 9, Train_Loss: 3.238551139831543, Test_Loss: 3.324205160140991\n",
      "Epoch: 9, Train_Loss: 3.239881992340088, Test_Loss: 4.403586387634277\n",
      "Epoch: 9, Train_Loss: 3.237112045288086, Test_Loss: 3.95192289352417 *\n",
      "Epoch: 9, Train_Loss: 3.2435615062713623, Test_Loss: 3.360114097595215 *\n",
      "Epoch: 9, Train_Loss: 3.2388432025909424, Test_Loss: 3.297775983810425 *\n",
      "Epoch: 9, Train_Loss: 3.2325611114501953, Test_Loss: 3.2745361328125 *\n",
      "Epoch: 9, Train_Loss: 3.2335851192474365, Test_Loss: 3.5931973457336426\n",
      "Epoch: 9, Train_Loss: 3.2259974479675293, Test_Loss: 3.440272808074951 *\n",
      "Epoch: 9, Train_Loss: 3.2304415702819824, Test_Loss: 4.430903911590576\n",
      "Epoch: 9, Train_Loss: 3.2430810928344727, Test_Loss: 4.123775482177734 *\n",
      "Epoch: 9, Train_Loss: 3.2311127185821533, Test_Loss: 3.392638683319092 *\n",
      "Epoch: 9, Train_Loss: 3.235481023788452, Test_Loss: 3.254514694213867 *\n",
      "Epoch: 9, Train_Loss: 3.2323615550994873, Test_Loss: 3.263146162033081\n",
      "Epoch: 9, Train_Loss: 11.917593002319336, Test_Loss: 3.280489683151245\n",
      "Epoch: 9, Train_Loss: 3.6279945373535156, Test_Loss: 3.294530153274536\n",
      "Epoch: 9, Train_Loss: 3.2337992191314697, Test_Loss: 3.5946338176727295\n",
      "Epoch: 9, Train_Loss: 3.235050678253174, Test_Loss: 3.6379454135894775\n",
      "Epoch: 9, Train_Loss: 3.2488274574279785, Test_Loss: 3.3142662048339844 *\n",
      "Epoch: 9, Train_Loss: 3.239079475402832, Test_Loss: 3.235236883163452 *\n",
      "Epoch: 9, Train_Loss: 3.2450809478759766, Test_Loss: 3.234334707260132 *\n",
      "Epoch: 9, Train_Loss: 3.2369000911712646, Test_Loss: 3.2623138427734375\n",
      "Epoch: 9, Train_Loss: 3.3880584239959717, Test_Loss: 3.4785032272338867\n",
      "Epoch: 9, Train_Loss: 3.4328773021698, Test_Loss: 4.691287994384766\n",
      "Epoch: 9, Train_Loss: 3.3914082050323486, Test_Loss: 4.258486747741699 *\n",
      "Epoch: 9, Train_Loss: 3.2252798080444336, Test_Loss: 3.2596614360809326 *\n",
      "Epoch: 9, Train_Loss: 3.3245980739593506, Test_Loss: 3.2577030658721924 *\n",
      "Epoch: 9, Train_Loss: 3.3014395236968994, Test_Loss: 3.22460675239563 *\n",
      "Epoch: 9, Train_Loss: 3.3447659015655518, Test_Loss: 3.2263970375061035\n",
      "Epoch: 9, Train_Loss: 3.3199498653411865, Test_Loss: 3.2286391258239746\n",
      "Epoch: 9, Train_Loss: 3.294584274291992, Test_Loss: 3.2410311698913574\n",
      "Epoch: 9, Train_Loss: 3.2482638359069824, Test_Loss: 3.308581590652466\n",
      "Epoch: 9, Train_Loss: 3.221057415008545, Test_Loss: 3.2087278366088867 *\n",
      "Epoch: 9, Train_Loss: 3.2893238067626953, Test_Loss: 3.234400510787964\n",
      "Epoch: 9, Train_Loss: 3.2295897006988525, Test_Loss: 3.3217239379882812\n",
      "Epoch: 9, Train_Loss: 3.208388090133667, Test_Loss: 3.5607874393463135\n",
      "Epoch: 9, Train_Loss: 3.2052323818206787, Test_Loss: 3.396193265914917 *\n",
      "Epoch: 9, Train_Loss: 3.2044732570648193, Test_Loss: 3.214226007461548 *\n",
      "Epoch: 9, Train_Loss: 3.2964961528778076, Test_Loss: 3.2003464698791504 *\n",
      "Epoch: 9, Train_Loss: 8.788944244384766, Test_Loss: 3.2043800354003906\n",
      "Epoch: 9, Train_Loss: 3.2072970867156982, Test_Loss: 3.20237135887146 *\n",
      "Epoch: 9, Train_Loss: 3.2099757194519043, Test_Loss: 3.2013888359069824 *\n",
      "Epoch: 9, Train_Loss: 3.2191457748413086, Test_Loss: 3.9350500106811523\n",
      "Epoch: 9, Train_Loss: 3.2181880474090576, Test_Loss: 8.101136207580566\n",
      "Epoch: 9, Train_Loss: 3.210850715637207, Test_Loss: 3.238450527191162 *\n",
      "Epoch: 9, Train_Loss: 3.1980414390563965, Test_Loss: 3.2072222232818604 *\n",
      "Epoch: 9, Train_Loss: 3.2080864906311035, Test_Loss: 3.1910626888275146 *\n",
      "Epoch: 9, Train_Loss: 3.22476863861084, Test_Loss: 3.1958813667297363\n",
      "Epoch: 9, Train_Loss: 3.212155818939209, Test_Loss: 3.1983466148376465\n",
      "Epoch: 9, Train_Loss: 3.200822353363037, Test_Loss: 3.199629068374634\n",
      "Epoch: 9, Train_Loss: 3.2059667110443115, Test_Loss: 3.2121999263763428\n",
      "Epoch: 9, Train_Loss: 3.1990230083465576, Test_Loss: 3.2061493396759033 *\n",
      "Epoch: 9, Train_Loss: 3.2139437198638916, Test_Loss: 3.2096259593963623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 3.1852071285247803, Test_Loss: 3.2003414630889893 *\n",
      "Epoch: 9, Train_Loss: 3.190368175506592, Test_Loss: 3.209364414215088\n",
      "Epoch: 9, Train_Loss: 3.2128443717956543, Test_Loss: 3.1968696117401123 *\n",
      "Epoch: 9, Train_Loss: 3.233304738998413, Test_Loss: 3.193589925765991 *\n",
      "Epoch: 9, Train_Loss: 3.21297287940979, Test_Loss: 3.199842929840088\n",
      "Epoch: 9, Train_Loss: 3.187700033187866, Test_Loss: 3.1929404735565186 *\n",
      "Epoch: 9, Train_Loss: 3.185487747192383, Test_Loss: 3.1863961219787598 *\n",
      "Epoch: 9, Train_Loss: 3.244704008102417, Test_Loss: 3.1807379722595215 *\n",
      "Epoch: 9, Train_Loss: 3.2744076251983643, Test_Loss: 3.18852162361145\n",
      "Epoch: 9, Train_Loss: 3.253221273422241, Test_Loss: 3.1856069564819336 *\n",
      "Epoch: 9, Train_Loss: 3.2184648513793945, Test_Loss: 3.1926300525665283\n",
      "Epoch: 9, Train_Loss: 3.203803062438965, Test_Loss: 3.187053918838501 *\n",
      "Epoch: 9, Train_Loss: 3.2506496906280518, Test_Loss: 3.190667152404785\n",
      "Epoch: 9, Train_Loss: 3.2447142601013184, Test_Loss: 3.186858654022217 *\n",
      "Epoch: 9, Train_Loss: 3.1947848796844482, Test_Loss: 3.184201955795288 *\n",
      "Epoch: 9, Train_Loss: 3.289788007736206, Test_Loss: 3.179004192352295 *\n",
      "Epoch: 9, Train_Loss: 3.196962833404541, Test_Loss: 3.190302610397339\n",
      "Epoch: 9, Train_Loss: 3.1831467151641846, Test_Loss: 3.1762893199920654 *\n",
      "Epoch: 9, Train_Loss: 3.1701624393463135, Test_Loss: 3.1772639751434326\n",
      "Epoch: 9, Train_Loss: 3.165914535522461, Test_Loss: 3.2007179260253906\n",
      "Epoch: 9, Train_Loss: 3.1702022552490234, Test_Loss: 3.220745086669922\n",
      "Epoch: 9, Train_Loss: 3.1665749549865723, Test_Loss: 6.2172040939331055\n",
      "Epoch: 9, Train_Loss: 3.206582546234131, Test_Loss: 5.8099894523620605 *\n",
      "Epoch: 9, Train_Loss: 7.839120864868164, Test_Loss: 3.1627252101898193 *\n",
      "Epoch: 9, Train_Loss: 3.3001601696014404, Test_Loss: 3.159330368041992 *\n",
      "Epoch: 9, Train_Loss: 3.168687343597412, Test_Loss: 3.2048048973083496\n",
      "Epoch: 9, Train_Loss: 3.175793409347534, Test_Loss: 3.2060933113098145\n",
      "Epoch: 9, Train_Loss: 3.1642541885375977, Test_Loss: 3.200610876083374 *\n",
      "Epoch: 9, Train_Loss: 3.15505051612854, Test_Loss: 3.210628032684326\n",
      "Epoch: 9, Train_Loss: 3.159072160720825, Test_Loss: 3.2632274627685547\n",
      "Epoch: 9, Train_Loss: 3.155963659286499, Test_Loss: 3.1610608100891113 *\n",
      "Epoch: 9, Train_Loss: 3.157470464706421, Test_Loss: 3.1850130558013916\n",
      "Epoch: 9, Train_Loss: 3.15387225151062, Test_Loss: 3.179334878921509 *\n",
      "Epoch: 9, Train_Loss: 3.2142109870910645, Test_Loss: 3.1662256717681885 *\n",
      "Epoch: 9, Train_Loss: 3.23814058303833, Test_Loss: 3.161574602127075 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 3.249833106994629, Test_Loss: 3.24354887008667\n",
      "Epoch: 9, Train_Loss: 3.220663070678711, Test_Loss: 3.2171080112457275 *\n",
      "Epoch: 9, Train_Loss: 3.148667097091675, Test_Loss: 3.2388246059417725\n",
      "Epoch: 9, Train_Loss: 3.2467715740203857, Test_Loss: 3.2492868900299072\n",
      "Epoch: 9, Train_Loss: 3.3603081703186035, Test_Loss: 3.1787185668945312 *\n",
      "Epoch: 9, Train_Loss: 3.3656845092773438, Test_Loss: 3.169477701187134 *\n",
      "Epoch: 9, Train_Loss: 3.3390824794769287, Test_Loss: 3.163235902786255 *\n",
      "Epoch: 9, Train_Loss: 3.148658037185669, Test_Loss: 3.1564836502075195 *\n",
      "Epoch: 9, Train_Loss: 3.143183946609497, Test_Loss: 3.1568009853363037\n",
      "Epoch: 9, Train_Loss: 3.1410112380981445, Test_Loss: 3.1660220623016357\n",
      "Epoch: 9, Train_Loss: 3.1497390270233154, Test_Loss: 3.1653857231140137 *\n",
      "Epoch: 9, Train_Loss: 3.150599479675293, Test_Loss: 3.146726369857788 *\n",
      "Epoch: 9, Train_Loss: 3.1486639976501465, Test_Loss: 3.1559438705444336\n",
      "Epoch: 9, Train_Loss: 3.1511824131011963, Test_Loss: 3.1580512523651123\n",
      "Epoch: 9, Train_Loss: 3.140627861022949, Test_Loss: 3.1631698608398438\n",
      "Epoch: 9, Train_Loss: 3.1414167881011963, Test_Loss: 3.1409928798675537 *\n",
      "Epoch: 9, Train_Loss: 3.149523973464966, Test_Loss: 3.160228967666626\n",
      "Epoch: 9, Train_Loss: 3.2849292755126953, Test_Loss: 3.1932146549224854\n",
      "Epoch: 9, Train_Loss: 3.3189432621002197, Test_Loss: 3.481692314147949\n",
      "Epoch: 9, Train_Loss: 3.3416125774383545, Test_Loss: 3.18330979347229 *\n",
      "Epoch: 9, Train_Loss: 3.170781135559082, Test_Loss: 3.1721179485321045 *\n",
      "Epoch: 9, Train_Loss: 3.2881226539611816, Test_Loss: 3.215536117553711\n",
      "Epoch: 9, Train_Loss: 3.2518320083618164, Test_Loss: 3.3949084281921387\n",
      "Epoch: 9, Train_Loss: 3.204216718673706, Test_Loss: 3.269695520401001 *\n",
      "Epoch: 9, Train_Loss: 3.266108989715576, Test_Loss: 3.163341760635376 *\n",
      "Epoch: 9, Train_Loss: 3.3713901042938232, Test_Loss: 3.365633964538574\n",
      "Epoch: 9, Train_Loss: 3.2300992012023926, Test_Loss: 3.4396958351135254\n",
      "Epoch: 9, Train_Loss: 3.143670082092285, Test_Loss: 3.153229236602783 *\n",
      "Epoch: 9, Train_Loss: 5.257747650146484, Test_Loss: 3.1739728450775146\n",
      "Epoch: 9, Train_Loss: 4.21507453918457, Test_Loss: 3.1287577152252197 *\n",
      "Epoch: 9, Train_Loss: 3.1565489768981934, Test_Loss: 3.1659300327301025\n",
      "Epoch: 9, Train_Loss: 3.1823811531066895, Test_Loss: 3.1476662158966064 *\n",
      "Epoch: 9, Train_Loss: 3.1923694610595703, Test_Loss: 3.9253435134887695\n",
      "Epoch: 9, Train_Loss: 3.165632724761963, Test_Loss: 3.3014934062957764 *\n",
      "Epoch: 9, Train_Loss: 3.121245861053467, Test_Loss: 3.9569332599639893\n",
      "Epoch: 9, Train_Loss: 3.150815963745117, Test_Loss: 3.811832904815674 *\n",
      "Epoch: 9, Train_Loss: 3.2370336055755615, Test_Loss: 3.393500804901123 *\n",
      "Epoch: 9, Train_Loss: 3.1901819705963135, Test_Loss: 3.5625767707824707\n",
      "Epoch: 9, Train_Loss: 3.195709466934204, Test_Loss: 3.1840837001800537 *\n",
      "Epoch: 9, Train_Loss: 3.206995964050293, Test_Loss: 3.132941722869873 *\n",
      "Epoch: 9, Train_Loss: 3.158003330230713, Test_Loss: 3.1444473266601562\n",
      "Epoch: 9, Train_Loss: 3.1439497470855713, Test_Loss: 3.29244065284729\n",
      "Epoch: 9, Train_Loss: 3.129438638687134, Test_Loss: 3.5653724670410156\n",
      "Epoch: 9, Train_Loss: 3.1366753578186035, Test_Loss: 3.677058219909668\n",
      "Epoch: 9, Train_Loss: 3.131847858428955, Test_Loss: 4.533653736114502\n",
      "Epoch: 9, Train_Loss: 3.1225240230560303, Test_Loss: 4.233146667480469 *\n",
      "Epoch: 9, Train_Loss: 3.1195309162139893, Test_Loss: 4.079423427581787 *\n",
      "Epoch: 9, Train_Loss: 3.148256540298462, Test_Loss: 3.460954189300537 *\n",
      "Epoch: 9, Train_Loss: 3.146719217300415, Test_Loss: 3.10870361328125 *\n",
      "Epoch: 9, Train_Loss: 3.119523525238037, Test_Loss: 3.1494805812835693\n",
      "Epoch: 9, Train_Loss: 3.10593843460083, Test_Loss: 4.0736894607543945\n",
      "Epoch: 9, Train_Loss: 3.1042215824127197, Test_Loss: 4.311878681182861\n",
      "Epoch: 9, Train_Loss: 3.1026813983917236, Test_Loss: 3.158409595489502 *\n",
      "Epoch: 9, Train_Loss: 3.105502128601074, Test_Loss: 3.169990301132202\n",
      "Epoch: 9, Train_Loss: 3.1048085689544678, Test_Loss: 3.135005474090576 *\n",
      "Epoch: 9, Train_Loss: 3.1007628440856934, Test_Loss: 3.456472396850586\n",
      "Epoch: 9, Train_Loss: 3.099879026412964, Test_Loss: 3.3017141819000244 *\n",
      "Epoch: 9, Train_Loss: 3.100989818572998, Test_Loss: 3.837585926055908\n",
      "Epoch: 9, Train_Loss: 3.095839500427246, Test_Loss: 4.072777271270752\n",
      "Epoch: 9, Train_Loss: 3.1006126403808594, Test_Loss: 3.3488845825195312 *\n",
      "Epoch: 9, Train_Loss: 3.1061911582946777, Test_Loss: 3.1009140014648438 *\n",
      "Epoch: 9, Train_Loss: 3.105072259902954, Test_Loss: 3.10155987739563\n",
      "Epoch: 9, Train_Loss: 3.1097257137298584, Test_Loss: 3.114213228225708\n",
      "Epoch: 9, Train_Loss: 3.1098556518554688, Test_Loss: 3.1261918544769287\n",
      "Epoch: 9, Train_Loss: 3.0978212356567383, Test_Loss: 3.511772394180298\n",
      "Epoch: 9, Train_Loss: 3.097432851791382, Test_Loss: 3.7275819778442383\n",
      "Epoch: 9, Train_Loss: 3.0894532203674316, Test_Loss: 3.317570209503174 *\n",
      "Epoch: 9, Train_Loss: 3.098186492919922, Test_Loss: 3.1528611183166504 *\n",
      "Epoch: 9, Train_Loss: 3.1162798404693604, Test_Loss: 3.128915309906006 *\n",
      "Epoch: 9, Train_Loss: 3.093461275100708, Test_Loss: 3.1032018661499023 *\n",
      "Epoch: 9, Train_Loss: 3.0871787071228027, Test_Loss: 3.230281352996826\n",
      "Epoch: 9, Train_Loss: 3.0854127407073975, Test_Loss: 4.109750747680664\n",
      "Epoch: 9, Train_Loss: 3.109893798828125, Test_Loss: 4.344168663024902\n",
      "Epoch: 9, Train_Loss: 3.1487910747528076, Test_Loss: 3.150550365447998 *\n",
      "Epoch: 9, Train_Loss: 3.1312646865844727, Test_Loss: 3.1784698963165283\n",
      "Epoch: 9, Train_Loss: 3.1146883964538574, Test_Loss: 3.0826261043548584 *\n",
      "Epoch: 9, Train_Loss: 3.079381227493286, Test_Loss: 3.0858638286590576\n",
      "Epoch: 9, Train_Loss: 3.1232106685638428, Test_Loss: 3.0895135402679443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 3.096605062484741, Test_Loss: 3.10335636138916\n",
      "Epoch: 9, Train_Loss: 3.082888126373291, Test_Loss: 3.1292104721069336\n",
      "Epoch: 9, Train_Loss: 3.113910436630249, Test_Loss: 3.106079339981079 *\n",
      "Epoch: 9, Train_Loss: 3.100295066833496, Test_Loss: 3.081281900405884 *\n",
      "Epoch: 9, Train_Loss: 3.200294017791748, Test_Loss: 3.1937780380249023\n",
      "Epoch: 9, Train_Loss: 3.1663761138916016, Test_Loss: 3.4892921447753906\n",
      "Epoch: 9, Train_Loss: 3.1393940448760986, Test_Loss: 3.2444381713867188 *\n",
      "Epoch: 9, Train_Loss: 3.09088134765625, Test_Loss: 3.150486469268799 *\n",
      "Epoch: 9, Train_Loss: 3.0765440464019775, Test_Loss: 3.0784482955932617 *\n",
      "Epoch: 9, Train_Loss: 3.0939388275146484, Test_Loss: 3.078401803970337 *\n",
      "Epoch: 9, Train_Loss: 3.0695674419403076, Test_Loss: 3.0729105472564697 *\n",
      "Epoch: 9, Train_Loss: 3.0733983516693115, Test_Loss: 3.0723159313201904 *\n",
      "Epoch: 9, Train_Loss: 3.0831234455108643, Test_Loss: 3.1627209186553955\n",
      "Epoch: 9, Train_Loss: 3.0935471057891846, Test_Loss: 8.258696556091309\n",
      "Epoch: 9, Train_Loss: 3.1644222736358643, Test_Loss: 3.2158095836639404 *\n",
      "Epoch: 9, Train_Loss: 3.070044755935669, Test_Loss: 3.072047233581543 *\n",
      "Epoch: 9, Train_Loss: 3.149693012237549, Test_Loss: 3.0690815448760986 *\n",
      "Epoch: 9, Train_Loss: 3.0782291889190674, Test_Loss: 3.070486545562744\n",
      "Epoch: 9, Train_Loss: 3.088677406311035, Test_Loss: 3.067089080810547 *\n",
      "Epoch: 9, Train_Loss: 3.1381120681762695, Test_Loss: 3.0639426708221436 *\n",
      "Epoch: 9, Train_Loss: 3.317152500152588, Test_Loss: 3.064026355743408\n",
      "Epoch: 9, Train_Loss: 3.067697763442993, Test_Loss: 3.064100742340088\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 3.088836193084717, Test_Loss: 3.0632760524749756 *\n",
      "Epoch: 9, Train_Loss: 3.0653598308563232, Test_Loss: 3.0632946491241455\n",
      "Epoch: 9, Train_Loss: 3.055776596069336, Test_Loss: 3.0624608993530273 *\n",
      "Epoch: 9, Train_Loss: 3.0568714141845703, Test_Loss: 3.069343090057373\n",
      "Epoch: 9, Train_Loss: 3.0538041591644287, Test_Loss: 3.073873281478882\n",
      "Epoch: 9, Train_Loss: 3.0694549083709717, Test_Loss: 3.0710666179656982 *\n",
      "Epoch: 9, Train_Loss: 3.07265305519104, Test_Loss: 3.0592565536499023 *\n",
      "Epoch: 9, Train_Loss: 3.0772132873535156, Test_Loss: 3.049961805343628 *\n",
      "Epoch: 9, Train_Loss: 3.0684642791748047, Test_Loss: 3.052638530731201\n",
      "Epoch: 9, Train_Loss: 3.0667872428894043, Test_Loss: 3.051751136779785 *\n",
      "Epoch: 9, Train_Loss: 3.066502094268799, Test_Loss: 3.0485894680023193 *\n",
      "Epoch: 9, Train_Loss: 3.054999828338623, Test_Loss: 3.053621292114258\n",
      "Epoch: 9, Train_Loss: 3.0449392795562744, Test_Loss: 3.0511045455932617 *\n",
      "Epoch: 9, Train_Loss: 3.06679368019104, Test_Loss: 3.0509798526763916 *\n",
      "Epoch: 9, Train_Loss: 3.0675208568573, Test_Loss: 3.0518412590026855\n",
      "Epoch: 9, Train_Loss: 3.068310022354126, Test_Loss: 3.048879861831665 *\n",
      "Epoch: 9, Train_Loss: 3.044142007827759, Test_Loss: 3.044414758682251 *\n",
      "Epoch: 9, Train_Loss: 3.1020805835723877, Test_Loss: 3.044750690460205\n",
      "Epoch: 9, Train_Loss: 3.0961155891418457, Test_Loss: 3.0450408458709717\n",
      "Epoch: 9, Train_Loss: 3.0815248489379883, Test_Loss: 3.0437989234924316 *\n",
      "Epoch: 9, Train_Loss: 3.042750835418701, Test_Loss: 3.0463504791259766\n",
      "Epoch: 9, Train_Loss: 3.0696120262145996, Test_Loss: 3.1002180576324463\n",
      "Epoch: 9, Train_Loss: 3.040024518966675, Test_Loss: 4.755320072174072\n",
      "Epoch: 9, Train_Loss: 3.0586912631988525, Test_Loss: 6.945887088775635\n",
      "Epoch: 9, Train_Loss: 3.044811964035034, Test_Loss: 3.0446183681488037 *\n",
      "Epoch: 9, Train_Loss: 3.065847158432007, Test_Loss: 3.0349912643432617 *\n",
      "Epoch: 9, Train_Loss: 3.920401096343994, Test_Loss: 3.0780110359191895\n",
      "Epoch: 9, Train_Loss: 7.029402256011963, Test_Loss: 3.08551287651062\n",
      "Epoch: 9, Train_Loss: 3.682483673095703, Test_Loss: 3.0848124027252197 *\n",
      "Epoch: 9, Train_Loss: 3.0566482543945312, Test_Loss: 3.0640318393707275 *\n",
      "Epoch: 9, Train_Loss: 3.043063163757324, Test_Loss: 3.1702418327331543\n",
      "Epoch: 9, Train_Loss: 3.2279553413391113, Test_Loss: 3.0411813259124756 *\n",
      "Epoch: 9, Train_Loss: 3.109632968902588, Test_Loss: 3.041703939437866\n",
      "Epoch: 9, Train_Loss: 3.039728879928589, Test_Loss: 3.0621423721313477\n",
      "Epoch: 9, Train_Loss: 3.0292809009552, Test_Loss: 3.0420215129852295 *\n",
      "Epoch: 9, Train_Loss: 3.0917418003082275, Test_Loss: 3.0361058712005615 *\n",
      "Epoch: 9, Train_Loss: 3.0414278507232666, Test_Loss: 3.0945589542388916\n",
      "Epoch: 9, Train_Loss: 3.0414886474609375, Test_Loss: 3.1197357177734375\n",
      "Epoch: 9, Train_Loss: 3.317359209060669, Test_Loss: 3.103976011276245 *\n",
      "Epoch: 9, Train_Loss: 4.479666709899902, Test_Loss: 3.1409335136413574\n",
      "Epoch: 9, Train_Loss: 4.199658393859863, Test_Loss: 3.0450387001037598 *\n",
      "Epoch: 9, Train_Loss: 3.157196521759033, Test_Loss: 3.061614513397217\n",
      "Epoch: 9, Train_Loss: 3.152924060821533, Test_Loss: 3.043745756149292 *\n",
      "Epoch: 9, Train_Loss: 5.348934173583984, Test_Loss: 3.033174514770508 *\n",
      "Epoch: 9, Train_Loss: 4.0853142738342285, Test_Loss: 3.0413410663604736\n",
      "Epoch: 9, Train_Loss: 3.06821346282959, Test_Loss: 3.0330188274383545 *\n",
      "Epoch: 9, Train_Loss: 3.0484094619750977, Test_Loss: 3.028449535369873 *\n",
      "Epoch: 9, Train_Loss: 3.726155996322632, Test_Loss: 3.0330517292022705\n",
      "Epoch: 9, Train_Loss: 4.801850318908691, Test_Loss: 3.039335012435913\n",
      "Epoch: 9, Train_Loss: 3.835178852081299, Test_Loss: 3.043962001800537\n",
      "Epoch: 9, Train_Loss: 3.0323662757873535, Test_Loss: 3.036663055419922 *\n",
      "Epoch: 9, Train_Loss: 3.0304200649261475, Test_Loss: 3.0104658603668213 *\n",
      "Epoch: 9, Train_Loss: 3.3482894897460938, Test_Loss: 3.0361528396606445\n",
      "Epoch: 9, Train_Loss: 3.4666192531585693, Test_Loss: 3.0985734462738037\n",
      "Epoch: 10, Train_Loss: 3.0304038524627686, Test_Loss: 3.2199246883392334 *\n",
      "Epoch: 10, Train_Loss: 3.059969425201416, Test_Loss: 3.1456246376037598 *\n",
      "Epoch: 10, Train_Loss: 3.1257236003875732, Test_Loss: 3.0317792892456055 *\n",
      "Epoch: 10, Train_Loss: 3.1725974082946777, Test_Loss: 3.0997910499572754\n",
      "Epoch: 10, Train_Loss: 3.091994285583496, Test_Loss: 3.211256742477417\n",
      "Epoch: 10, Train_Loss: 3.4462268352508545, Test_Loss: 3.280777931213379\n",
      "Epoch: 10, Train_Loss: 3.153106212615967, Test_Loss: 3.016967296600342 *\n",
      "Epoch: 10, Train_Loss: 3.0665442943573, Test_Loss: 3.191298484802246\n",
      "Epoch: 10, Train_Loss: 3.270881414413452, Test_Loss: 3.3123531341552734\n",
      "Epoch: 10, Train_Loss: 3.3334107398986816, Test_Loss: 3.0332343578338623 *\n",
      "Epoch: 10, Train_Loss: 3.4829368591308594, Test_Loss: 3.0635290145874023\n",
      "Epoch: 10, Train_Loss: 3.2461724281311035, Test_Loss: 3.0005340576171875 *\n",
      "Epoch: 10, Train_Loss: 3.061824321746826, Test_Loss: 3.0340847969055176\n",
      "Epoch: 10, Train_Loss: 3.1258485317230225, Test_Loss: 3.015228271484375 *\n",
      "Epoch: 10, Train_Loss: 3.0761220455169678, Test_Loss: 3.719559907913208\n",
      "Epoch: 10, Train_Loss: 3.0131309032440186, Test_Loss: 3.2782280445098877 *\n",
      "Epoch: 10, Train_Loss: 2.9950404167175293, Test_Loss: 3.658855676651001\n",
      "Epoch: 10, Train_Loss: 2.9967057704925537, Test_Loss: 3.7356584072113037\n",
      "Epoch: 10, Train_Loss: 2.996091365814209, Test_Loss: 3.1525797843933105 *\n",
      "Epoch: 10, Train_Loss: 2.993443489074707, Test_Loss: 3.563229560852051\n",
      "Epoch: 10, Train_Loss: 3.0173263549804688, Test_Loss: 3.10195255279541 *\n",
      "Epoch: 10, Train_Loss: 3.072880506515503, Test_Loss: 2.998110771179199 *\n",
      "Epoch: 10, Train_Loss: 3.0736491680145264, Test_Loss: 3.004981279373169\n",
      "Epoch: 10, Train_Loss: 3.105635166168213, Test_Loss: 3.128260612487793\n",
      "Epoch: 10, Train_Loss: 3.3261778354644775, Test_Loss: 3.2439188957214355\n",
      "Epoch: 10, Train_Loss: 3.2227213382720947, Test_Loss: 3.8548390865325928\n",
      "Epoch: 10, Train_Loss: 3.0058391094207764, Test_Loss: 3.7589194774627686 *\n",
      "Epoch: 10, Train_Loss: 3.0633656978607178, Test_Loss: 4.685070991516113\n",
      "Epoch: 10, Train_Loss: 3.3527488708496094, Test_Loss: 3.7210144996643066 *\n",
      "Epoch: 10, Train_Loss: 3.5342869758605957, Test_Loss: 3.605518341064453 *\n",
      "Epoch: 10, Train_Loss: 2.993623971939087, Test_Loss: 2.991950035095215 *\n",
      "Epoch: 10, Train_Loss: 2.983765125274658, Test_Loss: 2.99381422996521\n",
      "Epoch: 10, Train_Loss: 3.501265048980713, Test_Loss: 3.661776542663574\n",
      "Epoch: 10, Train_Loss: 3.6921043395996094, Test_Loss: 4.4274091720581055\n",
      "Epoch: 10, Train_Loss: 3.164778470993042, Test_Loss: 3.040572166442871 *\n",
      "Epoch: 10, Train_Loss: 3.0062646865844727, Test_Loss: 3.072993755340576\n",
      "Epoch: 10, Train_Loss: 2.9948275089263916, Test_Loss: 2.983114242553711 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 3.5012240409851074, Test_Loss: 3.1518442630767822\n",
      "Epoch: 10, Train_Loss: 4.610260963439941, Test_Loss: 3.2771315574645996\n",
      "Epoch: 10, Train_Loss: 3.167187213897705, Test_Loss: 3.4776484966278076\n",
      "Epoch: 10, Train_Loss: 3.0007948875427246, Test_Loss: 4.1831183433532715\n",
      "Epoch: 10, Train_Loss: 2.978510618209839, Test_Loss: 3.299816608428955 *\n",
      "Epoch: 10, Train_Loss: 2.9735138416290283, Test_Loss: 2.9747707843780518 *\n",
      "Epoch: 10, Train_Loss: 3.403867721557617, Test_Loss: 2.982028007507324\n",
      "Epoch: 10, Train_Loss: 3.0163912773132324, Test_Loss: 2.9931113719940186\n",
      "Epoch: 10, Train_Loss: 3.0250256061553955, Test_Loss: 3.011361598968506\n",
      "Epoch: 10, Train_Loss: 2.968986749649048, Test_Loss: 3.2380642890930176\n",
      "Epoch: 10, Train_Loss: 2.9863717555999756, Test_Loss: 3.5797924995422363\n",
      "Epoch: 10, Train_Loss: 20.068241119384766, Test_Loss: 3.2692980766296387 *\n",
      "Epoch: 10, Train_Loss: 3.4399218559265137, Test_Loss: 3.0539863109588623 *\n",
      "Epoch: 10, Train_Loss: 4.340148448944092, Test_Loss: 3.0010926723480225 *\n",
      "Epoch: 10, Train_Loss: 5.713232040405273, Test_Loss: 2.9787843227386475 *\n",
      "Epoch: 10, Train_Loss: 2.9728550910949707, Test_Loss: 3.0498549938201904\n",
      "Epoch: 10, Train_Loss: 3.023127794265747, Test_Loss: 3.697127103805542\n",
      "Epoch: 10, Train_Loss: 4.681546211242676, Test_Loss: 4.356803894042969\n",
      "Epoch: 10, Train_Loss: 13.235587120056152, Test_Loss: 3.1553661823272705 *\n",
      "Epoch: 10, Train_Loss: 3.285771369934082, Test_Loss: 3.058682680130005 *\n",
      "Epoch: 10, Train_Loss: 2.9759621620178223, Test_Loss: 2.961761474609375 *\n",
      "Epoch: 10, Train_Loss: 8.755292892456055, Test_Loss: 2.97757887840271\n",
      "Epoch: 10, Train_Loss: 3.4925906658172607, Test_Loss: 2.9707868099212646 *\n",
      "Epoch: 10, Train_Loss: 2.9705183506011963, Test_Loss: 2.988579750061035\n",
      "Epoch: 10, Train_Loss: 2.9716579914093018, Test_Loss: 3.024709701538086\n",
      "Epoch: 10, Train_Loss: 2.9639132022857666, Test_Loss: 3.021716594696045 *\n",
      "Epoch: 10, Train_Loss: 2.9600863456726074, Test_Loss: 2.982889413833618 *\n",
      "Epoch: 10, Train_Loss: 2.9646496772766113, Test_Loss: 3.027456045150757\n",
      "Epoch: 10, Train_Loss: 2.9569389820098877, Test_Loss: 3.3121163845062256\n",
      "Epoch: 10, Train_Loss: 2.9619553089141846, Test_Loss: 3.0262837409973145 *\n",
      "Epoch: 10, Train_Loss: 2.954564094543457, Test_Loss: 3.098336935043335\n",
      "Epoch: 10, Train_Loss: 2.965648651123047, Test_Loss: 2.9564383029937744 *\n",
      "Epoch: 10, Train_Loss: 2.9590487480163574, Test_Loss: 2.958080291748047\n",
      "Epoch: 10, Train_Loss: 2.957178831100464, Test_Loss: 2.957951784133911 *\n",
      "Epoch: 10, Train_Loss: 2.9909424781799316, Test_Loss: 2.9583628177642822\n",
      "Epoch: 10, Train_Loss: 3.0112743377685547, Test_Loss: 2.9532954692840576 *\n",
      "Epoch: 10, Train_Loss: 2.9795026779174805, Test_Loss: 7.919733047485352\n",
      "Epoch: 10, Train_Loss: 2.9599177837371826, Test_Loss: 3.964027166366577 *\n",
      "Epoch: 10, Train_Loss: 2.9503211975097656, Test_Loss: 2.964264392852783 *\n",
      "Epoch: 10, Train_Loss: 2.957443952560425, Test_Loss: 2.959879159927368 *\n",
      "Epoch: 10, Train_Loss: 2.95166015625, Test_Loss: 2.952054500579834 *\n",
      "Epoch: 10, Train_Loss: 2.9468870162963867, Test_Loss: 2.9533135890960693\n",
      "Epoch: 10, Train_Loss: 2.944624423980713, Test_Loss: 2.9611785411834717\n",
      "Epoch: 10, Train_Loss: 2.951568126678467, Test_Loss: 2.966437578201294\n",
      "Epoch: 10, Train_Loss: 2.9418890476226807, Test_Loss: 2.96919322013855\n",
      "Epoch: 10, Train_Loss: 2.939725399017334, Test_Loss: 2.9665069580078125 *\n",
      "Epoch: 10, Train_Loss: 2.935894012451172, Test_Loss: 2.980342149734497\n",
      "Epoch: 10, Train_Loss: 2.9397478103637695, Test_Loss: 2.9788818359375 *\n",
      "Epoch: 10, Train_Loss: 2.9440035820007324, Test_Loss: 2.9498324394226074 *\n",
      "Epoch: 10, Train_Loss: 2.9607741832733154, Test_Loss: 2.9485418796539307 *\n",
      "Epoch: 10, Train_Loss: 2.9479281902313232, Test_Loss: 2.9620745182037354\n",
      "Epoch: 10, Train_Loss: 2.9492862224578857, Test_Loss: 2.9620044231414795 *\n",
      "Epoch: 10, Train_Loss: 2.9787166118621826, Test_Loss: 2.9477086067199707 *\n",
      "Epoch: 10, Train_Loss: 11.853656768798828, Test_Loss: 2.963815212249756\n",
      "Epoch: 10, Train_Loss: 3.0586884021759033, Test_Loss: 2.957357883453369 *\n",
      "Epoch: 10, Train_Loss: 2.9515469074249268, Test_Loss: 2.954061985015869 *\n",
      "Epoch: 10, Train_Loss: 2.9597721099853516, Test_Loss: 2.964226245880127\n",
      "Epoch: 10, Train_Loss: 2.9596474170684814, Test_Loss: 2.9605977535247803 *\n",
      "Epoch: 10, Train_Loss: 2.958524227142334, Test_Loss: 2.9639763832092285\n",
      "Epoch: 10, Train_Loss: 2.9513673782348633, Test_Loss: 2.9748411178588867\n",
      "Epoch: 10, Train_Loss: 2.9667656421661377, Test_Loss: 2.9658589363098145 *\n",
      "Epoch: 10, Train_Loss: 3.150620222091675, Test_Loss: 2.9568333625793457 *\n",
      "Epoch: 10, Train_Loss: 3.1207642555236816, Test_Loss: 2.9586966037750244\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 3.085934638977051, Test_Loss: 2.945103406906128 *\n",
      "Epoch: 10, Train_Loss: 2.9349989891052246, Test_Loss: 2.946599245071411\n",
      "Epoch: 10, Train_Loss: 3.0628299713134766, Test_Loss: 2.9509503841400146\n",
      "Epoch: 10, Train_Loss: 3.0221810340881348, Test_Loss: 2.9963185787200928\n",
      "Epoch: 10, Train_Loss: 3.091986656188965, Test_Loss: 3.2367727756500244\n",
      "Epoch: 10, Train_Loss: 3.0366783142089844, Test_Loss: 8.550045013427734\n",
      "Epoch: 10, Train_Loss: 3.018887519836426, Test_Loss: 2.9480600357055664 *\n",
      "Epoch: 10, Train_Loss: 2.942539691925049, Test_Loss: 2.91648268699646 *\n",
      "Epoch: 10, Train_Loss: 2.9346535205841064, Test_Loss: 2.935699462890625\n",
      "Epoch: 10, Train_Loss: 2.990778923034668, Test_Loss: 2.9488816261291504\n",
      "Epoch: 10, Train_Loss: 2.9342246055603027, Test_Loss: 2.948070526123047 *\n",
      "Epoch: 10, Train_Loss: 2.917668581008911, Test_Loss: 2.927147150039673 *\n",
      "Epoch: 10, Train_Loss: 2.9124796390533447, Test_Loss: 3.0782504081726074\n",
      "Epoch: 10, Train_Loss: 2.912323474884033, Test_Loss: 2.956068277359009 *\n",
      "Epoch: 10, Train_Loss: 3.274658679962158, Test_Loss: 2.9056906700134277 *\n",
      "Epoch: 10, Train_Loss: 8.250556945800781, Test_Loss: 2.9608073234558105\n",
      "Epoch: 10, Train_Loss: 2.9128925800323486, Test_Loss: 2.9117989540100098 *\n",
      "Epoch: 10, Train_Loss: 2.924574613571167, Test_Loss: 2.9228179454803467\n",
      "Epoch: 10, Train_Loss: 2.93387770652771, Test_Loss: 2.938889503479004\n",
      "Epoch: 10, Train_Loss: 2.9233896732330322, Test_Loss: 2.983196973800659\n",
      "Epoch: 10, Train_Loss: 2.9110686779022217, Test_Loss: 2.9794018268585205 *\n",
      "Epoch: 10, Train_Loss: 2.9100165367126465, Test_Loss: 3.0269503593444824\n",
      "Epoch: 10, Train_Loss: 2.912294626235962, Test_Loss: 2.9520840644836426 *\n",
      "Epoch: 10, Train_Loss: 2.9332685470581055, Test_Loss: 2.917081356048584 *\n",
      "Epoch: 10, Train_Loss: 2.921067953109741, Test_Loss: 2.90287709236145 *\n",
      "Epoch: 10, Train_Loss: 2.918060302734375, Test_Loss: 2.9027113914489746 *\n",
      "Epoch: 10, Train_Loss: 2.913064956665039, Test_Loss: 2.9055187702178955\n",
      "Epoch: 10, Train_Loss: 2.8996846675872803, Test_Loss: 2.8954484462738037 *\n",
      "Epoch: 10, Train_Loss: 2.9242758750915527, Test_Loss: 2.896512746810913\n",
      "Epoch: 10, Train_Loss: 2.8982789516448975, Test_Loss: 2.8955013751983643 *\n",
      "Epoch: 10, Train_Loss: 2.8937928676605225, Test_Loss: 2.9053847789764404\n",
      "Epoch: 10, Train_Loss: 2.9299230575561523, Test_Loss: 2.9020910263061523 *\n",
      "Epoch: 10, Train_Loss: 2.9335336685180664, Test_Loss: 2.904952049255371\n",
      "Epoch: 10, Train_Loss: 2.9092297554016113, Test_Loss: 2.902580976486206 *\n",
      "Epoch: 10, Train_Loss: 2.890629529953003, Test_Loss: 2.897933006286621 *\n",
      "Epoch: 10, Train_Loss: 2.8948771953582764, Test_Loss: 2.9386250972747803\n",
      "Epoch: 10, Train_Loss: 2.9743099212646484, Test_Loss: 3.0021321773529053\n",
      "Epoch: 10, Train_Loss: 2.986304759979248, Test_Loss: 3.188408613204956\n",
      "Epoch: 10, Train_Loss: 2.9721450805664062, Test_Loss: 2.899549961090088 *\n",
      "Epoch: 10, Train_Loss: 2.923675775527954, Test_Loss: 2.9410369396209717\n",
      "Epoch: 10, Train_Loss: 2.9334828853607178, Test_Loss: 2.978196382522583\n",
      "Epoch: 10, Train_Loss: 2.931260108947754, Test_Loss: 3.187345266342163\n",
      "Epoch: 10, Train_Loss: 2.9429612159729004, Test_Loss: 2.905991315841675 *\n",
      "Epoch: 10, Train_Loss: 2.8891942501068115, Test_Loss: 3.056652069091797\n",
      "Epoch: 10, Train_Loss: 3.0083367824554443, Test_Loss: 3.161573648452759\n",
      "Epoch: 10, Train_Loss: 2.896089553833008, Test_Loss: 3.010235071182251 *\n",
      "Epoch: 10, Train_Loss: 2.8808672428131104, Test_Loss: 2.957137107849121 *\n",
      "Epoch: 10, Train_Loss: 2.880424737930298, Test_Loss: 2.8844754695892334 *\n",
      "Epoch: 10, Train_Loss: 2.8773136138916016, Test_Loss: 2.8917722702026367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 2.8799448013305664, Test_Loss: 2.8962972164154053\n",
      "Epoch: 10, Train_Loss: 2.877365827560425, Test_Loss: 3.4288170337677\n",
      "Epoch: 10, Train_Loss: 3.2372779846191406, Test_Loss: 3.3591129779815674 *\n",
      "Epoch: 10, Train_Loss: 7.2428178787231445, Test_Loss: 3.3150839805603027 *\n",
      "Epoch: 10, Train_Loss: 2.8917369842529297, Test_Loss: 3.6940276622772217\n",
      "Epoch: 10, Train_Loss: 2.8806519508361816, Test_Loss: 3.0267856121063232 *\n",
      "Epoch: 10, Train_Loss: 2.8810651302337646, Test_Loss: 3.422771453857422\n",
      "Epoch: 10, Train_Loss: 2.870790958404541, Test_Loss: 3.0637378692626953 *\n",
      "Epoch: 10, Train_Loss: 2.8680367469787598, Test_Loss: 2.879828453063965 *\n",
      "Epoch: 10, Train_Loss: 2.8722779750823975, Test_Loss: 2.8944292068481445\n",
      "Epoch: 10, Train_Loss: 2.8691298961639404, Test_Loss: 2.948307514190674\n",
      "Epoch: 10, Train_Loss: 2.865910530090332, Test_Loss: 3.03338360786438\n",
      "Epoch: 10, Train_Loss: 2.863672971725464, Test_Loss: 3.808866500854492\n",
      "Epoch: 10, Train_Loss: 2.940284252166748, Test_Loss: 3.2044456005096436 *\n",
      "Epoch: 10, Train_Loss: 2.9388649463653564, Test_Loss: 5.064696788787842\n",
      "Epoch: 10, Train_Loss: 2.952882766723633, Test_Loss: 3.347764492034912 *\n",
      "Epoch: 10, Train_Loss: 2.9149410724639893, Test_Loss: 3.726456880569458\n",
      "Epoch: 10, Train_Loss: 2.8686680793762207, Test_Loss: 2.8907127380371094 *\n",
      "Epoch: 10, Train_Loss: 2.995448350906372, Test_Loss: 2.871046543121338 *\n",
      "Epoch: 10, Train_Loss: 3.114325523376465, Test_Loss: 3.3238894939422607\n",
      "Epoch: 10, Train_Loss: 3.073385000228882, Test_Loss: 4.3664937019348145\n",
      "Epoch: 10, Train_Loss: 3.0190789699554443, Test_Loss: 3.1655192375183105 *\n",
      "Epoch: 10, Train_Loss: 2.858532428741455, Test_Loss: 2.982673406600952 *\n",
      "Epoch: 10, Train_Loss: 2.8532116413116455, Test_Loss: 2.8721354007720947 *\n",
      "Epoch: 10, Train_Loss: 2.854997158050537, Test_Loss: 2.9691596031188965\n",
      "Epoch: 10, Train_Loss: 2.8709118366241455, Test_Loss: 3.2419674396514893\n",
      "Epoch: 10, Train_Loss: 2.8601632118225098, Test_Loss: 3.1397292613983154 *\n",
      "Epoch: 10, Train_Loss: 2.8661553859710693, Test_Loss: 4.120604991912842\n",
      "Epoch: 10, Train_Loss: 2.849787712097168, Test_Loss: 3.306060552597046 *\n",
      "Epoch: 10, Train_Loss: 2.8485333919525146, Test_Loss: 2.8534762859344482 *\n",
      "Epoch: 10, Train_Loss: 2.8542184829711914, Test_Loss: 2.8590903282165527\n",
      "Epoch: 10, Train_Loss: 2.8753468990325928, Test_Loss: 2.8741259574890137\n",
      "Epoch: 10, Train_Loss: 3.0201973915100098, Test_Loss: 2.886730909347534\n",
      "Epoch: 10, Train_Loss: 3.0318124294281006, Test_Loss: 3.009098768234253\n",
      "Epoch: 10, Train_Loss: 3.046154260635376, Test_Loss: 3.3721303939819336\n",
      "Epoch: 10, Train_Loss: 2.9135568141937256, Test_Loss: 3.2064504623413086 *\n",
      "Epoch: 10, Train_Loss: 2.9901716709136963, Test_Loss: 2.940546989440918 *\n",
      "Epoch: 10, Train_Loss: 2.941011905670166, Test_Loss: 2.875807285308838 *\n",
      "Epoch: 10, Train_Loss: 2.947373628616333, Test_Loss: 2.855771064758301 *\n",
      "Epoch: 10, Train_Loss: 2.9817328453063965, Test_Loss: 2.9166688919067383\n",
      "Epoch: 10, Train_Loss: 3.1549072265625, Test_Loss: 3.358184576034546\n",
      "Epoch: 10, Train_Loss: 2.8484108448028564, Test_Loss: 4.280181407928467\n",
      "Epoch: 10, Train_Loss: 2.8473262786865234, Test_Loss: 3.3073928356170654 *\n",
      "Epoch: 10, Train_Loss: 5.533478260040283, Test_Loss: 2.932762861251831 *\n",
      "Epoch: 10, Train_Loss: 3.4098453521728516, Test_Loss: 2.845670461654663 *\n",
      "Epoch: 10, Train_Loss: 2.867663621902466, Test_Loss: 2.8392436504364014 *\n",
      "Epoch: 10, Train_Loss: 2.900730848312378, Test_Loss: 2.8352134227752686 *\n",
      "Epoch: 10, Train_Loss: 2.8848743438720703, Test_Loss: 2.8438990116119385\n",
      "Epoch: 10, Train_Loss: 2.8656389713287354, Test_Loss: 2.86847186088562\n",
      "Epoch: 10, Train_Loss: 2.8339626789093018, Test_Loss: 2.8847696781158447\n",
      "Epoch: 10, Train_Loss: 2.875269889831543, Test_Loss: 2.8349769115448 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 2.968834638595581, Test_Loss: 2.9214727878570557\n",
      "Epoch: 10, Train_Loss: 2.9223451614379883, Test_Loss: 3.099504232406616\n",
      "Epoch: 10, Train_Loss: 2.894951343536377, Test_Loss: 3.0509471893310547 *\n",
      "Epoch: 10, Train_Loss: 2.917045831680298, Test_Loss: 3.037808895111084 *\n",
      "Epoch: 10, Train_Loss: 2.8584320545196533, Test_Loss: 2.841301441192627 *\n",
      "Epoch: 10, Train_Loss: 2.8539557456970215, Test_Loss: 2.840528964996338 *\n",
      "Epoch: 10, Train_Loss: 2.837972402572632, Test_Loss: 2.834104061126709 *\n",
      "Epoch: 10, Train_Loss: 2.8744406700134277, Test_Loss: 2.8365488052368164\n",
      "Epoch: 10, Train_Loss: 2.8572559356689453, Test_Loss: 2.84275484085083\n",
      "Epoch: 10, Train_Loss: 2.8325982093811035, Test_Loss: 5.743261337280273\n",
      "Epoch: 10, Train_Loss: 2.832857131958008, Test_Loss: 5.267416954040527 *\n",
      "Epoch: 10, Train_Loss: 2.8624932765960693, Test_Loss: 2.832972764968872 *\n",
      "Epoch: 10, Train_Loss: 2.8633201122283936, Test_Loss: 2.825517416000366 *\n",
      "Epoch: 10, Train_Loss: 2.82161283493042, Test_Loss: 2.826417922973633\n",
      "Epoch: 10, Train_Loss: 2.8181421756744385, Test_Loss: 2.8346893787384033\n",
      "Epoch: 10, Train_Loss: 2.819596767425537, Test_Loss: 2.820984363555908 *\n",
      "Epoch: 10, Train_Loss: 2.8180224895477295, Test_Loss: 2.8224833011627197\n",
      "Epoch: 10, Train_Loss: 2.817495107650757, Test_Loss: 2.8164103031158447 *\n",
      "Epoch: 10, Train_Loss: 2.8146016597747803, Test_Loss: 2.8221492767333984\n",
      "Epoch: 10, Train_Loss: 2.813173770904541, Test_Loss: 2.817404270172119 *\n",
      "Epoch: 10, Train_Loss: 2.8139188289642334, Test_Loss: 2.8170325756073 *\n",
      "Epoch: 10, Train_Loss: 2.811649799346924, Test_Loss: 2.8194494247436523\n",
      "Epoch: 10, Train_Loss: 2.814326524734497, Test_Loss: 2.837162494659424\n",
      "Epoch: 10, Train_Loss: 2.814159631729126, Test_Loss: 2.829989433288574 *\n",
      "Epoch: 10, Train_Loss: 2.82232928276062, Test_Loss: 2.8132312297821045 *\n",
      "Epoch: 10, Train_Loss: 2.8185086250305176, Test_Loss: 2.808171510696411 *\n",
      "Epoch: 10, Train_Loss: 2.8163528442382812, Test_Loss: 2.810659408569336\n",
      "Epoch: 10, Train_Loss: 2.8273518085479736, Test_Loss: 2.8062362670898438 *\n",
      "Epoch: 10, Train_Loss: 2.82292103767395, Test_Loss: 2.805504083633423 *\n",
      "Epoch: 10, Train_Loss: 2.805161952972412, Test_Loss: 2.8076858520507812\n",
      "Epoch: 10, Train_Loss: 2.808450698852539, Test_Loss: 2.801886558532715 *\n",
      "Epoch: 10, Train_Loss: 2.8136374950408936, Test_Loss: 2.7991816997528076 *\n",
      "Epoch: 10, Train_Loss: 2.8270366191864014, Test_Loss: 2.8062145709991455\n",
      "Epoch: 10, Train_Loss: 2.8020265102386475, Test_Loss: 2.800051212310791 *\n",
      "Epoch: 10, Train_Loss: 2.806462049484253, Test_Loss: 2.802919864654541\n",
      "Epoch: 10, Train_Loss: 2.8002405166625977, Test_Loss: 2.8010764122009277 *\n",
      "Epoch: 10, Train_Loss: 2.837231159210205, Test_Loss: 2.798375129699707 *\n",
      "Epoch: 10, Train_Loss: 2.8557517528533936, Test_Loss: 2.804034471511841\n",
      "Epoch: 10, Train_Loss: 2.8416314125061035, Test_Loss: 2.7978479862213135 *\n",
      "Epoch: 10, Train_Loss: 2.8143203258514404, Test_Loss: 2.8360066413879395\n",
      "Epoch: 10, Train_Loss: 2.795055389404297, Test_Loss: 2.827563762664795 *\n",
      "Epoch: 10, Train_Loss: 2.8545100688934326, Test_Loss: 7.910567283630371\n",
      "Epoch: 10, Train_Loss: 2.8042171001434326, Test_Loss: 3.194450616836548 *\n",
      "Epoch: 10, Train_Loss: 2.8004472255706787, Test_Loss: 2.7947633266448975 *\n",
      "Epoch: 10, Train_Loss: 2.819357395172119, Test_Loss: 2.8133749961853027\n",
      "Epoch: 10, Train_Loss: 2.802969217300415, Test_Loss: 2.8549318313598633\n",
      "Epoch: 10, Train_Loss: 2.9374470710754395, Test_Loss: 2.858834743499756\n",
      "Epoch: 10, Train_Loss: 2.875209093093872, Test_Loss: 2.795924186706543 *\n",
      "Epoch: 10, Train_Loss: 2.838449478149414, Test_Loss: 2.8887364864349365\n",
      "Epoch: 10, Train_Loss: 2.801464080810547, Test_Loss: 2.863736629486084 *\n",
      "Epoch: 10, Train_Loss: 2.798452854156494, Test_Loss: 2.788800001144409 *\n",
      "Epoch: 10, Train_Loss: 2.7983007431030273, Test_Loss: 2.8282623291015625\n",
      "Epoch: 10, Train_Loss: 2.7889134883880615, Test_Loss: 2.7926011085510254 *\n",
      "Epoch: 10, Train_Loss: 2.7936999797821045, Test_Loss: 2.799583673477173\n",
      "Epoch: 10, Train_Loss: 2.799307346343994, Test_Loss: 2.7903449535369873 *\n",
      "Epoch: 10, Train_Loss: 2.812455177307129, Test_Loss: 2.913738250732422\n",
      "Epoch: 10, Train_Loss: 2.88348650932312, Test_Loss: 2.814814805984497 *\n",
      "Epoch: 10, Train_Loss: 2.7901105880737305, Test_Loss: 2.886573314666748\n",
      "Epoch: 10, Train_Loss: 2.8517045974731445, Test_Loss: 2.8476223945617676 *\n",
      "Epoch: 10, Train_Loss: 2.80115008354187, Test_Loss: 2.8150722980499268 *\n",
      "Epoch: 10, Train_Loss: 2.801762104034424, Test_Loss: 2.794858932495117 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 2.883666753768921, Test_Loss: 2.7909328937530518 *\n",
      "Epoch: 10, Train_Loss: 3.024859666824341, Test_Loss: 2.7846109867095947 *\n",
      "Epoch: 10, Train_Loss: 2.7918543815612793, Test_Loss: 2.784998655319214\n",
      "Epoch: 10, Train_Loss: 2.81210994720459, Test_Loss: 2.7908225059509277\n",
      "Epoch: 10, Train_Loss: 2.7725472450256348, Test_Loss: 2.789288282394409 *\n",
      "Epoch: 10, Train_Loss: 2.7718029022216797, Test_Loss: 2.782172918319702 *\n",
      "Epoch: 10, Train_Loss: 2.7721521854400635, Test_Loss: 2.7977192401885986\n",
      "Epoch: 10, Train_Loss: 2.7681350708007812, Test_Loss: 2.7966725826263428 *\n",
      "Epoch: 10, Train_Loss: 2.786541700363159, Test_Loss: 2.7808303833007812 *\n",
      "Epoch: 10, Train_Loss: 2.788743734359741, Test_Loss: 2.7774667739868164 *\n",
      "Epoch: 10, Train_Loss: 2.7803189754486084, Test_Loss: 2.826324224472046\n",
      "Epoch: 10, Train_Loss: 2.781733512878418, Test_Loss: 2.7879533767700195 *\n",
      "Epoch: 10, Train_Loss: 2.7943055629730225, Test_Loss: 3.127322196960449\n",
      "Epoch: 10, Train_Loss: 2.7714786529541016, Test_Loss: 2.778226137161255 *\n",
      "Epoch: 10, Train_Loss: 2.7685048580169678, Test_Loss: 2.8277411460876465\n",
      "Epoch: 10, Train_Loss: 2.7632007598876953, Test_Loss: 2.853405714035034\n",
      "Epoch: 10, Train_Loss: 2.7814927101135254, Test_Loss: 3.137990951538086\n",
      "Epoch: 10, Train_Loss: 2.788159132003784, Test_Loss: 2.807008981704712 *\n",
      "Epoch: 10, Train_Loss: 2.7827188968658447, Test_Loss: 2.8799257278442383\n",
      "Epoch: 10, Train_Loss: 2.763324737548828, Test_Loss: 3.007920265197754\n",
      "Epoch: 10, Train_Loss: 2.8206946849823, Test_Loss: 2.966229200363159 *\n",
      "Epoch: 10, Train_Loss: 2.813889741897583, Test_Loss: 2.7952561378479004 *\n",
      "Epoch: 10, Train_Loss: 2.785771131515503, Test_Loss: 2.776820182800293 *\n",
      "Epoch: 10, Train_Loss: 2.7675511837005615, Test_Loss: 2.7648749351501465 *\n",
      "Epoch: 10, Train_Loss: 2.782942533493042, Test_Loss: 2.795490026473999\n",
      "Epoch: 10, Train_Loss: 2.7581448554992676, Test_Loss: 2.9548044204711914\n",
      "Epoch: 10, Train_Loss: 2.772336721420288, Test_Loss: 3.494602680206299\n",
      "Epoch: 10, Train_Loss: 2.772275924682617, Test_Loss: 3.033616304397583 *\n",
      "Epoch: 10, Train_Loss: 2.785170078277588, Test_Loss: 3.621788740158081\n",
      "Epoch: 10, Train_Loss: 4.535639762878418, Test_Loss: 3.086895704269409 *\n",
      "Epoch: 10, Train_Loss: 6.443493843078613, Test_Loss: 3.226771593093872\n",
      "Epoch: 10, Train_Loss: 2.7841386795043945, Test_Loss: 3.075416326522827 *\n",
      "Epoch: 10, Train_Loss: 2.7659268379211426, Test_Loss: 2.7681326866149902 *\n",
      "Epoch: 10, Train_Loss: 2.770275831222534, Test_Loss: 2.763779878616333 *\n",
      "Epoch: 10, Train_Loss: 2.9466168880462646, Test_Loss: 2.7942419052124023\n",
      "Epoch: 10, Train_Loss: 2.791738986968994, Test_Loss: 2.8968536853790283\n",
      "Epoch: 10, Train_Loss: 2.7582223415374756, Test_Loss: 3.6804330348968506\n",
      "Epoch: 10, Train_Loss: 2.74272084236145, Test_Loss: 2.941873550415039 *\n",
      "Epoch: 10, Train_Loss: 2.815185070037842, Test_Loss: 4.84520959854126\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 2.7556395530700684, Test_Loss: 3.3191661834716797 *\n",
      "Epoch: 10, Train_Loss: 2.7551071643829346, Test_Loss: 3.823195457458496\n",
      "Epoch: 10, Train_Loss: 3.3098185062408447, Test_Loss: 2.8514819145202637 *\n",
      "Epoch: 10, Train_Loss: 4.145460605621338, Test_Loss: 2.744589328765869 *\n",
      "Epoch: 10, Train_Loss: 3.6787872314453125, Test_Loss: 2.9247007369995117\n",
      "Epoch: 10, Train_Loss: 2.8795838356018066, Test_Loss: 4.100279331207275\n",
      "Epoch: 10, Train_Loss: 3.0956623554229736, Test_Loss: 3.4370641708374023 *\n",
      "Epoch: 10, Train_Loss: 5.160094261169434, Test_Loss: 2.8251900672912598 *\n",
      "Epoch: 10, Train_Loss: 3.4767026901245117, Test_Loss: 2.7575058937072754 *\n",
      "Epoch: 10, Train_Loss: 2.78448748588562, Test_Loss: 2.83681321144104\n",
      "Epoch: 10, Train_Loss: 2.7600209712982178, Test_Loss: 3.1496410369873047\n",
      "Epoch: 10, Train_Loss: 3.8227062225341797, Test_Loss: 2.9259257316589355 *\n",
      "Epoch: 10, Train_Loss: 4.369146347045898, Test_Loss: 3.8601725101470947\n",
      "Epoch: 10, Train_Loss: 3.2746548652648926, Test_Loss: 3.4312751293182373 *\n",
      "Epoch: 10, Train_Loss: 2.7510077953338623, Test_Loss: 2.775771141052246 *\n",
      "Epoch: 10, Train_Loss: 2.73657488822937, Test_Loss: 2.7331836223602295 *\n",
      "Epoch: 10, Train_Loss: 3.2360270023345947, Test_Loss: 2.7436299324035645\n",
      "Epoch: 10, Train_Loss: 2.9901959896087646, Test_Loss: 2.748218297958374\n",
      "Epoch: 10, Train_Loss: 2.7372214794158936, Test_Loss: 2.788635015487671\n",
      "Epoch: 10, Train_Loss: 2.774604320526123, Test_Loss: 3.330549716949463\n",
      "Epoch: 10, Train_Loss: 2.8631269931793213, Test_Loss: 3.2693114280700684 *\n",
      "Epoch: 10, Train_Loss: 2.886021852493286, Test_Loss: 2.874023675918579 *\n",
      "Epoch: 10, Train_Loss: 2.807865858078003, Test_Loss: 2.7616801261901855 *\n",
      "Epoch: 10, Train_Loss: 3.1463398933410645, Test_Loss: 2.7514753341674805 *\n",
      "Epoch: 10, Train_Loss: 2.828892707824707, Test_Loss: 2.7568488121032715\n",
      "Epoch: 10, Train_Loss: 2.7920782566070557, Test_Loss: 3.026299238204956\n",
      "Epoch: 10, Train_Loss: 2.985339641571045, Test_Loss: 4.044663906097412\n",
      "Epoch: 10, Train_Loss: 3.077371835708618, Test_Loss: 3.4403598308563232 *\n",
      "Epoch: 10, Train_Loss: 3.1342508792877197, Test_Loss: 2.7899069786071777 *\n",
      "Epoch: 10, Train_Loss: 2.906076669692993, Test_Loss: 2.753354549407959 *\n",
      "Epoch: 10, Train_Loss: 2.7984678745269775, Test_Loss: 2.7184085845947266 *\n",
      "Epoch: 10, Train_Loss: 2.837472915649414, Test_Loss: 2.715240001678467 *\n",
      "Epoch: 10, Train_Loss: 2.7621684074401855, Test_Loss: 2.721961736679077\n",
      "Epoch: 10, Train_Loss: 2.733130693435669, Test_Loss: 2.730742931365967\n",
      "Epoch: 10, Train_Loss: 2.711547374725342, Test_Loss: 2.76792573928833\n",
      "Epoch: 10, Train_Loss: 2.71258544921875, Test_Loss: 2.7114222049713135 *\n",
      "Epoch: 10, Train_Loss: 2.7118289470672607, Test_Loss: 2.7575016021728516\n",
      "Epoch: 10, Train_Loss: 2.7112104892730713, Test_Loss: 2.831752300262451\n",
      "Epoch: 10, Train_Loss: 2.7495503425598145, Test_Loss: 3.1048998832702637\n",
      "Epoch: 10, Train_Loss: 2.786794662475586, Test_Loss: 2.9335134029388428 *\n",
      "Epoch: 10, Train_Loss: 2.7962489128112793, Test_Loss: 2.717996597290039 *\n",
      "Epoch: 10, Train_Loss: 2.7959635257720947, Test_Loss: 2.711068868637085 *\n",
      "Epoch: 10, Train_Loss: 3.156693935394287, Test_Loss: 2.712019205093384\n",
      "Epoch: 10, Train_Loss: 2.7681431770324707, Test_Loss: 2.710874080657959 *\n",
      "Epoch: 10, Train_Loss: 2.726893663406372, Test_Loss: 2.710890531539917\n",
      "Epoch: 10, Train_Loss: 2.8197808265686035, Test_Loss: 4.105050563812256\n",
      "Epoch: 10, Train_Loss: 3.178691864013672, Test_Loss: 6.752782344818115\n",
      "Epoch: 10, Train_Loss: 3.1238040924072266, Test_Loss: 2.715350866317749 *\n",
      "Epoch: 10, Train_Loss: 2.7040605545043945, Test_Loss: 2.7019925117492676 *\n",
      "Epoch: 10, Train_Loss: 2.7037506103515625, Test_Loss: 2.7011632919311523 *\n",
      "Epoch: 10, Train_Loss: 3.2731118202209473, Test_Loss: 2.704232931137085\n",
      "Epoch: 10, Train_Loss: 3.3455874919891357, Test_Loss: 2.7011966705322266 *\n",
      "Epoch: 10, Train_Loss: 2.82859206199646, Test_Loss: 2.7021121978759766\n",
      "Epoch: 10, Train_Loss: 2.7273693084716797, Test_Loss: 2.7051773071289062\n",
      "Epoch: 10, Train_Loss: 2.714948892593384, Test_Loss: 2.7026476860046387 *\n",
      "Epoch: 10, Train_Loss: 3.49515438079834, Test_Loss: 2.7177443504333496\n",
      "Epoch: 10, Train_Loss: 4.108968734741211, Test_Loss: 2.703856945037842 *\n",
      "Epoch: 10, Train_Loss: 2.7794501781463623, Test_Loss: 2.720221757888794\n",
      "Epoch: 10, Train_Loss: 2.721017360687256, Test_Loss: 2.703598976135254 *\n",
      "Epoch: 10, Train_Loss: 2.69671368598938, Test_Loss: 2.7026748657226562 *\n",
      "Epoch: 10, Train_Loss: 2.6970036029815674, Test_Loss: 2.7094459533691406\n",
      "Epoch: 10, Train_Loss: 3.172744035720825, Test_Loss: 2.691443681716919 *\n",
      "Epoch: 10, Train_Loss: 2.7174127101898193, Test_Loss: 2.6963140964508057\n",
      "Epoch: 10, Train_Loss: 2.7373006343841553, Test_Loss: 2.694934368133545 *\n",
      "Epoch: 10, Train_Loss: 2.696504831314087, Test_Loss: 2.6902856826782227 *\n",
      "Epoch: 10, Train_Loss: 2.703282356262207, Test_Loss: 2.6888175010681152 *\n",
      "Epoch: 10, Train_Loss: 20.087787628173828, Test_Loss: 2.701716661453247\n",
      "Epoch: 10, Train_Loss: 2.6872024536132812, Test_Loss: 2.694897174835205 *\n",
      "Epoch: 10, Train_Loss: 4.547525405883789, Test_Loss: 2.71146559715271\n",
      "Epoch: 10, Train_Loss: 4.961127281188965, Test_Loss: 2.7092127799987793 *\n",
      "Epoch: 10, Train_Loss: 2.691312551498413, Test_Loss: 2.701183319091797 *\n",
      "Epoch: 10, Train_Loss: 2.769911050796509, Test_Loss: 2.69150972366333 *\n",
      "Epoch: 10, Train_Loss: 5.844646453857422, Test_Loss: 2.7021517753601074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 11.477481842041016, Test_Loss: 2.69831919670105 *\n",
      "Epoch: 10, Train_Loss: 2.7652382850646973, Test_Loss: 2.7060654163360596\n",
      "Epoch: 10, Train_Loss: 2.709754705429077, Test_Loss: 2.7773287296295166\n",
      "Epoch: 10, Train_Loss: 8.665761947631836, Test_Loss: 2.7233381271362305 *\n",
      "Epoch: 10, Train_Loss: 2.7433364391326904, Test_Loss: 6.748688697814941\n",
      "Epoch: 10, Train_Loss: 2.7081544399261475, Test_Loss: 4.752945423126221 *\n",
      "Epoch: 10, Train_Loss: 2.6874442100524902, Test_Loss: 2.7045979499816895 *\n",
      "Epoch: 10, Train_Loss: 2.677520990371704, Test_Loss: 2.693753242492676 *\n",
      "Epoch: 10, Train_Loss: 2.7040812969207764, Test_Loss: 2.6927645206451416 *\n",
      "Epoch: 10, Train_Loss: 2.680845022201538, Test_Loss: 2.7164323329925537\n",
      "Epoch: 10, Train_Loss: 2.70741868019104, Test_Loss: 2.7020978927612305 *\n",
      "Epoch: 10, Train_Loss: 2.700871467590332, Test_Loss: 2.851189613342285\n",
      "Epoch: 10, Train_Loss: 2.6941025257110596, Test_Loss: 2.9100728034973145\n",
      "Epoch: 10, Train_Loss: 2.707003116607666, Test_Loss: 2.69998836517334 *\n",
      "Epoch: 10, Train_Loss: 2.702531337738037, Test_Loss: 2.7614309787750244\n",
      "Epoch: 10, Train_Loss: 2.6942319869995117, Test_Loss: 2.692776679992676 *\n",
      "Epoch: 10, Train_Loss: 2.731966018676758, Test_Loss: 2.7058727741241455\n",
      "Epoch: 10, Train_Loss: 2.7333388328552246, Test_Loss: 2.6896562576293945 *\n",
      "Epoch: 10, Train_Loss: 2.6901676654815674, Test_Loss: 2.732940912246704\n",
      "Epoch: 10, Train_Loss: 2.6815764904022217, Test_Loss: 2.7335195541381836\n",
      "Epoch: 10, Train_Loss: 2.670051097869873, Test_Loss: 2.891394853591919\n",
      "Epoch: 10, Train_Loss: 2.672095537185669, Test_Loss: 2.7982873916625977 *\n",
      "Epoch: 10, Train_Loss: 2.6720566749572754, Test_Loss: 2.681668281555176 *\n",
      "Epoch: 10, Train_Loss: 2.662121295928955, Test_Loss: 2.6820456981658936\n",
      "Epoch: 10, Train_Loss: 2.665715217590332, Test_Loss: 2.6787281036376953 *\n",
      "Epoch: 10, Train_Loss: 2.6648054122924805, Test_Loss: 2.6699399948120117 *\n",
      "Epoch: 10, Train_Loss: 2.661362886428833, Test_Loss: 2.6707911491394043\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 2.6635255813598633, Test_Loss: 2.6647918224334717 *\n",
      "Epoch: 10, Train_Loss: 2.660776138305664, Test_Loss: 2.66646146774292\n",
      "Epoch: 10, Train_Loss: 2.659515857696533, Test_Loss: 2.663680076599121 *\n",
      "Epoch: 10, Train_Loss: 2.6789000034332275, Test_Loss: 2.6768624782562256\n",
      "Epoch: 10, Train_Loss: 2.6974246501922607, Test_Loss: 2.668442726135254 *\n",
      "Epoch: 10, Train_Loss: 2.686784267425537, Test_Loss: 2.668409824371338 *\n",
      "Epoch: 10, Train_Loss: 2.6712372303009033, Test_Loss: 2.6902565956115723\n",
      "Epoch: 10, Train_Loss: 3.347698211669922, Test_Loss: 2.6665682792663574 *\n",
      "Epoch: 10, Train_Loss: 11.055387496948242, Test_Loss: 2.6844570636749268\n",
      "Epoch: 10, Train_Loss: 2.698004722595215, Test_Loss: 3.1082823276519775\n",
      "Epoch: 10, Train_Loss: 2.6685779094696045, Test_Loss: 2.764695644378662 *\n",
      "Epoch: 10, Train_Loss: 2.6962532997131348, Test_Loss: 2.6780483722686768 *\n",
      "Epoch: 10, Train_Loss: 2.6875414848327637, Test_Loss: 2.696485757827759\n",
      "Epoch: 10, Train_Loss: 2.679260015487671, Test_Loss: 2.8514344692230225\n",
      "Epoch: 10, Train_Loss: 2.685784101486206, Test_Loss: 2.707793951034546 *\n",
      "Epoch: 10, Train_Loss: 2.701295852661133, Test_Loss: 2.7428319454193115\n",
      "Epoch: 10, Train_Loss: 2.9041948318481445, Test_Loss: 2.9546966552734375\n",
      "Epoch: 10, Train_Loss: 2.817779302597046, Test_Loss: 2.994879961013794\n",
      "Epoch: 10, Train_Loss: 2.7432198524475098, Test_Loss: 2.6992406845092773 *\n",
      "Epoch: 10, Train_Loss: 2.668226957321167, Test_Loss: 2.7371339797973633\n",
      "Epoch: 10, Train_Loss: 2.811070203781128, Test_Loss: 2.6474344730377197 *\n",
      "Epoch: 10, Train_Loss: 2.7461957931518555, Test_Loss: 2.657472848892212\n",
      "Epoch: 10, Train_Loss: 2.8215861320495605, Test_Loss: 2.726534366607666\n",
      "Epoch: 10, Train_Loss: 2.7444725036621094, Test_Loss: 3.5530810356140137\n",
      "Epoch: 10, Train_Loss: 2.700859546661377, Test_Loss: 2.788301944732666 *\n",
      "Epoch: 10, Train_Loss: 2.64509916305542, Test_Loss: 3.3134701251983643\n",
      "Epoch: 10, Train_Loss: 2.6647095680236816, Test_Loss: 3.1133265495300293 *\n",
      "Epoch: 10, Train_Loss: 2.7168707847595215, Test_Loss: 3.004533290863037 *\n",
      "Epoch: 10, Train_Loss: 2.6599206924438477, Test_Loss: 3.1316637992858887\n",
      "Epoch: 10, Train_Loss: 2.6371374130249023, Test_Loss: 2.6908962726593018 *\n",
      "Epoch: 10, Train_Loss: 2.6379456520080566, Test_Loss: 2.6344494819641113 *\n",
      "Epoch: 10, Train_Loss: 2.6373870372772217, Test_Loss: 2.6449637413024902\n",
      "Epoch: 10, Train_Loss: 3.845036745071411, Test_Loss: 2.7311198711395264\n",
      "Epoch: 10, Train_Loss: 7.268527030944824, Test_Loss: 3.3518853187561035\n",
      "Epoch: 10, Train_Loss: 2.632939100265503, Test_Loss: 3.1270012855529785 *\n",
      "Epoch: 10, Train_Loss: 2.6544764041900635, Test_Loss: 4.096893787384033\n",
      "Epoch: 10, Train_Loss: 2.663355827331543, Test_Loss: 3.3657352924346924 *\n",
      "Epoch: 10, Train_Loss: 2.6525042057037354, Test_Loss: 3.780900478363037\n",
      "Epoch: 10, Train_Loss: 2.63645601272583, Test_Loss: 2.9336259365081787 *\n",
      "Epoch: 10, Train_Loss: 2.649860143661499, Test_Loss: 2.646353006362915 *\n",
      "Epoch: 10, Train_Loss: 2.6426196098327637, Test_Loss: 2.6885344982147217\n",
      "Epoch: 10, Train_Loss: 2.6454482078552246, Test_Loss: 3.516007900238037\n",
      "Epoch: 10, Train_Loss: 2.6483664512634277, Test_Loss: 3.497943639755249 *\n",
      "Epoch: 10, Train_Loss: 2.640714645385742, Test_Loss: 2.7031445503234863 *\n",
      "Epoch: 10, Train_Loss: 2.6292850971221924, Test_Loss: 2.6855666637420654 *\n",
      "Epoch: 10, Train_Loss: 2.623450517654419, Test_Loss: 2.654115915298462 *\n",
      "Epoch: 10, Train_Loss: 2.645399332046509, Test_Loss: 2.992814540863037\n",
      "Epoch: 10, Train_Loss: 2.6239404678344727, Test_Loss: 2.7834842205047607 *\n",
      "Epoch: 10, Train_Loss: 2.6195592880249023, Test_Loss: 3.6356852054595947\n",
      "Epoch: 10, Train_Loss: 2.6495108604431152, Test_Loss: 3.4987969398498535 *\n",
      "Epoch: 10, Train_Loss: 2.653106451034546, Test_Loss: 2.8210878372192383 *\n",
      "Epoch: 10, Train_Loss: 2.6264281272888184, Test_Loss: 2.630234718322754 *\n",
      "Epoch: 10, Train_Loss: 2.6124324798583984, Test_Loss: 2.6423275470733643\n",
      "Epoch: 10, Train_Loss: 2.623973846435547, Test_Loss: 2.63948655128479 *\n",
      "Epoch: 11, Train_Loss: 2.7065629959106445, Test_Loss: 2.687145471572876 *\n",
      "Epoch: 11, Train_Loss: 2.6627824306488037, Test_Loss: 2.9609408378601074\n",
      "Epoch: 11, Train_Loss: 2.683722972869873, Test_Loss: 3.097299337387085\n",
      "Epoch: 11, Train_Loss: 2.6504342555999756, Test_Loss: 2.782581329345703 *\n",
      "Epoch: 11, Train_Loss: 2.6643428802490234, Test_Loss: 2.635500431060791 *\n",
      "Epoch: 11, Train_Loss: 2.649552345275879, Test_Loss: 2.6327993869781494 *\n",
      "Epoch: 11, Train_Loss: 2.667787790298462, Test_Loss: 2.628701686859131 *\n",
      "Epoch: 11, Train_Loss: 2.6227166652679443, Test_Loss: 2.7607760429382324\n",
      "Epoch: 11, Train_Loss: 2.736515522003174, Test_Loss: 3.736785411834717\n",
      "Epoch: 11, Train_Loss: 2.620131731033325, Test_Loss: 3.6565866470336914 *\n",
      "Epoch: 11, Train_Loss: 2.602142810821533, Test_Loss: 2.6479249000549316 *\n",
      "Epoch: 11, Train_Loss: 2.605386972427368, Test_Loss: 2.6580255031585693\n",
      "Epoch: 11, Train_Loss: 2.6017374992370605, Test_Loss: 2.602274179458618 *\n",
      "Epoch: 11, Train_Loss: 2.596569776535034, Test_Loss: 2.607841968536377\n",
      "Epoch: 11, Train_Loss: 2.599226951599121, Test_Loss: 2.613722801208496\n",
      "Epoch: 11, Train_Loss: 3.8249423503875732, Test_Loss: 2.624356746673584\n",
      "Epoch: 11, Train_Loss: 6.172484397888184, Test_Loss: 2.6568405628204346\n",
      "Epoch: 11, Train_Loss: 2.5986177921295166, Test_Loss: 2.6067705154418945 *\n",
      "Epoch: 11, Train_Loss: 2.6047720909118652, Test_Loss: 2.6050312519073486 *\n",
      "Epoch: 11, Train_Loss: 2.6012051105499268, Test_Loss: 2.712921380996704\n",
      "Epoch: 11, Train_Loss: 2.5975983142852783, Test_Loss: 2.995466709136963\n",
      "Epoch: 11, Train_Loss: 2.5938868522644043, Test_Loss: 2.873929500579834 *\n",
      "Epoch: 11, Train_Loss: 2.596230983734131, Test_Loss: 2.6300652027130127 *\n",
      "Epoch: 11, Train_Loss: 2.594592571258545, Test_Loss: 2.6021625995635986 *\n",
      "Epoch: 11, Train_Loss: 2.5936520099639893, Test_Loss: 2.596616506576538 *\n",
      "Epoch: 11, Train_Loss: 2.5917608737945557, Test_Loss: 2.6030752658843994\n",
      "Epoch: 11, Train_Loss: 2.6611759662628174, Test_Loss: 2.5991804599761963 *\n",
      "Epoch: 11, Train_Loss: 2.650057554244995, Test_Loss: 2.8986353874206543\n",
      "Epoch: 11, Train_Loss: 2.6521267890930176, Test_Loss: 7.5721354484558105\n",
      "Epoch: 11, Train_Loss: 2.629192352294922, Test_Loss: 2.6471259593963623 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 2.5901131629943848, Test_Loss: 2.59906005859375 *\n",
      "Epoch: 11, Train_Loss: 2.7721505165100098, Test_Loss: 2.5881168842315674 *\n",
      "Epoch: 11, Train_Loss: 2.838259696960449, Test_Loss: 2.5965394973754883\n",
      "Epoch: 11, Train_Loss: 2.8137450218200684, Test_Loss: 2.5954415798187256 *\n",
      "Epoch: 11, Train_Loss: 2.730520725250244, Test_Loss: 2.5837314128875732 *\n",
      "Epoch: 11, Train_Loss: 2.5821409225463867, Test_Loss: 2.5843923091888428\n",
      "Epoch: 11, Train_Loss: 2.578211784362793, Test_Loss: 2.5820860862731934 *\n",
      "Epoch: 11, Train_Loss: 2.5848374366760254, Test_Loss: 2.5797576904296875 *\n",
      "Epoch: 11, Train_Loss: 2.6000173091888428, Test_Loss: 2.583375930786133\n",
      "Epoch: 11, Train_Loss: 2.593502998352051, Test_Loss: 2.583646535873413\n",
      "Epoch: 11, Train_Loss: 2.5842320919036865, Test_Loss: 2.5960710048675537\n",
      "Epoch: 11, Train_Loss: 2.5731067657470703, Test_Loss: 2.5989301204681396\n",
      "Epoch: 11, Train_Loss: 2.5718882083892822, Test_Loss: 2.593627691268921 *\n",
      "Epoch: 11, Train_Loss: 2.5818889141082764, Test_Loss: 2.5781779289245605 *\n",
      "Epoch: 11, Train_Loss: 2.623424768447876, Test_Loss: 2.5758471488952637 *\n",
      "Epoch: 11, Train_Loss: 2.7788994312286377, Test_Loss: 2.5766916275024414\n",
      "Epoch: 11, Train_Loss: 2.7547779083251953, Test_Loss: 2.574479818344116 *\n",
      "Epoch: 11, Train_Loss: 2.7206509113311768, Test_Loss: 2.571650743484497 *\n",
      "Epoch: 11, Train_Loss: 2.6426124572753906, Test_Loss: 2.572197675704956\n",
      "Epoch: 11, Train_Loss: 2.6934235095977783, Test_Loss: 2.5730438232421875\n",
      "Epoch: 11, Train_Loss: 2.6307880878448486, Test_Loss: 2.5863704681396484\n",
      "Epoch: 11, Train_Loss: 2.6894075870513916, Test_Loss: 2.579714298248291 *\n",
      "Epoch: 11, Train_Loss: 2.6928274631500244, Test_Loss: 2.5733680725097656 *\n",
      "Epoch: 11, Train_Loss: 2.8515965938568115, Test_Loss: 2.570539712905884 *\n",
      "Epoch: 11, Train_Loss: 2.578355550765991, Test_Loss: 2.568804979324341 *\n",
      "Epoch: 11, Train_Loss: 2.5792038440704346, Test_Loss: 2.5697507858276367\n",
      "Epoch: 11, Train_Loss: 5.4905524253845215, Test_Loss: 2.5642523765563965 *\n",
      "Epoch: 11, Train_Loss: 2.8375027179718018, Test_Loss: 2.570906162261963\n",
      "Epoch: 11, Train_Loss: 2.6140499114990234, Test_Loss: 2.6201040744781494\n",
      "Epoch: 11, Train_Loss: 2.6214277744293213, Test_Loss: 4.890246391296387\n",
      "Epoch: 11, Train_Loss: 2.62565279006958, Test_Loss: 5.63341760635376\n",
      "Epoch: 11, Train_Loss: 2.5759646892547607, Test_Loss: 2.559504508972168 *\n",
      "Epoch: 11, Train_Loss: 2.5593700408935547, Test_Loss: 2.5609540939331055\n",
      "Epoch: 11, Train_Loss: 2.6144423484802246, Test_Loss: 2.6210520267486572\n",
      "Epoch: 11, Train_Loss: 2.6846511363983154, Test_Loss: 2.6047401428222656 *\n",
      "Epoch: 11, Train_Loss: 2.646682024002075, Test_Loss: 2.617187976837158\n",
      "Epoch: 11, Train_Loss: 2.611328363418579, Test_Loss: 2.5956900119781494 *\n",
      "Epoch: 11, Train_Loss: 2.6229910850524902, Test_Loss: 2.662663221359253\n",
      "Epoch: 11, Train_Loss: 2.5784475803375244, Test_Loss: 2.556980848312378 *\n",
      "Epoch: 11, Train_Loss: 2.580393075942993, Test_Loss: 2.574150562286377\n",
      "Epoch: 11, Train_Loss: 2.5595288276672363, Test_Loss: 2.5718162059783936 *\n",
      "Epoch: 11, Train_Loss: 2.5944409370422363, Test_Loss: 2.578944683074951\n",
      "Epoch: 11, Train_Loss: 2.5790719985961914, Test_Loss: 2.5598080158233643 *\n",
      "Epoch: 11, Train_Loss: 2.5583229064941406, Test_Loss: 2.6389448642730713\n",
      "Epoch: 11, Train_Loss: 2.5659258365631104, Test_Loss: 2.649325132369995\n",
      "Epoch: 11, Train_Loss: 2.59309720993042, Test_Loss: 2.6074187755584717 *\n",
      "Epoch: 11, Train_Loss: 2.5824637413024902, Test_Loss: 2.6381514072418213\n",
      "Epoch: 11, Train_Loss: 2.5489161014556885, Test_Loss: 2.573272705078125 *\n",
      "Epoch: 11, Train_Loss: 2.548759937286377, Test_Loss: 2.5912585258483887\n",
      "Epoch: 11, Train_Loss: 2.5460050106048584, Test_Loss: 2.570834159851074 *\n",
      "Epoch: 11, Train_Loss: 2.542299270629883, Test_Loss: 2.5583300590515137 *\n",
      "Epoch: 11, Train_Loss: 2.5486886501312256, Test_Loss: 2.56276798248291\n",
      "Epoch: 11, Train_Loss: 2.5431697368621826, Test_Loss: 2.5606746673583984 *\n",
      "Epoch: 11, Train_Loss: 2.539700508117676, Test_Loss: 2.5667712688446045\n",
      "Epoch: 11, Train_Loss: 2.5435447692871094, Test_Loss: 2.558156728744507 *\n",
      "Epoch: 11, Train_Loss: 2.541785478591919, Test_Loss: 2.5663344860076904\n",
      "Epoch: 11, Train_Loss: 2.5425078868865967, Test_Loss: 2.563932418823242 *\n",
      "Epoch: 11, Train_Loss: 2.5443079471588135, Test_Loss: 2.563523292541504 *\n",
      "Epoch: 11, Train_Loss: 2.548579216003418, Test_Loss: 2.53731107711792 *\n",
      "Epoch: 11, Train_Loss: 2.55725359916687, Test_Loss: 2.5764858722686768\n",
      "Epoch: 11, Train_Loss: 2.5532751083374023, Test_Loss: 2.585214614868164\n",
      "Epoch: 11, Train_Loss: 2.5471646785736084, Test_Loss: 2.7997899055480957\n",
      "Epoch: 11, Train_Loss: 2.5556564331054688, Test_Loss: 2.6070148944854736 *\n",
      "Epoch: 11, Train_Loss: 2.540754795074463, Test_Loss: 2.567305088043213 *\n",
      "Epoch: 11, Train_Loss: 2.5348763465881348, Test_Loss: 2.6217329502105713\n",
      "Epoch: 11, Train_Loss: 2.5463149547576904, Test_Loss: 2.78945255279541\n",
      "Epoch: 11, Train_Loss: 2.5481412410736084, Test_Loss: 2.773193359375 *\n",
      "Epoch: 11, Train_Loss: 2.524369239807129, Test_Loss: 2.5462875366210938 *\n",
      "Epoch: 11, Train_Loss: 2.530111789703369, Test_Loss: 2.7548229694366455\n",
      "Epoch: 11, Train_Loss: 2.5274314880371094, Test_Loss: 2.8164567947387695\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 2.5871012210845947, Test_Loss: 2.5492916107177734 *\n",
      "Epoch: 11, Train_Loss: 2.5671660900115967, Test_Loss: 2.5696940422058105\n",
      "Epoch: 11, Train_Loss: 2.56864333152771, Test_Loss: 2.5277929306030273 *\n",
      "Epoch: 11, Train_Loss: 2.541989803314209, Test_Loss: 2.5695834159851074\n",
      "Epoch: 11, Train_Loss: 2.5239500999450684, Test_Loss: 2.539823293685913 *\n",
      "Epoch: 11, Train_Loss: 2.589906930923462, Test_Loss: 3.257089853286743\n",
      "Epoch: 11, Train_Loss: 2.5223026275634766, Test_Loss: 2.7383978366851807 *\n",
      "Epoch: 11, Train_Loss: 2.534717559814453, Test_Loss: 3.280304431915283\n",
      "Epoch: 11, Train_Loss: 2.547351121902466, Test_Loss: 3.24361252784729 *\n",
      "Epoch: 11, Train_Loss: 2.536097764968872, Test_Loss: 2.73397159576416 *\n",
      "Epoch: 11, Train_Loss: 2.6514534950256348, Test_Loss: 3.0465145111083984\n",
      "Epoch: 11, Train_Loss: 2.5841281414031982, Test_Loss: 2.5904223918914795 *\n",
      "Epoch: 11, Train_Loss: 2.5472474098205566, Test_Loss: 2.525035858154297 *\n",
      "Epoch: 11, Train_Loss: 2.5248398780822754, Test_Loss: 2.536409378051758\n",
      "Epoch: 11, Train_Loss: 2.5349326133728027, Test_Loss: 2.667574882507324\n",
      "Epoch: 11, Train_Loss: 2.5174179077148438, Test_Loss: 2.860487699508667\n",
      "Epoch: 11, Train_Loss: 2.5161681175231934, Test_Loss: 3.2856483459472656\n",
      "Epoch: 11, Train_Loss: 2.5223488807678223, Test_Loss: 3.617093086242676\n",
      "Epoch: 11, Train_Loss: 2.5247859954833984, Test_Loss: 3.899235486984253\n",
      "Epoch: 11, Train_Loss: 2.5560295581817627, Test_Loss: 3.3594284057617188 *\n",
      "Epoch: 11, Train_Loss: 2.5837490558624268, Test_Loss: 2.9643142223358154 *\n",
      "Epoch: 11, Train_Loss: 2.537031650543213, Test_Loss: 2.5117311477661133 *\n",
      "Epoch: 11, Train_Loss: 2.5618834495544434, Test_Loss: 2.5350122451782227\n",
      "Epoch: 11, Train_Loss: 2.5289971828460693, Test_Loss: 3.313427686691284\n",
      "Epoch: 11, Train_Loss: 2.5227530002593994, Test_Loss: 3.8522212505340576\n",
      "Epoch: 11, Train_Loss: 2.6131625175476074, Test_Loss: 2.556363344192505 *\n",
      "Epoch: 11, Train_Loss: 2.7342562675476074, Test_Loss: 2.5909483432769775\n",
      "Epoch: 11, Train_Loss: 2.505657911300659, Test_Loss: 2.507070302963257 *\n",
      "Epoch: 11, Train_Loss: 2.539557695388794, Test_Loss: 2.7591614723205566\n",
      "Epoch: 11, Train_Loss: 2.49855375289917, Test_Loss: 2.7529613971710205 *\n",
      "Epoch: 11, Train_Loss: 2.499734401702881, Test_Loss: 3.177450180053711\n",
      "Epoch: 11, Train_Loss: 2.5017220973968506, Test_Loss: 3.5775911808013916\n",
      "Epoch: 11, Train_Loss: 2.4998409748077393, Test_Loss: 2.8095157146453857 *\n",
      "Epoch: 11, Train_Loss: 2.5097146034240723, Test_Loss: 2.50183367729187 *\n",
      "Epoch: 11, Train_Loss: 2.5149123668670654, Test_Loss: 2.51139760017395\n",
      "Epoch: 11, Train_Loss: 2.509263038635254, Test_Loss: 2.520303249359131\n",
      "Epoch: 11, Train_Loss: 2.509974718093872, Test_Loss: 2.549212694168091\n",
      "Epoch: 11, Train_Loss: 2.518670082092285, Test_Loss: 2.8137192726135254\n",
      "Epoch: 11, Train_Loss: 2.4947075843811035, Test_Loss: 3.0569815635681152\n",
      "Epoch: 11, Train_Loss: 2.4953110218048096, Test_Loss: 2.729611396789551 *\n",
      "Epoch: 11, Train_Loss: 2.4930851459503174, Test_Loss: 2.556183099746704 *\n",
      "Epoch: 11, Train_Loss: 2.5169053077697754, Test_Loss: 2.5236141681671143 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 2.5136444568634033, Test_Loss: 2.5006115436553955 *\n",
      "Epoch: 11, Train_Loss: 2.506650686264038, Test_Loss: 2.602029800415039\n",
      "Epoch: 11, Train_Loss: 2.501631021499634, Test_Loss: 3.3901469707489014\n",
      "Epoch: 11, Train_Loss: 2.567227363586426, Test_Loss: 3.877030849456787\n",
      "Epoch: 11, Train_Loss: 2.544261932373047, Test_Loss: 2.594651937484741 *\n",
      "Epoch: 11, Train_Loss: 2.5068821907043457, Test_Loss: 2.589749813079834 *\n",
      "Epoch: 11, Train_Loss: 2.49869704246521, Test_Loss: 2.485302448272705 *\n",
      "Epoch: 11, Train_Loss: 2.503769636154175, Test_Loss: 2.489607334136963\n",
      "Epoch: 11, Train_Loss: 2.4928228855133057, Test_Loss: 2.4899332523345947\n",
      "Epoch: 11, Train_Loss: 2.4903719425201416, Test_Loss: 2.502474784851074\n",
      "Epoch: 11, Train_Loss: 2.504953145980835, Test_Loss: 2.5191726684570312\n",
      "Epoch: 11, Train_Loss: 2.530601978302002, Test_Loss: 2.51297664642334 *\n",
      "Epoch: 11, Train_Loss: 4.632441997528076, Test_Loss: 2.484532594680786 *\n",
      "Epoch: 11, Train_Loss: 5.78466796875, Test_Loss: 2.587137460708618\n",
      "Epoch: 11, Train_Loss: 2.491365671157837, Test_Loss: 2.876300811767578\n",
      "Epoch: 11, Train_Loss: 2.4879794120788574, Test_Loss: 2.5886197090148926 *\n",
      "Epoch: 11, Train_Loss: 2.5274181365966797, Test_Loss: 2.631319761276245\n",
      "Epoch: 11, Train_Loss: 2.6677401065826416, Test_Loss: 2.480663537979126 *\n",
      "Epoch: 11, Train_Loss: 2.5010435581207275, Test_Loss: 2.484741687774658\n",
      "Epoch: 11, Train_Loss: 2.482064723968506, Test_Loss: 2.48677134513855\n",
      "Epoch: 11, Train_Loss: 2.4809088706970215, Test_Loss: 2.4894750118255615\n",
      "Epoch: 11, Train_Loss: 2.5430521965026855, Test_Loss: 2.5062434673309326\n",
      "Epoch: 11, Train_Loss: 2.4801833629608154, Test_Loss: 7.546719551086426\n",
      "Epoch: 11, Train_Loss: 2.490640878677368, Test_Loss: 2.789092540740967 *\n",
      "Epoch: 11, Train_Loss: 3.2664082050323486, Test_Loss: 2.4818735122680664 *\n",
      "Epoch: 11, Train_Loss: 3.851323366165161, Test_Loss: 2.47593355178833 *\n",
      "Epoch: 11, Train_Loss: 3.118891477584839, Test_Loss: 2.4805281162261963\n",
      "Epoch: 11, Train_Loss: 2.592099905014038, Test_Loss: 2.4750523567199707 *\n",
      "Epoch: 11, Train_Loss: 3.1763803958892822, Test_Loss: 2.468876600265503 *\n",
      "Epoch: 11, Train_Loss: 4.82337760925293, Test_Loss: 2.4685075283050537 *\n",
      "Epoch: 11, Train_Loss: 2.896730899810791, Test_Loss: 2.466311454772949 *\n",
      "Epoch: 11, Train_Loss: 2.5066773891448975, Test_Loss: 2.464789628982544 *\n",
      "Epoch: 11, Train_Loss: 2.487663984298706, Test_Loss: 2.46903395652771\n",
      "Epoch: 11, Train_Loss: 3.850384473800659, Test_Loss: 2.4722394943237305\n",
      "Epoch: 11, Train_Loss: 4.063002109527588, Test_Loss: 2.4708783626556396 *\n",
      "Epoch: 11, Train_Loss: 2.677598237991333, Test_Loss: 2.4858458042144775\n",
      "Epoch: 11, Train_Loss: 2.4818265438079834, Test_Loss: 2.4729855060577393 *\n",
      "Epoch: 11, Train_Loss: 2.4640374183654785, Test_Loss: 2.4646177291870117 *\n",
      "Epoch: 11, Train_Loss: 3.103698253631592, Test_Loss: 2.462740182876587 *\n",
      "Epoch: 11, Train_Loss: 2.6022894382476807, Test_Loss: 2.4670968055725098\n",
      "Epoch: 11, Train_Loss: 2.496807098388672, Test_Loss: 2.4602928161621094 *\n",
      "Epoch: 11, Train_Loss: 2.5054023265838623, Test_Loss: 2.455979347229004 *\n",
      "Epoch: 11, Train_Loss: 2.6213366985321045, Test_Loss: 2.462677240371704\n",
      "Epoch: 11, Train_Loss: 2.605492353439331, Test_Loss: 2.457465171813965 *\n",
      "Epoch: 11, Train_Loss: 2.580880641937256, Test_Loss: 2.4694552421569824\n",
      "Epoch: 11, Train_Loss: 2.84305739402771, Test_Loss: 2.4674930572509766 *\n",
      "Epoch: 11, Train_Loss: 2.546149253845215, Test_Loss: 2.453589916229248 *\n",
      "Epoch: 11, Train_Loss: 2.5297393798828125, Test_Loss: 2.458137273788452\n",
      "Epoch: 11, Train_Loss: 2.709662437438965, Test_Loss: 2.4579837322235107 *\n",
      "Epoch: 11, Train_Loss: 2.820263147354126, Test_Loss: 2.4544107913970947 *\n",
      "Epoch: 11, Train_Loss: 2.8476028442382812, Test_Loss: 2.4555752277374268\n",
      "Epoch: 11, Train_Loss: 2.5455429553985596, Test_Loss: 2.466791868209839\n",
      "Epoch: 11, Train_Loss: 2.5537991523742676, Test_Loss: 2.5187673568725586\n",
      "Epoch: 11, Train_Loss: 2.5580997467041016, Test_Loss: 3.3915669918060303\n",
      "Epoch: 11, Train_Loss: 2.4863152503967285, Test_Loss: 7.1447834968566895\n",
      "Epoch: 11, Train_Loss: 2.460059642791748, Test_Loss: 2.4512786865234375 *\n",
      "Epoch: 11, Train_Loss: 2.4452545642852783, Test_Loss: 2.448549270629883 *\n",
      "Epoch: 11, Train_Loss: 2.4432754516601562, Test_Loss: 2.4815664291381836\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 2.4448654651641846, Test_Loss: 2.4778969287872314 *\n",
      "Epoch: 11, Train_Loss: 2.445755958557129, Test_Loss: 2.4815220832824707\n",
      "Epoch: 11, Train_Loss: 2.5145835876464844, Test_Loss: 2.4593312740325928 *\n",
      "Epoch: 11, Train_Loss: 2.510521173477173, Test_Loss: 2.596461057662964\n",
      "Epoch: 11, Train_Loss: 2.5364809036254883, Test_Loss: 2.459575653076172 *\n",
      "Epoch: 11, Train_Loss: 2.5527827739715576, Test_Loss: 2.445739507675171 *\n",
      "Epoch: 11, Train_Loss: 2.9184396266937256, Test_Loss: 2.470217704772949\n",
      "Epoch: 11, Train_Loss: 2.4432425498962402, Test_Loss: 2.4602909088134766 *\n",
      "Epoch: 11, Train_Loss: 2.463731527328491, Test_Loss: 2.4417357444763184 *\n",
      "Epoch: 11, Train_Loss: 2.578239917755127, Test_Loss: 2.496180772781372\n",
      "Epoch: 11, Train_Loss: 2.9417333602905273, Test_Loss: 2.5111641883850098\n",
      "Epoch: 11, Train_Loss: 2.797851085662842, Test_Loss: 2.5079360008239746 *\n",
      "Epoch: 11, Train_Loss: 2.44268798828125, Test_Loss: 2.52854323387146\n",
      "Epoch: 11, Train_Loss: 2.4782960414886475, Test_Loss: 2.466745615005493 *\n",
      "Epoch: 11, Train_Loss: 2.989720582962036, Test_Loss: 2.4629645347595215 *\n",
      "Epoch: 11, Train_Loss: 2.969416856765747, Test_Loss: 2.435131549835205 *\n",
      "Epoch: 11, Train_Loss: 2.50559401512146, Test_Loss: 2.4343948364257812 *\n",
      "Epoch: 11, Train_Loss: 2.458671808242798, Test_Loss: 2.4333817958831787 *\n",
      "Epoch: 11, Train_Loss: 2.4426674842834473, Test_Loss: 2.434077501296997\n",
      "Epoch: 11, Train_Loss: 3.3986897468566895, Test_Loss: 2.433638572692871 *\n",
      "Epoch: 11, Train_Loss: 3.692342758178711, Test_Loss: 2.4330644607543945 *\n",
      "Epoch: 11, Train_Loss: 2.456327438354492, Test_Loss: 2.43796706199646\n",
      "Epoch: 11, Train_Loss: 2.456812620162964, Test_Loss: 2.444133758544922\n",
      "Epoch: 11, Train_Loss: 2.4274702072143555, Test_Loss: 2.436413288116455 *\n",
      "Epoch: 11, Train_Loss: 2.4387099742889404, Test_Loss: 2.4463372230529785\n",
      "Epoch: 11, Train_Loss: 2.8446156978607178, Test_Loss: 2.4307901859283447 *\n",
      "Epoch: 11, Train_Loss: 2.4496469497680664, Test_Loss: 2.4538252353668213\n",
      "Epoch: 11, Train_Loss: 2.4523496627807617, Test_Loss: 2.6047239303588867\n",
      "Epoch: 11, Train_Loss: 2.445855140686035, Test_Loss: 2.6732418537139893\n",
      "Epoch: 11, Train_Loss: 2.4300222396850586, Test_Loss: 2.436500310897827 *\n",
      "Epoch: 11, Train_Loss: 19.78130340576172, Test_Loss: 2.468963146209717\n",
      "Epoch: 11, Train_Loss: 2.426449775695801, Test_Loss: 2.5271828174591064\n",
      "Epoch: 11, Train_Loss: 4.772347450256348, Test_Loss: 2.6595540046691895\n",
      "Epoch: 11, Train_Loss: 4.1699910163879395, Test_Loss: 2.4310829639434814 *\n",
      "Epoch: 11, Train_Loss: 2.4361093044281006, Test_Loss: 2.594196081161499\n",
      "Epoch: 11, Train_Loss: 2.5028953552246094, Test_Loss: 2.665167808532715\n",
      "Epoch: 11, Train_Loss: 7.121064186096191, Test_Loss: 2.5086138248443604 *\n",
      "Epoch: 11, Train_Loss: 9.484136581420898, Test_Loss: 2.514874219894409\n",
      "Epoch: 11, Train_Loss: 2.4447813034057617, Test_Loss: 2.4283933639526367 *\n",
      "Epoch: 11, Train_Loss: 2.4639601707458496, Test_Loss: 2.4260451793670654 *\n",
      "Epoch: 11, Train_Loss: 8.541577339172363, Test_Loss: 2.4482247829437256\n",
      "Epoch: 11, Train_Loss: 2.4525911808013916, Test_Loss: 3.267387628555298\n",
      "Epoch: 11, Train_Loss: 2.454710006713867, Test_Loss: 2.8474745750427246 *\n",
      "Epoch: 11, Train_Loss: 2.4309356212615967, Test_Loss: 2.9023184776306152\n",
      "Epoch: 11, Train_Loss: 2.4345927238464355, Test_Loss: 3.0487780570983887\n",
      "Epoch: 11, Train_Loss: 2.4632365703582764, Test_Loss: 2.6174840927124023 *\n",
      "Epoch: 11, Train_Loss: 2.439251661300659, Test_Loss: 3.1638023853302\n",
      "Epoch: 11, Train_Loss: 2.44502592086792, Test_Loss: 2.672559976577759 *\n",
      "Epoch: 11, Train_Loss: 2.4396424293518066, Test_Loss: 2.463048219680786 *\n",
      "Epoch: 11, Train_Loss: 2.4348936080932617, Test_Loss: 2.4467616081237793 *\n",
      "Epoch: 11, Train_Loss: 2.457292079925537, Test_Loss: 2.4921162128448486\n",
      "Epoch: 11, Train_Loss: 2.4396421909332275, Test_Loss: 2.6624093055725098\n",
      "Epoch: 11, Train_Loss: 2.4267148971557617, Test_Loss: 3.6165337562561035\n",
      "Epoch: 11, Train_Loss: 2.4821150302886963, Test_Loss: 2.9268221855163574 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 2.493415594100952, Test_Loss: 4.065605163574219\n",
      "Epoch: 11, Train_Loss: 2.4302499294281006, Test_Loss: 3.2259769439697266 *\n",
      "Epoch: 11, Train_Loss: 2.4180047512054443, Test_Loss: 3.266472816467285\n",
      "Epoch: 11, Train_Loss: 2.4222872257232666, Test_Loss: 2.461566686630249 *\n",
      "Epoch: 11, Train_Loss: 2.418058395385742, Test_Loss: 2.446808099746704 *\n",
      "Epoch: 11, Train_Loss: 2.4062695503234863, Test_Loss: 2.8593149185180664\n",
      "Epoch: 11, Train_Loss: 2.4126157760620117, Test_Loss: 3.641223430633545\n",
      "Epoch: 11, Train_Loss: 2.408175230026245, Test_Loss: 2.532571315765381 *\n",
      "Epoch: 11, Train_Loss: 2.4041082859039307, Test_Loss: 2.573091506958008\n",
      "Epoch: 11, Train_Loss: 2.3987228870391846, Test_Loss: 2.4147207736968994 *\n",
      "Epoch: 11, Train_Loss: 2.396153450012207, Test_Loss: 2.5142829418182373\n",
      "Epoch: 11, Train_Loss: 2.4003095626831055, Test_Loss: 2.71091628074646\n",
      "Epoch: 11, Train_Loss: 2.4017715454101562, Test_Loss: 2.903576612472534\n",
      "Epoch: 11, Train_Loss: 2.405836820602417, Test_Loss: 3.742253303527832\n",
      "Epoch: 11, Train_Loss: 2.4261977672576904, Test_Loss: 2.81852126121521 *\n",
      "Epoch: 11, Train_Loss: 2.4168403148651123, Test_Loss: 2.450368642807007 *\n",
      "Epoch: 11, Train_Loss: 2.404232978820801, Test_Loss: 2.436072587966919 *\n",
      "Epoch: 11, Train_Loss: 4.472599029541016, Test_Loss: 2.442110300064087\n",
      "Epoch: 11, Train_Loss: 9.099421501159668, Test_Loss: 2.4879515171051025\n",
      "Epoch: 11, Train_Loss: 2.4165163040161133, Test_Loss: 2.5520365238189697\n",
      "Epoch: 11, Train_Loss: 2.4307126998901367, Test_Loss: 2.7582080364227295\n",
      "Epoch: 11, Train_Loss: 2.4536094665527344, Test_Loss: 2.5676708221435547 *\n",
      "Epoch: 11, Train_Loss: 2.435263156890869, Test_Loss: 2.437929391860962 *\n",
      "Epoch: 11, Train_Loss: 2.434788942337036, Test_Loss: 2.3909356594085693 *\n",
      "Epoch: 11, Train_Loss: 2.45534086227417, Test_Loss: 2.4200148582458496\n",
      "Epoch: 11, Train_Loss: 2.4809370040893555, Test_Loss: 2.5008177757263184\n",
      "Epoch: 11, Train_Loss: 2.6872615814208984, Test_Loss: 3.1030890941619873\n",
      "Epoch: 11, Train_Loss: 2.5549514293670654, Test_Loss: 3.8904244899749756\n",
      "Epoch: 11, Train_Loss: 2.457514524459839, Test_Loss: 2.8186235427856445 *\n",
      "Epoch: 11, Train_Loss: 2.432133674621582, Test_Loss: 2.425446033477783 *\n",
      "Epoch: 11, Train_Loss: 2.5123226642608643, Test_Loss: 2.4050183296203613 *\n",
      "Epoch: 11, Train_Loss: 2.5011637210845947, Test_Loss: 2.4071836471557617\n",
      "Epoch: 11, Train_Loss: 2.568199396133423, Test_Loss: 2.4080264568328857\n",
      "Epoch: 11, Train_Loss: 2.4588241577148438, Test_Loss: 2.4133567810058594\n",
      "Epoch: 11, Train_Loss: 2.432708263397217, Test_Loss: 2.412423849105835 *\n",
      "Epoch: 11, Train_Loss: 2.3794710636138916, Test_Loss: 2.4310975074768066\n",
      "Epoch: 11, Train_Loss: 2.4135806560516357, Test_Loss: 2.384298801422119 *\n",
      "Epoch: 11, Train_Loss: 2.4450833797454834, Test_Loss: 2.44588041305542\n",
      "Epoch: 11, Train_Loss: 2.3947861194610596, Test_Loss: 2.713852643966675\n",
      "Epoch: 11, Train_Loss: 2.370088815689087, Test_Loss: 2.4841625690460205 *\n",
      "Epoch: 11, Train_Loss: 2.375088930130005, Test_Loss: 2.578749895095825\n",
      "Epoch: 11, Train_Loss: 2.378974676132202, Test_Loss: 2.3705763816833496 *\n",
      "Epoch: 11, Train_Loss: 4.640472888946533, Test_Loss: 2.368828058242798 *\n",
      "Epoch: 11, Train_Loss: 5.96769905090332, Test_Loss: 2.3677101135253906 *\n",
      "Epoch: 11, Train_Loss: 2.3651669025421143, Test_Loss: 2.3677690029144287\n",
      "Epoch: 11, Train_Loss: 2.407738447189331, Test_Loss: 2.380805015563965\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 2.4047586917877197, Test_Loss: 6.434717178344727\n",
      "Epoch: 11, Train_Loss: 2.38226580619812, Test_Loss: 3.9486069679260254 *\n",
      "Epoch: 11, Train_Loss: 2.372941255569458, Test_Loss: 2.3735718727111816 *\n",
      "Epoch: 11, Train_Loss: 2.386333703994751, Test_Loss: 2.372455358505249 *\n",
      "Epoch: 11, Train_Loss: 2.3827521800994873, Test_Loss: 2.370830535888672 *\n",
      "Epoch: 11, Train_Loss: 2.382951498031616, Test_Loss: 2.366461753845215 *\n",
      "Epoch: 11, Train_Loss: 2.395142078399658, Test_Loss: 2.3876826763153076\n",
      "Epoch: 11, Train_Loss: 2.3733742237091064, Test_Loss: 2.3970799446105957\n",
      "Epoch: 11, Train_Loss: 2.3624441623687744, Test_Loss: 2.3848373889923096 *\n",
      "Epoch: 11, Train_Loss: 2.366058588027954, Test_Loss: 2.3993961811065674\n",
      "Epoch: 11, Train_Loss: 2.381107807159424, Test_Loss: 2.3983304500579834 *\n",
      "Epoch: 11, Train_Loss: 2.3564281463623047, Test_Loss: 2.389794111251831 *\n",
      "Epoch: 11, Train_Loss: 2.3581855297088623, Test_Loss: 2.3806188106536865 *\n",
      "Epoch: 11, Train_Loss: 2.3880929946899414, Test_Loss: 2.3678112030029297 *\n",
      "Epoch: 11, Train_Loss: 2.3844542503356934, Test_Loss: 2.370816469192505\n",
      "Epoch: 11, Train_Loss: 2.354903221130371, Test_Loss: 2.379518985748291\n",
      "Epoch: 11, Train_Loss: 2.3512520790100098, Test_Loss: 2.357173204421997 *\n",
      "Epoch: 11, Train_Loss: 2.371187210083008, Test_Loss: 2.3771939277648926\n",
      "Epoch: 11, Train_Loss: 2.4359042644500732, Test_Loss: 2.3599774837493896 *\n",
      "Epoch: 11, Train_Loss: 2.405245780944824, Test_Loss: 2.3678855895996094\n",
      "Epoch: 11, Train_Loss: 2.399153232574463, Test_Loss: 2.3680245876312256\n",
      "Epoch: 11, Train_Loss: 2.393338203430176, Test_Loss: 2.380625009536743\n",
      "Epoch: 11, Train_Loss: 2.4166111946105957, Test_Loss: 2.37404727935791 *\n",
      "Epoch: 11, Train_Loss: 2.384002447128296, Test_Loss: 2.387631416320801\n",
      "Epoch: 11, Train_Loss: 2.3858351707458496, Test_Loss: 2.373413324356079 *\n",
      "Epoch: 11, Train_Loss: 2.388350486755371, Test_Loss: 2.3708245754241943 *\n",
      "Epoch: 11, Train_Loss: 2.455579996109009, Test_Loss: 2.352149486541748 *\n",
      "Epoch: 11, Train_Loss: 2.3598368167877197, Test_Loss: 2.3561935424804688\n",
      "Epoch: 11, Train_Loss: 2.343325614929199, Test_Loss: 2.358368158340454\n",
      "Epoch: 11, Train_Loss: 2.343247652053833, Test_Loss: 2.3591222763061523\n",
      "Epoch: 11, Train_Loss: 2.34222412109375, Test_Loss: 2.414336919784546\n",
      "Epoch: 11, Train_Loss: 2.3373959064483643, Test_Loss: 2.403934955596924 *\n",
      "Epoch: 11, Train_Loss: 2.3385684490203857, Test_Loss: 7.940681457519531\n",
      "Epoch: 11, Train_Loss: 4.694250106811523, Test_Loss: 2.4045114517211914 *\n",
      "Epoch: 11, Train_Loss: 4.7965240478515625, Test_Loss: 2.3366119861602783 *\n",
      "Epoch: 11, Train_Loss: 2.3372561931610107, Test_Loss: 2.363985538482666\n",
      "Epoch: 11, Train_Loss: 2.343576669692993, Test_Loss: 2.3707358837127686\n",
      "Epoch: 11, Train_Loss: 2.339893102645874, Test_Loss: 2.3952670097351074\n",
      "Epoch: 11, Train_Loss: 2.3361926078796387, Test_Loss: 2.3417718410491943 *\n",
      "Epoch: 11, Train_Loss: 2.336888313293457, Test_Loss: 2.415923833847046\n",
      "Epoch: 11, Train_Loss: 2.342072010040283, Test_Loss: 2.379040479660034 *\n",
      "Epoch: 11, Train_Loss: 2.33671498298645, Test_Loss: 2.3460183143615723 *\n",
      "Epoch: 11, Train_Loss: 2.3345131874084473, Test_Loss: 2.3656177520751953\n",
      "Epoch: 11, Train_Loss: 2.343797445297241, Test_Loss: 2.3666470050811768\n",
      "Epoch: 11, Train_Loss: 2.393775224685669, Test_Loss: 2.3383235931396484 *\n",
      "Epoch: 11, Train_Loss: 2.376343250274658, Test_Loss: 2.3732104301452637\n",
      "Epoch: 11, Train_Loss: 2.391336679458618, Test_Loss: 2.4854013919830322\n",
      "Epoch: 11, Train_Loss: 2.3643715381622314, Test_Loss: 2.3631019592285156 *\n",
      "Epoch: 11, Train_Loss: 2.3346798419952393, Test_Loss: 2.414435386657715\n",
      "Epoch: 11, Train_Loss: 2.5370655059814453, Test_Loss: 2.3711423873901367 *\n",
      "Epoch: 11, Train_Loss: 2.5830774307250977, Test_Loss: 2.3811070919036865\n",
      "Epoch: 11, Train_Loss: 2.548678398132324, Test_Loss: 2.3494973182678223 *\n",
      "Epoch: 11, Train_Loss: 2.415055990219116, Test_Loss: 2.343458890914917 *\n",
      "Epoch: 11, Train_Loss: 2.323639392852783, Test_Loss: 2.337751626968384 *\n",
      "Epoch: 11, Train_Loss: 2.321383237838745, Test_Loss: 2.3399760723114014\n",
      "Epoch: 11, Train_Loss: 2.330976963043213, Test_Loss: 2.335994243621826 *\n",
      "Epoch: 11, Train_Loss: 2.3350625038146973, Test_Loss: 2.335994005203247 *\n",
      "Epoch: 11, Train_Loss: 2.3323278427124023, Test_Loss: 2.328439474105835 *\n",
      "Epoch: 11, Train_Loss: 2.3200442790985107, Test_Loss: 2.334667205810547\n",
      "Epoch: 11, Train_Loss: 2.313272476196289, Test_Loss: 2.348816394805908\n",
      "Epoch: 11, Train_Loss: 2.315220355987549, Test_Loss: 2.323403835296631 *\n",
      "Epoch: 11, Train_Loss: 2.32096529006958, Test_Loss: 2.323864698410034\n",
      "Epoch: 11, Train_Loss: 2.393585205078125, Test_Loss: 2.357430934906006\n",
      "Epoch: 11, Train_Loss: 2.5261495113372803, Test_Loss: 2.367415428161621\n",
      "Epoch: 11, Train_Loss: 2.4708492755889893, Test_Loss: 2.670654296875\n",
      "Epoch: 11, Train_Loss: 2.413996458053589, Test_Loss: 2.3300397396087646 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 2.4347925186157227, Test_Loss: 2.368051290512085\n",
      "Epoch: 11, Train_Loss: 2.4757094383239746, Test_Loss: 2.390366315841675\n",
      "Epoch: 11, Train_Loss: 2.359396457672119, Test_Loss: 2.6128060817718506\n",
      "Epoch: 11, Train_Loss: 2.4914019107818604, Test_Loss: 2.3422062397003174 *\n",
      "Epoch: 11, Train_Loss: 2.4487714767456055, Test_Loss: 2.4735124111175537\n",
      "Epoch: 11, Train_Loss: 2.6002418994903564, Test_Loss: 2.58622407913208\n",
      "Epoch: 11, Train_Loss: 2.3185787200927734, Test_Loss: 2.4654736518859863 *\n",
      "Epoch: 11, Train_Loss: 2.4711647033691406, Test_Loss: 2.3637990951538086 *\n",
      "Epoch: 11, Train_Loss: 5.279343605041504, Test_Loss: 2.319246530532837 *\n",
      "Epoch: 11, Train_Loss: 2.4184792041778564, Test_Loss: 2.3133928775787354 *\n",
      "Epoch: 11, Train_Loss: 2.359255313873291, Test_Loss: 2.335747003555298\n",
      "Epoch: 11, Train_Loss: 2.364825487136841, Test_Loss: 2.6857616901397705\n",
      "Epoch: 11, Train_Loss: 2.3657729625701904, Test_Loss: 2.8604822158813477\n",
      "Epoch: 11, Train_Loss: 2.305420398712158, Test_Loss: 2.724987030029297 *\n",
      "Epoch: 11, Train_Loss: 2.302629232406616, Test_Loss: 3.215395450592041\n",
      "Epoch: 11, Train_Loss: 2.3767364025115967, Test_Loss: 2.5291857719421387 *\n",
      "Epoch: 11, Train_Loss: 2.3939239978790283, Test_Loss: 2.815768241882324\n",
      "Epoch: 11, Train_Loss: 2.3787660598754883, Test_Loss: 2.525696039199829 *\n",
      "Epoch: 11, Train_Loss: 2.357969045639038, Test_Loss: 2.3099734783172607 *\n",
      "Epoch: 11, Train_Loss: 2.3682773113250732, Test_Loss: 2.3179893493652344\n",
      "Epoch: 11, Train_Loss: 2.31453013420105, Test_Loss: 2.3714804649353027\n",
      "Epoch: 11, Train_Loss: 2.321627140045166, Test_Loss: 2.4641435146331787\n",
      "Epoch: 11, Train_Loss: 2.310328722000122, Test_Loss: 3.1926512718200684\n",
      "Epoch: 11, Train_Loss: 2.329399347305298, Test_Loss: 2.51877498626709 *\n",
      "Epoch: 11, Train_Loss: 2.318972587585449, Test_Loss: 4.479259490966797\n",
      "Epoch: 11, Train_Loss: 2.2984890937805176, Test_Loss: 2.8133511543273926 *\n",
      "Epoch: 11, Train_Loss: 2.3165628910064697, Test_Loss: 3.2094078063964844\n",
      "Epoch: 11, Train_Loss: 2.3383290767669678, Test_Loss: 2.34385085105896 *\n",
      "Epoch: 11, Train_Loss: 2.321674108505249, Test_Loss: 2.297677993774414 *\n",
      "Epoch: 11, Train_Loss: 2.2940080165863037, Test_Loss: 2.621075391769409\n",
      "Epoch: 11, Train_Loss: 2.289491653442383, Test_Loss: 3.7570812702178955\n",
      "Epoch: 11, Train_Loss: 2.2899835109710693, Test_Loss: 2.784496307373047 *\n",
      "Epoch: 11, Train_Loss: 2.286435127258301, Test_Loss: 2.3903393745422363 *\n",
      "Epoch: 11, Train_Loss: 2.286177158355713, Test_Loss: 2.3014047145843506 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 2.286262273788452, Test_Loss: 2.413532018661499\n",
      "Epoch: 11, Train_Loss: 2.288867235183716, Test_Loss: 2.721198797225952\n",
      "Epoch: 11, Train_Loss: 2.286311149597168, Test_Loss: 2.5138401985168457 *\n",
      "Epoch: 11, Train_Loss: 2.285612106323242, Test_Loss: 3.479248523712158\n",
      "Epoch: 11, Train_Loss: 2.286419630050659, Test_Loss: 2.7919113636016846 *\n",
      "Epoch: 11, Train_Loss: 2.2916743755340576, Test_Loss: 2.2860593795776367 *\n",
      "Epoch: 11, Train_Loss: 2.292832612991333, Test_Loss: 2.2843074798583984 *\n",
      "Epoch: 11, Train_Loss: 2.290841579437256, Test_Loss: 2.2919061183929443\n",
      "Epoch: 11, Train_Loss: 2.3040452003479004, Test_Loss: 2.2984116077423096\n",
      "Epoch: 11, Train_Loss: 2.2898402214050293, Test_Loss: 2.392806053161621\n",
      "Epoch: 11, Train_Loss: 2.291245460510254, Test_Loss: 2.909533739089966\n",
      "Epoch: 11, Train_Loss: 2.2761905193328857, Test_Loss: 2.7315847873687744 *\n",
      "Epoch: 11, Train_Loss: 2.275115489959717, Test_Loss: 2.409907341003418 *\n",
      "Epoch: 11, Train_Loss: 2.2932076454162598, Test_Loss: 2.3229780197143555 *\n",
      "Epoch: 11, Train_Loss: 2.290938138961792, Test_Loss: 2.2933425903320312 *\n",
      "Epoch: 11, Train_Loss: 2.2733757495880127, Test_Loss: 2.3161580562591553\n",
      "Epoch: 11, Train_Loss: 2.276900053024292, Test_Loss: 2.696746826171875\n",
      "Epoch: 11, Train_Loss: 2.272308349609375, Test_Loss: 3.612762451171875\n",
      "Epoch: 11, Train_Loss: 2.349731206893921, Test_Loss: 2.8383588790893555 *\n",
      "Epoch: 11, Train_Loss: 2.30564284324646, Test_Loss: 2.364239454269409 *\n",
      "Epoch: 11, Train_Loss: 2.3239338397979736, Test_Loss: 2.290283441543579 *\n",
      "Epoch: 11, Train_Loss: 2.276966094970703, Test_Loss: 2.2753210067749023 *\n",
      "Epoch: 11, Train_Loss: 2.279886484146118, Test_Loss: 2.268901824951172 *\n",
      "Epoch: 11, Train_Loss: 2.3334810733795166, Test_Loss: 2.2736120223999023\n",
      "Epoch: 11, Train_Loss: 2.266153335571289, Test_Loss: 2.295591354370117\n",
      "Epoch: 11, Train_Loss: 2.2811405658721924, Test_Loss: 2.3189897537231445\n",
      "Epoch: 11, Train_Loss: 2.292769432067871, Test_Loss: 2.2634646892547607 *\n",
      "Epoch: 11, Train_Loss: 2.313352108001709, Test_Loss: 2.3391644954681396\n",
      "Epoch: 11, Train_Loss: 2.3757681846618652, Test_Loss: 2.4218273162841797\n",
      "Epoch: 11, Train_Loss: 2.332705497741699, Test_Loss: 2.5919406414031982\n",
      "Epoch: 11, Train_Loss: 2.290999174118042, Test_Loss: 2.483619451522827 *\n",
      "Epoch: 11, Train_Loss: 2.263244867324829, Test_Loss: 2.276961088180542 *\n",
      "Epoch: 11, Train_Loss: 2.289285182952881, Test_Loss: 2.276052951812744 *\n",
      "Epoch: 11, Train_Loss: 2.2595291137695312, Test_Loss: 2.266967296600342 *\n",
      "Epoch: 11, Train_Loss: 2.2636067867279053, Test_Loss: 2.2734720706939697\n",
      "Epoch: 11, Train_Loss: 2.270034074783325, Test_Loss: 2.2747178077697754\n",
      "Epoch: 11, Train_Loss: 2.2760443687438965, Test_Loss: 4.413602828979492\n",
      "Epoch: 11, Train_Loss: 2.3217933177948, Test_Loss: 5.445387840270996\n",
      "Epoch: 11, Train_Loss: 2.3033018112182617, Test_Loss: 2.2671103477478027 *\n",
      "Epoch: 11, Train_Loss: 2.307969570159912, Test_Loss: 2.2549831867218018 *\n",
      "Epoch: 11, Train_Loss: 2.2800323963165283, Test_Loss: 2.265394687652588\n",
      "Epoch: 11, Train_Loss: 2.284334897994995, Test_Loss: 2.264836311340332 *\n",
      "Epoch: 11, Train_Loss: 2.2695155143737793, Test_Loss: 2.2556684017181396 *\n",
      "Epoch: 11, Train_Loss: 2.435872793197632, Test_Loss: 2.2553954124450684 *\n",
      "Epoch: 11, Train_Loss: 2.4100327491760254, Test_Loss: 2.2540299892425537 *\n",
      "Epoch: 11, Train_Loss: 2.2521305084228516, Test_Loss: 2.2532379627227783 *\n",
      "Epoch: 11, Train_Loss: 2.2786478996276855, Test_Loss: 2.250561237335205 *\n",
      "Epoch: 11, Train_Loss: 2.243572235107422, Test_Loss: 2.250415086746216 *\n",
      "Epoch: 11, Train_Loss: 2.246692657470703, Test_Loss: 2.258228302001953\n",
      "Epoch: 11, Train_Loss: 2.2464208602905273, Test_Loss: 2.2735350131988525\n",
      "Epoch: 11, Train_Loss: 2.2472198009490967, Test_Loss: 2.2653656005859375 *\n",
      "Epoch: 11, Train_Loss: 2.261765956878662, Test_Loss: 2.2507524490356445 *\n",
      "Epoch: 11, Train_Loss: 2.266428232192993, Test_Loss: 2.2437851428985596 *\n",
      "Epoch: 11, Train_Loss: 2.2554609775543213, Test_Loss: 2.2433881759643555 *\n",
      "Epoch: 12, Train_Loss: 2.2585132122039795, Test_Loss: 2.241302967071533 *\n",
      "Epoch: 12, Train_Loss: 2.2647953033447266, Test_Loss: 2.241926908493042\n",
      "Epoch: 12, Train_Loss: 2.2424631118774414, Test_Loss: 2.2419369220733643\n",
      "Epoch: 12, Train_Loss: 2.24113392829895, Test_Loss: 2.239800214767456 *\n",
      "Epoch: 12, Train_Loss: 2.238022565841675, Test_Loss: 2.240899085998535\n",
      "Epoch: 12, Train_Loss: 2.2764358520507812, Test_Loss: 2.246288537979126\n",
      "Epoch: 12, Train_Loss: 2.26273512840271, Test_Loss: 2.2435595989227295 *\n",
      "Epoch: 12, Train_Loss: 2.2494211196899414, Test_Loss: 2.238293409347534 *\n",
      "Epoch: 12, Train_Loss: 2.2541048526763916, Test_Loss: 2.2358341217041016 *\n",
      "Epoch: 12, Train_Loss: 2.2914299964904785, Test_Loss: 2.2340991497039795 *\n",
      "Epoch: 12, Train_Loss: 2.2799177169799805, Test_Loss: 2.234889507293701\n",
      "Epoch: 12, Train_Loss: 2.2425825595855713, Test_Loss: 2.2346620559692383 *\n",
      "Epoch: 12, Train_Loss: 2.258491277694702, Test_Loss: 2.2725844383239746\n",
      "Epoch: 12, Train_Loss: 2.2457852363586426, Test_Loss: 2.26922607421875 *\n",
      "Epoch: 12, Train_Loss: 2.2428579330444336, Test_Loss: 6.664130210876465\n",
      "Epoch: 12, Train_Loss: 2.23382568359375, Test_Loss: 3.257239818572998 *\n",
      "Epoch: 12, Train_Loss: 2.2586312294006348, Test_Loss: 2.2315120697021484 *\n",
      "Epoch: 12, Train_Loss: 2.2934463024139404, Test_Loss: 2.2455554008483887\n",
      "Epoch: 12, Train_Loss: 4.45536994934082, Test_Loss: 2.293410301208496\n",
      "Epoch: 12, Train_Loss: 5.356899738311768, Test_Loss: 2.2898998260498047 *\n",
      "Epoch: 12, Train_Loss: 2.2439351081848145, Test_Loss: 2.2388992309570312 *\n",
      "Epoch: 12, Train_Loss: 2.2283945083618164, Test_Loss: 2.306126356124878\n",
      "Epoch: 12, Train_Loss: 2.296266794204712, Test_Loss: 2.300863027572632 *\n",
      "Epoch: 12, Train_Loss: 2.387425422668457, Test_Loss: 2.2271153926849365 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 2.24483585357666, Test_Loss: 2.2612545490264893\n",
      "Epoch: 12, Train_Loss: 2.22603440284729, Test_Loss: 2.237018346786499 *\n",
      "Epoch: 12, Train_Loss: 2.239084243774414, Test_Loss: 2.2372024059295654\n",
      "Epoch: 12, Train_Loss: 2.281789779663086, Test_Loss: 2.225245237350464 *\n",
      "Epoch: 12, Train_Loss: 2.228646755218506, Test_Loss: 2.3565335273742676\n",
      "Epoch: 12, Train_Loss: 2.230165719985962, Test_Loss: 2.2606799602508545 *\n",
      "Epoch: 12, Train_Loss: 3.2681031227111816, Test_Loss: 2.324734926223755\n",
      "Epoch: 12, Train_Loss: 3.5682928562164307, Test_Loss: 2.28865122795105 *\n",
      "Epoch: 12, Train_Loss: 2.684180498123169, Test_Loss: 2.2608256340026855 *\n",
      "Epoch: 12, Train_Loss: 2.313128709793091, Test_Loss: 2.2431325912475586 *\n",
      "Epoch: 12, Train_Loss: 3.324112892150879, Test_Loss: 2.2349460124969482 *\n",
      "Epoch: 12, Train_Loss: 4.463125228881836, Test_Loss: 2.233433485031128 *\n",
      "Epoch: 12, Train_Loss: 2.3875508308410645, Test_Loss: 2.2384989261627197\n",
      "Epoch: 12, Train_Loss: 2.2588555812835693, Test_Loss: 2.230358362197876 *\n",
      "Epoch: 12, Train_Loss: 2.2628352642059326, Test_Loss: 2.2339091300964355\n",
      "Epoch: 12, Train_Loss: 3.7782514095306396, Test_Loss: 2.2290048599243164 *\n",
      "Epoch: 12, Train_Loss: 3.700373649597168, Test_Loss: 2.239611864089966\n",
      "Epoch: 12, Train_Loss: 2.254176616668701, Test_Loss: 2.22528076171875 *\n",
      "Epoch: 12, Train_Loss: 2.23211407661438, Test_Loss: 2.218214750289917 *\n",
      "Epoch: 12, Train_Loss: 2.2131409645080566, Test_Loss: 2.224095106124878\n",
      "Epoch: 12, Train_Loss: 2.9289133548736572, Test_Loss: 2.2511134147644043\n",
      "Epoch: 12, Train_Loss: 2.2942230701446533, Test_Loss: 2.238832950592041 *\n",
      "Epoch: 12, Train_Loss: 2.271787166595459, Test_Loss: 2.573575496673584\n",
      "Epoch: 12, Train_Loss: 2.236501455307007, Test_Loss: 2.225409984588623 *\n",
      "Epoch: 12, Train_Loss: 2.3979406356811523, Test_Loss: 2.2532219886779785\n",
      "Epoch: 12, Train_Loss: 2.3449058532714844, Test_Loss: 2.286501884460449\n",
      "Epoch: 12, Train_Loss: 2.4162209033966064, Test_Loss: 2.5562503337860107\n",
      "Epoch: 12, Train_Loss: 2.5584394931793213, Test_Loss: 2.266209363937378 *\n",
      "Epoch: 12, Train_Loss: 2.279651403427124, Test_Loss: 2.28739333152771\n",
      "Epoch: 12, Train_Loss: 2.3025388717651367, Test_Loss: 2.404881000518799\n",
      "Epoch: 12, Train_Loss: 2.438833236694336, Test_Loss: 2.4293673038482666\n",
      "Epoch: 12, Train_Loss: 2.5848493576049805, Test_Loss: 2.232410430908203 *\n",
      "Epoch: 12, Train_Loss: 2.547414779663086, Test_Loss: 2.2347841262817383\n",
      "Epoch: 12, Train_Loss: 2.23290753364563, Test_Loss: 2.20717191696167 *\n",
      "Epoch: 12, Train_Loss: 2.3050520420074463, Test_Loss: 2.2292404174804688\n",
      "Epoch: 12, Train_Loss: 2.3293724060058594, Test_Loss: 2.321981191635132\n",
      "Epoch: 12, Train_Loss: 2.2130253314971924, Test_Loss: 2.92525053024292\n",
      "Epoch: 12, Train_Loss: 2.214195728302002, Test_Loss: 2.3734185695648193 *\n",
      "Epoch: 12, Train_Loss: 2.198845148086548, Test_Loss: 2.925140619277954\n",
      "Epoch: 12, Train_Loss: 2.1983642578125, Test_Loss: 2.5813021659851074 *\n",
      "Epoch: 12, Train_Loss: 2.199720859527588, Test_Loss: 2.6358871459960938\n",
      "Epoch: 12, Train_Loss: 2.2030608654022217, Test_Loss: 2.5227184295654297 *\n",
      "Epoch: 12, Train_Loss: 2.289532423019409, Test_Loss: 2.253324270248413 *\n",
      "Epoch: 12, Train_Loss: 2.2534430027008057, Test_Loss: 2.199540853500366 *\n",
      "Epoch: 12, Train_Loss: 2.3034987449645996, Test_Loss: 2.212528705596924\n",
      "Epoch: 12, Train_Loss: 2.3124358654022217, Test_Loss: 2.2917981147766113\n",
      "Epoch: 12, Train_Loss: 2.6167492866516113, Test_Loss: 3.0038247108459473\n",
      "Epoch: 12, Train_Loss: 2.1929826736450195, Test_Loss: 2.4884538650512695 *\n",
      "Epoch: 12, Train_Loss: 2.2229502201080322, Test_Loss: 3.946568489074707\n",
      "Epoch: 12, Train_Loss: 2.4041948318481445, Test_Loss: 2.8022587299346924 *\n",
      "Epoch: 12, Train_Loss: 2.6711843013763428, Test_Loss: 3.2916882038116455\n",
      "Epoch: 12, Train_Loss: 2.484999418258667, Test_Loss: 2.37469744682312 *\n",
      "Epoch: 12, Train_Loss: 2.1981449127197266, Test_Loss: 2.202194929122925 *\n",
      "Epoch: 12, Train_Loss: 2.346958637237549, Test_Loss: 2.3012654781341553\n",
      "Epoch: 12, Train_Loss: 2.6727070808410645, Test_Loss: 3.347653865814209\n",
      "Epoch: 12, Train_Loss: 2.638195037841797, Test_Loss: 2.9806461334228516 *\n",
      "Epoch: 12, Train_Loss: 2.2279212474823, Test_Loss: 2.2674152851104736 *\n",
      "Epoch: 12, Train_Loss: 2.209045886993408, Test_Loss: 2.2158560752868652 *\n",
      "Epoch: 12, Train_Loss: 2.1962473392486572, Test_Loss: 2.2470240592956543\n",
      "Epoch: 12, Train_Loss: 3.4273056983947754, Test_Loss: 2.6410961151123047\n",
      "Epoch: 12, Train_Loss: 3.1529054641723633, Test_Loss: 2.348630666732788 *\n",
      "Epoch: 12, Train_Loss: 2.1962687969207764, Test_Loss: 3.1962156295776367\n",
      "Epoch: 12, Train_Loss: 2.2071287631988525, Test_Loss: 2.816070079803467 *\n",
      "Epoch: 12, Train_Loss: 2.1766107082366943, Test_Loss: 2.3069286346435547 *\n",
      "Epoch: 12, Train_Loss: 2.240551471710205, Test_Loss: 2.1888957023620605 *\n",
      "Epoch: 12, Train_Loss: 2.5614736080169678, Test_Loss: 2.182816505432129 *\n",
      "Epoch: 12, Train_Loss: 2.1977052688598633, Test_Loss: 2.1852200031280518\n",
      "Epoch: 12, Train_Loss: 2.2171411514282227, Test_Loss: 2.248603105545044\n",
      "Epoch: 12, Train_Loss: 2.197338342666626, Test_Loss: 2.6210832595825195\n",
      "Epoch: 12, Train_Loss: 2.187584638595581, Test_Loss: 2.6486942768096924\n",
      "Epoch: 12, Train_Loss: 19.708236694335938, Test_Loss: 2.350207567214966 *\n",
      "Epoch: 12, Train_Loss: 2.182276487350464, Test_Loss: 2.205005645751953 *\n",
      "Epoch: 12, Train_Loss: 4.968194484710693, Test_Loss: 2.2098753452301025\n",
      "Epoch: 12, Train_Loss: 3.397531032562256, Test_Loss: 2.193363904953003 *\n",
      "Epoch: 12, Train_Loss: 2.1825265884399414, Test_Loss: 2.3569259643554688\n",
      "Epoch: 12, Train_Loss: 2.252211093902588, Test_Loss: 3.3055200576782227\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 9.030662536621094, Test_Loss: 2.9000039100646973 *\n",
      "Epoch: 12, Train_Loss: 7.255921363830566, Test_Loss: 2.223850727081299 *\n",
      "Epoch: 12, Train_Loss: 2.1931345462799072, Test_Loss: 2.2081656455993652 *\n",
      "Epoch: 12, Train_Loss: 2.415816307067871, Test_Loss: 2.177957057952881 *\n",
      "Epoch: 12, Train_Loss: 8.129179954528809, Test_Loss: 2.18792724609375\n",
      "Epoch: 12, Train_Loss: 2.200747013092041, Test_Loss: 2.2003397941589355\n",
      "Epoch: 12, Train_Loss: 2.174079656600952, Test_Loss: 2.19584584236145 *\n",
      "Epoch: 12, Train_Loss: 2.1708693504333496, Test_Loss: 2.314875364303589\n",
      "Epoch: 12, Train_Loss: 2.166717529296875, Test_Loss: 2.170034885406494 *\n",
      "Epoch: 12, Train_Loss: 2.1788878440856934, Test_Loss: 2.2139594554901123\n",
      "Epoch: 12, Train_Loss: 2.161733388900757, Test_Loss: 2.2899792194366455\n",
      "Epoch: 12, Train_Loss: 2.1676559448242188, Test_Loss: 2.460174083709717\n",
      "Epoch: 12, Train_Loss: 2.1657469272613525, Test_Loss: 2.338696241378784 *\n",
      "Epoch: 12, Train_Loss: 2.1676218509674072, Test_Loss: 2.165959596633911 *\n",
      "Epoch: 12, Train_Loss: 2.174375534057617, Test_Loss: 2.164036989212036 *\n",
      "Epoch: 12, Train_Loss: 2.1670970916748047, Test_Loss: 2.1601006984710693 *\n",
      "Epoch: 12, Train_Loss: 2.1694865226745605, Test_Loss: 2.16020131111145\n",
      "Epoch: 12, Train_Loss: 2.2103123664855957, Test_Loss: 2.158130168914795 *\n",
      "Epoch: 12, Train_Loss: 2.209794759750366, Test_Loss: 3.057490110397339\n",
      "Epoch: 12, Train_Loss: 2.1680264472961426, Test_Loss: 7.220808029174805\n",
      "Epoch: 12, Train_Loss: 2.1702144145965576, Test_Loss: 2.2117502689361572 *\n",
      "Epoch: 12, Train_Loss: 2.1560184955596924, Test_Loss: 2.1741390228271484 *\n",
      "Epoch: 12, Train_Loss: 2.1585514545440674, Test_Loss: 2.1740753650665283 *\n",
      "Epoch: 12, Train_Loss: 2.1524505615234375, Test_Loss: 2.161595344543457 *\n",
      "Epoch: 12, Train_Loss: 2.1493403911590576, Test_Loss: 2.173823595046997\n",
      "Epoch: 12, Train_Loss: 2.146454334259033, Test_Loss: 2.171722650527954 *\n",
      "Epoch: 12, Train_Loss: 2.1506309509277344, Test_Loss: 2.196960926055908\n",
      "Epoch: 12, Train_Loss: 2.1447551250457764, Test_Loss: 2.1655588150024414 *\n",
      "Epoch: 12, Train_Loss: 2.1465394496917725, Test_Loss: 2.178799629211426\n",
      "Epoch: 12, Train_Loss: 2.144008159637451, Test_Loss: 2.174075126647949 *\n",
      "Epoch: 12, Train_Loss: 2.1474609375, Test_Loss: 2.1864612102508545\n",
      "Epoch: 12, Train_Loss: 2.1654016971588135, Test_Loss: 2.1601457595825195 *\n",
      "Epoch: 12, Train_Loss: 2.1761224269866943, Test_Loss: 2.15887713432312 *\n",
      "Epoch: 12, Train_Loss: 2.1569271087646484, Test_Loss: 2.189640522003174\n",
      "Epoch: 12, Train_Loss: 2.1470277309417725, Test_Loss: 2.16660737991333 *\n",
      "Epoch: 12, Train_Loss: 6.474109172821045, Test_Loss: 2.1635196208953857 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 6.909297943115234, Test_Loss: 2.1751163005828857\n",
      "Epoch: 12, Train_Loss: 2.1620421409606934, Test_Loss: 2.185194969177246\n",
      "Epoch: 12, Train_Loss: 2.166875123977661, Test_Loss: 2.175623655319214 *\n",
      "Epoch: 12, Train_Loss: 2.1975460052490234, Test_Loss: 2.199671983718872\n",
      "Epoch: 12, Train_Loss: 2.179804563522339, Test_Loss: 2.211467742919922\n",
      "Epoch: 12, Train_Loss: 2.1814072132110596, Test_Loss: 2.226290464401245\n",
      "Epoch: 12, Train_Loss: 2.194077253341675, Test_Loss: 2.2095868587493896 *\n",
      "Epoch: 12, Train_Loss: 2.262988567352295, Test_Loss: 2.192896604537964 *\n",
      "Epoch: 12, Train_Loss: 2.420537233352661, Test_Loss: 2.1886768341064453 *\n",
      "Epoch: 12, Train_Loss: 2.3059473037719727, Test_Loss: 2.190218210220337\n",
      "Epoch: 12, Train_Loss: 2.2049739360809326, Test_Loss: 2.1861188411712646 *\n",
      "Epoch: 12, Train_Loss: 2.2069668769836426, Test_Loss: 2.1745240688323975 *\n",
      "Epoch: 12, Train_Loss: 2.2825417518615723, Test_Loss: 2.2179203033447266\n",
      "Epoch: 12, Train_Loss: 2.296510696411133, Test_Loss: 2.194624423980713 *\n",
      "Epoch: 12, Train_Loss: 2.3079161643981934, Test_Loss: 5.531702995300293\n",
      "Epoch: 12, Train_Loss: 2.1969211101531982, Test_Loss: 4.745393753051758 *\n",
      "Epoch: 12, Train_Loss: 2.192389488220215, Test_Loss: 2.1355063915252686 *\n",
      "Epoch: 12, Train_Loss: 2.133993148803711, Test_Loss: 2.1287801265716553 *\n",
      "Epoch: 12, Train_Loss: 2.175947904586792, Test_Loss: 2.1662209033966064\n",
      "Epoch: 12, Train_Loss: 2.180284023284912, Test_Loss: 2.1451148986816406 *\n",
      "Epoch: 12, Train_Loss: 2.1412620544433594, Test_Loss: 2.150639295578003\n",
      "Epoch: 12, Train_Loss: 2.1273815631866455, Test_Loss: 2.210576295852661\n",
      "Epoch: 12, Train_Loss: 2.1305718421936035, Test_Loss: 2.2399959564208984\n",
      "Epoch: 12, Train_Loss: 2.140869379043579, Test_Loss: 2.1262083053588867 *\n",
      "Epoch: 12, Train_Loss: 5.917145729064941, Test_Loss: 2.153944492340088\n",
      "Epoch: 12, Train_Loss: 4.045380115509033, Test_Loss: 2.1469178199768066 *\n",
      "Epoch: 12, Train_Loss: 2.1294853687286377, Test_Loss: 2.1385059356689453 *\n",
      "Epoch: 12, Train_Loss: 2.1548964977264404, Test_Loss: 2.1283390522003174 *\n",
      "Epoch: 12, Train_Loss: 2.149366617202759, Test_Loss: 2.1902239322662354\n",
      "Epoch: 12, Train_Loss: 2.1319499015808105, Test_Loss: 2.1632676124572754 *\n",
      "Epoch: 12, Train_Loss: 2.124488353729248, Test_Loss: 2.23746657371521\n",
      "Epoch: 12, Train_Loss: 2.1386561393737793, Test_Loss: 2.2082834243774414 *\n",
      "Epoch: 12, Train_Loss: 2.1379823684692383, Test_Loss: 2.1452956199645996 *\n",
      "Epoch: 12, Train_Loss: 2.139387369155884, Test_Loss: 2.1325151920318604 *\n",
      "Epoch: 12, Train_Loss: 2.1454670429229736, Test_Loss: 2.1211163997650146 *\n",
      "Epoch: 12, Train_Loss: 2.13090181350708, Test_Loss: 2.120067834854126 *\n",
      "Epoch: 12, Train_Loss: 2.1153829097747803, Test_Loss: 2.1223864555358887\n",
      "Epoch: 12, Train_Loss: 2.1363115310668945, Test_Loss: 2.125058174133301\n",
      "Epoch: 12, Train_Loss: 2.1187140941619873, Test_Loss: 2.115788698196411 *\n",
      "Epoch: 12, Train_Loss: 2.115056037902832, Test_Loss: 2.113778829574585 *\n",
      "Epoch: 12, Train_Loss: 2.1183574199676514, Test_Loss: 2.120488166809082\n",
      "Epoch: 12, Train_Loss: 2.145911931991577, Test_Loss: 2.1163558959960938 *\n",
      "Epoch: 12, Train_Loss: 2.139265298843384, Test_Loss: 2.1173367500305176\n",
      "Epoch: 12, Train_Loss: 2.1132073402404785, Test_Loss: 2.1448490619659424\n",
      "Epoch: 12, Train_Loss: 2.1159603595733643, Test_Loss: 2.1229753494262695 *\n",
      "Epoch: 12, Train_Loss: 2.1421868801116943, Test_Loss: 2.1387832164764404\n",
      "Epoch: 12, Train_Loss: 2.192202568054199, Test_Loss: 2.4882750511169434\n",
      "Epoch: 12, Train_Loss: 2.1530075073242188, Test_Loss: 2.1837995052337646 *\n",
      "Epoch: 12, Train_Loss: 2.162036895751953, Test_Loss: 2.1281659603118896 *\n",
      "Epoch: 12, Train_Loss: 2.1513519287109375, Test_Loss: 2.1566710472106934\n",
      "Epoch: 12, Train_Loss: 2.1727025508880615, Test_Loss: 2.340327024459839\n",
      "Epoch: 12, Train_Loss: 2.1439640522003174, Test_Loss: 2.2282867431640625 *\n",
      "Epoch: 12, Train_Loss: 2.144345760345459, Test_Loss: 2.146484375 *\n",
      "Epoch: 12, Train_Loss: 2.2046167850494385, Test_Loss: 2.3099193572998047\n",
      "Epoch: 12, Train_Loss: 2.1551921367645264, Test_Loss: 2.3885304927825928\n",
      "Epoch: 12, Train_Loss: 2.112586259841919, Test_Loss: 2.1493067741394043 *\n",
      "Epoch: 12, Train_Loss: 2.1020750999450684, Test_Loss: 2.1732189655303955\n",
      "Epoch: 12, Train_Loss: 2.0996766090393066, Test_Loss: 2.105811834335327 *\n",
      "Epoch: 12, Train_Loss: 2.097670316696167, Test_Loss: 2.1253106594085693\n",
      "Epoch: 12, Train_Loss: 2.1007027626037598, Test_Loss: 2.1322412490844727\n",
      "Epoch: 12, Train_Loss: 2.097940683364868, Test_Loss: 2.835653066635132\n",
      "Epoch: 12, Train_Loss: 5.661764144897461, Test_Loss: 2.279916286468506 *\n",
      "Epoch: 12, Train_Loss: 3.3868484497070312, Test_Loss: 2.8109657764434814\n",
      "Epoch: 12, Train_Loss: 2.0953915119171143, Test_Loss: 2.6487560272216797 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 2.1044533252716064, Test_Loss: 2.376305341720581 *\n",
      "Epoch: 12, Train_Loss: 2.1001534461975098, Test_Loss: 2.5155138969421387\n",
      "Epoch: 12, Train_Loss: 2.0940051078796387, Test_Loss: 2.1470179557800293 *\n",
      "Epoch: 12, Train_Loss: 2.0934982299804688, Test_Loss: 2.1033904552459717 *\n",
      "Epoch: 12, Train_Loss: 2.0921356678009033, Test_Loss: 2.118978977203369\n",
      "Epoch: 12, Train_Loss: 2.0927529335021973, Test_Loss: 2.265685558319092\n",
      "Epoch: 12, Train_Loss: 2.0948233604431152, Test_Loss: 2.5833182334899902\n",
      "Epoch: 12, Train_Loss: 2.112887144088745, Test_Loss: 2.630507707595825\n",
      "Epoch: 12, Train_Loss: 2.1515684127807617, Test_Loss: 3.5527141094207764\n",
      "Epoch: 12, Train_Loss: 2.1585335731506348, Test_Loss: 3.2061171531677246 *\n",
      "Epoch: 12, Train_Loss: 2.157200336456299, Test_Loss: 3.014913320541382 *\n",
      "Epoch: 12, Train_Loss: 2.1045238971710205, Test_Loss: 2.4300410747528076 *\n",
      "Epoch: 12, Train_Loss: 2.1208364963531494, Test_Loss: 2.0905697345733643 *\n",
      "Epoch: 12, Train_Loss: 2.322108268737793, Test_Loss: 2.1543681621551514\n",
      "Epoch: 12, Train_Loss: 2.3460426330566406, Test_Loss: 3.0946264266967773\n",
      "Epoch: 12, Train_Loss: 2.3186309337615967, Test_Loss: 3.2702436447143555\n",
      "Epoch: 12, Train_Loss: 2.1359610557556152, Test_Loss: 2.13435697555542 *\n",
      "Epoch: 12, Train_Loss: 2.0863990783691406, Test_Loss: 2.1415343284606934\n",
      "Epoch: 12, Train_Loss: 2.0810859203338623, Test_Loss: 2.1168243885040283 *\n",
      "Epoch: 12, Train_Loss: 2.093096971511841, Test_Loss: 2.4498233795166016\n",
      "Epoch: 12, Train_Loss: 2.0959887504577637, Test_Loss: 2.267085075378418 *\n",
      "Epoch: 12, Train_Loss: 2.0911190509796143, Test_Loss: 2.878385066986084\n",
      "Epoch: 12, Train_Loss: 2.085116147994995, Test_Loss: 3.009300470352173\n",
      "Epoch: 12, Train_Loss: 2.076059103012085, Test_Loss: 2.3263118267059326 *\n",
      "Epoch: 12, Train_Loss: 2.075267791748047, Test_Loss: 2.0835986137390137 *\n",
      "Epoch: 12, Train_Loss: 2.084336042404175, Test_Loss: 2.0877318382263184\n",
      "Epoch: 12, Train_Loss: 2.174534797668457, Test_Loss: 2.0924270153045654\n",
      "Epoch: 12, Train_Loss: 2.2752726078033447, Test_Loss: 2.131500482559204\n",
      "Epoch: 12, Train_Loss: 2.2360072135925293, Test_Loss: 2.4278104305267334\n",
      "Epoch: 12, Train_Loss: 2.155914783477783, Test_Loss: 2.6081552505493164\n",
      "Epoch: 12, Train_Loss: 2.1905529499053955, Test_Loss: 2.2981886863708496 *\n",
      "Epoch: 12, Train_Loss: 2.232473134994507, Test_Loss: 2.1213996410369873 *\n",
      "Epoch: 12, Train_Loss: 2.0826871395111084, Test_Loss: 2.1064300537109375 *\n",
      "Epoch: 12, Train_Loss: 2.247981309890747, Test_Loss: 2.083308696746826 *\n",
      "Epoch: 12, Train_Loss: 2.2041313648223877, Test_Loss: 2.1780502796173096\n",
      "Epoch: 12, Train_Loss: 2.3324050903320312, Test_Loss: 3.09919810295105\n",
      "Epoch: 12, Train_Loss: 2.0761566162109375, Test_Loss: 3.2660858631134033\n",
      "Epoch: 12, Train_Loss: 2.718820095062256, Test_Loss: 2.1269655227661133 *\n",
      "Epoch: 12, Train_Loss: 4.635862350463867, Test_Loss: 2.1465365886688232\n",
      "Epoch: 12, Train_Loss: 2.1091792583465576, Test_Loss: 2.0710175037384033 *\n",
      "Epoch: 12, Train_Loss: 2.1232848167419434, Test_Loss: 2.0712475776672363\n",
      "Epoch: 12, Train_Loss: 2.1184184551239014, Test_Loss: 2.0777549743652344\n",
      "Epoch: 12, Train_Loss: 2.130605459213257, Test_Loss: 2.081251621246338\n",
      "Epoch: 12, Train_Loss: 2.060978889465332, Test_Loss: 2.109544038772583\n",
      "Epoch: 12, Train_Loss: 2.0626723766326904, Test_Loss: 2.082167387008667 *\n",
      "Epoch: 12, Train_Loss: 2.1692147254943848, Test_Loss: 2.065769672393799 *\n",
      "Epoch: 12, Train_Loss: 2.1447319984436035, Test_Loss: 2.1946804523468018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 2.144848585128784, Test_Loss: 2.500670909881592\n",
      "Epoch: 12, Train_Loss: 2.112783670425415, Test_Loss: 2.2496180534362793 *\n",
      "Epoch: 12, Train_Loss: 2.1209428310394287, Test_Loss: 2.143752098083496 *\n",
      "Epoch: 12, Train_Loss: 2.0779359340667725, Test_Loss: 2.075089693069458 *\n",
      "Epoch: 12, Train_Loss: 2.0806684494018555, Test_Loss: 2.0814545154571533\n",
      "Epoch: 12, Train_Loss: 2.0728094577789307, Test_Loss: 2.0697250366210938 *\n",
      "Epoch: 12, Train_Loss: 2.094207286834717, Test_Loss: 2.0743346214294434\n",
      "Epoch: 12, Train_Loss: 2.071972608566284, Test_Loss: 2.172593832015991\n",
      "Epoch: 12, Train_Loss: 2.058748483657837, Test_Loss: 7.069220066070557\n",
      "Epoch: 12, Train_Loss: 2.0837461948394775, Test_Loss: 2.1773762702941895 *\n",
      "Epoch: 12, Train_Loss: 2.0967705249786377, Test_Loss: 2.067019462585449 *\n",
      "Epoch: 12, Train_Loss: 2.077054977416992, Test_Loss: 2.058089017868042 *\n",
      "Epoch: 12, Train_Loss: 2.052889347076416, Test_Loss: 2.0642004013061523\n",
      "Epoch: 12, Train_Loss: 2.057298183441162, Test_Loss: 2.0614702701568604 *\n",
      "Epoch: 12, Train_Loss: 2.049787759780884, Test_Loss: 2.054323673248291 *\n",
      "Epoch: 12, Train_Loss: 2.051406145095825, Test_Loss: 2.0519747734069824 *\n",
      "Epoch: 12, Train_Loss: 2.0489325523376465, Test_Loss: 2.04903507232666 *\n",
      "Epoch: 12, Train_Loss: 2.047386646270752, Test_Loss: 2.047886371612549 *\n",
      "Epoch: 12, Train_Loss: 2.050483465194702, Test_Loss: 2.0522282123565674\n",
      "Epoch: 12, Train_Loss: 2.0485076904296875, Test_Loss: 2.0537147521972656\n",
      "Epoch: 12, Train_Loss: 2.0482099056243896, Test_Loss: 2.06683087348938\n",
      "Epoch: 12, Train_Loss: 2.049943447113037, Test_Loss: 2.0712249279022217\n",
      "Epoch: 12, Train_Loss: 2.055959701538086, Test_Loss: 2.0632941722869873 *\n",
      "Epoch: 12, Train_Loss: 2.062222480773926, Test_Loss: 2.0452067852020264 *\n",
      "Epoch: 12, Train_Loss: 2.0538556575775146, Test_Loss: 2.042482614517212 *\n",
      "Epoch: 12, Train_Loss: 2.068535089492798, Test_Loss: 2.042081117630005 *\n",
      "Epoch: 12, Train_Loss: 2.0563313961029053, Test_Loss: 2.0447638034820557\n",
      "Epoch: 12, Train_Loss: 2.063671112060547, Test_Loss: 2.0419764518737793 *\n",
      "Epoch: 12, Train_Loss: 2.0440707206726074, Test_Loss: 2.0416479110717773 *\n",
      "Epoch: 12, Train_Loss: 2.044806718826294, Test_Loss: 2.0370469093322754 *\n",
      "Epoch: 12, Train_Loss: 2.062021017074585, Test_Loss: 2.0431203842163086\n",
      "Epoch: 12, Train_Loss: 2.0545358657836914, Test_Loss: 2.0411126613616943 *\n",
      "Epoch: 12, Train_Loss: 2.039994478225708, Test_Loss: 2.037853717803955 *\n",
      "Epoch: 12, Train_Loss: 2.0388894081115723, Test_Loss: 2.0363504886627197 *\n",
      "Epoch: 12, Train_Loss: 2.043811559677124, Test_Loss: 2.0378756523132324\n",
      "Epoch: 12, Train_Loss: 2.115438222885132, Test_Loss: 2.034613609313965 *\n",
      "Epoch: 12, Train_Loss: 2.0628821849823, Test_Loss: 2.0357985496520996\n",
      "Epoch: 12, Train_Loss: 2.078742504119873, Test_Loss: 2.0395851135253906\n",
      "Epoch: 12, Train_Loss: 2.0327212810516357, Test_Loss: 2.0951578617095947\n",
      "Epoch: 12, Train_Loss: 2.0612075328826904, Test_Loss: 3.78208589553833\n",
      "Epoch: 12, Train_Loss: 2.0756988525390625, Test_Loss: 5.785821914672852\n",
      "Epoch: 12, Train_Loss: 2.0336191654205322, Test_Loss: 2.0344431400299072 *\n",
      "Epoch: 12, Train_Loss: 2.0480165481567383, Test_Loss: 2.0337443351745605 *\n",
      "Epoch: 12, Train_Loss: 2.0571494102478027, Test_Loss: 2.0811495780944824\n",
      "Epoch: 12, Train_Loss: 2.1025357246398926, Test_Loss: 2.0819499492645264\n",
      "Epoch: 12, Train_Loss: 2.1338021755218506, Test_Loss: 2.0908684730529785\n",
      "Epoch: 12, Train_Loss: 2.0999035835266113, Test_Loss: 2.057264566421509 *\n",
      "Epoch: 12, Train_Loss: 2.054616689682007, Test_Loss: 2.1456003189086914\n",
      "Epoch: 12, Train_Loss: 2.026538372039795, Test_Loss: 2.0336592197418213 *\n",
      "Epoch: 12, Train_Loss: 2.0490505695343018, Test_Loss: 2.0386202335357666\n",
      "Epoch: 12, Train_Loss: 2.0243680477142334, Test_Loss: 2.054654121398926\n",
      "Epoch: 12, Train_Loss: 2.028108835220337, Test_Loss: 2.041846513748169 *\n",
      "Epoch: 12, Train_Loss: 2.044807195663452, Test_Loss: 2.0313408374786377 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 2.0448930263519287, Test_Loss: 2.0974504947662354\n",
      "Epoch: 12, Train_Loss: 2.121169328689575, Test_Loss: 2.1252331733703613\n",
      "Epoch: 12, Train_Loss: 2.0355212688446045, Test_Loss: 2.097177505493164 *\n",
      "Epoch: 12, Train_Loss: 2.0906782150268555, Test_Loss: 2.1365387439727783\n",
      "Epoch: 12, Train_Loss: 2.0288822650909424, Test_Loss: 2.0406670570373535 *\n",
      "Epoch: 12, Train_Loss: 2.056339740753174, Test_Loss: 2.0488831996917725\n",
      "Epoch: 12, Train_Loss: 2.030223846435547, Test_Loss: 2.0353171825408936 *\n",
      "Epoch: 12, Train_Loss: 2.278237819671631, Test_Loss: 2.0280487537384033 *\n",
      "Epoch: 12, Train_Loss: 2.0965445041656494, Test_Loss: 2.0299646854400635\n",
      "Epoch: 12, Train_Loss: 2.033985137939453, Test_Loss: 2.0381479263305664\n",
      "Epoch: 12, Train_Loss: 2.0291571617126465, Test_Loss: 2.0325510501861572 *\n",
      "Epoch: 12, Train_Loss: 2.013805866241455, Test_Loss: 2.028134822845459 *\n",
      "Epoch: 12, Train_Loss: 2.0156214237213135, Test_Loss: 2.0400688648223877\n",
      "Epoch: 12, Train_Loss: 2.010756492614746, Test_Loss: 2.0289840698242188 *\n",
      "Epoch: 12, Train_Loss: 2.018782377243042, Test_Loss: 2.0357649326324463\n",
      "Epoch: 12, Train_Loss: 2.0275111198425293, Test_Loss: 2.01774525642395 *\n",
      "Epoch: 12, Train_Loss: 2.0356032848358154, Test_Loss: 2.033034086227417\n",
      "Epoch: 12, Train_Loss: 2.023141860961914, Test_Loss: 2.0784411430358887\n",
      "Epoch: 12, Train_Loss: 2.0243678092956543, Test_Loss: 2.234508514404297\n",
      "Epoch: 12, Train_Loss: 2.0278098583221436, Test_Loss: 2.129009246826172 *\n",
      "Epoch: 12, Train_Loss: 2.0111021995544434, Test_Loss: 2.0303399562835693 *\n",
      "Epoch: 12, Train_Loss: 2.006225109100342, Test_Loss: 2.096381187438965\n",
      "Epoch: 12, Train_Loss: 2.0152945518493652, Test_Loss: 2.2216429710388184\n",
      "Epoch: 12, Train_Loss: 2.037464141845703, Test_Loss: 2.266268491744995\n",
      "Epoch: 12, Train_Loss: 2.0401484966278076, Test_Loss: 2.0106654167175293 *\n",
      "Epoch: 12, Train_Loss: 2.008857488632202, Test_Loss: 2.1965668201446533\n",
      "Epoch: 12, Train_Loss: 2.033020257949829, Test_Loss: 2.32157039642334\n",
      "Epoch: 12, Train_Loss: 2.0664477348327637, Test_Loss: 2.036349058151245 *\n",
      "Epoch: 12, Train_Loss: 2.0605709552764893, Test_Loss: 2.051483154296875\n",
      "Epoch: 12, Train_Loss: 2.004281520843506, Test_Loss: 2.0033066272735596 *\n",
      "Epoch: 12, Train_Loss: 2.029986619949341, Test_Loss: 2.0353353023529053\n",
      "Epoch: 12, Train_Loss: 2.009042501449585, Test_Loss: 2.016840696334839 *\n",
      "Epoch: 12, Train_Loss: 2.0191140174865723, Test_Loss: 2.720773458480835\n",
      "Epoch: 12, Train_Loss: 1.9999279975891113, Test_Loss: 2.2663381099700928 *\n",
      "Epoch: 12, Train_Loss: 2.021704912185669, Test_Loss: 2.698817491531372\n",
      "Epoch: 12, Train_Loss: 2.0573017597198486, Test_Loss: 2.762568235397339\n",
      "Epoch: 12, Train_Loss: 4.342303276062012, Test_Loss: 2.1642141342163086 *\n",
      "Epoch: 12, Train_Loss: 5.005179405212402, Test_Loss: 2.53879976272583\n",
      "Epoch: 12, Train_Loss: 2.0160274505615234, Test_Loss: 2.0950098037719727 *\n",
      "Epoch: 12, Train_Loss: 1.9944214820861816, Test_Loss: 2.0038464069366455 *\n",
      "Epoch: 12, Train_Loss: 2.106309175491333, Test_Loss: 2.0208375453948975\n",
      "Epoch: 12, Train_Loss: 2.1317474842071533, Test_Loss: 2.130190849304199\n",
      "Epoch: 12, Train_Loss: 2.014862537384033, Test_Loss: 2.248178482055664\n",
      "Epoch: 12, Train_Loss: 1.9932243824005127, Test_Loss: 2.814910888671875\n",
      "Epoch: 12, Train_Loss: 2.0287702083587646, Test_Loss: 2.7789108753204346 *\n",
      "Epoch: 12, Train_Loss: 2.0273818969726562, Test_Loss: 3.6336374282836914\n",
      "Epoch: 12, Train_Loss: 2.0067005157470703, Test_Loss: 2.742252826690674 *\n",
      "Epoch: 12, Train_Loss: 2.01542329788208, Test_Loss: 2.536863088607788 *\n",
      "Epoch: 12, Train_Loss: 3.2175188064575195, Test_Loss: 1.9939167499542236 *\n",
      "Epoch: 12, Train_Loss: 3.436478614807129, Test_Loss: 1.9983570575714111\n",
      "Epoch: 12, Train_Loss: 2.223924160003662, Test_Loss: 2.71199107170105\n",
      "Epoch: 12, Train_Loss: 2.072399139404297, Test_Loss: 3.4307467937469482\n",
      "Epoch: 12, Train_Loss: 3.4625022411346436, Test_Loss: 2.0476911067962646 *\n",
      "Epoch: 12, Train_Loss: 3.888045072555542, Test_Loss: 2.0729870796203613\n",
      "Epoch: 12, Train_Loss: 2.0493764877319336, Test_Loss: 1.9958012104034424 *\n",
      "Epoch: 12, Train_Loss: 2.0303616523742676, Test_Loss: 2.2022290229797363\n",
      "Epoch: 12, Train_Loss: 2.1531331539154053, Test_Loss: 2.3129348754882812\n",
      "Epoch: 12, Train_Loss: 3.6763052940368652, Test_Loss: 2.478931427001953\n",
      "Epoch: 12, Train_Loss: 3.3101110458374023, Test_Loss: 3.1116013526916504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 2.000765085220337, Test_Loss: 2.2972798347473145 *\n",
      "Epoch: 12, Train_Loss: 2.0036635398864746, Test_Loss: 1.9838588237762451 *\n",
      "Epoch: 12, Train_Loss: 2.024535894393921, Test_Loss: 1.9874142408370972\n",
      "Epoch: 12, Train_Loss: 2.7409796714782715, Test_Loss: 1.9872575998306274 *\n",
      "Epoch: 12, Train_Loss: 2.0131478309631348, Test_Loss: 2.029709815979004\n",
      "Epoch: 12, Train_Loss: 2.0522654056549072, Test_Loss: 2.224797248840332\n",
      "Epoch: 12, Train_Loss: 2.0124523639678955, Test_Loss: 2.531402826309204\n",
      "Epoch: 12, Train_Loss: 2.1531825065612793, Test_Loss: 2.261282444000244 *\n",
      "Epoch: 12, Train_Loss: 2.0971555709838867, Test_Loss: 2.0755488872528076 *\n",
      "Epoch: 12, Train_Loss: 2.2307400703430176, Test_Loss: 2.0088489055633545 *\n",
      "Epoch: 12, Train_Loss: 2.2294862270355225, Test_Loss: 1.9869693517684937 *\n",
      "Epoch: 12, Train_Loss: 2.036424398422241, Test_Loss: 2.0572125911712646\n",
      "Epoch: 12, Train_Loss: 2.1089327335357666, Test_Loss: 2.6545608043670654\n",
      "Epoch: 12, Train_Loss: 2.1969571113586426, Test_Loss: 3.2227249145507812\n",
      "Epoch: 12, Train_Loss: 2.3671419620513916, Test_Loss: 2.1556015014648438 *\n",
      "Epoch: 12, Train_Loss: 2.2753148078918457, Test_Loss: 2.054450750350952 *\n",
      "Epoch: 12, Train_Loss: 1.9961040019989014, Test_Loss: 1.9755144119262695 *\n",
      "Epoch: 12, Train_Loss: 2.074188470840454, Test_Loss: 1.9840025901794434\n",
      "Epoch: 12, Train_Loss: 2.111067056655884, Test_Loss: 1.982844352722168 *\n",
      "Epoch: 12, Train_Loss: 1.9798439741134644, Test_Loss: 2.008107900619507\n",
      "Epoch: 12, Train_Loss: 1.9900609254837036, Test_Loss: 2.008596897125244\n",
      "Epoch: 12, Train_Loss: 1.968231439590454, Test_Loss: 2.007430076599121 *\n",
      "Epoch: 12, Train_Loss: 1.9707846641540527, Test_Loss: 1.9773653745651245 *\n",
      "Epoch: 12, Train_Loss: 1.9744521379470825, Test_Loss: 2.0464553833007812\n",
      "Epoch: 12, Train_Loss: 1.9773670434951782, Test_Loss: 2.3422322273254395\n",
      "Epoch: 12, Train_Loss: 2.065582752227783, Test_Loss: 2.0735678672790527 *\n",
      "Epoch: 12, Train_Loss: 2.0158705711364746, Test_Loss: 2.1529531478881836\n",
      "Epoch: 12, Train_Loss: 2.090771436691284, Test_Loss: 1.9633828401565552 *\n",
      "Epoch: 12, Train_Loss: 2.0762596130371094, Test_Loss: 1.964053750038147\n",
      "Epoch: 12, Train_Loss: 2.3740434646606445, Test_Loss: 1.9663784503936768\n",
      "Epoch: 12, Train_Loss: 1.9734461307525635, Test_Loss: 1.96694815158844\n",
      "Epoch: 12, Train_Loss: 1.995470643043518, Test_Loss: 1.9850962162017822\n",
      "Epoch: 12, Train_Loss: 2.2232306003570557, Test_Loss: 6.769071578979492\n",
      "Epoch: 12, Train_Loss: 2.498804807662964, Test_Loss: 2.728435516357422 *\n",
      "Epoch: 12, Train_Loss: 2.1819164752960205, Test_Loss: 1.9739047288894653 *\n",
      "Epoch: 12, Train_Loss: 1.971923589706421, Test_Loss: 1.9635863304138184 *\n",
      "Epoch: 12, Train_Loss: 2.2115070819854736, Test_Loss: 1.9617196321487427 *\n",
      "Epoch: 12, Train_Loss: 2.490962028503418, Test_Loss: 1.9619617462158203\n",
      "Epoch: 12, Train_Loss: 2.3459248542785645, Test_Loss: 1.9712669849395752\n",
      "Epoch: 12, Train_Loss: 1.9791711568832397, Test_Loss: 1.9852232933044434\n",
      "Epoch: 12, Train_Loss: 1.9804171323776245, Test_Loss: 1.9590243101119995 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 2.0183513164520264, Test_Loss: 1.9757471084594727\n",
      "Epoch: 12, Train_Loss: 3.353384017944336, Test_Loss: 1.9762167930603027\n",
      "Epoch: 12, Train_Loss: 2.699403762817383, Test_Loss: 1.9985872507095337\n",
      "Epoch: 12, Train_Loss: 1.9703972339630127, Test_Loss: 1.9693570137023926 *\n",
      "Epoch: 12, Train_Loss: 1.9816992282867432, Test_Loss: 1.9581466913223267 *\n",
      "Epoch: 12, Train_Loss: 1.9526959657669067, Test_Loss: 1.967819094657898\n",
      "Epoch: 12, Train_Loss: 2.1172003746032715, Test_Loss: 1.9708960056304932\n",
      "Epoch: 12, Train_Loss: 2.2590274810791016, Test_Loss: 1.9529345035552979 *\n",
      "Epoch: 12, Train_Loss: 1.974753737449646, Test_Loss: 1.975361704826355\n",
      "Epoch: 12, Train_Loss: 1.9821876287460327, Test_Loss: 1.9536552429199219 *\n",
      "Epoch: 12, Train_Loss: 1.9739279747009277, Test_Loss: 1.9572482109069824\n",
      "Epoch: 12, Train_Loss: 3.238197088241577, Test_Loss: 1.9610804319381714\n",
      "Epoch: 12, Train_Loss: 18.158586502075195, Test_Loss: 1.96531343460083\n",
      "Epoch: 12, Train_Loss: 2.2199926376342773, Test_Loss: 1.9800831079483032\n",
      "Epoch: 12, Train_Loss: 4.964813709259033, Test_Loss: 1.973045825958252 *\n",
      "Epoch: 12, Train_Loss: 2.6654088497161865, Test_Loss: 1.9663937091827393 *\n",
      "Epoch: 12, Train_Loss: 1.9961118698120117, Test_Loss: 1.964127540588379 *\n",
      "Epoch: 12, Train_Loss: 1.9883860349655151, Test_Loss: 1.9575955867767334 *\n",
      "Epoch: 12, Train_Loss: 10.783677101135254, Test_Loss: 1.9630606174468994\n",
      "Epoch: 12, Train_Loss: 5.012539863586426, Test_Loss: 1.9684972763061523\n",
      "Epoch: 12, Train_Loss: 1.971763253211975, Test_Loss: 1.9826147556304932\n",
      "Epoch: 12, Train_Loss: 3.3141653537750244, Test_Loss: 2.040173292160034\n",
      "Epoch: 12, Train_Loss: 6.763322830200195, Test_Loss: 2.388800859451294\n",
      "Epoch: 12, Train_Loss: 2.0191638469696045, Test_Loss: 7.803488731384277\n",
      "Epoch: 12, Train_Loss: 1.9485102891921997, Test_Loss: 2.003387451171875 *\n",
      "Epoch: 12, Train_Loss: 1.9537568092346191, Test_Loss: 1.962467908859253 *\n",
      "Epoch: 12, Train_Loss: 1.939579725265503, Test_Loss: 1.9660000801086426\n",
      "Epoch: 12, Train_Loss: 1.950690746307373, Test_Loss: 1.974005103111267\n",
      "Epoch: 12, Train_Loss: 1.9413561820983887, Test_Loss: 1.9652814865112305 *\n",
      "Epoch: 12, Train_Loss: 1.9493157863616943, Test_Loss: 2.0103063583374023\n",
      "Epoch: 12, Train_Loss: 1.9395387172698975, Test_Loss: 2.2609431743621826\n",
      "Epoch: 12, Train_Loss: 1.9510349035263062, Test_Loss: 2.038193464279175 *\n",
      "Epoch: 12, Train_Loss: 1.9651150703430176, Test_Loss: 1.9638638496398926 *\n",
      "Epoch: 12, Train_Loss: 1.9485467672348022, Test_Loss: 2.0175282955169678\n",
      "Epoch: 12, Train_Loss: 1.9506968259811401, Test_Loss: 1.9360982179641724 *\n",
      "Epoch: 12, Train_Loss: 2.0084331035614014, Test_Loss: 1.9634888172149658\n",
      "Epoch: 12, Train_Loss: 2.0090513229370117, Test_Loss: 1.9671963453292847\n",
      "Epoch: 12, Train_Loss: 1.9369564056396484, Test_Loss: 1.9727777242660522\n",
      "Epoch: 12, Train_Loss: 1.935333013534546, Test_Loss: 2.0817084312438965\n",
      "Epoch: 12, Train_Loss: 1.9399241209030151, Test_Loss: 2.0832180976867676\n",
      "Epoch: 12, Train_Loss: 1.9290012121200562, Test_Loss: 1.972820520401001 *\n",
      "Epoch: 12, Train_Loss: 1.926175594329834, Test_Loss: 1.9412903785705566 *\n",
      "Epoch: 12, Train_Loss: 1.9263428449630737, Test_Loss: 1.935541033744812 *\n",
      "Epoch: 12, Train_Loss: 1.9257276058197021, Test_Loss: 1.9421051740646362\n",
      "Epoch: 12, Train_Loss: 1.9232653379440308, Test_Loss: 1.9363981485366821 *\n",
      "Epoch: 12, Train_Loss: 1.9217137098312378, Test_Loss: 1.9362038373947144 *\n",
      "Epoch: 12, Train_Loss: 1.9193811416625977, Test_Loss: 1.927651286125183 *\n",
      "Epoch: 12, Train_Loss: 1.919244647026062, Test_Loss: 1.9404981136322021\n",
      "Epoch: 12, Train_Loss: 1.926769495010376, Test_Loss: 1.9374828338623047 *\n",
      "Epoch: 12, Train_Loss: 1.937052607536316, Test_Loss: 1.9326422214508057 *\n",
      "Epoch: 12, Train_Loss: 1.9869678020477295, Test_Loss: 1.9356945753097534\n",
      "Epoch: 12, Train_Loss: 1.935996174812317, Test_Loss: 1.9785163402557373\n",
      "Epoch: 12, Train_Loss: 1.925007939338684, Test_Loss: 1.9658664464950562 *\n",
      "Epoch: 12, Train_Loss: 8.912652015686035, Test_Loss: 1.9463152885437012 *\n",
      "Epoch: 13, Train_Loss: 3.8693971633911133, Test_Loss: 2.099663496017456 *\n",
      "Epoch: 13, Train_Loss: 1.9764920473098755, Test_Loss: 2.333003520965576\n",
      "Epoch: 13, Train_Loss: 1.9693039655685425, Test_Loss: 1.9615691900253296 *\n",
      "Epoch: 13, Train_Loss: 1.9871807098388672, Test_Loss: 1.9570943117141724 *\n",
      "Epoch: 13, Train_Loss: 1.9467190504074097, Test_Loss: 1.9779719114303589\n",
      "Epoch: 13, Train_Loss: 1.9649420976638794, Test_Loss: 2.122734546661377\n",
      "Epoch: 13, Train_Loss: 1.9609533548355103, Test_Loss: 1.9329633712768555 *\n",
      "Epoch: 13, Train_Loss: 2.062218189239502, Test_Loss: 2.1036934852600098\n",
      "Epoch: 13, Train_Loss: 2.1666431427001953, Test_Loss: 2.230281352996826\n",
      "Epoch: 13, Train_Loss: 2.0551583766937256, Test_Loss: 2.123789072036743 *\n",
      "Epoch: 13, Train_Loss: 1.930620551109314, Test_Loss: 2.037810802459717 *\n",
      "Epoch: 13, Train_Loss: 1.9964982271194458, Test_Loss: 1.923701524734497 *\n",
      "Epoch: 13, Train_Loss: 2.030808210372925, Test_Loss: 1.919721245765686 *\n",
      "Epoch: 13, Train_Loss: 2.091026544570923, Test_Loss: 1.920098066329956\n",
      "Epoch: 13, Train_Loss: 2.030025005340576, Test_Loss: 2.4975740909576416\n",
      "Epoch: 13, Train_Loss: 1.9733506441116333, Test_Loss: 2.3916549682617188 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 1.9581764936447144, Test_Loss: 2.3071019649505615 *\n",
      "Epoch: 13, Train_Loss: 1.9156103134155273, Test_Loss: 2.561460018157959\n",
      "Epoch: 13, Train_Loss: 1.9790011644363403, Test_Loss: 2.046842336654663 *\n",
      "Epoch: 13, Train_Loss: 1.9459593296051025, Test_Loss: 2.515292167663574\n",
      "Epoch: 13, Train_Loss: 1.9070390462875366, Test_Loss: 2.1163291931152344 *\n",
      "Epoch: 13, Train_Loss: 1.9042772054672241, Test_Loss: 1.9214814901351929 *\n",
      "Epoch: 13, Train_Loss: 1.9060921669006348, Test_Loss: 1.9122501611709595 *\n",
      "Epoch: 13, Train_Loss: 1.9358530044555664, Test_Loss: 1.9492501020431519\n",
      "Epoch: 13, Train_Loss: 7.069644451141357, Test_Loss: 2.0702199935913086\n",
      "Epoch: 13, Train_Loss: 2.2798590660095215, Test_Loss: 2.8749570846557617\n",
      "Epoch: 13, Train_Loss: 1.9040151834487915, Test_Loss: 2.2514030933380127 *\n",
      "Epoch: 13, Train_Loss: 1.9488250017166138, Test_Loss: 3.808685541152954\n",
      "Epoch: 13, Train_Loss: 1.9278109073638916, Test_Loss: 2.44386625289917 *\n",
      "Epoch: 13, Train_Loss: 1.9017168283462524, Test_Loss: 2.783729314804077\n",
      "Epoch: 13, Train_Loss: 1.9042730331420898, Test_Loss: 1.955176591873169 *\n",
      "Epoch: 13, Train_Loss: 1.9170504808425903, Test_Loss: 1.9158480167388916 *\n",
      "Epoch: 13, Train_Loss: 1.9127533435821533, Test_Loss: 2.2482588291168213\n",
      "Epoch: 13, Train_Loss: 1.9130886793136597, Test_Loss: 3.2135863304138184\n",
      "Epoch: 13, Train_Loss: 1.931422233581543, Test_Loss: 2.13828444480896 *\n",
      "Epoch: 13, Train_Loss: 1.9010405540466309, Test_Loss: 2.0623068809509277 *\n",
      "Epoch: 13, Train_Loss: 1.8925429582595825, Test_Loss: 1.89788818359375 *\n",
      "Epoch: 13, Train_Loss: 1.922406792640686, Test_Loss: 2.0062668323516846\n",
      "Epoch: 13, Train_Loss: 1.8943357467651367, Test_Loss: 2.238003730773926\n",
      "Epoch: 13, Train_Loss: 1.8897018432617188, Test_Loss: 2.2546887397766113\n",
      "Epoch: 13, Train_Loss: 1.8993940353393555, Test_Loss: 3.2024688720703125\n",
      "Epoch: 13, Train_Loss: 1.9236165285110474, Test_Loss: 2.344576358795166 *\n",
      "Epoch: 13, Train_Loss: 1.9122194051742554, Test_Loss: 1.9037798643112183 *\n",
      "Epoch: 13, Train_Loss: 1.8899402618408203, Test_Loss: 1.8977560997009277 *\n",
      "Epoch: 13, Train_Loss: 1.8908582925796509, Test_Loss: 1.908902645111084\n",
      "Epoch: 13, Train_Loss: 1.9361965656280518, Test_Loss: 1.9328229427337646\n",
      "Epoch: 13, Train_Loss: 1.9418219327926636, Test_Loss: 2.04425311088562\n",
      "Epoch: 13, Train_Loss: 1.9324586391448975, Test_Loss: 2.325511932373047\n",
      "Epoch: 13, Train_Loss: 1.9259988069534302, Test_Loss: 2.218762159347534 *\n",
      "Epoch: 13, Train_Loss: 1.9442291259765625, Test_Loss: 2.0026984214782715 *\n",
      "Epoch: 13, Train_Loss: 1.943153738975525, Test_Loss: 1.9011166095733643 *\n",
      "Epoch: 13, Train_Loss: 1.9162944555282593, Test_Loss: 1.8964753150939941 *\n",
      "Epoch: 13, Train_Loss: 1.9154607057571411, Test_Loss: 1.9509468078613281\n",
      "Epoch: 13, Train_Loss: 2.040728807449341, Test_Loss: 2.44700026512146\n",
      "Epoch: 13, Train_Loss: 1.9147738218307495, Test_Loss: 3.1748156547546387\n",
      "Epoch: 13, Train_Loss: 1.889607310295105, Test_Loss: 2.305602550506592 *\n",
      "Epoch: 13, Train_Loss: 1.8788543939590454, Test_Loss: 1.9491195678710938 *\n",
      "Epoch: 13, Train_Loss: 1.8779901266098022, Test_Loss: 1.894043207168579 *\n",
      "Epoch: 13, Train_Loss: 1.879327654838562, Test_Loss: 1.8920669555664062 *\n",
      "Epoch: 13, Train_Loss: 1.8778481483459473, Test_Loss: 1.8877016305923462 *\n",
      "Epoch: 13, Train_Loss: 1.8774157762527466, Test_Loss: 1.9030312299728394\n",
      "Epoch: 13, Train_Loss: 6.116403102874756, Test_Loss: 1.9064944982528687\n",
      "Epoch: 13, Train_Loss: 2.474992275238037, Test_Loss: 1.9203226566314697\n",
      "Epoch: 13, Train_Loss: 1.877098560333252, Test_Loss: 1.8775309324264526 *\n",
      "Epoch: 13, Train_Loss: 1.886420488357544, Test_Loss: 1.9813265800476074\n",
      "Epoch: 13, Train_Loss: 1.8784003257751465, Test_Loss: 2.1694767475128174\n",
      "Epoch: 13, Train_Loss: 1.876293659210205, Test_Loss: 2.0742852687835693 *\n",
      "Epoch: 13, Train_Loss: 1.8727282285690308, Test_Loss: 2.097224712371826\n",
      "Epoch: 13, Train_Loss: 1.870439887046814, Test_Loss: 1.8863273859024048 *\n",
      "Epoch: 13, Train_Loss: 1.8702653646469116, Test_Loss: 1.8867088556289673\n",
      "Epoch: 13, Train_Loss: 1.8781557083129883, Test_Loss: 1.8887099027633667\n",
      "Epoch: 13, Train_Loss: 1.9035742282867432, Test_Loss: 1.890899658203125\n",
      "Epoch: 13, Train_Loss: 1.9413366317749023, Test_Loss: 1.904700756072998\n",
      "Epoch: 13, Train_Loss: 1.9378106594085693, Test_Loss: 4.927182197570801\n",
      "Epoch: 13, Train_Loss: 1.9365687370300293, Test_Loss: 4.106006145477295 *\n",
      "Epoch: 13, Train_Loss: 1.871362328529358, Test_Loss: 1.8845975399017334 *\n",
      "Epoch: 13, Train_Loss: 1.9301964044570923, Test_Loss: 1.8726927042007446 *\n",
      "Epoch: 13, Train_Loss: 2.093062162399292, Test_Loss: 1.8819199800491333\n",
      "Epoch: 13, Train_Loss: 2.105156421661377, Test_Loss: 1.8835219144821167\n",
      "Epoch: 13, Train_Loss: 2.0998876094818115, Test_Loss: 1.866407871246338 *\n",
      "Epoch: 13, Train_Loss: 1.8799753189086914, Test_Loss: 1.8674920797348022\n",
      "Epoch: 13, Train_Loss: 1.861562728881836, Test_Loss: 1.861474871635437 *\n",
      "Epoch: 13, Train_Loss: 1.8628226518630981, Test_Loss: 1.8654371500015259\n",
      "Epoch: 13, Train_Loss: 1.8677016496658325, Test_Loss: 1.8636220693588257 *\n",
      "Epoch: 13, Train_Loss: 1.869755506515503, Test_Loss: 1.8591265678405762 *\n",
      "Epoch: 13, Train_Loss: 1.8724522590637207, Test_Loss: 1.8711305856704712\n",
      "Epoch: 13, Train_Loss: 1.8655511140823364, Test_Loss: 1.8831137418746948\n",
      "Epoch: 13, Train_Loss: 1.8581809997558594, Test_Loss: 1.8783504962921143 *\n",
      "Epoch: 13, Train_Loss: 1.859782338142395, Test_Loss: 1.8641831874847412 *\n",
      "Epoch: 13, Train_Loss: 1.865836262702942, Test_Loss: 1.8554420471191406 *\n",
      "Epoch: 13, Train_Loss: 1.9933408498764038, Test_Loss: 1.8584407567977905\n",
      "Epoch: 13, Train_Loss: 2.046374797821045, Test_Loss: 1.8587217330932617\n",
      "Epoch: 13, Train_Loss: 2.046384334564209, Test_Loss: 1.8558825254440308 *\n",
      "Epoch: 13, Train_Loss: 1.9147508144378662, Test_Loss: 1.8579120635986328\n",
      "Epoch: 13, Train_Loss: 2.0125210285186768, Test_Loss: 1.8538488149642944 *\n",
      "Epoch: 13, Train_Loss: 1.992980718612671, Test_Loss: 1.8597615957260132\n",
      "Epoch: 13, Train_Loss: 1.8726476430892944, Test_Loss: 1.8663150072097778\n",
      "Epoch: 13, Train_Loss: 2.03147029876709, Test_Loss: 1.8552159070968628 *\n",
      "Epoch: 13, Train_Loss: 1.9782485961914062, Test_Loss: 1.855346918106079\n",
      "Epoch: 13, Train_Loss: 2.094909191131592, Test_Loss: 1.8502706289291382 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 1.8627238273620605, Test_Loss: 1.8521955013275146\n",
      "Epoch: 13, Train_Loss: 3.0838515758514404, Test_Loss: 1.8511937856674194 *\n",
      "Epoch: 13, Train_Loss: 3.794130802154541, Test_Loss: 1.852229356765747\n",
      "Epoch: 13, Train_Loss: 1.8871521949768066, Test_Loss: 1.8858013153076172\n",
      "Epoch: 13, Train_Loss: 1.9095211029052734, Test_Loss: 1.8797498941421509 *\n",
      "Epoch: 13, Train_Loss: 1.922020673751831, Test_Loss: 6.932806491851807\n",
      "Epoch: 13, Train_Loss: 1.9047311544418335, Test_Loss: 2.1593897342681885 *\n",
      "Epoch: 13, Train_Loss: 1.8448784351348877, Test_Loss: 1.8444886207580566 *\n",
      "Epoch: 13, Train_Loss: 1.8486838340759277, Test_Loss: 1.86554753780365\n",
      "Epoch: 13, Train_Loss: 1.9577689170837402, Test_Loss: 1.907138466835022\n",
      "Epoch: 13, Train_Loss: 1.9277037382125854, Test_Loss: 1.9192942380905151\n",
      "Epoch: 13, Train_Loss: 1.9173643589019775, Test_Loss: 1.855051875114441 *\n",
      "Epoch: 13, Train_Loss: 1.9069745540618896, Test_Loss: 1.9241327047348022\n",
      "Epoch: 13, Train_Loss: 1.8933271169662476, Test_Loss: 1.901533842086792 *\n",
      "Epoch: 13, Train_Loss: 1.861395239830017, Test_Loss: 1.8423627614974976 *\n",
      "Epoch: 13, Train_Loss: 1.8602770566940308, Test_Loss: 1.878928542137146\n",
      "Epoch: 13, Train_Loss: 1.8628240823745728, Test_Loss: 1.862247109413147 *\n",
      "Epoch: 13, Train_Loss: 1.8707047700881958, Test_Loss: 1.8482073545455933 *\n",
      "Epoch: 13, Train_Loss: 1.852311372756958, Test_Loss: 1.8563973903656006\n",
      "Epoch: 13, Train_Loss: 1.8409724235534668, Test_Loss: 2.014573335647583\n",
      "Epoch: 13, Train_Loss: 1.8665573596954346, Test_Loss: 1.8739820718765259 *\n",
      "Epoch: 13, Train_Loss: 1.877509593963623, Test_Loss: 1.9295660257339478\n",
      "Epoch: 13, Train_Loss: 1.8519387245178223, Test_Loss: 1.885563611984253 *\n",
      "Epoch: 13, Train_Loss: 1.834208369255066, Test_Loss: 1.8916617631912231\n",
      "Epoch: 13, Train_Loss: 1.8377549648284912, Test_Loss: 1.8631047010421753 *\n",
      "Epoch: 13, Train_Loss: 1.8304731845855713, Test_Loss: 1.854594349861145 *\n",
      "Epoch: 13, Train_Loss: 1.8305141925811768, Test_Loss: 1.858950138092041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 1.836880087852478, Test_Loss: 1.8547189235687256 *\n",
      "Epoch: 13, Train_Loss: 1.8341668844223022, Test_Loss: 1.8608556985855103\n",
      "Epoch: 13, Train_Loss: 1.835016131401062, Test_Loss: 1.8495166301727295 *\n",
      "Epoch: 13, Train_Loss: 1.8288086652755737, Test_Loss: 1.8516918420791626\n",
      "Epoch: 13, Train_Loss: 1.8322724103927612, Test_Loss: 1.8714866638183594\n",
      "Epoch: 13, Train_Loss: 1.8335524797439575, Test_Loss: 1.8696627616882324 *\n",
      "Epoch: 13, Train_Loss: 1.8419302701950073, Test_Loss: 1.8354514837265015 *\n",
      "Epoch: 13, Train_Loss: 1.8563085794448853, Test_Loss: 1.8408666849136353\n",
      "Epoch: 13, Train_Loss: 1.8425657749176025, Test_Loss: 1.8943207263946533\n",
      "Epoch: 13, Train_Loss: 1.860133171081543, Test_Loss: 1.8476594686508179 *\n",
      "Epoch: 13, Train_Loss: 1.8373507261276245, Test_Loss: 2.143050193786621\n",
      "Epoch: 13, Train_Loss: 1.8385471105575562, Test_Loss: 1.8324087858200073 *\n",
      "Epoch: 13, Train_Loss: 1.8260830640792847, Test_Loss: 1.8889070749282837\n",
      "Epoch: 13, Train_Loss: 1.8282220363616943, Test_Loss: 1.9166240692138672\n",
      "Epoch: 13, Train_Loss: 1.8472541570663452, Test_Loss: 2.2162575721740723\n",
      "Epoch: 13, Train_Loss: 1.8349943161010742, Test_Loss: 1.879255771636963 *\n",
      "Epoch: 13, Train_Loss: 1.8235334157943726, Test_Loss: 1.935590386390686\n",
      "Epoch: 13, Train_Loss: 1.820678949356079, Test_Loss: 2.062061071395874\n",
      "Epoch: 13, Train_Loss: 1.827309250831604, Test_Loss: 2.005131244659424 *\n",
      "Epoch: 13, Train_Loss: 1.8917495012283325, Test_Loss: 1.8559709787368774 *\n",
      "Epoch: 13, Train_Loss: 1.8543177843093872, Test_Loss: 1.8352108001708984 *\n",
      "Epoch: 13, Train_Loss: 1.8629473447799683, Test_Loss: 1.8317753076553345 *\n",
      "Epoch: 13, Train_Loss: 1.8156769275665283, Test_Loss: 1.8684346675872803\n",
      "Epoch: 13, Train_Loss: 1.8580224514007568, Test_Loss: 2.03326678276062\n",
      "Epoch: 13, Train_Loss: 1.8457196950912476, Test_Loss: 2.5228466987609863\n",
      "Epoch: 13, Train_Loss: 1.8159557580947876, Test_Loss: 2.1104438304901123 *\n",
      "Epoch: 13, Train_Loss: 1.830568790435791, Test_Loss: 2.7047057151794434\n",
      "Epoch: 13, Train_Loss: 1.8387664556503296, Test_Loss: 2.1431479454040527 *\n",
      "Epoch: 13, Train_Loss: 1.9068163633346558, Test_Loss: 2.3146309852600098\n",
      "Epoch: 13, Train_Loss: 1.8978506326675415, Test_Loss: 2.1029536724090576 *\n",
      "Epoch: 13, Train_Loss: 1.8672850131988525, Test_Loss: 1.8228716850280762 *\n",
      "Epoch: 13, Train_Loss: 1.8361237049102783, Test_Loss: 1.8282482624053955\n",
      "Epoch: 13, Train_Loss: 1.8134472370147705, Test_Loss: 1.8734211921691895\n",
      "Epoch: 13, Train_Loss: 1.826918363571167, Test_Loss: 1.9638416767120361\n",
      "Epoch: 13, Train_Loss: 1.8064465522766113, Test_Loss: 2.7204439640045166\n",
      "Epoch: 13, Train_Loss: 1.8122204542160034, Test_Loss: 1.9913606643676758 *\n",
      "Epoch: 13, Train_Loss: 1.8265864849090576, Test_Loss: 3.908865451812744\n",
      "Epoch: 13, Train_Loss: 1.8262665271759033, Test_Loss: 2.3730549812316895 *\n",
      "Epoch: 13, Train_Loss: 1.905720591545105, Test_Loss: 2.878056526184082\n",
      "Epoch: 13, Train_Loss: 1.8084805011749268, Test_Loss: 1.906491756439209 *\n",
      "Epoch: 13, Train_Loss: 1.8744086027145386, Test_Loss: 1.8105106353759766 *\n",
      "Epoch: 13, Train_Loss: 1.8106621503829956, Test_Loss: 2.0029125213623047\n",
      "Epoch: 13, Train_Loss: 1.8429943323135376, Test_Loss: 3.150897741317749\n",
      "Epoch: 13, Train_Loss: 1.8284529447555542, Test_Loss: 2.4483492374420166 *\n",
      "Epoch: 13, Train_Loss: 2.0961265563964844, Test_Loss: 1.9048091173171997 *\n",
      "Epoch: 13, Train_Loss: 1.8240411281585693, Test_Loss: 1.8200284242630005 *\n",
      "Epoch: 13, Train_Loss: 1.83711576461792, Test_Loss: 1.883592963218689\n",
      "Epoch: 13, Train_Loss: 1.8013224601745605, Test_Loss: 2.206704616546631\n",
      "Epoch: 13, Train_Loss: 1.79848051071167, Test_Loss: 2.018249034881592 *\n",
      "Epoch: 13, Train_Loss: 1.798493504524231, Test_Loss: 2.9858999252319336\n",
      "Epoch: 13, Train_Loss: 1.798302173614502, Test_Loss: 2.4343881607055664 *\n",
      "Epoch: 13, Train_Loss: 1.805240273475647, Test_Loss: 1.8368782997131348 *\n",
      "Epoch: 13, Train_Loss: 1.8083746433258057, Test_Loss: 1.8060659170150757 *\n",
      "Epoch: 13, Train_Loss: 1.813655138015747, Test_Loss: 1.8109036684036255\n",
      "Epoch: 13, Train_Loss: 1.8061103820800781, Test_Loss: 1.8183672428131104\n",
      "Epoch: 13, Train_Loss: 1.804248332977295, Test_Loss: 1.8638496398925781\n",
      "Epoch: 13, Train_Loss: 1.8125050067901611, Test_Loss: 2.3217177391052246\n",
      "Epoch: 13, Train_Loss: 1.7965151071548462, Test_Loss: 2.267167568206787 *\n",
      "Epoch: 13, Train_Loss: 1.791364312171936, Test_Loss: 1.9486171007156372 *\n",
      "Epoch: 13, Train_Loss: 1.8096566200256348, Test_Loss: 1.8288098573684692 *\n",
      "Epoch: 13, Train_Loss: 1.8092265129089355, Test_Loss: 1.8108667135238647 *\n",
      "Epoch: 13, Train_Loss: 1.820207118988037, Test_Loss: 1.8304307460784912\n",
      "Epoch: 13, Train_Loss: 1.7883743047714233, Test_Loss: 2.1089487075805664\n",
      "Epoch: 13, Train_Loss: 1.8266777992248535, Test_Loss: 3.129549026489258\n",
      "Epoch: 13, Train_Loss: 1.8508421182632446, Test_Loss: 2.4564311504364014 *\n",
      "Epoch: 13, Train_Loss: 1.8347095251083374, Test_Loss: 1.8792771100997925 *\n",
      "Epoch: 13, Train_Loss: 1.787382960319519, Test_Loss: 1.8197565078735352 *\n",
      "Epoch: 13, Train_Loss: 1.8226503133773804, Test_Loss: 1.791231632232666 *\n",
      "Epoch: 13, Train_Loss: 1.785902738571167, Test_Loss: 1.7875362634658813 *\n",
      "Epoch: 13, Train_Loss: 1.800718903541565, Test_Loss: 1.794535517692566\n",
      "Epoch: 13, Train_Loss: 1.787662148475647, Test_Loss: 1.8051023483276367\n",
      "Epoch: 13, Train_Loss: 1.8108210563659668, Test_Loss: 1.8385224342346191\n",
      "Epoch: 13, Train_Loss: 1.8618782758712769, Test_Loss: 1.7856078147888184 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 4.8724188804626465, Test_Loss: 1.8356190919876099\n",
      "Epoch: 13, Train_Loss: 4.052536487579346, Test_Loss: 1.9012022018432617\n",
      "Epoch: 13, Train_Loss: 1.805353045463562, Test_Loss: 2.176940679550171\n",
      "Epoch: 13, Train_Loss: 1.784280776977539, Test_Loss: 2.0069079399108887 *\n",
      "Epoch: 13, Train_Loss: 1.9470493793487549, Test_Loss: 1.8010475635528564 *\n",
      "Epoch: 13, Train_Loss: 1.9064481258392334, Test_Loss: 1.7884238958358765 *\n",
      "Epoch: 13, Train_Loss: 1.7919305562973022, Test_Loss: 1.789047360420227\n",
      "Epoch: 13, Train_Loss: 1.7791098356246948, Test_Loss: 1.7848786115646362 *\n",
      "Epoch: 13, Train_Loss: 1.8273274898529053, Test_Loss: 1.7884528636932373\n",
      "Epoch: 13, Train_Loss: 1.8033828735351562, Test_Loss: 3.2460708618164062\n",
      "Epoch: 13, Train_Loss: 1.791501522064209, Test_Loss: 5.68698787689209\n",
      "Epoch: 13, Train_Loss: 1.912577509880066, Test_Loss: 1.7876689434051514 *\n",
      "Epoch: 13, Train_Loss: 3.0627388954162598, Test_Loss: 1.7760779857635498 *\n",
      "Epoch: 13, Train_Loss: 3.167853355407715, Test_Loss: 1.7752662897109985 *\n",
      "Epoch: 13, Train_Loss: 1.8995897769927979, Test_Loss: 1.7818641662597656\n",
      "Epoch: 13, Train_Loss: 1.8311883211135864, Test_Loss: 1.7769806385040283 *\n",
      "Epoch: 13, Train_Loss: 3.6750669479370117, Test_Loss: 1.7759954929351807 *\n",
      "Epoch: 13, Train_Loss: 3.2317843437194824, Test_Loss: 1.7813351154327393\n",
      "Epoch: 13, Train_Loss: 1.8206757307052612, Test_Loss: 1.774990200996399 *\n",
      "Epoch: 13, Train_Loss: 1.8042737245559692, Test_Loss: 1.779020071029663\n",
      "Epoch: 13, Train_Loss: 2.1244471073150635, Test_Loss: 1.7882955074310303\n",
      "Epoch: 13, Train_Loss: 3.465804100036621, Test_Loss: 1.7900242805480957\n",
      "Epoch: 13, Train_Loss: 2.92380428314209, Test_Loss: 1.781434178352356 *\n",
      "Epoch: 13, Train_Loss: 1.7808293104171753, Test_Loss: 1.7850548028945923\n",
      "Epoch: 13, Train_Loss: 1.7933249473571777, Test_Loss: 1.7920185327529907\n",
      "Epoch: 13, Train_Loss: 1.9135262966156006, Test_Loss: 1.7679176330566406 *\n",
      "Epoch: 13, Train_Loss: 2.4328761100769043, Test_Loss: 1.7773045301437378\n",
      "Epoch: 13, Train_Loss: 1.7981185913085938, Test_Loss: 1.7699366807937622 *\n",
      "Epoch: 13, Train_Loss: 1.8387725353240967, Test_Loss: 1.773147702217102\n",
      "Epoch: 13, Train_Loss: 1.8578870296478271, Test_Loss: 1.768228530883789 *\n",
      "Epoch: 13, Train_Loss: 1.883758544921875, Test_Loss: 1.780973196029663\n",
      "Epoch: 13, Train_Loss: 1.8735569715499878, Test_Loss: 1.7896885871887207\n",
      "Epoch: 13, Train_Loss: 2.076063394546509, Test_Loss: 1.7961410284042358\n",
      "Epoch: 13, Train_Loss: 1.9946115016937256, Test_Loss: 1.7789101600646973 *\n",
      "Epoch: 13, Train_Loss: 1.8224036693572998, Test_Loss: 1.7809959650039673\n",
      "Epoch: 13, Train_Loss: 1.9207037687301636, Test_Loss: 1.768066644668579 *\n",
      "Epoch: 13, Train_Loss: 1.9310097694396973, Test_Loss: 1.7762606143951416\n",
      "Epoch: 13, Train_Loss: 2.1225483417510986, Test_Loss: 1.7859114408493042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 2.0018951892852783, Test_Loss: 1.767443299293518 *\n",
      "Epoch: 13, Train_Loss: 1.7963300943374634, Test_Loss: 1.8566341400146484\n",
      "Epoch: 13, Train_Loss: 1.8746166229248047, Test_Loss: 1.7937132120132446 *\n",
      "Epoch: 13, Train_Loss: 1.8718777894973755, Test_Loss: 5.741145133972168\n",
      "Epoch: 13, Train_Loss: 1.7723798751831055, Test_Loss: 3.541473388671875 *\n",
      "Epoch: 13, Train_Loss: 1.7680859565734863, Test_Loss: 1.7608078718185425 *\n",
      "Epoch: 13, Train_Loss: 1.7623212337493896, Test_Loss: 1.7638092041015625\n",
      "Epoch: 13, Train_Loss: 1.7603949308395386, Test_Loss: 1.7800568342208862\n",
      "Epoch: 13, Train_Loss: 1.7654329538345337, Test_Loss: 1.7776230573654175 *\n",
      "Epoch: 13, Train_Loss: 1.7665444612503052, Test_Loss: 1.7702867984771729 *\n",
      "Epoch: 13, Train_Loss: 1.8246313333511353, Test_Loss: 1.8304762840270996\n",
      "Epoch: 13, Train_Loss: 1.8041223287582397, Test_Loss: 1.83707857131958\n",
      "Epoch: 13, Train_Loss: 1.8589171171188354, Test_Loss: 1.7557283639907837 *\n",
      "Epoch: 13, Train_Loss: 1.867918848991394, Test_Loss: 1.7808409929275513\n",
      "Epoch: 13, Train_Loss: 2.1181130409240723, Test_Loss: 1.7735521793365479 *\n",
      "Epoch: 13, Train_Loss: 1.7678909301757812, Test_Loss: 1.7652767896652222 *\n",
      "Epoch: 13, Train_Loss: 1.791316032409668, Test_Loss: 1.7541085481643677 *\n",
      "Epoch: 13, Train_Loss: 2.0929183959960938, Test_Loss: 1.8054227828979492\n",
      "Epoch: 13, Train_Loss: 2.246854782104492, Test_Loss: 1.7933393716812134 *\n",
      "Epoch: 13, Train_Loss: 1.8755977153778076, Test_Loss: 1.834938406944275\n",
      "Epoch: 13, Train_Loss: 1.7712013721466064, Test_Loss: 1.8101495504379272 *\n",
      "Epoch: 13, Train_Loss: 2.053959846496582, Test_Loss: 1.802534580230713 *\n",
      "Epoch: 13, Train_Loss: 2.308088541030884, Test_Loss: 1.7574598789215088 *\n",
      "Epoch: 13, Train_Loss: 2.0239932537078857, Test_Loss: 1.7513104677200317 *\n",
      "Epoch: 13, Train_Loss: 1.7652029991149902, Test_Loss: 1.7492856979370117 *\n",
      "Epoch: 13, Train_Loss: 1.7720305919647217, Test_Loss: 1.7521326541900635\n",
      "Epoch: 13, Train_Loss: 1.9189761877059937, Test_Loss: 1.7531836032867432\n",
      "Epoch: 13, Train_Loss: 3.2903921604156494, Test_Loss: 1.75747549533844\n",
      "Epoch: 13, Train_Loss: 2.260773181915283, Test_Loss: 1.7443138360977173 *\n",
      "Epoch: 13, Train_Loss: 1.7562263011932373, Test_Loss: 1.7510533332824707\n",
      "Epoch: 13, Train_Loss: 1.7555675506591797, Test_Loss: 1.747328758239746 *\n",
      "Epoch: 13, Train_Loss: 1.7466164827346802, Test_Loss: 1.747910499572754\n",
      "Epoch: 13, Train_Loss: 2.044529914855957, Test_Loss: 1.7769778966903687\n",
      "Epoch: 13, Train_Loss: 1.918703317642212, Test_Loss: 1.755405068397522 *\n",
      "Epoch: 13, Train_Loss: 1.7654927968978882, Test_Loss: 1.7641351222991943\n",
      "Epoch: 13, Train_Loss: 1.7621921300888062, Test_Loss: 2.1116487979888916\n",
      "Epoch: 13, Train_Loss: 1.7570644617080688, Test_Loss: 1.7686609029769897 *\n",
      "Epoch: 13, Train_Loss: 12.000099182128906, Test_Loss: 1.7606264352798462 *\n",
      "Epoch: 13, Train_Loss: 9.000354766845703, Test_Loss: 1.809075117111206\n",
      "Epoch: 13, Train_Loss: 2.497603416442871, Test_Loss: 1.987828254699707\n",
      "Epoch: 13, Train_Loss: 4.611474990844727, Test_Loss: 1.7945536375045776 *\n",
      "Epoch: 13, Train_Loss: 1.9731063842773438, Test_Loss: 1.8089640140533447\n",
      "Epoch: 13, Train_Loss: 1.8063414096832275, Test_Loss: 1.8988951444625854\n",
      "Epoch: 13, Train_Loss: 2.0946269035339355, Test_Loss: 1.9481145143508911\n",
      "Epoch: 13, Train_Loss: 11.670049667358398, Test_Loss: 1.7779988050460815 *\n",
      "Epoch: 13, Train_Loss: 3.2419981956481934, Test_Loss: 1.819500207901001\n",
      "Epoch: 13, Train_Loss: 1.7557947635650635, Test_Loss: 1.7361154556274414 *\n",
      "Epoch: 13, Train_Loss: 5.086365222930908, Test_Loss: 1.7474066019058228\n",
      "Epoch: 13, Train_Loss: 4.4522705078125, Test_Loss: 1.8242193460464478\n",
      "Epoch: 13, Train_Loss: 1.781596064567566, Test_Loss: 2.5640223026275635\n",
      "Epoch: 13, Train_Loss: 1.7400672435760498, Test_Loss: 1.9015682935714722 *\n",
      "Epoch: 13, Train_Loss: 1.7344365119934082, Test_Loss: 2.2546029090881348\n",
      "Epoch: 13, Train_Loss: 1.7351081371307373, Test_Loss: 2.1069226264953613 *\n",
      "Epoch: 13, Train_Loss: 1.7474586963653564, Test_Loss: 2.201300859451294\n",
      "Epoch: 13, Train_Loss: 1.7392569780349731, Test_Loss: 2.241480827331543\n",
      "Epoch: 13, Train_Loss: 1.730026125907898, Test_Loss: 1.8666160106658936 *\n",
      "Epoch: 13, Train_Loss: 1.7286038398742676, Test_Loss: 1.7565058469772339 *\n",
      "Epoch: 13, Train_Loss: 1.7466806173324585, Test_Loss: 1.7696950435638428\n",
      "Epoch: 13, Train_Loss: 1.7592757940292358, Test_Loss: 1.793228030204773\n",
      "Epoch: 13, Train_Loss: 1.7371526956558228, Test_Loss: 2.5610780715942383\n",
      "Epoch: 13, Train_Loss: 1.7613173723220825, Test_Loss: 2.224350690841675 *\n",
      "Epoch: 13, Train_Loss: 1.814424991607666, Test_Loss: 3.0505263805389404\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 1.8001372814178467, Test_Loss: 2.3741860389709473 *\n",
      "Epoch: 13, Train_Loss: 1.7337418794631958, Test_Loss: 2.9420857429504395\n",
      "Epoch: 13, Train_Loss: 1.7330451011657715, Test_Loss: 2.0607540607452393 *\n",
      "Epoch: 13, Train_Loss: 1.7254256010055542, Test_Loss: 1.7646020650863647 *\n",
      "Epoch: 13, Train_Loss: 1.7206213474273682, Test_Loss: 1.785561442375183\n",
      "Epoch: 13, Train_Loss: 1.7204254865646362, Test_Loss: 2.6232247352600098\n",
      "Epoch: 13, Train_Loss: 1.7169694900512695, Test_Loss: 2.5391738414764404 *\n",
      "Epoch: 13, Train_Loss: 1.7134840488433838, Test_Loss: 1.815876841545105 *\n",
      "Epoch: 13, Train_Loss: 1.7149674892425537, Test_Loss: 1.7998507022857666 *\n",
      "Epoch: 13, Train_Loss: 1.7147077322006226, Test_Loss: 1.7529808282852173 *\n",
      "Epoch: 13, Train_Loss: 1.714686393737793, Test_Loss: 2.067042827606201\n",
      "Epoch: 13, Train_Loss: 1.7137917280197144, Test_Loss: 1.8837729692459106 *\n",
      "Epoch: 13, Train_Loss: 1.7209347486495972, Test_Loss: 2.7302660942077637\n",
      "Epoch: 13, Train_Loss: 1.7366502285003662, Test_Loss: 2.519366979598999 *\n",
      "Epoch: 13, Train_Loss: 1.7613576650619507, Test_Loss: 1.9874980449676514 *\n",
      "Epoch: 13, Train_Loss: 1.7198066711425781, Test_Loss: 1.7385631799697876 *\n",
      "Epoch: 13, Train_Loss: 1.7139919996261597, Test_Loss: 1.751157522201538\n",
      "Epoch: 13, Train_Loss: 10.159978866577148, Test_Loss: 1.7535111904144287\n",
      "Epoch: 13, Train_Loss: 2.187079906463623, Test_Loss: 1.839016318321228\n",
      "Epoch: 13, Train_Loss: 1.7462328672409058, Test_Loss: 1.995131254196167\n",
      "Epoch: 13, Train_Loss: 1.805802345275879, Test_Loss: 2.0232629776000977\n",
      "Epoch: 13, Train_Loss: 1.784116506576538, Test_Loss: 1.7879506349563599 *\n",
      "Epoch: 13, Train_Loss: 1.7370340824127197, Test_Loss: 1.7304491996765137 *\n",
      "Epoch: 13, Train_Loss: 1.749380350112915, Test_Loss: 1.7382416725158691\n",
      "Epoch: 13, Train_Loss: 1.7932403087615967, Test_Loss: 1.7808715105056763\n",
      "Epoch: 13, Train_Loss: 1.9598342180252075, Test_Loss: 1.880306601524353\n",
      "Epoch: 13, Train_Loss: 1.9519860744476318, Test_Loss: 2.9710092544555664\n",
      "Epoch: 13, Train_Loss: 1.8574128150939941, Test_Loss: 2.8715033531188965 *\n",
      "Epoch: 13, Train_Loss: 1.717434048652649, Test_Loss: 1.773620367050171 *\n",
      "Epoch: 13, Train_Loss: 1.8318296670913696, Test_Loss: 1.7545088529586792 *\n",
      "Epoch: 13, Train_Loss: 1.8247544765472412, Test_Loss: 1.7208999395370483 *\n",
      "Epoch: 13, Train_Loss: 1.8884501457214355, Test_Loss: 1.739809274673462\n",
      "Epoch: 13, Train_Loss: 1.8163337707519531, Test_Loss: 1.7313196659088135 *\n",
      "Epoch: 13, Train_Loss: 1.7660053968429565, Test_Loss: 1.7263528108596802 *\n",
      "Epoch: 13, Train_Loss: 1.7368102073669434, Test_Loss: 1.7844558954238892\n",
      "Epoch: 13, Train_Loss: 1.7198983430862427, Test_Loss: 1.7080971002578735 *\n",
      "Epoch: 13, Train_Loss: 1.8130091428756714, Test_Loss: 1.7204656600952148\n",
      "Epoch: 13, Train_Loss: 1.7319213151931763, Test_Loss: 1.7953109741210938\n",
      "Epoch: 13, Train_Loss: 1.707066297531128, Test_Loss: 2.1015512943267822\n",
      "Epoch: 13, Train_Loss: 1.7048022747039795, Test_Loss: 1.9320822954177856 *\n",
      "Epoch: 13, Train_Loss: 1.7024592161178589, Test_Loss: 1.709814190864563 *\n",
      "Epoch: 13, Train_Loss: 1.7695006132125854, Test_Loss: 1.7008136510849 *\n",
      "Epoch: 13, Train_Loss: 7.168710708618164, Test_Loss: 1.6966043710708618 *\n",
      "Epoch: 13, Train_Loss: 1.7122989892959595, Test_Loss: 1.6951347589492798 *\n",
      "Epoch: 13, Train_Loss: 1.7125121355056763, Test_Loss: 1.692679524421692 *\n",
      "Epoch: 13, Train_Loss: 1.7516788244247437, Test_Loss: 2.0835368633270264\n",
      "Epoch: 13, Train_Loss: 1.7208614349365234, Test_Loss: 7.012630939483643\n",
      "Epoch: 13, Train_Loss: 1.7046449184417725, Test_Loss: 1.791198968887329 *\n",
      "Epoch: 13, Train_Loss: 1.7034600973129272, Test_Loss: 1.715797781944275 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 1.7088364362716675, Test_Loss: 1.7063612937927246 *\n",
      "Epoch: 13, Train_Loss: 1.7114489078521729, Test_Loss: 1.6986351013183594 *\n",
      "Epoch: 13, Train_Loss: 1.7076126337051392, Test_Loss: 1.7087650299072266\n",
      "Epoch: 13, Train_Loss: 1.7300951480865479, Test_Loss: 1.7034016847610474 *\n",
      "Epoch: 13, Train_Loss: 1.6935292482376099, Test_Loss: 1.7381744384765625\n",
      "Epoch: 13, Train_Loss: 1.6923733949661255, Test_Loss: 1.7136285305023193 *\n",
      "Epoch: 13, Train_Loss: 1.714914083480835, Test_Loss: 1.735719084739685\n",
      "Epoch: 13, Train_Loss: 1.6883931159973145, Test_Loss: 1.720910668373108 *\n",
      "Epoch: 13, Train_Loss: 1.6881705522537231, Test_Loss: 1.7393441200256348\n",
      "Epoch: 13, Train_Loss: 1.7054611444473267, Test_Loss: 1.7010297775268555 *\n",
      "Epoch: 13, Train_Loss: 1.7256624698638916, Test_Loss: 1.6968002319335938 *\n",
      "Epoch: 13, Train_Loss: 1.7014657258987427, Test_Loss: 1.7170530557632446\n",
      "Epoch: 13, Train_Loss: 1.6882879734039307, Test_Loss: 1.7090613842010498 *\n",
      "Epoch: 13, Train_Loss: 1.6902120113372803, Test_Loss: 1.6983628273010254 *\n",
      "Epoch: 13, Train_Loss: 1.74122953414917, Test_Loss: 1.699798583984375\n",
      "Epoch: 13, Train_Loss: 1.749863862991333, Test_Loss: 1.703984260559082\n",
      "Epoch: 13, Train_Loss: 1.7476024627685547, Test_Loss: 1.6985502243041992 *\n",
      "Epoch: 13, Train_Loss: 1.7140425443649292, Test_Loss: 1.7084753513336182\n",
      "Epoch: 13, Train_Loss: 1.7313404083251953, Test_Loss: 1.7034249305725098 *\n",
      "Epoch: 13, Train_Loss: 1.7318153381347656, Test_Loss: 1.705100178718567\n",
      "Epoch: 13, Train_Loss: 1.7213472127914429, Test_Loss: 1.7059556245803833\n",
      "Epoch: 13, Train_Loss: 1.7078320980072021, Test_Loss: 1.696131944656372 *\n",
      "Epoch: 13, Train_Loss: 1.831822395324707, Test_Loss: 1.6951982975006104 *\n",
      "Epoch: 13, Train_Loss: 1.7028006315231323, Test_Loss: 1.6899031400680542 *\n",
      "Epoch: 13, Train_Loss: 1.6876362562179565, Test_Loss: 1.6841092109680176 *\n",
      "Epoch: 13, Train_Loss: 1.6752727031707764, Test_Loss: 1.6870276927947998\n",
      "Epoch: 13, Train_Loss: 1.6737489700317383, Test_Loss: 1.7093828916549683\n",
      "Epoch: 13, Train_Loss: 1.676952838897705, Test_Loss: 1.7396652698516846\n",
      "Epoch: 13, Train_Loss: 1.67278254032135, Test_Loss: 4.261868476867676\n",
      "Epoch: 13, Train_Loss: 1.6887942552566528, Test_Loss: 4.730185508728027\n",
      "Epoch: 13, Train_Loss: 6.179080963134766, Test_Loss: 1.6767487525939941 *\n",
      "Epoch: 13, Train_Loss: 1.8626757860183716, Test_Loss: 1.6729447841644287 *\n",
      "Epoch: 13, Train_Loss: 1.6757817268371582, Test_Loss: 1.7333769798278809\n",
      "Epoch: 13, Train_Loss: 1.6793361902236938, Test_Loss: 1.7190098762512207 *\n",
      "Epoch: 13, Train_Loss: 1.6729761362075806, Test_Loss: 1.72043776512146\n",
      "Epoch: 13, Train_Loss: 1.6687365770339966, Test_Loss: 1.7106108665466309 *\n",
      "Epoch: 13, Train_Loss: 1.6724821329116821, Test_Loss: 1.7629650831222534\n",
      "Epoch: 13, Train_Loss: 1.6668782234191895, Test_Loss: 1.673607349395752 *\n",
      "Epoch: 13, Train_Loss: 1.6678825616836548, Test_Loss: 1.6928893327713013\n",
      "Epoch: 13, Train_Loss: 1.6686079502105713, Test_Loss: 1.688873052597046 *\n",
      "Epoch: 13, Train_Loss: 1.7158188819885254, Test_Loss: 1.6875810623168945 *\n",
      "Epoch: 13, Train_Loss: 1.7205225229263306, Test_Loss: 1.6772023439407349 *\n",
      "Epoch: 13, Train_Loss: 1.7288016080856323, Test_Loss: 1.7645020484924316\n",
      "Epoch: 13, Train_Loss: 1.7209680080413818, Test_Loss: 1.7595582008361816 *\n",
      "Epoch: 13, Train_Loss: 1.6683167219161987, Test_Loss: 1.7335374355316162 *\n",
      "Epoch: 13, Train_Loss: 1.7653913497924805, Test_Loss: 1.7492389678955078\n",
      "Epoch: 13, Train_Loss: 1.8957480192184448, Test_Loss: 1.6833897829055786 *\n",
      "Epoch: 13, Train_Loss: 1.887923240661621, Test_Loss: 1.7100893259048462\n",
      "Epoch: 13, Train_Loss: 1.8804233074188232, Test_Loss: 1.6791118383407593 *\n",
      "Epoch: 13, Train_Loss: 1.6602336168289185, Test_Loss: 1.6773394346237183 *\n",
      "Epoch: 13, Train_Loss: 1.6601028442382812, Test_Loss: 1.679883599281311\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 1.658301591873169, Test_Loss: 1.6773028373718262 *\n",
      "Epoch: 13, Train_Loss: 1.6721724271774292, Test_Loss: 1.67086923122406 *\n",
      "Epoch: 13, Train_Loss: 1.6718029975891113, Test_Loss: 1.6738426685333252\n",
      "Epoch: 13, Train_Loss: 1.66201913356781, Test_Loss: 1.6780282258987427\n",
      "Epoch: 13, Train_Loss: 1.6589800119400024, Test_Loss: 1.672622799873352 *\n",
      "Epoch: 13, Train_Loss: 1.6580907106399536, Test_Loss: 1.6719399690628052 *\n",
      "Epoch: 13, Train_Loss: 1.6601265668869019, Test_Loss: 1.66068696975708 *\n",
      "Epoch: 13, Train_Loss: 1.6644865274429321, Test_Loss: 1.6790266036987305\n",
      "Epoch: 13, Train_Loss: 1.823056936264038, Test_Loss: 1.704957127571106\n",
      "Epoch: 13, Train_Loss: 1.8126914501190186, Test_Loss: 1.9657461643218994\n",
      "Epoch: 13, Train_Loss: 1.8329131603240967, Test_Loss: 1.75153386592865 *\n",
      "Epoch: 13, Train_Loss: 1.6956467628479004, Test_Loss: 1.6752052307128906 *\n",
      "Epoch: 13, Train_Loss: 1.8113738298416138, Test_Loss: 1.7199350595474243\n",
      "Epoch: 13, Train_Loss: 1.785697102546692, Test_Loss: 1.9062203168869019\n",
      "Epoch: 13, Train_Loss: 1.7095799446105957, Test_Loss: 1.8413857221603394 *\n",
      "Epoch: 13, Train_Loss: 1.8068244457244873, Test_Loss: 1.678985595703125 *\n",
      "Epoch: 13, Train_Loss: 1.8703882694244385, Test_Loss: 1.887459635734558\n",
      "Epoch: 13, Train_Loss: 1.789915919303894, Test_Loss: 1.9535605907440186\n",
      "Epoch: 13, Train_Loss: 1.661146640777588, Test_Loss: 1.6777034997940063 *\n",
      "Epoch: 13, Train_Loss: 3.6300277709960938, Test_Loss: 1.725211501121521\n",
      "Epoch: 13, Train_Loss: 2.9014782905578613, Test_Loss: 1.6503909826278687 *\n",
      "Epoch: 13, Train_Loss: 1.6783133745193481, Test_Loss: 1.6908791065216064\n",
      "Epoch: 13, Train_Loss: 1.6964447498321533, Test_Loss: 1.6672654151916504 *\n",
      "Epoch: 13, Train_Loss: 1.7061113119125366, Test_Loss: 2.422107696533203\n",
      "Epoch: 13, Train_Loss: 1.69025456905365, Test_Loss: 1.8556889295578003 *\n",
      "Epoch: 13, Train_Loss: 1.6423932313919067, Test_Loss: 2.4063262939453125\n",
      "Epoch: 13, Train_Loss: 1.6656429767608643, Test_Loss: 2.3427538871765137 *\n",
      "Epoch: 13, Train_Loss: 1.7622085809707642, Test_Loss: 1.873381495475769 *\n",
      "Epoch: 13, Train_Loss: 1.7254717350006104, Test_Loss: 2.135969877243042\n",
      "Epoch: 13, Train_Loss: 1.7150967121124268, Test_Loss: 1.7069876194000244 *\n",
      "Epoch: 13, Train_Loss: 1.7095807790756226, Test_Loss: 1.6521440744400024 *\n",
      "Epoch: 13, Train_Loss: 1.6756631135940552, Test_Loss: 1.666954517364502\n",
      "Epoch: 13, Train_Loss: 1.6609212160110474, Test_Loss: 1.8161121606826782\n",
      "Epoch: 13, Train_Loss: 1.6528509855270386, Test_Loss: 1.9889023303985596\n",
      "Epoch: 13, Train_Loss: 1.6696559190750122, Test_Loss: 2.303342819213867\n",
      "Epoch: 13, Train_Loss: 1.6740598678588867, Test_Loss: 2.7761034965515137\n",
      "Epoch: 13, Train_Loss: 1.644464373588562, Test_Loss: 3.067155361175537\n",
      "Epoch: 13, Train_Loss: 1.6369868516921997, Test_Loss: 2.480485439300537 *\n",
      "Epoch: 13, Train_Loss: 1.6700414419174194, Test_Loss: 2.0486581325531006 *\n",
      "Epoch: 13, Train_Loss: 1.6769541501998901, Test_Loss: 1.6405003070831299 *\n",
      "Epoch: 13, Train_Loss: 1.6476365327835083, Test_Loss: 1.6749869585037231\n",
      "Epoch: 13, Train_Loss: 1.6366803646087646, Test_Loss: 2.507674217224121\n",
      "Epoch: 13, Train_Loss: 1.63471519947052, Test_Loss: 2.9749813079833984\n",
      "Epoch: 13, Train_Loss: 1.636478066444397, Test_Loss: 1.6811232566833496 *\n",
      "Epoch: 13, Train_Loss: 1.6327532529830933, Test_Loss: 1.7025213241577148\n",
      "Epoch: 13, Train_Loss: 1.6322733163833618, Test_Loss: 1.6494807004928589 *\n",
      "Epoch: 13, Train_Loss: 1.6313763856887817, Test_Loss: 1.9502317905426025\n",
      "Epoch: 13, Train_Loss: 1.6297239065170288, Test_Loss: 1.894873023033142 *\n",
      "Epoch: 13, Train_Loss: 1.6288121938705444, Test_Loss: 2.293558120727539\n",
      "Epoch: 13, Train_Loss: 1.6294474601745605, Test_Loss: 2.565579414367676\n",
      "Epoch: 13, Train_Loss: 1.6329667568206787, Test_Loss: 1.8881911039352417 *\n",
      "Epoch: 13, Train_Loss: 1.6452665328979492, Test_Loss: 1.6299127340316772 *\n",
      "Epoch: 13, Train_Loss: 1.6404166221618652, Test_Loss: 1.636580467224121\n",
      "Epoch: 13, Train_Loss: 1.642632007598877, Test_Loss: 1.639897108078003\n",
      "Epoch: 14, Train_Loss: 1.6520099639892578, Test_Loss: 1.658722996711731 *\n",
      "Epoch: 14, Train_Loss: 1.6378560066223145, Test_Loss: 1.9872918128967285\n",
      "Epoch: 14, Train_Loss: 1.6302547454833984, Test_Loss: 2.268747568130493\n",
      "Epoch: 14, Train_Loss: 1.6264092922210693, Test_Loss: 1.8825483322143555 *\n",
      "Epoch: 14, Train_Loss: 1.6318327188491821, Test_Loss: 1.7043354511260986 *\n",
      "Epoch: 14, Train_Loss: 1.6540488004684448, Test_Loss: 1.6751677989959717 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 1.6278711557388306, Test_Loss: 1.6294530630111694 *\n",
      "Epoch: 14, Train_Loss: 1.6241074800491333, Test_Loss: 1.6995465755462646\n",
      "Epoch: 14, Train_Loss: 1.6183165311813354, Test_Loss: 2.4790596961975098\n",
      "Epoch: 14, Train_Loss: 1.6426899433135986, Test_Loss: 2.91191029548645\n",
      "Epoch: 14, Train_Loss: 1.6868977546691895, Test_Loss: 1.7110655307769775 *\n",
      "Epoch: 14, Train_Loss: 1.6626989841461182, Test_Loss: 1.7218092679977417\n",
      "Epoch: 14, Train_Loss: 1.648770809173584, Test_Loss: 1.6199337244033813 *\n",
      "Epoch: 14, Train_Loss: 1.6162751913070679, Test_Loss: 1.6227531433105469\n",
      "Epoch: 14, Train_Loss: 1.6743875741958618, Test_Loss: 1.6277589797973633\n",
      "Epoch: 14, Train_Loss: 1.6416953802108765, Test_Loss: 1.6326143741607666\n",
      "Epoch: 14, Train_Loss: 1.61775541305542, Test_Loss: 1.651593804359436\n",
      "Epoch: 14, Train_Loss: 1.6326528787612915, Test_Loss: 1.6437337398529053 *\n",
      "Epoch: 14, Train_Loss: 1.6372517347335815, Test_Loss: 1.6204549074172974 *\n",
      "Epoch: 14, Train_Loss: 1.717100739479065, Test_Loss: 1.7370185852050781\n",
      "Epoch: 14, Train_Loss: 1.6947029829025269, Test_Loss: 2.031198024749756\n",
      "Epoch: 14, Train_Loss: 1.6637256145477295, Test_Loss: 1.719149112701416 *\n",
      "Epoch: 14, Train_Loss: 1.629141926765442, Test_Loss: 1.7626943588256836\n",
      "Epoch: 14, Train_Loss: 1.6191293001174927, Test_Loss: 1.6215318441390991 *\n",
      "Epoch: 14, Train_Loss: 1.6345694065093994, Test_Loss: 1.6218938827514648\n",
      "Epoch: 14, Train_Loss: 1.6128047704696655, Test_Loss: 1.6225464344024658\n",
      "Epoch: 14, Train_Loss: 1.6161019802093506, Test_Loss: 1.6218829154968262 *\n",
      "Epoch: 14, Train_Loss: 1.6281044483184814, Test_Loss: 1.6458576917648315\n",
      "Epoch: 14, Train_Loss: 1.6313371658325195, Test_Loss: 6.68309211730957\n",
      "Epoch: 14, Train_Loss: 1.7190799713134766, Test_Loss: 1.8729509115219116 *\n",
      "Epoch: 14, Train_Loss: 1.6110224723815918, Test_Loss: 1.6173208951950073 *\n",
      "Epoch: 14, Train_Loss: 1.672797441482544, Test_Loss: 1.6108039617538452 *\n",
      "Epoch: 14, Train_Loss: 1.6195600032806396, Test_Loss: 1.6120425462722778\n",
      "Epoch: 14, Train_Loss: 1.645904302597046, Test_Loss: 1.615622639656067\n",
      "Epoch: 14, Train_Loss: 1.665250301361084, Test_Loss: 1.6067051887512207 *\n",
      "Epoch: 14, Train_Loss: 1.8665192127227783, Test_Loss: 1.6110576391220093\n",
      "Epoch: 14, Train_Loss: 1.6190035343170166, Test_Loss: 1.6045044660568237 *\n",
      "Epoch: 14, Train_Loss: 1.6443803310394287, Test_Loss: 1.6044410467147827 *\n",
      "Epoch: 14, Train_Loss: 1.6018972396850586, Test_Loss: 1.6061224937438965\n",
      "Epoch: 14, Train_Loss: 1.6014020442962646, Test_Loss: 1.6106457710266113\n",
      "Epoch: 14, Train_Loss: 1.602731704711914, Test_Loss: 1.6145646572113037\n",
      "Epoch: 14, Train_Loss: 1.5983718633651733, Test_Loss: 1.6285643577575684\n",
      "Epoch: 14, Train_Loss: 1.6143673658370972, Test_Loss: 1.6129242181777954 *\n",
      "Epoch: 14, Train_Loss: 1.6107059717178345, Test_Loss: 1.6015866994857788 *\n",
      "Epoch: 14, Train_Loss: 1.611144781112671, Test_Loss: 1.599733591079712 *\n",
      "Epoch: 14, Train_Loss: 1.6103994846343994, Test_Loss: 1.603975534439087\n",
      "Epoch: 14, Train_Loss: 1.608742117881775, Test_Loss: 1.5980572700500488 *\n",
      "Epoch: 14, Train_Loss: 1.61024808883667, Test_Loss: 1.5964802503585815 *\n",
      "Epoch: 14, Train_Loss: 1.5993574857711792, Test_Loss: 1.5990718603134155\n",
      "Epoch: 14, Train_Loss: 1.5962302684783936, Test_Loss: 1.598721981048584 *\n",
      "Epoch: 14, Train_Loss: 1.6186681985855103, Test_Loss: 1.6010452508926392\n",
      "Epoch: 14, Train_Loss: 1.620047688484192, Test_Loss: 1.6033762693405151\n",
      "Epoch: 14, Train_Loss: 1.6284089088439941, Test_Loss: 1.5969101190567017 *\n",
      "Epoch: 14, Train_Loss: 1.5922675132751465, Test_Loss: 1.5953291654586792 *\n",
      "Epoch: 14, Train_Loss: 1.6444040536880493, Test_Loss: 1.5940724611282349 *\n",
      "Epoch: 14, Train_Loss: 1.6490014791488647, Test_Loss: 1.59213125705719 *\n",
      "Epoch: 14, Train_Loss: 1.6313737630844116, Test_Loss: 1.5948529243469238\n",
      "Epoch: 14, Train_Loss: 1.5932674407958984, Test_Loss: 1.5946382284164429 *\n",
      "Epoch: 14, Train_Loss: 1.640405297279358, Test_Loss: 1.6526124477386475\n",
      "Epoch: 14, Train_Loss: 1.5907024145126343, Test_Loss: 2.650125026702881\n",
      "Epoch: 14, Train_Loss: 1.6061939001083374, Test_Loss: 5.971292495727539\n",
      "Epoch: 14, Train_Loss: 1.5919026136398315, Test_Loss: 1.5939830541610718 *\n",
      "Epoch: 14, Train_Loss: 1.6143711805343628, Test_Loss: 1.589404821395874 *\n",
      "Epoch: 14, Train_Loss: 2.190430164337158, Test_Loss: 1.6345391273498535\n",
      "Epoch: 14, Train_Loss: 5.511791706085205, Test_Loss: 1.6470495462417603\n",
      "Epoch: 14, Train_Loss: 2.5753564834594727, Test_Loss: 1.6501314640045166\n",
      "Epoch: 14, Train_Loss: 1.5993720293045044, Test_Loss: 1.598710060119629 *\n",
      "Epoch: 14, Train_Loss: 1.5902881622314453, Test_Loss: 1.7091264724731445\n",
      "Epoch: 14, Train_Loss: 1.7389343976974487, Test_Loss: 1.6049681901931763 *\n",
      "Epoch: 14, Train_Loss: 1.6637065410614014, Test_Loss: 1.5906075239181519 *\n",
      "Epoch: 14, Train_Loss: 1.5970656871795654, Test_Loss: 1.6187254190444946\n",
      "Epoch: 14, Train_Loss: 1.5808485746383667, Test_Loss: 1.598814845085144 *\n",
      "Epoch: 14, Train_Loss: 1.6353487968444824, Test_Loss: 1.5872637033462524 *\n",
      "Epoch: 14, Train_Loss: 1.5978245735168457, Test_Loss: 1.6460654735565186\n",
      "Epoch: 14, Train_Loss: 1.6031887531280518, Test_Loss: 1.681918740272522\n",
      "Epoch: 14, Train_Loss: 1.8189823627471924, Test_Loss: 1.638933539390564 *\n",
      "Epoch: 14, Train_Loss: 2.931607484817505, Test_Loss: 1.662665843963623\n",
      "Epoch: 14, Train_Loss: 2.7322731018066406, Test_Loss: 1.6095478534698486 *\n",
      "Epoch: 14, Train_Loss: 1.6875065565109253, Test_Loss: 1.63168203830719\n",
      "Epoch: 14, Train_Loss: 1.6826345920562744, Test_Loss: 1.5960279703140259 *\n",
      "Epoch: 14, Train_Loss: 3.8851969242095947, Test_Loss: 1.5914396047592163 *\n",
      "Epoch: 14, Train_Loss: 2.7139031887054443, Test_Loss: 1.5956887006759644\n",
      "Epoch: 14, Train_Loss: 1.624864935874939, Test_Loss: 1.59352445602417 *\n",
      "Epoch: 14, Train_Loss: 1.609379529953003, Test_Loss: 1.5938305854797363\n",
      "Epoch: 14, Train_Loss: 2.183779001235962, Test_Loss: 1.5896742343902588 *\n",
      "Epoch: 14, Train_Loss: 3.206753730773926, Test_Loss: 1.5849136114120483 *\n",
      "Epoch: 14, Train_Loss: 2.4885287284851074, Test_Loss: 1.582582950592041 *\n",
      "Epoch: 14, Train_Loss: 1.5863804817199707, Test_Loss: 1.5829509496688843\n",
      "Epoch: 14, Train_Loss: 1.5926493406295776, Test_Loss: 1.6008739471435547\n",
      "Epoch: 14, Train_Loss: 1.8656105995178223, Test_Loss: 1.5818017721176147 *\n",
      "Epoch: 14, Train_Loss: 2.0520870685577393, Test_Loss: 1.614941954612732\n",
      "Epoch: 14, Train_Loss: 1.6010103225708008, Test_Loss: 1.8028285503387451\n",
      "Epoch: 14, Train_Loss: 1.6425001621246338, Test_Loss: 1.8264566659927368\n",
      "Epoch: 14, Train_Loss: 1.6977261304855347, Test_Loss: 1.5909322500228882 *\n",
      "Epoch: 14, Train_Loss: 1.6715484857559204, Test_Loss: 1.6146197319030762\n",
      "Epoch: 14, Train_Loss: 1.6563467979431152, Test_Loss: 1.718926191329956\n",
      "Epoch: 14, Train_Loss: 1.9367179870605469, Test_Loss: 1.7966667413711548\n",
      "Epoch: 14, Train_Loss: 1.7747846841812134, Test_Loss: 1.5811506509780884 *\n",
      "Epoch: 14, Train_Loss: 1.6377549171447754, Test_Loss: 1.7090795040130615\n",
      "Epoch: 14, Train_Loss: 1.7474851608276367, Test_Loss: 1.8482460975646973\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 1.7563279867172241, Test_Loss: 1.6575069427490234 *\n",
      "Epoch: 14, Train_Loss: 1.910516619682312, Test_Loss: 1.6963235139846802\n",
      "Epoch: 14, Train_Loss: 1.7994990348815918, Test_Loss: 1.5692814588546753 *\n",
      "Epoch: 14, Train_Loss: 1.6150580644607544, Test_Loss: 1.5803003311157227\n",
      "Epoch: 14, Train_Loss: 1.7091004848480225, Test_Loss: 1.5770208835601807 *\n",
      "Epoch: 14, Train_Loss: 1.6630204916000366, Test_Loss: 2.1528303623199463\n",
      "Epoch: 14, Train_Loss: 1.5794459581375122, Test_Loss: 1.9312132596969604 *\n",
      "Epoch: 14, Train_Loss: 1.5713824033737183, Test_Loss: 2.0553674697875977\n",
      "Epoch: 14, Train_Loss: 1.568369746208191, Test_Loss: 2.146794319152832\n",
      "Epoch: 14, Train_Loss: 1.5643481016159058, Test_Loss: 1.6985960006713867 *\n",
      "Epoch: 14, Train_Loss: 1.5721559524536133, Test_Loss: 2.106415033340454\n",
      "Epoch: 14, Train_Loss: 1.5852079391479492, Test_Loss: 1.6977877616882324 *\n",
      "Epoch: 14, Train_Loss: 1.6264466047286987, Test_Loss: 1.5734941959381104 *\n",
      "Epoch: 14, Train_Loss: 1.6120431423187256, Test_Loss: 1.5706788301467896 *\n",
      "Epoch: 14, Train_Loss: 1.6806007623672485, Test_Loss: 1.6246227025985718\n",
      "Epoch: 14, Train_Loss: 1.7784233093261719, Test_Loss: 1.7728030681610107\n",
      "Epoch: 14, Train_Loss: 1.8238862752914429, Test_Loss: 2.429217576980591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 1.5850911140441895, Test_Loss: 2.0992441177368164 *\n",
      "Epoch: 14, Train_Loss: 1.6209440231323242, Test_Loss: 3.348564863204956\n",
      "Epoch: 14, Train_Loss: 1.9272277355194092, Test_Loss: 2.1737639904022217 *\n",
      "Epoch: 14, Train_Loss: 2.047240734100342, Test_Loss: 2.1861135959625244\n",
      "Epoch: 14, Train_Loss: 1.5884963274002075, Test_Loss: 1.589158535003662 *\n",
      "Epoch: 14, Train_Loss: 1.566910743713379, Test_Loss: 1.5616129636764526 *\n",
      "Epoch: 14, Train_Loss: 1.9710655212402344, Test_Loss: 2.122814893722534\n",
      "Epoch: 14, Train_Loss: 2.1357109546661377, Test_Loss: 3.0515620708465576\n",
      "Epoch: 14, Train_Loss: 1.7307987213134766, Test_Loss: 1.6875431537628174 *\n",
      "Epoch: 14, Train_Loss: 1.5766528844833374, Test_Loss: 1.6648143529891968 *\n",
      "Epoch: 14, Train_Loss: 1.581032633781433, Test_Loss: 1.5546724796295166 *\n",
      "Epoch: 14, Train_Loss: 1.9455955028533936, Test_Loss: 1.7188178300857544\n",
      "Epoch: 14, Train_Loss: 3.099090337753296, Test_Loss: 1.8927569389343262\n",
      "Epoch: 14, Train_Loss: 1.7989296913146973, Test_Loss: 1.9364129304885864\n",
      "Epoch: 14, Train_Loss: 1.5741448402404785, Test_Loss: 2.631333589553833\n",
      "Epoch: 14, Train_Loss: 1.5529550313949585, Test_Loss: 1.8786072731018066 *\n",
      "Epoch: 14, Train_Loss: 1.5555325746536255, Test_Loss: 1.5668327808380127 *\n",
      "Epoch: 14, Train_Loss: 1.9617502689361572, Test_Loss: 1.5506502389907837 *\n",
      "Epoch: 14, Train_Loss: 1.6196963787078857, Test_Loss: 1.5552449226379395\n",
      "Epoch: 14, Train_Loss: 1.5817134380340576, Test_Loss: 1.5866141319274902\n",
      "Epoch: 14, Train_Loss: 1.5563879013061523, Test_Loss: 1.7578626871109009\n",
      "Epoch: 14, Train_Loss: 1.5751347541809082, Test_Loss: 1.9864060878753662\n",
      "Epoch: 14, Train_Loss: 17.761520385742188, Test_Loss: 1.8024606704711914 *\n",
      "Epoch: 14, Train_Loss: 2.591017007827759, Test_Loss: 1.6434273719787598 *\n",
      "Epoch: 14, Train_Loss: 2.7789018154144287, Test_Loss: 1.5685616731643677 *\n",
      "Epoch: 14, Train_Loss: 4.059133529663086, Test_Loss: 1.5621989965438843 *\n",
      "Epoch: 14, Train_Loss: 1.5793445110321045, Test_Loss: 1.6073942184448242\n",
      "Epoch: 14, Train_Loss: 1.64999520778656, Test_Loss: 2.074042320251465\n",
      "Epoch: 14, Train_Loss: 2.8201982975006104, Test_Loss: 2.70880126953125\n",
      "Epoch: 14, Train_Loss: 11.324790954589844, Test_Loss: 1.8071699142456055 *\n",
      "Epoch: 14, Train_Loss: 1.980170488357544, Test_Loss: 1.5983250141143799 *\n",
      "Epoch: 14, Train_Loss: 1.5625547170639038, Test_Loss: 1.563001036643982 *\n",
      "Epoch: 14, Train_Loss: 6.67108678817749, Test_Loss: 1.5770677328109741\n",
      "Epoch: 14, Train_Loss: 2.3141915798187256, Test_Loss: 1.6052216291427612\n",
      "Epoch: 14, Train_Loss: 1.628313660621643, Test_Loss: 1.6597257852554321\n",
      "Epoch: 14, Train_Loss: 1.5576757192611694, Test_Loss: 1.6271326541900635 *\n",
      "Epoch: 14, Train_Loss: 1.5486159324645996, Test_Loss: 1.6796259880065918\n",
      "Epoch: 14, Train_Loss: 1.5553656816482544, Test_Loss: 1.6423665285110474 *\n",
      "Epoch: 14, Train_Loss: 1.5545454025268555, Test_Loss: 1.627794861793518 *\n",
      "Epoch: 14, Train_Loss: 1.5592819452285767, Test_Loss: 1.9005426168441772\n",
      "Epoch: 14, Train_Loss: 1.5487873554229736, Test_Loss: 1.6150927543640137 *\n",
      "Epoch: 14, Train_Loss: 1.539910912513733, Test_Loss: 1.6559661626815796\n",
      "Epoch: 14, Train_Loss: 1.5959111452102661, Test_Loss: 1.6105189323425293 *\n",
      "Epoch: 14, Train_Loss: 1.594468593597412, Test_Loss: 1.6183968782424927\n",
      "Epoch: 14, Train_Loss: 1.5795284509658813, Test_Loss: 1.606237769126892 *\n",
      "Epoch: 14, Train_Loss: 1.5949432849884033, Test_Loss: 1.5795989036560059 *\n",
      "Epoch: 14, Train_Loss: 1.6134731769561768, Test_Loss: 1.544921875 *\n",
      "Epoch: 14, Train_Loss: 1.5993409156799316, Test_Loss: 6.179780006408691\n",
      "Epoch: 14, Train_Loss: 1.538710355758667, Test_Loss: 3.1854476928710938 *\n",
      "Epoch: 14, Train_Loss: 1.5552877187728882, Test_Loss: 1.584557056427002 *\n",
      "Epoch: 14, Train_Loss: 1.5432530641555786, Test_Loss: 1.5747188329696655 *\n",
      "Epoch: 14, Train_Loss: 1.534237027168274, Test_Loss: 1.5730613470077515 *\n",
      "Epoch: 14, Train_Loss: 1.5258971452713013, Test_Loss: 1.5494383573532104 *\n",
      "Epoch: 14, Train_Loss: 1.529049277305603, Test_Loss: 1.5821713209152222\n",
      "Epoch: 14, Train_Loss: 1.5273998975753784, Test_Loss: 1.5880537033081055\n",
      "Epoch: 14, Train_Loss: 1.5277682542800903, Test_Loss: 1.5727999210357666 *\n",
      "Epoch: 14, Train_Loss: 1.5267049074172974, Test_Loss: 1.5724797248840332 *\n",
      "Epoch: 14, Train_Loss: 1.5241494178771973, Test_Loss: 1.578634262084961\n",
      "Epoch: 14, Train_Loss: 1.5257561206817627, Test_Loss: 1.5873099565505981\n",
      "Epoch: 14, Train_Loss: 1.5491948127746582, Test_Loss: 1.5516250133514404 *\n",
      "Epoch: 14, Train_Loss: 1.5583688020706177, Test_Loss: 1.5353567600250244 *\n",
      "Epoch: 14, Train_Loss: 1.566536784172058, Test_Loss: 1.5522736310958862\n",
      "Epoch: 14, Train_Loss: 1.5334011316299438, Test_Loss: 1.5648301839828491\n",
      "Epoch: 14, Train_Loss: 1.539406180381775, Test_Loss: 1.5461921691894531 *\n",
      "Epoch: 14, Train_Loss: 10.27684211730957, Test_Loss: 1.584599256515503\n",
      "Epoch: 14, Train_Loss: 1.6524908542633057, Test_Loss: 1.5675830841064453 *\n",
      "Epoch: 14, Train_Loss: 1.5585006475448608, Test_Loss: 1.5812609195709229\n",
      "Epoch: 14, Train_Loss: 1.5820564031600952, Test_Loss: 1.5716001987457275 *\n",
      "Epoch: 14, Train_Loss: 1.6062874794006348, Test_Loss: 1.579360008239746\n",
      "Epoch: 14, Train_Loss: 1.5475808382034302, Test_Loss: 1.615803837776184\n",
      "Epoch: 14, Train_Loss: 1.565530776977539, Test_Loss: 1.6138173341751099 *\n",
      "Epoch: 14, Train_Loss: 1.624812126159668, Test_Loss: 1.6105421781539917 *\n",
      "Epoch: 14, Train_Loss: 1.776499629020691, Test_Loss: 1.6092729568481445 *\n",
      "Epoch: 14, Train_Loss: 1.7123316526412964, Test_Loss: 1.58211088180542 *\n",
      "Epoch: 14, Train_Loss: 1.6263192892074585, Test_Loss: 1.584545373916626\n",
      "Epoch: 14, Train_Loss: 1.517993688583374, Test_Loss: 1.5791484117507935 *\n",
      "Epoch: 14, Train_Loss: 1.677215337753296, Test_Loss: 1.5727498531341553 *\n",
      "Epoch: 14, Train_Loss: 1.643966794013977, Test_Loss: 1.6550034284591675\n",
      "Epoch: 14, Train_Loss: 1.7184970378875732, Test_Loss: 1.6066051721572876 *\n",
      "Epoch: 14, Train_Loss: 1.6400421857833862, Test_Loss: 7.536830425262451\n",
      "Epoch: 14, Train_Loss: 1.5750974416732788, Test_Loss: 1.6043459177017212 *\n",
      "Epoch: 14, Train_Loss: 1.536315679550171, Test_Loss: 1.5171399116516113 *\n",
      "Epoch: 14, Train_Loss: 1.533507227897644, Test_Loss: 1.5323995351791382\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 1.6106517314910889, Test_Loss: 1.5339224338531494\n",
      "Epoch: 14, Train_Loss: 1.5385158061981201, Test_Loss: 1.537046194076538\n",
      "Epoch: 14, Train_Loss: 1.52375328540802, Test_Loss: 1.5234935283660889 *\n",
      "Epoch: 14, Train_Loss: 1.520837664604187, Test_Loss: 1.6555418968200684\n",
      "Epoch: 14, Train_Loss: 1.5303782224655151, Test_Loss: 1.5652873516082764 *\n",
      "Epoch: 14, Train_Loss: 1.7171566486358643, Test_Loss: 1.5078836679458618 *\n",
      "Epoch: 14, Train_Loss: 6.8770012855529785, Test_Loss: 1.5457159280776978\n",
      "Epoch: 14, Train_Loss: 1.5093392133712769, Test_Loss: 1.5275859832763672 *\n",
      "Epoch: 14, Train_Loss: 1.5386614799499512, Test_Loss: 1.5148155689239502 *\n",
      "Epoch: 14, Train_Loss: 1.5487148761749268, Test_Loss: 1.5375714302062988\n",
      "Epoch: 14, Train_Loss: 1.5267101526260376, Test_Loss: 1.5593370199203491\n",
      "Epoch: 14, Train_Loss: 1.5139225721359253, Test_Loss: 1.5787138938903809\n",
      "Epoch: 14, Train_Loss: 1.5143660306930542, Test_Loss: 1.6182596683502197\n",
      "Epoch: 14, Train_Loss: 1.5299105644226074, Test_Loss: 1.555344581604004 *\n",
      "Epoch: 14, Train_Loss: 1.5355494022369385, Test_Loss: 1.5280160903930664 *\n",
      "Epoch: 14, Train_Loss: 1.5265953540802002, Test_Loss: 1.5089200735092163 *\n",
      "Epoch: 14, Train_Loss: 1.5313425064086914, Test_Loss: 1.5060981512069702 *\n",
      "Epoch: 14, Train_Loss: 1.5061873197555542, Test_Loss: 1.505715250968933 *\n",
      "Epoch: 14, Train_Loss: 1.5007318258285522, Test_Loss: 1.5026569366455078 *\n",
      "Epoch: 14, Train_Loss: 1.524653434753418, Test_Loss: 1.5047112703323364\n",
      "Epoch: 14, Train_Loss: 1.5033295154571533, Test_Loss: 1.5076000690460205\n",
      "Epoch: 14, Train_Loss: 1.5004627704620361, Test_Loss: 1.5051310062408447 *\n",
      "Epoch: 14, Train_Loss: 1.5247434377670288, Test_Loss: 1.5091770887374878\n",
      "Epoch: 14, Train_Loss: 1.530438780784607, Test_Loss: 1.5052176713943481 *\n",
      "Epoch: 14, Train_Loss: 1.5134862661361694, Test_Loss: 1.5276432037353516\n",
      "Epoch: 14, Train_Loss: 1.5018199682235718, Test_Loss: 1.5187740325927734 *\n",
      "Epoch: 14, Train_Loss: 1.5026600360870361, Test_Loss: 1.5332778692245483\n",
      "Epoch: 14, Train_Loss: 1.557326316833496, Test_Loss: 1.5739877223968506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 1.546440839767456, Test_Loss: 1.9108071327209473\n",
      "Epoch: 14, Train_Loss: 1.5435842275619507, Test_Loss: 1.522032380104065 *\n",
      "Epoch: 14, Train_Loss: 1.5302045345306396, Test_Loss: 1.5354037284851074\n",
      "Epoch: 14, Train_Loss: 1.5486303567886353, Test_Loss: 1.5667742490768433\n",
      "Epoch: 14, Train_Loss: 1.5262234210968018, Test_Loss: 1.7915343046188354\n",
      "Epoch: 14, Train_Loss: 1.5419631004333496, Test_Loss: 1.5207668542861938 *\n",
      "Epoch: 14, Train_Loss: 1.5183250904083252, Test_Loss: 1.6348271369934082\n",
      "Epoch: 14, Train_Loss: 1.667914628982544, Test_Loss: 1.7465076446533203\n",
      "Epoch: 14, Train_Loss: 1.512708067893982, Test_Loss: 1.6712232828140259 *\n",
      "Epoch: 14, Train_Loss: 1.4981003999710083, Test_Loss: 1.5760475397109985 *\n",
      "Epoch: 14, Train_Loss: 1.48704993724823, Test_Loss: 1.4991463422775269 *\n",
      "Epoch: 14, Train_Loss: 1.4881746768951416, Test_Loss: 1.5030289888381958\n",
      "Epoch: 14, Train_Loss: 1.4847415685653687, Test_Loss: 1.5139930248260498\n",
      "Epoch: 14, Train_Loss: 1.4867606163024902, Test_Loss: 1.8969414234161377\n",
      "Epoch: 14, Train_Loss: 1.724102258682251, Test_Loss: 2.0289154052734375\n",
      "Epoch: 14, Train_Loss: 5.924963474273682, Test_Loss: 1.8643394708633423 *\n",
      "Epoch: 14, Train_Loss: 1.5153084993362427, Test_Loss: 2.241014003753662\n",
      "Epoch: 14, Train_Loss: 1.4914827346801758, Test_Loss: 1.6881909370422363 *\n",
      "Epoch: 14, Train_Loss: 1.4922279119491577, Test_Loss: 2.0132646560668945\n",
      "Epoch: 14, Train_Loss: 1.4853841066360474, Test_Loss: 1.68439781665802 *\n",
      "Epoch: 14, Train_Loss: 1.4820964336395264, Test_Loss: 1.5004165172576904 *\n",
      "Epoch: 14, Train_Loss: 1.482002854347229, Test_Loss: 1.4961546659469604 *\n",
      "Epoch: 14, Train_Loss: 1.4812960624694824, Test_Loss: 1.538190245628357\n",
      "Epoch: 14, Train_Loss: 1.4816961288452148, Test_Loss: 1.637786626815796\n",
      "Epoch: 14, Train_Loss: 1.4833866357803345, Test_Loss: 2.4216690063476562\n",
      "Epoch: 14, Train_Loss: 1.5432777404785156, Test_Loss: 1.7345688343048096 *\n",
      "Epoch: 14, Train_Loss: 1.542790174484253, Test_Loss: 3.5741477012634277\n",
      "Epoch: 14, Train_Loss: 1.5576403141021729, Test_Loss: 1.994262933731079 *\n",
      "Epoch: 14, Train_Loss: 1.5286260843276978, Test_Loss: 2.3991832733154297\n",
      "Epoch: 14, Train_Loss: 1.480177402496338, Test_Loss: 1.5443177223205566 *\n",
      "Epoch: 14, Train_Loss: 1.6226998567581177, Test_Loss: 1.4841508865356445 *\n",
      "Epoch: 14, Train_Loss: 1.6979824304580688, Test_Loss: 1.79099440574646\n",
      "Epoch: 14, Train_Loss: 1.6942033767700195, Test_Loss: 2.9556825160980225\n",
      "Epoch: 14, Train_Loss: 1.6180287599563599, Test_Loss: 1.9396300315856934 *\n",
      "Epoch: 14, Train_Loss: 1.4767100811004639, Test_Loss: 1.5822373628616333 *\n",
      "Epoch: 14, Train_Loss: 1.4742261171340942, Test_Loss: 1.4847233295440674 *\n",
      "Epoch: 14, Train_Loss: 1.475214958190918, Test_Loss: 1.591894507408142\n",
      "Epoch: 14, Train_Loss: 1.4899146556854248, Test_Loss: 1.8820061683654785\n",
      "Epoch: 14, Train_Loss: 1.480528473854065, Test_Loss: 1.7334647178649902 *\n",
      "Epoch: 14, Train_Loss: 1.4814509153366089, Test_Loss: 2.6823720932006836\n",
      "Epoch: 14, Train_Loss: 1.4753272533416748, Test_Loss: 1.9729145765304565 *\n",
      "Epoch: 14, Train_Loss: 1.4733617305755615, Test_Loss: 1.4933185577392578 *\n",
      "Epoch: 14, Train_Loss: 1.4767730236053467, Test_Loss: 1.4847660064697266 *\n",
      "Epoch: 14, Train_Loss: 1.4920017719268799, Test_Loss: 1.4904320240020752\n",
      "Epoch: 14, Train_Loss: 1.588993787765503, Test_Loss: 1.513378620147705\n",
      "Epoch: 14, Train_Loss: 1.5849967002868652, Test_Loss: 1.5858454704284668\n",
      "Epoch: 14, Train_Loss: 1.5978074073791504, Test_Loss: 1.9476537704467773\n",
      "Epoch: 14, Train_Loss: 1.5528970956802368, Test_Loss: 1.8294050693511963 *\n",
      "Epoch: 14, Train_Loss: 1.6097586154937744, Test_Loss: 1.596492886543274 *\n",
      "Epoch: 14, Train_Loss: 1.6001677513122559, Test_Loss: 1.4881137609481812 *\n",
      "Epoch: 14, Train_Loss: 1.5907658338546753, Test_Loss: 1.4865437746047974 *\n",
      "Epoch: 14, Train_Loss: 1.657448410987854, Test_Loss: 1.5079549551010132\n",
      "Epoch: 14, Train_Loss: 1.8394893407821655, Test_Loss: 1.8588365316390991\n",
      "Epoch: 14, Train_Loss: 1.4804844856262207, Test_Loss: 2.7667083740234375\n",
      "Epoch: 14, Train_Loss: 1.4733933210372925, Test_Loss: 1.9764351844787598 *\n",
      "Epoch: 14, Train_Loss: 4.087747097015381, Test_Loss: 1.5448864698410034 *\n",
      "Epoch: 14, Train_Loss: 2.1837382316589355, Test_Loss: 1.4769439697265625 *\n",
      "Epoch: 14, Train_Loss: 1.4970802068710327, Test_Loss: 1.470540165901184 *\n",
      "Epoch: 14, Train_Loss: 1.512332797050476, Test_Loss: 1.4652687311172485 *\n",
      "Epoch: 14, Train_Loss: 1.5244287252426147, Test_Loss: 1.4692655801773071\n",
      "Epoch: 14, Train_Loss: 1.4967840909957886, Test_Loss: 1.4878768920898438\n",
      "Epoch: 14, Train_Loss: 1.4640107154846191, Test_Loss: 1.5066373348236084\n",
      "Epoch: 14, Train_Loss: 1.5021663904190063, Test_Loss: 1.4616903066635132 *\n",
      "Epoch: 14, Train_Loss: 1.5849196910858154, Test_Loss: 1.5374038219451904\n",
      "Epoch: 14, Train_Loss: 1.5453619956970215, Test_Loss: 1.6459566354751587\n",
      "Epoch: 14, Train_Loss: 1.5261750221252441, Test_Loss: 1.7897872924804688\n",
      "Epoch: 14, Train_Loss: 1.5325199365615845, Test_Loss: 1.7075616121292114 *\n",
      "Epoch: 14, Train_Loss: 1.4896376132965088, Test_Loss: 1.4882608652114868 *\n",
      "Epoch: 14, Train_Loss: 1.4804795980453491, Test_Loss: 1.4767524003982544 *\n",
      "Epoch: 14, Train_Loss: 1.4673917293548584, Test_Loss: 1.4788825511932373\n",
      "Epoch: 14, Train_Loss: 1.4947123527526855, Test_Loss: 1.470883846282959 *\n",
      "Epoch: 14, Train_Loss: 1.492289423942566, Test_Loss: 1.495652675628662\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 1.4622642993927002, Test_Loss: 3.6833009719848633\n",
      "Epoch: 14, Train_Loss: 1.4603043794631958, Test_Loss: 4.450882434844971\n",
      "Epoch: 14, Train_Loss: 1.4891691207885742, Test_Loss: 1.467855453491211 *\n",
      "Epoch: 14, Train_Loss: 1.4950660467147827, Test_Loss: 1.458760380744934 *\n",
      "Epoch: 14, Train_Loss: 1.4574737548828125, Test_Loss: 1.4607970714569092\n",
      "Epoch: 14, Train_Loss: 1.453170895576477, Test_Loss: 1.4768344163894653\n",
      "Epoch: 14, Train_Loss: 1.4533288478851318, Test_Loss: 1.454187035560608 *\n",
      "Epoch: 14, Train_Loss: 1.4585880041122437, Test_Loss: 1.4563624858856201\n",
      "Epoch: 14, Train_Loss: 1.4522876739501953, Test_Loss: 1.4514288902282715 *\n",
      "Epoch: 14, Train_Loss: 1.455154538154602, Test_Loss: 1.450993299484253 *\n",
      "Epoch: 14, Train_Loss: 1.4517474174499512, Test_Loss: 1.4538308382034302\n",
      "Epoch: 14, Train_Loss: 1.4530739784240723, Test_Loss: 1.4472274780273438 *\n",
      "Epoch: 14, Train_Loss: 1.452856183052063, Test_Loss: 1.4570801258087158\n",
      "Epoch: 14, Train_Loss: 1.4481984376907349, Test_Loss: 1.4842419624328613\n",
      "Epoch: 14, Train_Loss: 1.4521454572677612, Test_Loss: 1.4712846279144287 *\n",
      "Epoch: 14, Train_Loss: 1.463861107826233, Test_Loss: 1.4500313997268677 *\n",
      "Epoch: 14, Train_Loss: 1.4669394493103027, Test_Loss: 1.4469102621078491 *\n",
      "Epoch: 14, Train_Loss: 1.4618407487869263, Test_Loss: 1.446631669998169 *\n",
      "Epoch: 14, Train_Loss: 1.4736653566360474, Test_Loss: 1.446808099746704\n",
      "Epoch: 14, Train_Loss: 1.4524893760681152, Test_Loss: 1.4430397748947144 *\n",
      "Epoch: 14, Train_Loss: 1.4444618225097656, Test_Loss: 1.447493553161621\n",
      "Epoch: 14, Train_Loss: 1.445294737815857, Test_Loss: 1.4412360191345215 *\n",
      "Epoch: 14, Train_Loss: 1.4526002407073975, Test_Loss: 1.4449000358581543\n",
      "Epoch: 14, Train_Loss: 1.464868187904358, Test_Loss: 1.4469741582870483\n",
      "Epoch: 14, Train_Loss: 1.4443963766098022, Test_Loss: 1.44240140914917 *\n",
      "Epoch: 14, Train_Loss: 1.4426597356796265, Test_Loss: 1.4408702850341797 *\n",
      "Epoch: 14, Train_Loss: 1.4416582584381104, Test_Loss: 1.441807746887207\n",
      "Epoch: 14, Train_Loss: 1.4766433238983154, Test_Loss: 1.4415812492370605 *\n",
      "Epoch: 14, Train_Loss: 1.4994802474975586, Test_Loss: 1.4418786764144897\n",
      "Epoch: 14, Train_Loss: 1.4899827241897583, Test_Loss: 1.4377723932266235 *\n",
      "Epoch: 14, Train_Loss: 1.4602437019348145, Test_Loss: 1.4730092287063599\n",
      "Epoch: 14, Train_Loss: 1.4363945722579956, Test_Loss: 1.4762097597122192\n",
      "Epoch: 14, Train_Loss: 1.5007394552230835, Test_Loss: 5.975090503692627\n",
      "Epoch: 14, Train_Loss: 1.4517704248428345, Test_Loss: 2.311249017715454 *\n",
      "Epoch: 14, Train_Loss: 1.4437769651412964, Test_Loss: 1.4364515542984009 *\n",
      "Epoch: 14, Train_Loss: 1.466600775718689, Test_Loss: 1.4535514116287231\n",
      "Epoch: 14, Train_Loss: 1.4524624347686768, Test_Loss: 1.5007625818252563\n",
      "Epoch: 14, Train_Loss: 1.5487128496170044, Test_Loss: 1.512349247932434\n",
      "Epoch: 14, Train_Loss: 1.5227504968643188, Test_Loss: 1.4486889839172363 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 1.4684252738952637, Test_Loss: 1.510297417640686\n",
      "Epoch: 14, Train_Loss: 1.4481148719787598, Test_Loss: 1.5068730115890503 *\n",
      "Epoch: 14, Train_Loss: 1.444137692451477, Test_Loss: 1.4363237619400024 *\n",
      "Epoch: 14, Train_Loss: 1.4473724365234375, Test_Loss: 1.4749842882156372\n",
      "Epoch: 14, Train_Loss: 1.4340393543243408, Test_Loss: 1.4462368488311768 *\n",
      "Epoch: 14, Train_Loss: 1.4405454397201538, Test_Loss: 1.441274642944336 *\n",
      "Epoch: 14, Train_Loss: 1.4520421028137207, Test_Loss: 1.4344793558120728 *\n",
      "Epoch: 14, Train_Loss: 1.4605984687805176, Test_Loss: 1.579240322113037\n",
      "Epoch: 14, Train_Loss: 1.5387001037597656, Test_Loss: 1.4725605249404907 *\n",
      "Epoch: 14, Train_Loss: 1.4382432699203491, Test_Loss: 1.542177438735962\n",
      "Epoch: 14, Train_Loss: 1.4885133504867554, Test_Loss: 1.494004487991333 *\n",
      "Epoch: 14, Train_Loss: 1.4494457244873047, Test_Loss: 1.4646228551864624 *\n",
      "Epoch: 14, Train_Loss: 1.4516997337341309, Test_Loss: 1.4490294456481934 *\n",
      "Epoch: 14, Train_Loss: 1.5195486545562744, Test_Loss: 1.450271487236023\n",
      "Epoch: 14, Train_Loss: 1.664167046546936, Test_Loss: 1.4496078491210938 *\n",
      "Epoch: 14, Train_Loss: 1.4376353025436401, Test_Loss: 1.4430091381072998 *\n",
      "Epoch: 14, Train_Loss: 1.4654940366744995, Test_Loss: 1.4395252466201782 *\n",
      "Epoch: 14, Train_Loss: 1.423629641532898, Test_Loss: 1.4395734071731567\n",
      "Epoch: 14, Train_Loss: 1.4235153198242188, Test_Loss: 1.4403077363967896\n",
      "Epoch: 14, Train_Loss: 1.4237357378005981, Test_Loss: 1.4497482776641846\n",
      "Epoch: 14, Train_Loss: 1.421615481376648, Test_Loss: 1.4571621417999268\n",
      "Epoch: 14, Train_Loss: 1.4346495866775513, Test_Loss: 1.4366503953933716 *\n",
      "Epoch: 14, Train_Loss: 1.4357866048812866, Test_Loss: 1.4347580671310425 *\n",
      "Epoch: 14, Train_Loss: 1.4332739114761353, Test_Loss: 1.4804000854492188\n",
      "Epoch: 14, Train_Loss: 1.4309735298156738, Test_Loss: 1.4528032541275024 *\n",
      "Epoch: 14, Train_Loss: 1.4383732080459595, Test_Loss: 1.740609884262085\n",
      "Epoch: 14, Train_Loss: 1.425252079963684, Test_Loss: 1.4307588338851929 *\n",
      "Epoch: 14, Train_Loss: 1.4202555418014526, Test_Loss: 1.478068470954895\n",
      "Epoch: 14, Train_Loss: 1.4161858558654785, Test_Loss: 1.5033029317855835\n",
      "Epoch: 14, Train_Loss: 1.4485138654708862, Test_Loss: 1.822419285774231\n",
      "Epoch: 14, Train_Loss: 1.4451239109039307, Test_Loss: 1.4893869161605835 *\n",
      "Epoch: 14, Train_Loss: 1.441711187362671, Test_Loss: 1.5037591457366943\n",
      "Epoch: 14, Train_Loss: 1.4185010194778442, Test_Loss: 1.644118070602417\n",
      "Epoch: 14, Train_Loss: 1.4675862789154053, Test_Loss: 1.6406328678131104 *\n",
      "Epoch: 14, Train_Loss: 1.461518406867981, Test_Loss: 1.4331486225128174 *\n",
      "Epoch: 14, Train_Loss: 1.448157548904419, Test_Loss: 1.4488017559051514\n",
      "Epoch: 14, Train_Loss: 1.4230432510375977, Test_Loss: 1.422248125076294 *\n",
      "Epoch: 14, Train_Loss: 1.4517220258712769, Test_Loss: 1.4598047733306885\n",
      "Epoch: 14, Train_Loss: 1.4123173952102661, Test_Loss: 1.5344653129577637\n",
      "Epoch: 14, Train_Loss: 1.425460934638977, Test_Loss: 2.1920056343078613\n",
      "Epoch: 14, Train_Loss: 1.4290837049484253, Test_Loss: 1.640379548072815 *\n",
      "Epoch: 14, Train_Loss: 1.44854736328125, Test_Loss: 2.3275864124298096\n",
      "Epoch: 14, Train_Loss: 2.9865708351135254, Test_Loss: 1.847185730934143 *\n",
      "Epoch: 14, Train_Loss: 5.290131568908691, Test_Loss: 1.8381503820419312 *\n",
      "Epoch: 14, Train_Loss: 1.4974216222763062, Test_Loss: 1.7433046102523804 *\n",
      "Epoch: 14, Train_Loss: 1.4233651161193848, Test_Loss: 1.4218776226043701 *\n",
      "Epoch: 14, Train_Loss: 1.4229587316513062, Test_Loss: 1.4258880615234375\n",
      "Epoch: 14, Train_Loss: 1.5810610055923462, Test_Loss: 1.461501121520996\n",
      "Epoch: 14, Train_Loss: 1.4556922912597656, Test_Loss: 1.5823874473571777\n",
      "Epoch: 14, Train_Loss: 1.417089819908142, Test_Loss: 2.223690986633301\n",
      "Epoch: 14, Train_Loss: 1.4052523374557495, Test_Loss: 1.6349220275878906 *\n",
      "Epoch: 14, Train_Loss: 1.4551708698272705, Test_Loss: 3.3901262283325195\n",
      "Epoch: 14, Train_Loss: 1.413352131843567, Test_Loss: 2.0415761470794678 *\n",
      "Epoch: 14, Train_Loss: 1.429207444190979, Test_Loss: 2.4671387672424316\n",
      "Epoch: 14, Train_Loss: 1.863654613494873, Test_Loss: 1.548088788986206 *\n",
      "Epoch: 14, Train_Loss: 2.7414774894714355, Test_Loss: 1.408547043800354 *\n",
      "Epoch: 14, Train_Loss: 2.3140335083007812, Test_Loss: 1.5707805156707764\n",
      "Epoch: 14, Train_Loss: 1.5189043283462524, Test_Loss: 2.745850086212158\n",
      "Epoch: 14, Train_Loss: 1.6922698020935059, Test_Loss: 2.22455096244812 *\n",
      "Epoch: 14, Train_Loss: 3.9482784271240234, Test_Loss: 1.4755085706710815 *\n",
      "Epoch: 14, Train_Loss: 2.291658401489258, Test_Loss: 1.4323514699935913 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 1.4698669910430908, Test_Loss: 1.4877067804336548\n",
      "Epoch: 14, Train_Loss: 1.436660885810852, Test_Loss: 1.8706092834472656\n",
      "Epoch: 14, Train_Loss: 2.3240504264831543, Test_Loss: 1.550302267074585 *\n",
      "Epoch: 14, Train_Loss: 2.961672782897949, Test_Loss: 2.371403217315674\n",
      "Epoch: 14, Train_Loss: 1.9723246097564697, Test_Loss: 2.0662660598754883 *\n",
      "Epoch: 14, Train_Loss: 1.4126418828964233, Test_Loss: 1.488112449645996 *\n",
      "Epoch: 14, Train_Loss: 1.4059745073318481, Test_Loss: 1.3998494148254395 *\n",
      "Epoch: 14, Train_Loss: 1.8758552074432373, Test_Loss: 1.3988707065582275 *\n",
      "Epoch: 14, Train_Loss: 1.759061336517334, Test_Loss: 1.4075310230255127\n",
      "Epoch: 14, Train_Loss: 1.4275699853897095, Test_Loss: 1.4445106983184814\n",
      "Epoch: 14, Train_Loss: 1.4660230875015259, Test_Loss: 1.9316372871398926\n",
      "Epoch: 14, Train_Loss: 1.569098711013794, Test_Loss: 1.9529333114624023\n",
      "Epoch: 14, Train_Loss: 1.5227230787277222, Test_Loss: 1.5734988451004028 *\n",
      "Epoch: 14, Train_Loss: 1.4823143482208252, Test_Loss: 1.4391621351242065 *\n",
      "Epoch: 14, Train_Loss: 1.7855297327041626, Test_Loss: 1.4261618852615356 *\n",
      "Epoch: 14, Train_Loss: 1.4963200092315674, Test_Loss: 1.4136638641357422 *\n",
      "Epoch: 14, Train_Loss: 1.4671941995620728, Test_Loss: 1.5709857940673828\n",
      "Epoch: 14, Train_Loss: 1.6627131700515747, Test_Loss: 2.5493736267089844\n",
      "Epoch: 14, Train_Loss: 1.6625739336013794, Test_Loss: 2.132922887802124 *\n",
      "Epoch: 14, Train_Loss: 1.7413108348846436, Test_Loss: 1.456169605255127 *\n",
      "Epoch: 14, Train_Loss: 1.5951459407806396, Test_Loss: 1.4382573366165161 *\n",
      "Epoch: 14, Train_Loss: 1.4531805515289307, Test_Loss: 1.3938143253326416 *\n",
      "Epoch: 14, Train_Loss: 1.4979230165481567, Test_Loss: 1.3922556638717651 *\n",
      "Epoch: 14, Train_Loss: 1.4503862857818604, Test_Loss: 1.401145339012146\n",
      "Epoch: 14, Train_Loss: 1.4028756618499756, Test_Loss: 1.3992127180099487 *\n",
      "Epoch: 14, Train_Loss: 1.3887449502944946, Test_Loss: 1.4429800510406494\n",
      "Epoch: 14, Train_Loss: 1.3921936750411987, Test_Loss: 1.3941750526428223 *\n",
      "Epoch: 14, Train_Loss: 1.3888949155807495, Test_Loss: 1.4199473857879639\n",
      "Epoch: 14, Train_Loss: 1.3926618099212646, Test_Loss: 1.5134748220443726\n",
      "Epoch: 14, Train_Loss: 1.4221941232681274, Test_Loss: 1.7909572124481201\n",
      "Epoch: 14, Train_Loss: 1.4391462802886963, Test_Loss: 1.6629910469055176 *\n",
      "Epoch: 14, Train_Loss: 1.456274390220642, Test_Loss: 1.405163288116455 *\n",
      "Epoch: 14, Train_Loss: 1.4942094087600708, Test_Loss: 1.3865716457366943 *\n",
      "Epoch: 14, Train_Loss: 1.7462280988693237, Test_Loss: 1.3829665184020996 *\n",
      "Epoch: 14, Train_Loss: 1.488957166671753, Test_Loss: 1.38224458694458 *\n",
      "Epoch: 14, Train_Loss: 1.4072473049163818, Test_Loss: 1.389298439025879\n",
      "Epoch: 14, Train_Loss: 1.4663822650909424, Test_Loss: 2.252579927444458\n",
      "Epoch: 14, Train_Loss: 1.8107218742370605, Test_Loss: 5.979494571685791\n",
      "Epoch: 14, Train_Loss: 1.7901614904403687, Test_Loss: 1.4188966751098633 *\n",
      "Epoch: 14, Train_Loss: 1.3957213163375854, Test_Loss: 1.3926639556884766 *\n",
      "Epoch: 14, Train_Loss: 1.3947210311889648, Test_Loss: 1.3917726278305054 *\n",
      "Epoch: 14, Train_Loss: 1.8231627941131592, Test_Loss: 1.3839753866195679 *\n",
      "Epoch: 14, Train_Loss: 1.9026211500167847, Test_Loss: 1.399300217628479\n",
      "Epoch: 14, Train_Loss: 1.499572515487671, Test_Loss: 1.3934894800186157 *\n",
      "Epoch: 14, Train_Loss: 1.4017596244812012, Test_Loss: 1.416824460029602\n",
      "Epoch: 14, Train_Loss: 1.397399663925171, Test_Loss: 1.3971102237701416 *\n",
      "Epoch: 14, Train_Loss: 2.066385269165039, Test_Loss: 1.422494888305664\n",
      "Epoch: 14, Train_Loss: 2.7120072841644287, Test_Loss: 1.4119071960449219 *\n",
      "Epoch: 14, Train_Loss: 1.4852604866027832, Test_Loss: 1.4281904697418213\n",
      "Epoch: 14, Train_Loss: 1.4040155410766602, Test_Loss: 1.3879870176315308 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 1.378448247909546, Test_Loss: 1.4006823301315308\n",
      "Epoch: 14, Train_Loss: 1.3931398391723633, Test_Loss: 1.4078030586242676\n",
      "Epoch: 14, Train_Loss: 1.7605314254760742, Test_Loss: 1.3850754499435425 *\n",
      "Epoch: 14, Train_Loss: 1.4071557521820068, Test_Loss: 1.3926570415496826\n",
      "Epoch: 15, Train_Loss: 1.4021508693695068, Test_Loss: 1.3977006673812866 *\n",
      "Epoch: 15, Train_Loss: 1.3942898511886597, Test_Loss: 1.3973147869110107 *\n",
      "Epoch: 15, Train_Loss: 1.406374216079712, Test_Loss: 1.377759575843811 *\n",
      "Epoch: 15, Train_Loss: 18.42563247680664, Test_Loss: 1.4048264026641846\n",
      "Epoch: 15, Train_Loss: 1.3928399085998535, Test_Loss: 1.4054194688796997\n",
      "Epoch: 15, Train_Loss: 2.998171329498291, Test_Loss: 1.4320241212844849\n",
      "Epoch: 15, Train_Loss: 3.3452296257019043, Test_Loss: 1.4224841594696045 *\n",
      "Epoch: 15, Train_Loss: 1.3821362257003784, Test_Loss: 1.409062147140503 *\n",
      "Epoch: 15, Train_Loss: 1.4785106182098389, Test_Loss: 1.4014283418655396 *\n",
      "Epoch: 15, Train_Loss: 3.932237148284912, Test_Loss: 1.4075876474380493\n",
      "Epoch: 15, Train_Loss: 10.191680908203125, Test_Loss: 1.3996257781982422 *\n",
      "Epoch: 15, Train_Loss: 1.507324457168579, Test_Loss: 1.3829612731933594 *\n",
      "Epoch: 15, Train_Loss: 1.3910150527954102, Test_Loss: 1.4449628591537476\n",
      "Epoch: 15, Train_Loss: 7.338718414306641, Test_Loss: 1.4208685159683228 *\n",
      "Epoch: 15, Train_Loss: 1.4839508533477783, Test_Loss: 4.6765546798706055\n",
      "Epoch: 15, Train_Loss: 1.3807576894760132, Test_Loss: 3.851024627685547 *\n",
      "Epoch: 15, Train_Loss: 1.3706855773925781, Test_Loss: 1.3833578824996948 *\n",
      "Epoch: 15, Train_Loss: 1.3626549243927002, Test_Loss: 1.378946304321289 *\n",
      "Epoch: 15, Train_Loss: 1.3723703622817993, Test_Loss: 1.3763421773910522 *\n",
      "Epoch: 15, Train_Loss: 1.3634964227676392, Test_Loss: 1.3853766918182373\n",
      "Epoch: 15, Train_Loss: 1.3733865022659302, Test_Loss: 1.3878122568130493\n",
      "Epoch: 15, Train_Loss: 1.3626713752746582, Test_Loss: 1.4704798460006714\n",
      "Epoch: 15, Train_Loss: 1.366585612297058, Test_Loss: 1.5701117515563965\n",
      "Epoch: 15, Train_Loss: 1.3714061975479126, Test_Loss: 1.3806630373001099 *\n",
      "Epoch: 15, Train_Loss: 1.371827244758606, Test_Loss: 1.4388161897659302\n",
      "Epoch: 15, Train_Loss: 1.3710966110229492, Test_Loss: 1.3892532587051392 *\n",
      "Epoch: 15, Train_Loss: 1.390602707862854, Test_Loss: 1.3856041431427002 *\n",
      "Epoch: 15, Train_Loss: 1.4042290449142456, Test_Loss: 1.3919106721878052\n",
      "Epoch: 15, Train_Loss: 1.3801281452178955, Test_Loss: 1.3996894359588623\n",
      "Epoch: 15, Train_Loss: 1.3683712482452393, Test_Loss: 1.4080634117126465\n",
      "Epoch: 15, Train_Loss: 1.362042784690857, Test_Loss: 1.5199840068817139\n",
      "Epoch: 15, Train_Loss: 1.3644574880599976, Test_Loss: 1.4919886589050293 *\n",
      "Epoch: 15, Train_Loss: 1.3574916124343872, Test_Loss: 1.3730792999267578 *\n",
      "Epoch: 15, Train_Loss: 1.3525176048278809, Test_Loss: 1.3690723180770874 *\n",
      "Epoch: 15, Train_Loss: 1.3548295497894287, Test_Loss: 1.3622915744781494 *\n",
      "Epoch: 15, Train_Loss: 1.3560316562652588, Test_Loss: 1.364661455154419\n",
      "Epoch: 15, Train_Loss: 1.3529026508331299, Test_Loss: 1.3573518991470337 *\n",
      "Epoch: 15, Train_Loss: 1.3536025285720825, Test_Loss: 1.3573336601257324 *\n",
      "Epoch: 15, Train_Loss: 1.3529162406921387, Test_Loss: 1.3596035242080688\n",
      "Epoch: 15, Train_Loss: 1.3547756671905518, Test_Loss: 1.3544776439666748 *\n",
      "Epoch: 15, Train_Loss: 1.3738683462142944, Test_Loss: 1.3652268648147583\n",
      "Epoch: 15, Train_Loss: 1.3759382963180542, Test_Loss: 1.3596632480621338 *\n",
      "Epoch: 15, Train_Loss: 1.3902133703231812, Test_Loss: 1.3629124164581299\n",
      "Epoch: 15, Train_Loss: 1.360680103302002, Test_Loss: 1.3927382230758667\n",
      "Epoch: 15, Train_Loss: 1.8074126243591309, Test_Loss: 1.3663618564605713 *\n",
      "Epoch: 15, Train_Loss: 9.535913467407227, Test_Loss: 1.3857895135879517\n",
      "Epoch: 15, Train_Loss: 1.3971017599105835, Test_Loss: 1.9431604146957397\n",
      "Epoch: 15, Train_Loss: 1.409178376197815, Test_Loss: 1.514159917831421 *\n",
      "Epoch: 15, Train_Loss: 1.4521496295928955, Test_Loss: 1.4037971496582031 *\n",
      "Epoch: 15, Train_Loss: 1.441472053527832, Test_Loss: 1.394415259361267 *\n",
      "Epoch: 15, Train_Loss: 1.3952538967132568, Test_Loss: 1.5020818710327148\n",
      "Epoch: 15, Train_Loss: 1.423311710357666, Test_Loss: 1.4273520708084106 *\n",
      "Epoch: 15, Train_Loss: 1.488551378250122, Test_Loss: 1.48252272605896\n",
      "Epoch: 15, Train_Loss: 1.7169641256332397, Test_Loss: 1.6800642013549805\n",
      "Epoch: 15, Train_Loss: 1.539486289024353, Test_Loss: 1.7362834215164185\n",
      "Epoch: 15, Train_Loss: 1.4150407314300537, Test_Loss: 1.445807695388794 *\n",
      "Epoch: 15, Train_Loss: 1.3640923500061035, Test_Loss: 1.5190991163253784\n",
      "Epoch: 15, Train_Loss: 1.530328392982483, Test_Loss: 1.3700424432754517 *\n",
      "Epoch: 15, Train_Loss: 1.5042532682418823, Test_Loss: 1.3684719800949097 *\n",
      "Epoch: 15, Train_Loss: 1.5590673685073853, Test_Loss: 1.4290082454681396\n",
      "Epoch: 15, Train_Loss: 1.419476866722107, Test_Loss: 2.052304983139038\n",
      "Epoch: 15, Train_Loss: 1.3978383541107178, Test_Loss: 1.5402073860168457 *\n",
      "Epoch: 15, Train_Loss: 1.3579684495925903, Test_Loss: 1.9118049144744873\n",
      "Epoch: 15, Train_Loss: 1.370405673980713, Test_Loss: 1.7943041324615479 *\n",
      "Epoch: 15, Train_Loss: 1.476723313331604, Test_Loss: 1.6180870532989502 *\n",
      "Epoch: 15, Train_Loss: 1.3923982381820679, Test_Loss: 1.7487496137619019\n",
      "Epoch: 15, Train_Loss: 1.3569341897964478, Test_Loss: 1.420353889465332 *\n",
      "Epoch: 15, Train_Loss: 1.3525972366333008, Test_Loss: 1.3531279563903809 *\n",
      "Epoch: 15, Train_Loss: 1.3538310527801514, Test_Loss: 1.352331519126892 *\n",
      "Epoch: 15, Train_Loss: 2.31880259513855, Test_Loss: 1.4315637350082397\n",
      "Epoch: 15, Train_Loss: 5.981208324432373, Test_Loss: 1.8533108234405518\n",
      "Epoch: 15, Train_Loss: 1.3470865488052368, Test_Loss: 1.9392471313476562\n",
      "Epoch: 15, Train_Loss: 1.3941489458084106, Test_Loss: 2.556499481201172\n",
      "Epoch: 15, Train_Loss: 1.380624532699585, Test_Loss: 2.1784706115722656 *\n",
      "Epoch: 15, Train_Loss: 1.3753204345703125, Test_Loss: 2.473487615585327\n",
      "Epoch: 15, Train_Loss: 1.3510043621063232, Test_Loss: 1.749321699142456 *\n",
      "Epoch: 15, Train_Loss: 1.3670464754104614, Test_Loss: 1.3598358631134033 *\n",
      "Epoch: 15, Train_Loss: 1.3592472076416016, Test_Loss: 1.3831971883773804\n",
      "Epoch: 15, Train_Loss: 1.349558711051941, Test_Loss: 2.1198019981384277\n",
      "Epoch: 15, Train_Loss: 1.368514895439148, Test_Loss: 2.2847490310668945\n",
      "Epoch: 15, Train_Loss: 1.3680189847946167, Test_Loss: 1.4253507852554321 *\n",
      "Epoch: 15, Train_Loss: 1.3429559469223022, Test_Loss: 1.4378890991210938\n",
      "Epoch: 15, Train_Loss: 1.336388349533081, Test_Loss: 1.3550852537155151 *\n",
      "Epoch: 15, Train_Loss: 1.3581122159957886, Test_Loss: 1.6056420803070068\n",
      "Epoch: 15, Train_Loss: 1.3355557918548584, Test_Loss: 1.5131139755249023 *\n",
      "Epoch: 15, Train_Loss: 1.3349066972732544, Test_Loss: 2.2749147415161133\n",
      "Epoch: 15, Train_Loss: 1.3626091480255127, Test_Loss: 2.2748100757598877 *\n",
      "Epoch: 15, Train_Loss: 1.373795747756958, Test_Loss: 1.5858378410339355 *\n",
      "Epoch: 15, Train_Loss: 1.345179796218872, Test_Loss: 1.3390153646469116 *\n",
      "Epoch: 15, Train_Loss: 1.3277597427368164, Test_Loss: 1.3461123704910278\n",
      "Epoch: 15, Train_Loss: 1.3374847173690796, Test_Loss: 1.3433773517608643 *\n",
      "Epoch: 15, Train_Loss: 1.39729905128479, Test_Loss: 1.4154208898544312\n",
      "Epoch: 15, Train_Loss: 1.3802400827407837, Test_Loss: 1.5971356630325317\n",
      "Epoch: 15, Train_Loss: 1.3740659952163696, Test_Loss: 1.764304757118225\n",
      "Epoch: 15, Train_Loss: 1.3642088174819946, Test_Loss: 1.4655321836471558 *\n",
      "Epoch: 15, Train_Loss: 1.40224027633667, Test_Loss: 1.3513470888137817 *\n",
      "Epoch: 15, Train_Loss: 1.3580844402313232, Test_Loss: 1.3479728698730469 *\n",
      "Epoch: 15, Train_Loss: 1.3868995904922485, Test_Loss: 1.3572816848754883\n",
      "Epoch: 15, Train_Loss: 1.355872631072998, Test_Loss: 1.4514715671539307\n",
      "Epoch: 15, Train_Loss: 1.4908498525619507, Test_Loss: 2.3321118354797363\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 1.351204514503479, Test_Loss: 2.4247148036956787\n",
      "Epoch: 15, Train_Loss: 1.3230619430541992, Test_Loss: 1.3816040754318237 *\n",
      "Epoch: 15, Train_Loss: 1.325095295906067, Test_Loss: 1.3796674013137817 *\n",
      "Epoch: 15, Train_Loss: 1.321911096572876, Test_Loss: 1.3264373540878296 *\n",
      "Epoch: 15, Train_Loss: 1.3196417093276978, Test_Loss: 1.331710934638977\n",
      "Epoch: 15, Train_Loss: 1.322401523590088, Test_Loss: 1.3350857496261597\n",
      "Epoch: 15, Train_Loss: 2.296034097671509, Test_Loss: 1.3430376052856445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 5.103365421295166, Test_Loss: 1.3732171058654785\n",
      "Epoch: 15, Train_Loss: 1.319005012512207, Test_Loss: 1.3398798704147339 *\n",
      "Epoch: 15, Train_Loss: 1.3245435953140259, Test_Loss: 1.325472116470337 *\n",
      "Epoch: 15, Train_Loss: 1.322064757347107, Test_Loss: 1.419694185256958\n",
      "Epoch: 15, Train_Loss: 1.3189609050750732, Test_Loss: 1.7447224855422974\n",
      "Epoch: 15, Train_Loss: 1.318260908126831, Test_Loss: 1.5574445724487305 *\n",
      "Epoch: 15, Train_Loss: 1.3155312538146973, Test_Loss: 1.38237464427948 *\n",
      "Epoch: 15, Train_Loss: 1.3136656284332275, Test_Loss: 1.32591712474823 *\n",
      "Epoch: 15, Train_Loss: 1.3137030601501465, Test_Loss: 1.328037142753601\n",
      "Epoch: 15, Train_Loss: 1.3218278884887695, Test_Loss: 1.3281341791152954\n",
      "Epoch: 15, Train_Loss: 1.3703824281692505, Test_Loss: 1.330929160118103\n",
      "Epoch: 15, Train_Loss: 1.3631551265716553, Test_Loss: 1.4593572616577148\n",
      "Epoch: 15, Train_Loss: 1.3868443965911865, Test_Loss: 6.348165512084961\n",
      "Epoch: 15, Train_Loss: 1.3522077798843384, Test_Loss: 1.4154959917068481 *\n",
      "Epoch: 15, Train_Loss: 1.3161697387695312, Test_Loss: 1.3206504583358765 *\n",
      "Epoch: 15, Train_Loss: 1.4784067869186401, Test_Loss: 1.3146092891693115 *\n",
      "Epoch: 15, Train_Loss: 1.5496604442596436, Test_Loss: 1.315350890159607\n",
      "Epoch: 15, Train_Loss: 1.5252723693847656, Test_Loss: 1.3173964023590088\n",
      "Epoch: 15, Train_Loss: 1.4413048028945923, Test_Loss: 1.3135160207748413 *\n",
      "Epoch: 15, Train_Loss: 1.3117343187332153, Test_Loss: 1.3202979564666748\n",
      "Epoch: 15, Train_Loss: 1.3067102432250977, Test_Loss: 1.3103793859481812 *\n",
      "Epoch: 15, Train_Loss: 1.3142694234848022, Test_Loss: 1.309910774230957 *\n",
      "Epoch: 15, Train_Loss: 1.3190579414367676, Test_Loss: 1.3185219764709473\n",
      "Epoch: 15, Train_Loss: 1.3133388757705688, Test_Loss: 1.323451280593872\n",
      "Epoch: 15, Train_Loss: 1.3079701662063599, Test_Loss: 1.3176254034042358 *\n",
      "Epoch: 15, Train_Loss: 1.3065860271453857, Test_Loss: 1.3146028518676758 *\n",
      "Epoch: 15, Train_Loss: 1.310091495513916, Test_Loss: 1.3250970840454102\n",
      "Epoch: 15, Train_Loss: 1.3100285530090332, Test_Loss: 1.3174223899841309 *\n",
      "Epoch: 15, Train_Loss: 1.3324841260910034, Test_Loss: 1.3110336065292358 *\n",
      "Epoch: 15, Train_Loss: 1.4676530361175537, Test_Loss: 1.3247226476669312\n",
      "Epoch: 15, Train_Loss: 1.4240080118179321, Test_Loss: 1.3220739364624023 *\n",
      "Epoch: 15, Train_Loss: 1.4093974828720093, Test_Loss: 1.3154098987579346 *\n",
      "Epoch: 15, Train_Loss: 1.4189374446868896, Test_Loss: 1.3172154426574707\n",
      "Epoch: 15, Train_Loss: 1.4386552572250366, Test_Loss: 1.333764672279358\n",
      "Epoch: 15, Train_Loss: 1.4231648445129395, Test_Loss: 1.3431121110916138\n",
      "Epoch: 15, Train_Loss: 1.488358497619629, Test_Loss: 1.3420097827911377 *\n",
      "Epoch: 15, Train_Loss: 1.4918140172958374, Test_Loss: 1.3241163492202759 *\n",
      "Epoch: 15, Train_Loss: 1.6664361953735352, Test_Loss: 1.3315141201019287\n",
      "Epoch: 15, Train_Loss: 1.3099868297576904, Test_Loss: 1.3250346183776855 *\n",
      "Epoch: 15, Train_Loss: 1.3118526935577393, Test_Loss: 1.3170720338821411 *\n",
      "Epoch: 15, Train_Loss: 4.419611930847168, Test_Loss: 1.3081902265548706 *\n",
      "Epoch: 15, Train_Loss: 1.6424901485443115, Test_Loss: 1.3172991275787354\n",
      "Epoch: 15, Train_Loss: 1.3355337381362915, Test_Loss: 1.3627755641937256\n",
      "Epoch: 15, Train_Loss: 1.343724012374878, Test_Loss: 3.2264509201049805\n",
      "Epoch: 15, Train_Loss: 1.334344506263733, Test_Loss: 4.902225971221924\n",
      "Epoch: 15, Train_Loss: 1.3177728652954102, Test_Loss: 1.3004628419876099 *\n",
      "Epoch: 15, Train_Loss: 1.2978368997573853, Test_Loss: 1.295018196105957 *\n",
      "Epoch: 15, Train_Loss: 1.3413275480270386, Test_Loss: 1.3526653051376343\n",
      "Epoch: 15, Train_Loss: 1.4123964309692383, Test_Loss: 1.3372236490249634 *\n",
      "Epoch: 15, Train_Loss: 1.3749970197677612, Test_Loss: 1.346863031387329\n",
      "Epoch: 15, Train_Loss: 1.3479032516479492, Test_Loss: 1.3256287574768066 *\n",
      "Epoch: 15, Train_Loss: 1.3610833883285522, Test_Loss: 1.3945083618164062\n",
      "Epoch: 15, Train_Loss: 1.310878872871399, Test_Loss: 1.3029201030731201 *\n",
      "Epoch: 15, Train_Loss: 1.3183934688568115, Test_Loss: 1.3075448274612427\n",
      "Epoch: 15, Train_Loss: 1.2989275455474854, Test_Loss: 1.3162885904312134\n",
      "Epoch: 15, Train_Loss: 1.3251893520355225, Test_Loss: 1.3253700733184814\n",
      "Epoch: 15, Train_Loss: 1.321597933769226, Test_Loss: 1.3009930849075317 *\n",
      "Epoch: 15, Train_Loss: 1.2930206060409546, Test_Loss: 1.3744544982910156\n",
      "Epoch: 15, Train_Loss: 1.3022664785385132, Test_Loss: 1.3860892057418823\n",
      "Epoch: 15, Train_Loss: 1.322057843208313, Test_Loss: 1.3441619873046875 *\n",
      "Epoch: 15, Train_Loss: 1.3256529569625854, Test_Loss: 1.3714449405670166\n",
      "Epoch: 15, Train_Loss: 1.291791558265686, Test_Loss: 1.3189047574996948 *\n",
      "Epoch: 15, Train_Loss: 1.287043809890747, Test_Loss: 1.3354504108428955\n",
      "Epoch: 15, Train_Loss: 1.288010835647583, Test_Loss: 1.306931734085083 *\n",
      "Epoch: 15, Train_Loss: 1.288476824760437, Test_Loss: 1.2987418174743652 *\n",
      "Epoch: 15, Train_Loss: 1.287115216255188, Test_Loss: 1.3071867227554321\n",
      "Epoch: 15, Train_Loss: 1.287869930267334, Test_Loss: 1.3188965320587158\n",
      "Epoch: 15, Train_Loss: 1.2869517803192139, Test_Loss: 1.3106257915496826 *\n",
      "Epoch: 15, Train_Loss: 1.285956859588623, Test_Loss: 1.3091202974319458 *\n",
      "Epoch: 15, Train_Loss: 1.2876509428024292, Test_Loss: 1.3115298748016357\n",
      "Epoch: 15, Train_Loss: 1.28850257396698, Test_Loss: 1.3032491207122803 *\n",
      "Epoch: 15, Train_Loss: 1.290164828300476, Test_Loss: 1.3117820024490356\n",
      "Epoch: 15, Train_Loss: 1.3056819438934326, Test_Loss: 1.286783218383789 *\n",
      "Epoch: 15, Train_Loss: 1.3036558628082275, Test_Loss: 1.302474021911621\n",
      "Epoch: 15, Train_Loss: 1.3004518747329712, Test_Loss: 1.329858660697937\n",
      "Epoch: 15, Train_Loss: 1.3014191389083862, Test_Loss: 1.509883165359497\n",
      "Epoch: 15, Train_Loss: 1.2985931634902954, Test_Loss: 1.3798280954360962 *\n",
      "Epoch: 15, Train_Loss: 1.2862454652786255, Test_Loss: 1.313727617263794 *\n",
      "Epoch: 15, Train_Loss: 1.2833921909332275, Test_Loss: 1.3664085865020752\n",
      "Epoch: 15, Train_Loss: 1.297470211982727, Test_Loss: 1.5176690816879272\n",
      "Epoch: 15, Train_Loss: 1.306014895439148, Test_Loss: 1.5235956907272339\n",
      "Epoch: 15, Train_Loss: 1.2794908285140991, Test_Loss: 1.289710283279419 *\n",
      "Epoch: 15, Train_Loss: 1.285778522491455, Test_Loss: 1.4505904912948608\n",
      "Epoch: 15, Train_Loss: 1.2790800333023071, Test_Loss: 1.5491886138916016\n",
      "Epoch: 15, Train_Loss: 1.3224148750305176, Test_Loss: 1.3107202053070068 *\n",
      "Epoch: 15, Train_Loss: 1.3224161863327026, Test_Loss: 1.3261114358901978\n",
      "Epoch: 15, Train_Loss: 1.3312978744506836, Test_Loss: 1.2777920961380005 *\n",
      "Epoch: 15, Train_Loss: 1.2996528148651123, Test_Loss: 1.3203741312026978\n",
      "Epoch: 15, Train_Loss: 1.2779333591461182, Test_Loss: 1.2945832014083862 *\n",
      "Epoch: 15, Train_Loss: 1.3456774950027466, Test_Loss: 1.9606324434280396\n",
      "Epoch: 15, Train_Loss: 1.2786033153533936, Test_Loss: 1.5190162658691406 *\n",
      "Epoch: 15, Train_Loss: 1.2810611724853516, Test_Loss: 1.9977242946624756\n",
      "Epoch: 15, Train_Loss: 1.3006267547607422, Test_Loss: 2.0516719818115234\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 1.285502552986145, Test_Loss: 1.4429017305374146 *\n",
      "Epoch: 15, Train_Loss: 1.4090815782546997, Test_Loss: 1.8294272422790527\n",
      "Epoch: 15, Train_Loss: 1.341779112815857, Test_Loss: 1.3625285625457764 *\n",
      "Epoch: 15, Train_Loss: 1.307236671447754, Test_Loss: 1.2821582555770874 *\n",
      "Epoch: 15, Train_Loss: 1.2839572429656982, Test_Loss: 1.2949881553649902\n",
      "Epoch: 15, Train_Loss: 1.2930893898010254, Test_Loss: 1.4369370937347412\n",
      "Epoch: 15, Train_Loss: 1.275818109512329, Test_Loss: 1.5396535396575928\n",
      "Epoch: 15, Train_Loss: 1.273900032043457, Test_Loss: 2.0593528747558594\n",
      "Epoch: 15, Train_Loss: 1.279705286026001, Test_Loss: 2.155608654022217\n",
      "Epoch: 15, Train_Loss: 1.2826329469680786, Test_Loss: 2.882120132446289\n",
      "Epoch: 15, Train_Loss: 1.3113491535186768, Test_Loss: 2.025585651397705 *\n",
      "Epoch: 15, Train_Loss: 1.3457751274108887, Test_Loss: 1.8007137775421143 *\n",
      "Epoch: 15, Train_Loss: 1.292130947113037, Test_Loss: 1.2717196941375732 *\n",
      "Epoch: 15, Train_Loss: 1.3170149326324463, Test_Loss: 1.2843519449234009\n",
      "Epoch: 15, Train_Loss: 1.294744610786438, Test_Loss: 1.9842443466186523\n",
      "Epoch: 15, Train_Loss: 1.2845345735549927, Test_Loss: 2.638571262359619\n",
      "Epoch: 15, Train_Loss: 1.3704415559768677, Test_Loss: 1.3258401155471802 *\n",
      "Epoch: 15, Train_Loss: 1.4985625743865967, Test_Loss: 1.3639644384384155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 1.268784999847412, Test_Loss: 1.2670215368270874 *\n",
      "Epoch: 15, Train_Loss: 1.3054604530334473, Test_Loss: 1.480412483215332\n",
      "Epoch: 15, Train_Loss: 1.2617690563201904, Test_Loss: 1.556125521659851\n",
      "Epoch: 15, Train_Loss: 1.2618637084960938, Test_Loss: 1.8014788627624512\n",
      "Epoch: 15, Train_Loss: 1.261350393295288, Test_Loss: 2.4332327842712402\n",
      "Epoch: 15, Train_Loss: 1.2623060941696167, Test_Loss: 1.5798144340515137 *\n",
      "Epoch: 15, Train_Loss: 1.2705860137939453, Test_Loss: 1.2628378868103027 *\n",
      "Epoch: 15, Train_Loss: 1.2782542705535889, Test_Loss: 1.268259048461914\n",
      "Epoch: 15, Train_Loss: 1.2731436491012573, Test_Loss: 1.281855583190918\n",
      "Epoch: 15, Train_Loss: 1.2717604637145996, Test_Loss: 1.3010313510894775\n",
      "Epoch: 15, Train_Loss: 1.2776209115982056, Test_Loss: 1.5523103475570679\n",
      "Epoch: 15, Train_Loss: 1.2606762647628784, Test_Loss: 1.8141217231750488\n",
      "Epoch: 15, Train_Loss: 1.2596772909164429, Test_Loss: 1.5117638111114502 *\n",
      "Epoch: 15, Train_Loss: 1.258131742477417, Test_Loss: 1.3466233015060425 *\n",
      "Epoch: 15, Train_Loss: 1.2839508056640625, Test_Loss: 1.2936660051345825 *\n",
      "Epoch: 15, Train_Loss: 1.2871893644332886, Test_Loss: 1.265740156173706 *\n",
      "Epoch: 15, Train_Loss: 1.2745418548583984, Test_Loss: 1.3397988080978394\n",
      "Epoch: 15, Train_Loss: 1.2646796703338623, Test_Loss: 1.9848599433898926\n",
      "Epoch: 15, Train_Loss: 1.32399582862854, Test_Loss: 2.5212717056274414\n",
      "Epoch: 15, Train_Loss: 1.2953423261642456, Test_Loss: 1.4059851169586182 *\n",
      "Epoch: 15, Train_Loss: 1.2776139974594116, Test_Loss: 1.3534109592437744 *\n",
      "Epoch: 15, Train_Loss: 1.2723195552825928, Test_Loss: 1.2543290853500366 *\n",
      "Epoch: 15, Train_Loss: 1.2816075086593628, Test_Loss: 1.2608402967453003\n",
      "Epoch: 15, Train_Loss: 1.258256196975708, Test_Loss: 1.2562450170516968 *\n",
      "Epoch: 15, Train_Loss: 1.26144540309906, Test_Loss: 1.2685703039169312\n",
      "Epoch: 15, Train_Loss: 1.274898886680603, Test_Loss: 1.2810442447662354\n",
      "Epoch: 15, Train_Loss: 1.2947447299957275, Test_Loss: 1.2885777950286865\n",
      "Epoch: 15, Train_Loss: 3.312865734100342, Test_Loss: 1.256945013999939 *\n",
      "Epoch: 15, Train_Loss: 4.646035671234131, Test_Loss: 1.3636842966079712\n",
      "Epoch: 15, Train_Loss: 1.2616546154022217, Test_Loss: 1.6432652473449707\n",
      "Epoch: 15, Train_Loss: 1.2621663808822632, Test_Loss: 1.3576867580413818 *\n",
      "Epoch: 15, Train_Loss: 1.2934153079986572, Test_Loss: 1.4362826347351074\n",
      "Epoch: 15, Train_Loss: 1.4380640983581543, Test_Loss: 1.2611689567565918 *\n",
      "Epoch: 15, Train_Loss: 1.2803372144699097, Test_Loss: 1.258469581604004 *\n",
      "Epoch: 15, Train_Loss: 1.2533526420593262, Test_Loss: 1.2564935684204102 *\n",
      "Epoch: 15, Train_Loss: 1.2473220825195312, Test_Loss: 1.2558131217956543 *\n",
      "Epoch: 15, Train_Loss: 1.312432885169983, Test_Loss: 1.2775957584381104\n",
      "Epoch: 15, Train_Loss: 1.2538360357284546, Test_Loss: 5.947524547576904\n",
      "Epoch: 15, Train_Loss: 1.2688555717468262, Test_Loss: 1.8638551235198975 *\n",
      "Epoch: 15, Train_Loss: 1.957620620727539, Test_Loss: 1.254744529724121 *\n",
      "Epoch: 15, Train_Loss: 2.5866875648498535, Test_Loss: 1.2471038103103638 *\n",
      "Epoch: 15, Train_Loss: 1.9466309547424316, Test_Loss: 1.2485098838806152\n",
      "Epoch: 15, Train_Loss: 1.3675282001495361, Test_Loss: 1.2534971237182617\n",
      "Epoch: 15, Train_Loss: 1.8421630859375, Test_Loss: 1.2460418939590454 *\n",
      "Epoch: 15, Train_Loss: 3.6723403930664062, Test_Loss: 1.2482619285583496\n",
      "Epoch: 15, Train_Loss: 1.7522386312484741, Test_Loss: 1.241879940032959 *\n",
      "Epoch: 15, Train_Loss: 1.2947572469711304, Test_Loss: 1.2434991598129272\n",
      "Epoch: 15, Train_Loss: 1.2698830366134644, Test_Loss: 1.2485007047653198\n",
      "Epoch: 15, Train_Loss: 2.4773430824279785, Test_Loss: 1.26016104221344\n",
      "Epoch: 15, Train_Loss: 2.6978416442871094, Test_Loss: 1.2465988397598267 *\n",
      "Epoch: 15, Train_Loss: 1.4985921382904053, Test_Loss: 1.256034016609192\n",
      "Epoch: 15, Train_Loss: 1.2551475763320923, Test_Loss: 1.2550628185272217 *\n",
      "Epoch: 15, Train_Loss: 1.2422627210617065, Test_Loss: 1.2490944862365723 *\n",
      "Epoch: 15, Train_Loss: 1.8726356029510498, Test_Loss: 1.2438745498657227 *\n",
      "Epoch: 15, Train_Loss: 1.4702162742614746, Test_Loss: 1.2612730264663696\n",
      "Epoch: 15, Train_Loss: 1.2841746807098389, Test_Loss: 1.244997262954712 *\n",
      "Epoch: 15, Train_Loss: 1.3173775672912598, Test_Loss: 1.241359829902649 *\n",
      "Epoch: 15, Train_Loss: 1.4295063018798828, Test_Loss: 1.2436400651931763\n",
      "Epoch: 15, Train_Loss: 1.3710072040557861, Test_Loss: 1.245385766029358\n",
      "Epoch: 15, Train_Loss: 1.344282627105713, Test_Loss: 1.2613575458526611\n",
      "Epoch: 15, Train_Loss: 1.6436249017715454, Test_Loss: 1.2630187273025513\n",
      "Epoch: 15, Train_Loss: 1.3085064888000488, Test_Loss: 1.2474908828735352 *\n",
      "Epoch: 15, Train_Loss: 1.3126883506774902, Test_Loss: 1.2522755861282349\n",
      "Epoch: 15, Train_Loss: 1.4747906923294067, Test_Loss: 1.2398638725280762 *\n",
      "Epoch: 15, Train_Loss: 1.520413875579834, Test_Loss: 1.2390094995498657 *\n",
      "Epoch: 15, Train_Loss: 1.539368987083435, Test_Loss: 1.2466470003128052\n",
      "Epoch: 15, Train_Loss: 1.372595191001892, Test_Loss: 1.2509163618087769\n",
      "Epoch: 15, Train_Loss: 1.3012124300003052, Test_Loss: 1.313569188117981\n",
      "Epoch: 15, Train_Loss: 1.346031904220581, Test_Loss: 1.7405142784118652\n",
      "Epoch: 15, Train_Loss: 1.2770626544952393, Test_Loss: 6.493298053741455\n",
      "Epoch: 15, Train_Loss: 1.2470293045043945, Test_Loss: 1.2400603294372559 *\n",
      "Epoch: 15, Train_Loss: 1.2309796810150146, Test_Loss: 1.247695803642273\n",
      "Epoch: 15, Train_Loss: 1.2333165407180786, Test_Loss: 1.2710233926773071\n",
      "Epoch: 15, Train_Loss: 1.2319462299346924, Test_Loss: 1.250532627105713 *\n",
      "Epoch: 15, Train_Loss: 1.2371577024459839, Test_Loss: 1.2594678401947021\n",
      "Epoch: 15, Train_Loss: 1.2871249914169312, Test_Loss: 1.2406044006347656 *\n",
      "Epoch: 15, Train_Loss: 1.2713954448699951, Test_Loss: 1.342680811882019\n",
      "Epoch: 15, Train_Loss: 1.2886569499969482, Test_Loss: 1.2566945552825928 *\n",
      "Epoch: 15, Train_Loss: 1.3388378620147705, Test_Loss: 1.2424256801605225 *\n",
      "Epoch: 15, Train_Loss: 1.6413891315460205, Test_Loss: 1.2523064613342285\n",
      "Epoch: 15, Train_Loss: 1.2443667650222778, Test_Loss: 1.2735486030578613\n",
      "Epoch: 15, Train_Loss: 1.2836145162582397, Test_Loss: 1.2328184843063354 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 1.3591700792312622, Test_Loss: 1.2870744466781616\n",
      "Epoch: 15, Train_Loss: 1.742664098739624, Test_Loss: 1.2698644399642944 *\n",
      "Epoch: 15, Train_Loss: 1.5611389875411987, Test_Loss: 1.2946231365203857\n",
      "Epoch: 15, Train_Loss: 1.236754059791565, Test_Loss: 1.2842159271240234 *\n",
      "Epoch: 15, Train_Loss: 1.257771372795105, Test_Loss: 1.261308193206787 *\n",
      "Epoch: 15, Train_Loss: 1.740605115890503, Test_Loss: 1.2861067056655884\n",
      "Epoch: 15, Train_Loss: 1.6904762983322144, Test_Loss: 1.2246023416519165 *\n",
      "Epoch: 15, Train_Loss: 1.3030102252960205, Test_Loss: 1.22554349899292\n",
      "Epoch: 15, Train_Loss: 1.246047854423523, Test_Loss: 1.2316731214523315\n",
      "Epoch: 15, Train_Loss: 1.2355867624282837, Test_Loss: 1.2346199750900269\n",
      "Epoch: 15, Train_Loss: 2.069582939147949, Test_Loss: 1.2421538829803467\n",
      "Epoch: 15, Train_Loss: 2.3224573135375977, Test_Loss: 1.2308881282806396 *\n",
      "Epoch: 15, Train_Loss: 1.2617506980895996, Test_Loss: 1.2251135110855103 *\n",
      "Epoch: 15, Train_Loss: 1.2487643957138062, Test_Loss: 1.2266918420791626\n",
      "Epoch: 15, Train_Loss: 1.2247871160507202, Test_Loss: 1.226719856262207\n",
      "Epoch: 15, Train_Loss: 1.2310316562652588, Test_Loss: 1.2668848037719727\n",
      "Epoch: 15, Train_Loss: 1.6384987831115723, Test_Loss: 1.231703758239746 *\n",
      "Epoch: 15, Train_Loss: 1.2357531785964966, Test_Loss: 1.2401182651519775\n",
      "Epoch: 15, Train_Loss: 1.2473746538162231, Test_Loss: 1.3957664966583252\n",
      "Epoch: 15, Train_Loss: 1.2462743520736694, Test_Loss: 1.5415937900543213\n",
      "Epoch: 15, Train_Loss: 1.24647057056427, Test_Loss: 1.2478485107421875 *\n",
      "Epoch: 15, Train_Loss: 18.357786178588867, Test_Loss: 1.2541165351867676\n",
      "Epoch: 15, Train_Loss: 1.2194889783859253, Test_Loss: 1.3244032859802246\n",
      "Epoch: 15, Train_Loss: 3.2518348693847656, Test_Loss: 1.399160385131836\n",
      "Epoch: 15, Train_Loss: 2.9025802612304688, Test_Loss: 1.23513925075531 *\n",
      "Epoch: 15, Train_Loss: 1.226352334022522, Test_Loss: 1.318111538887024\n",
      "Epoch: 15, Train_Loss: 1.3289778232574463, Test_Loss: 1.361890435218811\n",
      "Epoch: 15, Train_Loss: 5.200439453125, Test_Loss: 1.3801820278167725\n",
      "Epoch: 15, Train_Loss: 8.47423267364502, Test_Loss: 1.3386521339416504 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 1.2902593612670898, Test_Loss: 1.2283519506454468 *\n",
      "Epoch: 15, Train_Loss: 1.2654651403427124, Test_Loss: 1.2299062013626099\n",
      "Epoch: 15, Train_Loss: 7.131446838378906, Test_Loss: 1.23389732837677\n",
      "Epoch: 15, Train_Loss: 1.2854187488555908, Test_Loss: 1.7401683330535889\n",
      "Epoch: 15, Train_Loss: 1.2529929876327515, Test_Loss: 1.7366933822631836 *\n",
      "Epoch: 15, Train_Loss: 1.2180023193359375, Test_Loss: 1.5373127460479736 *\n",
      "Epoch: 15, Train_Loss: 1.2172980308532715, Test_Loss: 1.7349481582641602\n",
      "Epoch: 15, Train_Loss: 1.2369529008865356, Test_Loss: 1.38724946975708 *\n",
      "Epoch: 15, Train_Loss: 1.2131555080413818, Test_Loss: 1.7994916439056396\n",
      "Epoch: 15, Train_Loss: 1.2280993461608887, Test_Loss: 1.5160614252090454 *\n",
      "Epoch: 15, Train_Loss: 1.2154006958007812, Test_Loss: 1.294630765914917 *\n",
      "Epoch: 15, Train_Loss: 1.2198830842971802, Test_Loss: 1.2659615278244019 *\n",
      "Epoch: 15, Train_Loss: 1.2623369693756104, Test_Loss: 1.247238039970398 *\n",
      "Epoch: 15, Train_Loss: 1.2438607215881348, Test_Loss: 1.434728741645813\n",
      "Epoch: 15, Train_Loss: 1.2468512058258057, Test_Loss: 2.3147802352905273\n",
      "Epoch: 15, Train_Loss: 1.2948561906814575, Test_Loss: 1.5908803939819336 *\n",
      "Epoch: 15, Train_Loss: 1.322845458984375, Test_Loss: 2.664701461791992\n",
      "Epoch: 15, Train_Loss: 1.2216365337371826, Test_Loss: 1.915855884552002 *\n",
      "Epoch: 15, Train_Loss: 1.218920111656189, Test_Loss: 2.1520352363586426\n",
      "Epoch: 15, Train_Loss: 1.2157942056655884, Test_Loss: 1.3336490392684937 *\n",
      "Epoch: 15, Train_Loss: 1.2208274602890015, Test_Loss: 1.2407419681549072 *\n",
      "Epoch: 15, Train_Loss: 1.208166241645813, Test_Loss: 1.5225780010223389\n",
      "Epoch: 15, Train_Loss: 1.203542947769165, Test_Loss: 2.40854811668396\n",
      "Epoch: 15, Train_Loss: 1.202439785003662, Test_Loss: 1.4593260288238525 *\n",
      "Epoch: 15, Train_Loss: 1.2019281387329102, Test_Loss: 1.423802137374878 *\n",
      "Epoch: 15, Train_Loss: 1.201434850692749, Test_Loss: 1.2071911096572876 *\n",
      "Epoch: 15, Train_Loss: 1.1987885236740112, Test_Loss: 1.2860027551651\n",
      "Epoch: 15, Train_Loss: 1.2014888525009155, Test_Loss: 1.5426791906356812\n",
      "Epoch: 15, Train_Loss: 1.2068960666656494, Test_Loss: 1.6248605251312256\n",
      "Epoch: 15, Train_Loss: 1.2140578031539917, Test_Loss: 2.404425859451294\n",
      "Epoch: 15, Train_Loss: 1.2417699098587036, Test_Loss: 1.635615348815918 *\n",
      "Epoch: 15, Train_Loss: 1.2454204559326172, Test_Loss: 1.2316272258758545 *\n",
      "Epoch: 15, Train_Loss: 1.213163137435913, Test_Loss: 1.232181429862976\n",
      "Epoch: 15, Train_Loss: 2.885685443878174, Test_Loss: 1.2404576539993286\n",
      "Epoch: 15, Train_Loss: 8.243027687072754, Test_Loss: 1.2830685377120972\n",
      "Epoch: 15, Train_Loss: 1.214598536491394, Test_Loss: 1.3671811819076538\n",
      "Epoch: 15, Train_Loss: 1.2457919120788574, Test_Loss: 1.523937463760376\n",
      "Epoch: 15, Train_Loss: 1.2801580429077148, Test_Loss: 1.416499137878418 *\n",
      "Epoch: 15, Train_Loss: 1.250538945198059, Test_Loss: 1.2593927383422852 *\n",
      "Epoch: 15, Train_Loss: 1.2335530519485474, Test_Loss: 1.212756633758545 *\n",
      "Epoch: 15, Train_Loss: 1.2531777620315552, Test_Loss: 1.21938157081604\n",
      "Epoch: 15, Train_Loss: 1.3040412664413452, Test_Loss: 1.2896811962127686\n",
      "Epoch: 15, Train_Loss: 1.4788615703582764, Test_Loss: 1.7645045518875122\n",
      "Epoch: 15, Train_Loss: 1.3516918420791626, Test_Loss: 2.594961166381836\n",
      "Epoch: 15, Train_Loss: 1.2571566104888916, Test_Loss: 1.6556819677352905 *\n",
      "Epoch: 15, Train_Loss: 1.2324941158294678, Test_Loss: 1.2520853281021118 *\n",
      "Epoch: 15, Train_Loss: 1.3742568492889404, Test_Loss: 1.2244843244552612 *\n",
      "Epoch: 15, Train_Loss: 1.368513584136963, Test_Loss: 1.2359994649887085\n",
      "Epoch: 15, Train_Loss: 1.4021506309509277, Test_Loss: 1.238282322883606\n",
      "Epoch: 15, Train_Loss: 1.278770923614502, Test_Loss: 1.2386353015899658\n",
      "Epoch: 15, Train_Loss: 1.2545017004013062, Test_Loss: 1.2354543209075928 *\n",
      "Epoch: 15, Train_Loss: 1.1930592060089111, Test_Loss: 1.258621096611023\n",
      "Epoch: 15, Train_Loss: 1.219982624053955, Test_Loss: 1.2009257078170776 *\n",
      "Epoch: 15, Train_Loss: 1.2651491165161133, Test_Loss: 1.254989743232727\n",
      "Epoch: 15, Train_Loss: 1.219329595565796, Test_Loss: 1.483882188796997\n",
      "Epoch: 15, Train_Loss: 1.193845510482788, Test_Loss: 1.3464149236679077 *\n",
      "Epoch: 15, Train_Loss: 1.1983176469802856, Test_Loss: 1.3984074592590332\n",
      "Epoch: 15, Train_Loss: 1.208286166191101, Test_Loss: 1.1951605081558228 *\n",
      "Epoch: 15, Train_Loss: 3.001634120941162, Test_Loss: 1.1867810487747192 *\n",
      "Epoch: 15, Train_Loss: 5.086780548095703, Test_Loss: 1.186563491821289 *\n",
      "Epoch: 15, Train_Loss: 1.1903012990951538, Test_Loss: 1.187656283378601\n",
      "Epoch: 15, Train_Loss: 1.2369869947433472, Test_Loss: 1.1984602212905884\n",
      "Epoch: 15, Train_Loss: 1.2131178379058838, Test_Loss: 4.5887250900268555\n",
      "Epoch: 15, Train_Loss: 1.1967324018478394, Test_Loss: 3.4160914421081543 *\n",
      "Epoch: 15, Train_Loss: 1.188901662826538, Test_Loss: 1.2023276090621948 *\n",
      "Epoch: 15, Train_Loss: 1.1910408735275269, Test_Loss: 1.2103976011276245\n",
      "Epoch: 15, Train_Loss: 1.2018061876296997, Test_Loss: 1.2104952335357666\n",
      "Epoch: 15, Train_Loss: 1.2020975351333618, Test_Loss: 1.186547875404358 *\n",
      "Epoch: 15, Train_Loss: 1.243683934211731, Test_Loss: 1.2312216758728027\n",
      "Epoch: 15, Train_Loss: 1.1939783096313477, Test_Loss: 1.238539695739746\n",
      "Epoch: 15, Train_Loss: 1.181786298751831, Test_Loss: 1.226266622543335 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 1.1803100109100342, Test_Loss: 1.234216570854187\n",
      "Epoch: 15, Train_Loss: 1.1978555917739868, Test_Loss: 1.23795485496521\n",
      "Epoch: 15, Train_Loss: 1.1800975799560547, Test_Loss: 1.2294913530349731 *\n",
      "Epoch: 15, Train_Loss: 1.1804335117340088, Test_Loss: 1.2397394180297852\n",
      "Epoch: 15, Train_Loss: 1.2094833850860596, Test_Loss: 1.1936239004135132 *\n",
      "Epoch: 15, Train_Loss: 1.205830693244934, Test_Loss: 1.2070071697235107\n",
      "Epoch: 15, Train_Loss: 1.1835256814956665, Test_Loss: 1.2207072973251343\n",
      "Epoch: 15, Train_Loss: 1.1785738468170166, Test_Loss: 1.1811550855636597 *\n",
      "Epoch: 15, Train_Loss: 1.1977715492248535, Test_Loss: 1.215015172958374\n",
      "Epoch: 15, Train_Loss: 1.2259018421173096, Test_Loss: 1.1848297119140625 *\n",
      "Epoch: 15, Train_Loss: 1.2096149921417236, Test_Loss: 1.202650785446167\n",
      "Epoch: 15, Train_Loss: 1.2219579219818115, Test_Loss: 1.1993354558944702 *\n",
      "Epoch: 15, Train_Loss: 1.228531002998352, Test_Loss: 1.2047632932662964\n",
      "Epoch: 15, Train_Loss: 1.258474588394165, Test_Loss: 1.2072179317474365\n",
      "Epoch: 15, Train_Loss: 1.2018063068389893, Test_Loss: 1.2221368551254272\n",
      "Epoch: 15, Train_Loss: 1.2237215042114258, Test_Loss: 1.1949011087417603 *\n",
      "Epoch: 15, Train_Loss: 1.222319483757019, Test_Loss: 1.1994820833206177\n",
      "Epoch: 15, Train_Loss: 1.3185474872589111, Test_Loss: 1.1810023784637451 *\n",
      "Epoch: 15, Train_Loss: 1.1910903453826904, Test_Loss: 1.1902334690093994\n",
      "Epoch: 15, Train_Loss: 1.1725194454193115, Test_Loss: 1.186549425125122 *\n",
      "Epoch: 15, Train_Loss: 1.1703940629959106, Test_Loss: 1.1789038181304932 *\n",
      "Epoch: 15, Train_Loss: 1.1695363521575928, Test_Loss: 1.2523729801177979\n",
      "Epoch: 15, Train_Loss: 1.172531008720398, Test_Loss: 1.2010183334350586 *\n",
      "Epoch: 15, Train_Loss: 1.1700338125228882, Test_Loss: 6.573692321777344\n",
      "Epoch: 15, Train_Loss: 3.2417545318603516, Test_Loss: 1.412919282913208 *\n",
      "Epoch: 15, Train_Loss: 3.83290958404541, Test_Loss: 1.1717694997787476 *\n",
      "Epoch: 15, Train_Loss: 1.1743005514144897, Test_Loss: 1.1970487833023071\n",
      "Epoch: 15, Train_Loss: 1.1768319606781006, Test_Loss: 1.2292835712432861\n",
      "Epoch: 15, Train_Loss: 1.17288339138031, Test_Loss: 1.236424207687378\n",
      "Epoch: 15, Train_Loss: 1.1727783679962158, Test_Loss: 1.1758112907409668 *\n",
      "Epoch: 15, Train_Loss: 1.1715630292892456, Test_Loss: 1.2413454055786133\n",
      "Epoch: 15, Train_Loss: 1.1704156398773193, Test_Loss: 1.2196050882339478 *\n",
      "Epoch: 15, Train_Loss: 1.173340916633606, Test_Loss: 1.1696938276290894 *\n",
      "Epoch: 15, Train_Loss: 1.1657240390777588, Test_Loss: 1.1947951316833496\n",
      "Epoch: 15, Train_Loss: 1.1755627393722534, Test_Loss: 1.1861989498138428 *\n",
      "Epoch: 15, Train_Loss: 1.224753737449646, Test_Loss: 1.1737464666366577 *\n",
      "Epoch: 15, Train_Loss: 1.2143933773040771, Test_Loss: 1.18421471118927\n",
      "Epoch: 15, Train_Loss: 1.2272976636886597, Test_Loss: 1.3309582471847534\n",
      "Epoch: 15, Train_Loss: 1.1964647769927979, Test_Loss: 1.200232744216919 *\n",
      "Epoch: 15, Train_Loss: 1.1736187934875488, Test_Loss: 1.2584590911865234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 1.3891032934188843, Test_Loss: 1.2091599702835083 *\n",
      "Epoch: 15, Train_Loss: 1.41389799118042, Test_Loss: 1.2050447463989258 *\n",
      "Epoch: 15, Train_Loss: 1.3895362615585327, Test_Loss: 1.189606785774231 *\n",
      "Epoch: 15, Train_Loss: 1.2670632600784302, Test_Loss: 1.1819169521331787 *\n",
      "Epoch: 15, Train_Loss: 1.1629700660705566, Test_Loss: 1.1786603927612305 *\n",
      "Epoch: 15, Train_Loss: 1.1602321863174438, Test_Loss: 1.1801578998565674\n",
      "Epoch: 15, Train_Loss: 1.1666024923324585, Test_Loss: 1.1804262399673462\n",
      "Epoch: 15, Train_Loss: 1.1749240159988403, Test_Loss: 1.1810038089752197\n",
      "Epoch: 15, Train_Loss: 1.1750425100326538, Test_Loss: 1.1773375272750854 *\n",
      "Epoch: 15, Train_Loss: 1.168927550315857, Test_Loss: 1.1883772611618042\n",
      "Epoch: 15, Train_Loss: 1.1582648754119873, Test_Loss: 1.1897616386413574\n",
      "Epoch: 15, Train_Loss: 1.1571835279464722, Test_Loss: 1.1678297519683838 *\n",
      "Epoch: 15, Train_Loss: 1.1648651361465454, Test_Loss: 1.1728171110153198\n",
      "Epoch: 15, Train_Loss: 1.2226698398590088, Test_Loss: 1.2310583591461182\n",
      "Epoch: 16, Train_Loss: 1.355977177619934, Test_Loss: 1.1811803579330444 *\n",
      "Epoch: 16, Train_Loss: 1.3316304683685303, Test_Loss: 1.4933087825775146\n",
      "Epoch: 16, Train_Loss: 1.2846482992172241, Test_Loss: 1.1702109575271606 *\n",
      "Epoch: 16, Train_Loss: 1.261294960975647, Test_Loss: 1.2272098064422607\n",
      "Epoch: 16, Train_Loss: 1.3013139963150024, Test_Loss: 1.2349151372909546\n",
      "Epoch: 16, Train_Loss: 1.2029173374176025, Test_Loss: 1.5439776182174683\n",
      "Epoch: 16, Train_Loss: 1.3021608591079712, Test_Loss: 1.2017019987106323 *\n",
      "Epoch: 16, Train_Loss: 1.2748926877975464, Test_Loss: 1.2898716926574707\n",
      "Epoch: 16, Train_Loss: 1.4472591876983643, Test_Loss: 1.4193744659423828\n",
      "Epoch: 16, Train_Loss: 1.1646212339401245, Test_Loss: 1.3464349508285522 *\n",
      "Epoch: 16, Train_Loss: 1.2588835954666138, Test_Loss: 1.193580150604248 *\n",
      "Epoch: 16, Train_Loss: 4.15247106552124, Test_Loss: 1.1740760803222656 *\n",
      "Epoch: 16, Train_Loss: 1.3117209672927856, Test_Loss: 1.1652002334594727 *\n",
      "Epoch: 16, Train_Loss: 1.197920799255371, Test_Loss: 1.2006504535675049\n",
      "Epoch: 16, Train_Loss: 1.2111252546310425, Test_Loss: 1.4182487726211548\n",
      "Epoch: 16, Train_Loss: 1.2145179510116577, Test_Loss: 1.8354672193527222\n",
      "Epoch: 16, Train_Loss: 1.157266616821289, Test_Loss: 1.5162287950515747 *\n",
      "Epoch: 16, Train_Loss: 1.1510634422302246, Test_Loss: 2.084388017654419\n",
      "Epoch: 16, Train_Loss: 1.2262012958526611, Test_Loss: 1.44575834274292 *\n",
      "Epoch: 16, Train_Loss: 1.2589893341064453, Test_Loss: 1.6595032215118408\n",
      "Epoch: 16, Train_Loss: 1.2401889562606812, Test_Loss: 1.4198896884918213 *\n",
      "Epoch: 16, Train_Loss: 1.2132378816604614, Test_Loss: 1.1593680381774902 *\n",
      "Epoch: 16, Train_Loss: 1.2279292345046997, Test_Loss: 1.1665681600570679\n",
      "Epoch: 16, Train_Loss: 1.1706112623214722, Test_Loss: 1.2117960453033447\n",
      "Epoch: 16, Train_Loss: 1.1789783239364624, Test_Loss: 1.3173812627792358\n",
      "Epoch: 16, Train_Loss: 1.1556100845336914, Test_Loss: 2.0808839797973633\n",
      "Epoch: 16, Train_Loss: 1.1868077516555786, Test_Loss: 1.3263888359069824 *\n",
      "Epoch: 16, Train_Loss: 1.1640348434448242, Test_Loss: 3.320918560028076\n",
      "Epoch: 16, Train_Loss: 1.143918514251709, Test_Loss: 1.70687735080719 *\n",
      "Epoch: 16, Train_Loss: 1.163989782333374, Test_Loss: 2.1330556869506836\n",
      "Epoch: 16, Train_Loss: 1.1885021924972534, Test_Loss: 1.2192564010620117 *\n",
      "Epoch: 16, Train_Loss: 1.1760494709014893, Test_Loss: 1.1475578546524048 *\n",
      "Epoch: 16, Train_Loss: 1.1453139781951904, Test_Loss: 1.3766834735870361\n",
      "Epoch: 16, Train_Loss: 1.1422125101089478, Test_Loss: 2.539203405380249\n",
      "Epoch: 16, Train_Loss: 1.1413891315460205, Test_Loss: 1.7927498817443848 *\n",
      "Epoch: 16, Train_Loss: 1.140771746635437, Test_Loss: 1.2311362028121948 *\n",
      "Epoch: 16, Train_Loss: 1.1422603130340576, Test_Loss: 1.1567590236663818 *\n",
      "Epoch: 16, Train_Loss: 1.141701579093933, Test_Loss: 1.254483938217163\n",
      "Epoch: 16, Train_Loss: 1.1410531997680664, Test_Loss: 1.579551339149475\n",
      "Epoch: 16, Train_Loss: 1.1402746438980103, Test_Loss: 1.330172061920166 *\n",
      "Epoch: 16, Train_Loss: 1.139122486114502, Test_Loss: 2.263795852661133\n",
      "Epoch: 16, Train_Loss: 1.1409639120101929, Test_Loss: 1.693100929260254 *\n",
      "Epoch: 16, Train_Loss: 1.143728494644165, Test_Loss: 1.1650121212005615 *\n",
      "Epoch: 16, Train_Loss: 1.1540958881378174, Test_Loss: 1.1441023349761963 *\n",
      "Epoch: 16, Train_Loss: 1.155932068824768, Test_Loss: 1.145422339439392\n",
      "Epoch: 16, Train_Loss: 1.1650006771087646, Test_Loss: 1.1532602310180664\n",
      "Epoch: 16, Train_Loss: 1.1502419710159302, Test_Loss: 1.210126519203186\n",
      "Epoch: 16, Train_Loss: 1.1523234844207764, Test_Loss: 1.7316455841064453\n",
      "Epoch: 16, Train_Loss: 1.1376543045043945, Test_Loss: 1.6509250402450562 *\n",
      "Epoch: 16, Train_Loss: 1.1376640796661377, Test_Loss: 1.3007285594940186 *\n",
      "Epoch: 16, Train_Loss: 1.154411792755127, Test_Loss: 1.1827044486999512 *\n",
      "Epoch: 16, Train_Loss: 1.158125877380371, Test_Loss: 1.1622651815414429 *\n",
      "Epoch: 16, Train_Loss: 1.135833740234375, Test_Loss: 1.1640851497650146\n",
      "Epoch: 16, Train_Loss: 1.1370065212249756, Test_Loss: 1.437967300415039\n",
      "Epoch: 16, Train_Loss: 1.1331355571746826, Test_Loss: 2.4324491024017334\n",
      "Epoch: 16, Train_Loss: 1.1989402770996094, Test_Loss: 1.7540733814239502 *\n",
      "Epoch: 16, Train_Loss: 1.1685808897018433, Test_Loss: 1.2229973077774048 *\n",
      "Epoch: 16, Train_Loss: 1.1836063861846924, Test_Loss: 1.1690888404846191 *\n",
      "Epoch: 16, Train_Loss: 1.1389825344085693, Test_Loss: 1.1374574899673462 *\n",
      "Epoch: 16, Train_Loss: 1.1392053365707397, Test_Loss: 1.1308400630950928 *\n",
      "Epoch: 16, Train_Loss: 1.2043581008911133, Test_Loss: 1.1386395692825317\n",
      "Epoch: 16, Train_Loss: 1.1327046155929565, Test_Loss: 1.1489789485931396\n",
      "Epoch: 16, Train_Loss: 1.1409695148468018, Test_Loss: 1.1754900217056274\n",
      "Epoch: 16, Train_Loss: 1.1501598358154297, Test_Loss: 1.1305888891220093 *\n",
      "Epoch: 16, Train_Loss: 1.1682685613632202, Test_Loss: 1.1876285076141357\n",
      "Epoch: 16, Train_Loss: 1.2301051616668701, Test_Loss: 1.2492178678512573\n",
      "Epoch: 16, Train_Loss: 1.1946946382522583, Test_Loss: 1.5024405717849731\n",
      "Epoch: 16, Train_Loss: 1.151550531387329, Test_Loss: 1.3753905296325684 *\n",
      "Epoch: 16, Train_Loss: 1.1329689025878906, Test_Loss: 1.1457228660583496 *\n",
      "Epoch: 16, Train_Loss: 1.1507899761199951, Test_Loss: 1.1396043300628662 *\n",
      "Epoch: 16, Train_Loss: 1.1254907846450806, Test_Loss: 1.140260100364685\n",
      "Epoch: 16, Train_Loss: 1.1291592121124268, Test_Loss: 1.13882577419281 *\n",
      "Epoch: 16, Train_Loss: 1.1381161212921143, Test_Loss: 1.1504392623901367\n",
      "Epoch: 16, Train_Loss: 1.149237871170044, Test_Loss: 2.7267932891845703\n",
      "Epoch: 16, Train_Loss: 1.1870192289352417, Test_Loss: 4.893173694610596\n",
      "Epoch: 16, Train_Loss: 1.1806883811950684, Test_Loss: 1.139667272567749 *\n",
      "Epoch: 16, Train_Loss: 1.163821816444397, Test_Loss: 1.1254929304122925 *\n",
      "Epoch: 16, Train_Loss: 1.1490247249603271, Test_Loss: 1.123533010482788 *\n",
      "Epoch: 16, Train_Loss: 1.155012607574463, Test_Loss: 1.132631540298462\n",
      "Epoch: 16, Train_Loss: 1.1385297775268555, Test_Loss: 1.1260106563568115 *\n",
      "Epoch: 16, Train_Loss: 1.2804372310638428, Test_Loss: 1.1277886629104614\n",
      "Epoch: 16, Train_Loss: 1.3083478212356567, Test_Loss: 1.1307265758514404\n",
      "Epoch: 16, Train_Loss: 1.1262184381484985, Test_Loss: 1.1209518909454346 *\n",
      "Epoch: 16, Train_Loss: 1.1619434356689453, Test_Loss: 1.1275297403335571\n",
      "Epoch: 16, Train_Loss: 1.1180996894836426, Test_Loss: 1.1247810125350952 *\n",
      "Epoch: 16, Train_Loss: 1.1177481412887573, Test_Loss: 1.129921555519104\n",
      "Epoch: 16, Train_Loss: 1.1193742752075195, Test_Loss: 1.1330126523971558\n",
      "Epoch: 16, Train_Loss: 1.1212157011032104, Test_Loss: 1.1279518604278564 *\n",
      "Epoch: 16, Train_Loss: 1.1259090900421143, Test_Loss: 1.129662275314331\n",
      "Epoch: 16, Train_Loss: 1.131614089012146, Test_Loss: 1.1181855201721191 *\n",
      "Epoch: 16, Train_Loss: 1.1230119466781616, Test_Loss: 1.1230556964874268\n",
      "Epoch: 16, Train_Loss: 1.126041293144226, Test_Loss: 1.119265079498291 *\n",
      "Epoch: 16, Train_Loss: 1.1363621950149536, Test_Loss: 1.1205343008041382\n",
      "Epoch: 16, Train_Loss: 1.1143598556518555, Test_Loss: 1.117872953414917 *\n",
      "Epoch: 16, Train_Loss: 1.116445779800415, Test_Loss: 1.1182864904403687\n",
      "Epoch: 16, Train_Loss: 1.1142361164093018, Test_Loss: 1.1171470880508423 *\n",
      "Epoch: 16, Train_Loss: 1.1450015306472778, Test_Loss: 1.1299792528152466\n",
      "Epoch: 16, Train_Loss: 1.1463497877120972, Test_Loss: 1.1236989498138428 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 1.1278588771820068, Test_Loss: 1.1190155744552612 *\n",
      "Epoch: 16, Train_Loss: 1.1280158758163452, Test_Loss: 1.1133999824523926 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 1.1717630624771118, Test_Loss: 1.118047833442688\n",
      "Epoch: 16, Train_Loss: 1.156497836112976, Test_Loss: 1.1152586936950684 *\n",
      "Epoch: 16, Train_Loss: 1.1228724718093872, Test_Loss: 1.1137877702713013 *\n",
      "Epoch: 16, Train_Loss: 1.1371757984161377, Test_Loss: 1.1622703075408936\n",
      "Epoch: 16, Train_Loss: 1.1283657550811768, Test_Loss: 1.1451750993728638 *\n",
      "Epoch: 16, Train_Loss: 1.1216621398925781, Test_Loss: 4.9787797927856445\n",
      "Epoch: 16, Train_Loss: 1.1167097091674805, Test_Loss: 2.692173480987549 *\n",
      "Epoch: 16, Train_Loss: 1.1392171382904053, Test_Loss: 1.1129482984542847 *\n",
      "Epoch: 16, Train_Loss: 1.1631889343261719, Test_Loss: 1.1172301769256592\n",
      "Epoch: 16, Train_Loss: 3.351820945739746, Test_Loss: 1.1602798700332642\n",
      "Epoch: 16, Train_Loss: 4.2709126472473145, Test_Loss: 1.1537009477615356 *\n",
      "Epoch: 16, Train_Loss: 1.123921513557434, Test_Loss: 1.1265374422073364 *\n",
      "Epoch: 16, Train_Loss: 1.1105362176895142, Test_Loss: 1.172049880027771\n",
      "Epoch: 16, Train_Loss: 1.1788078546524048, Test_Loss: 1.1992926597595215\n",
      "Epoch: 16, Train_Loss: 1.2673993110656738, Test_Loss: 1.109636902809143 *\n",
      "Epoch: 16, Train_Loss: 1.1262234449386597, Test_Loss: 1.1367895603179932\n",
      "Epoch: 16, Train_Loss: 1.1090335845947266, Test_Loss: 1.1273624897003174 *\n",
      "Epoch: 16, Train_Loss: 1.1164640188217163, Test_Loss: 1.1171116828918457 *\n",
      "Epoch: 16, Train_Loss: 1.172398567199707, Test_Loss: 1.109210729598999 *\n",
      "Epoch: 16, Train_Loss: 1.1143298149108887, Test_Loss: 1.1959638595581055\n",
      "Epoch: 16, Train_Loss: 1.1194844245910645, Test_Loss: 1.1492818593978882 *\n",
      "Epoch: 16, Train_Loss: 2.057617664337158, Test_Loss: 1.1948250532150269\n",
      "Epoch: 16, Train_Loss: 2.4657225608825684, Test_Loss: 1.1741070747375488 *\n",
      "Epoch: 16, Train_Loss: 1.6133760213851929, Test_Loss: 1.1470599174499512 *\n",
      "Epoch: 16, Train_Loss: 1.2030752897262573, Test_Loss: 1.1213358640670776 *\n",
      "Epoch: 16, Train_Loss: 2.095287322998047, Test_Loss: 1.1128489971160889 *\n",
      "Epoch: 16, Train_Loss: 3.220127582550049, Test_Loss: 1.1120795011520386 *\n",
      "Epoch: 16, Train_Loss: 1.3098578453063965, Test_Loss: 1.1146066188812256\n",
      "Epoch: 16, Train_Loss: 1.1343021392822266, Test_Loss: 1.1124845743179321 *\n",
      "Epoch: 16, Train_Loss: 1.1334781646728516, Test_Loss: 1.1085048913955688 *\n",
      "Epoch: 16, Train_Loss: 2.5389835834503174, Test_Loss: 1.1010491847991943 *\n",
      "Epoch: 16, Train_Loss: 2.5534534454345703, Test_Loss: 1.1098284721374512\n",
      "Epoch: 16, Train_Loss: 1.1634292602539062, Test_Loss: 1.1021301746368408 *\n",
      "Epoch: 16, Train_Loss: 1.1146172285079956, Test_Loss: 1.1211071014404297\n",
      "Epoch: 16, Train_Loss: 1.1106334924697876, Test_Loss: 1.1427193880081177\n",
      "Epoch: 16, Train_Loss: 1.7603347301483154, Test_Loss: 1.1228140592575073 *\n",
      "Epoch: 16, Train_Loss: 1.2247596979141235, Test_Loss: 1.1361255645751953\n",
      "Epoch: 16, Train_Loss: 1.1619863510131836, Test_Loss: 1.518726110458374\n",
      "Epoch: 16, Train_Loss: 1.1265000104904175, Test_Loss: 1.1351430416107178 *\n",
      "Epoch: 16, Train_Loss: 1.2554476261138916, Test_Loss: 1.1278127431869507 *\n",
      "Epoch: 16, Train_Loss: 1.1880745887756348, Test_Loss: 1.174572229385376\n",
      "Epoch: 16, Train_Loss: 1.3060182332992554, Test_Loss: 1.3438398838043213\n",
      "Epoch: 16, Train_Loss: 1.5249953269958496, Test_Loss: 1.142214059829712 *\n",
      "Epoch: 16, Train_Loss: 1.1827913522720337, Test_Loss: 1.1697771549224854\n",
      "Epoch: 16, Train_Loss: 1.1624420881271362, Test_Loss: 1.2133876085281372\n",
      "Epoch: 16, Train_Loss: 1.2408713102340698, Test_Loss: 1.3086739778518677\n",
      "Epoch: 16, Train_Loss: 1.3994817733764648, Test_Loss: 1.152266263961792 *\n",
      "Epoch: 16, Train_Loss: 1.3377299308776855, Test_Loss: 1.1708370447158813\n",
      "Epoch: 16, Train_Loss: 1.1382977962493896, Test_Loss: 1.10874605178833 *\n",
      "Epoch: 16, Train_Loss: 1.210580587387085, Test_Loss: 1.108676552772522 *\n",
      "Epoch: 16, Train_Loss: 1.2283475399017334, Test_Loss: 1.1926292181015015\n",
      "Epoch: 16, Train_Loss: 1.1242488622665405, Test_Loss: 1.7196097373962402\n",
      "Epoch: 16, Train_Loss: 1.1065467596054077, Test_Loss: 1.245072364807129 *\n",
      "Epoch: 16, Train_Loss: 1.097890019416809, Test_Loss: 1.603846549987793\n",
      "Epoch: 16, Train_Loss: 1.0948269367218018, Test_Loss: 1.4896231889724731 *\n",
      "Epoch: 16, Train_Loss: 1.094903588294983, Test_Loss: 1.3676800727844238 *\n",
      "Epoch: 16, Train_Loss: 1.0999568700790405, Test_Loss: 1.3733818531036377\n",
      "Epoch: 16, Train_Loss: 1.146733283996582, Test_Loss: 1.1638139486312866 *\n",
      "Epoch: 16, Train_Loss: 1.1168450117111206, Test_Loss: 1.1008440256118774 *\n",
      "Epoch: 16, Train_Loss: 1.1671204566955566, Test_Loss: 1.103147268295288\n",
      "Epoch: 16, Train_Loss: 1.2410515546798706, Test_Loss: 1.166127324104309\n",
      "Epoch: 16, Train_Loss: 1.473002552986145, Test_Loss: 1.693875789642334\n",
      "Epoch: 16, Train_Loss: 1.097015619277954, Test_Loss: 1.482234239578247 *\n",
      "Epoch: 16, Train_Loss: 1.1418774127960205, Test_Loss: 2.501049518585205\n",
      "Epoch: 16, Train_Loss: 1.2682164907455444, Test_Loss: 1.8154867887496948 *\n",
      "Epoch: 16, Train_Loss: 1.5431267023086548, Test_Loss: 2.005805015563965\n",
      "Epoch: 16, Train_Loss: 1.3414340019226074, Test_Loss: 1.3136918544769287 *\n",
      "Epoch: 16, Train_Loss: 1.1066129207611084, Test_Loss: 1.0970568656921387 *\n",
      "Epoch: 16, Train_Loss: 1.2144474983215332, Test_Loss: 1.1794986724853516\n",
      "Epoch: 16, Train_Loss: 1.5382773876190186, Test_Loss: 2.05704402923584\n",
      "Epoch: 16, Train_Loss: 1.4427425861358643, Test_Loss: 2.105600357055664\n",
      "Epoch: 16, Train_Loss: 1.1306167840957642, Test_Loss: 1.1569571495056152 *\n",
      "Epoch: 16, Train_Loss: 1.0965086221694946, Test_Loss: 1.1381968259811401 *\n",
      "Epoch: 16, Train_Loss: 1.1008498668670654, Test_Loss: 1.165443778038025\n",
      "Epoch: 16, Train_Loss: 2.0940439701080322, Test_Loss: 1.581416130065918\n",
      "Epoch: 16, Train_Loss: 1.922123670578003, Test_Loss: 1.2445528507232666 *\n",
      "Epoch: 16, Train_Loss: 1.1019538640975952, Test_Loss: 1.9016575813293457\n",
      "Epoch: 16, Train_Loss: 1.1152945756912231, Test_Loss: 1.6094920635223389 *\n",
      "Epoch: 16, Train_Loss: 1.0899019241333008, Test_Loss: 1.2834875583648682 *\n",
      "Epoch: 16, Train_Loss: 1.1484463214874268, Test_Loss: 1.1031817197799683 *\n",
      "Epoch: 16, Train_Loss: 1.4785163402557373, Test_Loss: 1.0962661504745483 *\n",
      "Epoch: 16, Train_Loss: 1.0941224098205566, Test_Loss: 1.102895975112915\n",
      "Epoch: 16, Train_Loss: 1.1086546182632446, Test_Loss: 1.1780668497085571\n",
      "Epoch: 16, Train_Loss: 1.114194631576538, Test_Loss: 1.3821732997894287\n",
      "Epoch: 16, Train_Loss: 1.1316766738891602, Test_Loss: 1.4271330833435059\n",
      "Epoch: 16, Train_Loss: 18.339441299438477, Test_Loss: 1.2267733812332153 *\n",
      "Epoch: 16, Train_Loss: 1.0935001373291016, Test_Loss: 1.119930624961853 *\n",
      "Epoch: 16, Train_Loss: 3.264432191848755, Test_Loss: 1.1234642267227173\n",
      "Epoch: 16, Train_Loss: 2.279970407485962, Test_Loss: 1.0954471826553345 *\n",
      "Epoch: 16, Train_Loss: 1.0871330499649048, Test_Loss: 1.1764183044433594\n",
      "Epoch: 16, Train_Loss: 1.1613630056381226, Test_Loss: 2.0298147201538086\n",
      "Epoch: 16, Train_Loss: 7.050905704498291, Test_Loss: 1.831315279006958 *\n",
      "Epoch: 16, Train_Loss: 6.580453872680664, Test_Loss: 1.1392666101455688 *\n",
      "Epoch: 16, Train_Loss: 1.1119446754455566, Test_Loss: 1.138049840927124 *\n",
      "Epoch: 16, Train_Loss: 1.2369805574417114, Test_Loss: 1.0983550548553467 *\n",
      "Epoch: 16, Train_Loss: 6.906902313232422, Test_Loss: 1.1206464767456055\n",
      "Epoch: 16, Train_Loss: 1.158797264099121, Test_Loss: 1.1592024564743042\n",
      "Epoch: 16, Train_Loss: 1.081360936164856, Test_Loss: 1.1207135915756226 *\n",
      "Epoch: 16, Train_Loss: 1.0791256427764893, Test_Loss: 1.2591997385025024\n",
      "Epoch: 16, Train_Loss: 1.082143783569336, Test_Loss: 1.0934044122695923 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 1.0956783294677734, Test_Loss: 1.1590657234191895\n",
      "Epoch: 16, Train_Loss: 1.0794031620025635, Test_Loss: 1.1896241903305054\n",
      "Epoch: 16, Train_Loss: 1.079727292060852, Test_Loss: 1.3699736595153809\n",
      "Epoch: 16, Train_Loss: 1.0701463222503662, Test_Loss: 1.2417542934417725 *\n",
      "Epoch: 16, Train_Loss: 1.0728719234466553, Test_Loss: 1.10169517993927 *\n",
      "Epoch: 16, Train_Loss: 1.1234753131866455, Test_Loss: 1.1345603466033936\n",
      "Epoch: 16, Train_Loss: 1.0975888967514038, Test_Loss: 1.1297261714935303 *\n",
      "Epoch: 16, Train_Loss: 1.1038986444473267, Test_Loss: 1.1243280172348022 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 1.1835609674453735, Test_Loss: 1.1142643690109253 *\n",
      "Epoch: 16, Train_Loss: 1.202530860900879, Test_Loss: 1.604666829109192\n",
      "Epoch: 16, Train_Loss: 1.0864789485931396, Test_Loss: 6.6630539894104\n",
      "Epoch: 16, Train_Loss: 1.0873711109161377, Test_Loss: 1.2615453004837036 *\n",
      "Epoch: 16, Train_Loss: 1.082190990447998, Test_Loss: 1.1526871919631958 *\n",
      "Epoch: 16, Train_Loss: 1.0838769674301147, Test_Loss: 1.1394542455673218 *\n",
      "Epoch: 16, Train_Loss: 1.0721681118011475, Test_Loss: 1.0980998277664185 *\n",
      "Epoch: 16, Train_Loss: 1.066394567489624, Test_Loss: 1.1686912775039673\n",
      "Epoch: 16, Train_Loss: 1.0650075674057007, Test_Loss: 1.1430726051330566 *\n",
      "Epoch: 16, Train_Loss: 1.0671411752700806, Test_Loss: 1.1746151447296143\n",
      "Epoch: 16, Train_Loss: 1.0641504526138306, Test_Loss: 1.1522972583770752 *\n",
      "Epoch: 16, Train_Loss: 1.0671143531799316, Test_Loss: 1.181544542312622\n",
      "Epoch: 16, Train_Loss: 1.0628927946090698, Test_Loss: 1.151084303855896 *\n",
      "Epoch: 16, Train_Loss: 1.0708513259887695, Test_Loss: 1.1965605020523071\n",
      "Epoch: 16, Train_Loss: 1.0768003463745117, Test_Loss: 1.1007410287857056 *\n",
      "Epoch: 16, Train_Loss: 1.1079317331314087, Test_Loss: 1.1318451166152954\n",
      "Epoch: 16, Train_Loss: 1.0924818515777588, Test_Loss: 1.1467429399490356\n",
      "Epoch: 16, Train_Loss: 1.0788923501968384, Test_Loss: 1.1081088781356812 *\n",
      "Epoch: 16, Train_Loss: 4.606687068939209, Test_Loss: 1.1059298515319824 *\n",
      "Epoch: 16, Train_Loss: 6.124360084533691, Test_Loss: 1.1202809810638428\n",
      "Epoch: 16, Train_Loss: 1.0942462682724, Test_Loss: 1.1266483068466187\n",
      "Epoch: 16, Train_Loss: 1.1144402027130127, Test_Loss: 1.1265747547149658 *\n",
      "Epoch: 16, Train_Loss: 1.163605809211731, Test_Loss: 1.1666067838668823\n",
      "Epoch: 16, Train_Loss: 1.1084340810775757, Test_Loss: 1.164302945137024 *\n",
      "Epoch: 16, Train_Loss: 1.0929641723632812, Test_Loss: 1.1667152643203735\n",
      "Epoch: 16, Train_Loss: 1.140854001045227, Test_Loss: 1.1709259748458862\n",
      "Epoch: 16, Train_Loss: 1.2116700410842896, Test_Loss: 1.1385866403579712 *\n",
      "Epoch: 16, Train_Loss: 1.3477180004119873, Test_Loss: 1.125708818435669 *\n",
      "Epoch: 16, Train_Loss: 1.2424057722091675, Test_Loss: 1.1276357173919678\n",
      "Epoch: 16, Train_Loss: 1.1189428567886353, Test_Loss: 1.124885082244873 *\n",
      "Epoch: 16, Train_Loss: 1.1131932735443115, Test_Loss: 1.1126282215118408 *\n",
      "Epoch: 16, Train_Loss: 1.2152818441390991, Test_Loss: 1.1597843170166016\n",
      "Epoch: 16, Train_Loss: 1.2101396322250366, Test_Loss: 1.140447735786438 *\n",
      "Epoch: 16, Train_Loss: 1.2075397968292236, Test_Loss: 4.033993244171143\n",
      "Epoch: 16, Train_Loss: 1.1190605163574219, Test_Loss: 4.2502851486206055\n",
      "Epoch: 16, Train_Loss: 1.102918028831482, Test_Loss: 1.069077730178833 *\n",
      "Epoch: 16, Train_Loss: 1.0563558340072632, Test_Loss: 1.0589677095413208 *\n",
      "Epoch: 16, Train_Loss: 1.0909719467163086, Test_Loss: 1.077898383140564\n",
      "Epoch: 16, Train_Loss: 1.1262480020523071, Test_Loss: 1.0708969831466675 *\n",
      "Epoch: 16, Train_Loss: 1.0770719051361084, Test_Loss: 1.0809663534164429\n",
      "Epoch: 16, Train_Loss: 1.0646930932998657, Test_Loss: 1.109573245048523\n",
      "Epoch: 16, Train_Loss: 1.0678898096084595, Test_Loss: 1.183355450630188\n",
      "Epoch: 16, Train_Loss: 1.0677516460418701, Test_Loss: 1.0564920902252197 *\n",
      "Epoch: 16, Train_Loss: 4.20443058013916, Test_Loss: 1.0725349187850952\n",
      "Epoch: 16, Train_Loss: 3.4279704093933105, Test_Loss: 1.069745659828186 *\n",
      "Epoch: 16, Train_Loss: 1.063984990119934, Test_Loss: 1.0718858242034912\n",
      "Epoch: 16, Train_Loss: 1.128241777420044, Test_Loss: 1.059707522392273 *\n",
      "Epoch: 16, Train_Loss: 1.0790033340454102, Test_Loss: 1.1108158826828003\n",
      "Epoch: 16, Train_Loss: 1.0561809539794922, Test_Loss: 1.0898207426071167 *\n",
      "Epoch: 16, Train_Loss: 1.050536870956421, Test_Loss: 1.137248158454895\n",
      "Epoch: 16, Train_Loss: 1.0566999912261963, Test_Loss: 1.129136562347412 *\n",
      "Epoch: 16, Train_Loss: 1.069473385810852, Test_Loss: 1.075041651725769 *\n",
      "Epoch: 16, Train_Loss: 1.0637710094451904, Test_Loss: 1.0746710300445557 *\n",
      "Epoch: 16, Train_Loss: 1.092911958694458, Test_Loss: 1.0523675680160522 *\n",
      "Epoch: 16, Train_Loss: 1.054112434387207, Test_Loss: 1.0526361465454102\n",
      "Epoch: 16, Train_Loss: 1.0469411611557007, Test_Loss: 1.0529614686965942\n",
      "Epoch: 16, Train_Loss: 1.057097315788269, Test_Loss: 1.058907389640808\n",
      "Epoch: 16, Train_Loss: 1.0520927906036377, Test_Loss: 1.0586276054382324 *\n",
      "Epoch: 16, Train_Loss: 1.045608639717102, Test_Loss: 1.052886962890625 *\n",
      "Epoch: 16, Train_Loss: 1.048553466796875, Test_Loss: 1.0566569566726685\n",
      "Epoch: 16, Train_Loss: 1.0714173316955566, Test_Loss: 1.0530591011047363 *\n",
      "Epoch: 16, Train_Loss: 1.0670005083084106, Test_Loss: 1.0503555536270142 *\n",
      "Epoch: 16, Train_Loss: 1.047166109085083, Test_Loss: 1.072628140449524\n",
      "Epoch: 16, Train_Loss: 1.04631507396698, Test_Loss: 1.052830457687378 *\n",
      "Epoch: 16, Train_Loss: 1.0847022533416748, Test_Loss: 1.069657564163208\n",
      "Epoch: 16, Train_Loss: 1.0901901721954346, Test_Loss: 1.3960654735565186\n",
      "Epoch: 16, Train_Loss: 1.0860254764556885, Test_Loss: 1.1288126707077026 *\n",
      "Epoch: 16, Train_Loss: 1.0809695720672607, Test_Loss: 1.0689419507980347 *\n",
      "Epoch: 16, Train_Loss: 1.1060305833816528, Test_Loss: 1.0910500288009644\n",
      "Epoch: 16, Train_Loss: 1.0999212265014648, Test_Loss: 1.2821619510650635\n",
      "Epoch: 16, Train_Loss: 1.0685467720031738, Test_Loss: 1.1900447607040405 *\n",
      "Epoch: 16, Train_Loss: 1.0840834379196167, Test_Loss: 1.0686304569244385 *\n",
      "Epoch: 16, Train_Loss: 1.1479977369308472, Test_Loss: 1.2260222434997559\n",
      "Epoch: 16, Train_Loss: 1.109445333480835, Test_Loss: 1.297600507736206\n",
      "Epoch: 16, Train_Loss: 1.0537317991256714, Test_Loss: 1.0861655473709106 *\n",
      "Epoch: 16, Train_Loss: 1.0380021333694458, Test_Loss: 1.1181539297103882\n",
      "Epoch: 16, Train_Loss: 1.0380403995513916, Test_Loss: 1.045331358909607 *\n",
      "Epoch: 16, Train_Loss: 1.039040207862854, Test_Loss: 1.0655267238616943\n",
      "Epoch: 16, Train_Loss: 1.0369752645492554, Test_Loss: 1.0623522996902466 *\n",
      "Epoch: 16, Train_Loss: 1.0388922691345215, Test_Loss: 1.7440240383148193\n",
      "Epoch: 16, Train_Loss: 4.156712532043457, Test_Loss: 1.260726809501648 *\n",
      "Epoch: 16, Train_Loss: 2.5654215812683105, Test_Loss: 1.73048996925354\n",
      "Epoch: 16, Train_Loss: 1.0361930131912231, Test_Loss: 1.6332874298095703 *\n",
      "Epoch: 16, Train_Loss: 1.0419673919677734, Test_Loss: 1.256609559059143 *\n",
      "Epoch: 16, Train_Loss: 1.0388716459274292, Test_Loss: 1.4808824062347412\n",
      "Epoch: 16, Train_Loss: 1.0408954620361328, Test_Loss: 1.0895195007324219 *\n",
      "Epoch: 16, Train_Loss: 1.0368783473968506, Test_Loss: 1.054882287979126 *\n",
      "Epoch: 16, Train_Loss: 1.0394636392593384, Test_Loss: 1.0624308586120605\n",
      "Epoch: 16, Train_Loss: 1.039515733718872, Test_Loss: 1.2115635871887207\n",
      "Epoch: 16, Train_Loss: 1.0364519357681274, Test_Loss: 1.3799983263015747\n",
      "Epoch: 16, Train_Loss: 1.052012324333191, Test_Loss: 1.658766746520996\n",
      "Epoch: 16, Train_Loss: 1.089216947555542, Test_Loss: 2.2135307788848877\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 1.0796806812286377, Test_Loss: 2.3753764629364014\n",
      "Epoch: 16, Train_Loss: 1.1059645414352417, Test_Loss: 1.8376097679138184 *\n",
      "Epoch: 16, Train_Loss: 1.0506993532180786, Test_Loss: 1.3901457786560059 *\n",
      "Epoch: 16, Train_Loss: 1.0580910444259644, Test_Loss: 1.0315039157867432 *\n",
      "Epoch: 16, Train_Loss: 1.2817516326904297, Test_Loss: 1.0812169313430786\n",
      "Epoch: 16, Train_Loss: 1.2846217155456543, Test_Loss: 1.9548976421356201\n",
      "Epoch: 16, Train_Loss: 1.2771718502044678, Test_Loss: 2.375755548477173\n",
      "Epoch: 16, Train_Loss: 1.0959553718566895, Test_Loss: 1.0767271518707275 *\n",
      "Epoch: 16, Train_Loss: 1.0280673503875732, Test_Loss: 1.0916006565093994\n",
      "Epoch: 16, Train_Loss: 1.0286169052124023, Test_Loss: 1.050510287284851 *\n",
      "Epoch: 16, Train_Loss: 1.0390833616256714, Test_Loss: 1.3886051177978516\n",
      "Epoch: 16, Train_Loss: 1.0407800674438477, Test_Loss: 1.2701997756958008 *\n",
      "Epoch: 16, Train_Loss: 1.0443429946899414, Test_Loss: 1.6698710918426514\n",
      "Epoch: 16, Train_Loss: 1.0345097780227661, Test_Loss: 1.9226734638214111\n",
      "Epoch: 16, Train_Loss: 1.0283397436141968, Test_Loss: 1.296980619430542 *\n",
      "Epoch: 16, Train_Loss: 1.0282611846923828, Test_Loss: 1.0296987295150757 *\n",
      "Epoch: 16, Train_Loss: 1.0416686534881592, Test_Loss: 1.0345046520233154\n",
      "Epoch: 16, Train_Loss: 1.1218712329864502, Test_Loss: 1.0388168096542358\n",
      "Epoch: 16, Train_Loss: 1.2034028768539429, Test_Loss: 1.0680800676345825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 1.1663342714309692, Test_Loss: 1.3375952243804932\n",
      "Epoch: 16, Train_Loss: 1.115600347518921, Test_Loss: 1.541811466217041\n",
      "Epoch: 16, Train_Loss: 1.170792579650879, Test_Loss: 1.2320884466171265 *\n",
      "Epoch: 16, Train_Loss: 1.1830434799194336, Test_Loss: 1.0872793197631836 *\n",
      "Epoch: 16, Train_Loss: 1.0472679138183594, Test_Loss: 1.0515371561050415 *\n",
      "Epoch: 16, Train_Loss: 1.2433817386627197, Test_Loss: 1.0337178707122803 *\n",
      "Epoch: 16, Train_Loss: 1.1703567504882812, Test_Loss: 1.1123816967010498\n",
      "Epoch: 16, Train_Loss: 1.314526915550232, Test_Loss: 1.831508994102478\n",
      "Epoch: 16, Train_Loss: 1.0329134464263916, Test_Loss: 2.212174892425537\n",
      "Epoch: 16, Train_Loss: 1.5147022008895874, Test_Loss: 1.097287893295288 *\n",
      "Epoch: 16, Train_Loss: 3.6756372451782227, Test_Loss: 1.1067843437194824\n",
      "Epoch: 16, Train_Loss: 1.0723609924316406, Test_Loss: 1.024354100227356 *\n",
      "Epoch: 16, Train_Loss: 1.0728144645690918, Test_Loss: 1.0270038843154907\n",
      "Epoch: 16, Train_Loss: 1.0805904865264893, Test_Loss: 1.0305908918380737\n",
      "Epoch: 16, Train_Loss: 1.077644944190979, Test_Loss: 1.0369834899902344\n",
      "Epoch: 16, Train_Loss: 1.0228334665298462, Test_Loss: 1.0561089515686035\n",
      "Epoch: 16, Train_Loss: 1.0203408002853394, Test_Loss: 1.0439903736114502 *\n",
      "Epoch: 16, Train_Loss: 1.1017680168151855, Test_Loss: 1.0243533849716187 *\n",
      "Epoch: 16, Train_Loss: 1.114011526107788, Test_Loss: 1.134921908378601\n",
      "Epoch: 16, Train_Loss: 1.1022366285324097, Test_Loss: 1.4349786043167114\n",
      "Epoch: 16, Train_Loss: 1.0713061094284058, Test_Loss: 1.1576972007751465 *\n",
      "Epoch: 16, Train_Loss: 1.0697075128555298, Test_Loss: 1.1823898553848267\n",
      "Epoch: 16, Train_Loss: 1.033348560333252, Test_Loss: 1.037368893623352 *\n",
      "Epoch: 16, Train_Loss: 1.0386000871658325, Test_Loss: 1.0332180261611938 *\n",
      "Epoch: 16, Train_Loss: 1.0319081544876099, Test_Loss: 1.031292200088501 *\n",
      "Epoch: 16, Train_Loss: 1.0630546808242798, Test_Loss: 1.0347840785980225\n",
      "Epoch: 16, Train_Loss: 1.0359771251678467, Test_Loss: 1.0725547075271606\n",
      "Epoch: 16, Train_Loss: 1.0202968120574951, Test_Loss: 5.960870742797852\n",
      "Epoch: 16, Train_Loss: 1.037416696548462, Test_Loss: 1.2193019390106201 *\n",
      "Epoch: 16, Train_Loss: 1.043073058128357, Test_Loss: 1.027402400970459 *\n",
      "Epoch: 16, Train_Loss: 1.036799430847168, Test_Loss: 1.0164144039154053 *\n",
      "Epoch: 16, Train_Loss: 1.0209485292434692, Test_Loss: 1.0223366022109985\n",
      "Epoch: 16, Train_Loss: 1.016270637512207, Test_Loss: 1.0250567197799683\n",
      "Epoch: 16, Train_Loss: 1.0137883424758911, Test_Loss: 1.0163776874542236 *\n",
      "Epoch: 16, Train_Loss: 1.01471745967865, Test_Loss: 1.018457055091858\n",
      "Epoch: 16, Train_Loss: 1.0145570039749146, Test_Loss: 1.0132404565811157 *\n",
      "Epoch: 16, Train_Loss: 1.015377163887024, Test_Loss: 1.0163050889968872\n",
      "Epoch: 16, Train_Loss: 1.019281029701233, Test_Loss: 1.0170786380767822\n",
      "Epoch: 16, Train_Loss: 1.0154924392700195, Test_Loss: 1.0260725021362305\n",
      "Epoch: 16, Train_Loss: 1.0127135515213013, Test_Loss: 1.0258212089538574 *\n",
      "Epoch: 16, Train_Loss: 1.015081763267517, Test_Loss: 1.0330952405929565\n",
      "Epoch: 16, Train_Loss: 1.0213040113449097, Test_Loss: 1.02969491481781 *\n",
      "Epoch: 16, Train_Loss: 1.025998592376709, Test_Loss: 1.0131525993347168 *\n",
      "Epoch: 16, Train_Loss: 1.0310540199279785, Test_Loss: 1.0110118389129639 *\n",
      "Epoch: 16, Train_Loss: 1.0433506965637207, Test_Loss: 1.0104354619979858 *\n",
      "Epoch: 16, Train_Loss: 1.0151829719543457, Test_Loss: 1.0097012519836426 *\n",
      "Epoch: 16, Train_Loss: 1.0251350402832031, Test_Loss: 1.008324384689331 *\n",
      "Epoch: 16, Train_Loss: 1.011011004447937, Test_Loss: 1.0093095302581787\n",
      "Epoch: 16, Train_Loss: 1.0111312866210938, Test_Loss: 1.0063225030899048 *\n",
      "Epoch: 16, Train_Loss: 1.0297943353652954, Test_Loss: 1.0078684091567993\n",
      "Epoch: 16, Train_Loss: 1.0243463516235352, Test_Loss: 1.0109940767288208\n",
      "Epoch: 16, Train_Loss: 1.0079588890075684, Test_Loss: 1.0072417259216309 *\n",
      "Epoch: 16, Train_Loss: 1.0047979354858398, Test_Loss: 1.0071215629577637 *\n",
      "Epoch: 16, Train_Loss: 1.0104584693908691, Test_Loss: 1.0037566423416138 *\n",
      "Epoch: 16, Train_Loss: 1.0747768878936768, Test_Loss: 1.0053071975708008\n",
      "Epoch: 16, Train_Loss: 1.030543565750122, Test_Loss: 1.004770040512085 *\n",
      "Epoch: 16, Train_Loss: 1.0491023063659668, Test_Loss: 1.0104665756225586\n",
      "Epoch: 16, Train_Loss: 1.0049790143966675, Test_Loss: 1.0680263042449951\n",
      "Epoch: 16, Train_Loss: 1.027766227722168, Test_Loss: 2.219552993774414\n",
      "Epoch: 16, Train_Loss: 1.0649020671844482, Test_Loss: 5.269371032714844\n",
      "Epoch: 16, Train_Loss: 1.0054216384887695, Test_Loss: 1.0071040391921997 *\n",
      "Epoch: 16, Train_Loss: 1.01460599899292, Test_Loss: 1.002387523651123 *\n",
      "Epoch: 16, Train_Loss: 1.0285828113555908, Test_Loss: 1.0469520092010498\n",
      "Epoch: 16, Train_Loss: 1.075107216835022, Test_Loss: 1.0528923273086548\n",
      "Epoch: 16, Train_Loss: 1.0858138799667358, Test_Loss: 1.0611273050308228\n",
      "Epoch: 16, Train_Loss: 1.078211784362793, Test_Loss: 1.0185295343399048 *\n",
      "Epoch: 16, Train_Loss: 1.0324225425720215, Test_Loss: 1.126283884048462\n",
      "Epoch: 16, Train_Loss: 1.003287434577942, Test_Loss: 1.0172125101089478 *\n",
      "Epoch: 16, Train_Loss: 1.0228559970855713, Test_Loss: 1.006714105606079 *\n",
      "Epoch: 16, Train_Loss: 0.9982758164405823, Test_Loss: 1.0344501733779907\n",
      "Epoch: 16, Train_Loss: 1.0066087245941162, Test_Loss: 1.0167542695999146 *\n",
      "Epoch: 16, Train_Loss: 1.0110692977905273, Test_Loss: 1.0066643953323364 *\n",
      "Epoch: 16, Train_Loss: 1.0200228691101074, Test_Loss: 1.0652198791503906\n",
      "Epoch: 16, Train_Loss: 1.0861985683441162, Test_Loss: 1.0983777046203613\n",
      "Epoch: 16, Train_Loss: 1.020358681678772, Test_Loss: 1.058424711227417 *\n",
      "Epoch: 16, Train_Loss: 1.0559775829315186, Test_Loss: 1.0883325338363647\n",
      "Epoch: 16, Train_Loss: 1.003108263015747, Test_Loss: 1.0225474834442139 *\n",
      "Epoch: 16, Train_Loss: 1.0341206789016724, Test_Loss: 1.0344157218933105\n",
      "Epoch: 16, Train_Loss: 1.0081465244293213, Test_Loss: 1.0177448987960815 *\n",
      "Epoch: 16, Train_Loss: 1.2403388023376465, Test_Loss: 1.0115232467651367 *\n",
      "Epoch: 16, Train_Loss: 1.0852771997451782, Test_Loss: 1.0101407766342163 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 1.0149405002593994, Test_Loss: 1.0123034715652466\n",
      "Epoch: 16, Train_Loss: 1.0146781206130981, Test_Loss: 1.0089390277862549 *\n",
      "Epoch: 16, Train_Loss: 0.9922309517860413, Test_Loss: 1.0082616806030273 *\n",
      "Epoch: 16, Train_Loss: 0.99363774061203, Test_Loss: 1.0144951343536377\n",
      "Epoch: 16, Train_Loss: 0.9931674003601074, Test_Loss: 1.0084737539291382 *\n",
      "Epoch: 16, Train_Loss: 0.9969490766525269, Test_Loss: 1.022055745124817\n",
      "Epoch: 16, Train_Loss: 1.0020617246627808, Test_Loss: 0.9962121844291687 *\n",
      "Epoch: 16, Train_Loss: 1.00797438621521, Test_Loss: 1.0056370496749878\n",
      "Epoch: 16, Train_Loss: 0.9999051690101624, Test_Loss: 1.0458019971847534\n",
      "Epoch: 16, Train_Loss: 1.0008108615875244, Test_Loss: 1.1707481145858765\n",
      "Epoch: 16, Train_Loss: 1.007713794708252, Test_Loss: 1.1499056816101074 *\n",
      "Epoch: 16, Train_Loss: 0.9907177686691284, Test_Loss: 1.0143065452575684 *\n",
      "Epoch: 16, Train_Loss: 0.9914805293083191, Test_Loss: 1.0758392810821533\n",
      "Epoch: 16, Train_Loss: 0.9945870041847229, Test_Loss: 1.1689594984054565\n",
      "Epoch: 16, Train_Loss: 1.0171648263931274, Test_Loss: 1.281392216682434\n",
      "Epoch: 16, Train_Loss: 1.0185281038284302, Test_Loss: 1.0008082389831543 *\n",
      "Epoch: 16, Train_Loss: 0.9923354983329773, Test_Loss: 1.1791493892669678\n",
      "Epoch: 16, Train_Loss: 1.0118006467819214, Test_Loss: 1.269134521484375\n",
      "Epoch: 16, Train_Loss: 1.0314006805419922, Test_Loss: 1.036704421043396 *\n",
      "Epoch: 16, Train_Loss: 1.0311094522476196, Test_Loss: 1.0443322658538818\n",
      "Epoch: 16, Train_Loss: 0.9869133234024048, Test_Loss: 0.991176962852478 *\n",
      "Epoch: 16, Train_Loss: 1.0201892852783203, Test_Loss: 1.016116976737976\n",
      "Epoch: 16, Train_Loss: 0.9965707063674927, Test_Loss: 1.0113393068313599 *\n",
      "Epoch: 16, Train_Loss: 0.9976874589920044, Test_Loss: 1.5993807315826416\n",
      "Epoch: 16, Train_Loss: 0.9855712652206421, Test_Loss: 1.2820124626159668 *\n",
      "Epoch: 16, Train_Loss: 1.0097970962524414, Test_Loss: 1.6150214672088623\n",
      "Epoch: 16, Train_Loss: 1.0481442213058472, Test_Loss: 1.7394709587097168\n",
      "Epoch: 16, Train_Loss: 3.273451805114746, Test_Loss: 1.1131749153137207 *\n",
      "Epoch: 16, Train_Loss: 4.10905647277832, Test_Loss: 1.5226199626922607\n",
      "Epoch: 16, Train_Loss: 1.009825587272644, Test_Loss: 1.1002628803253174 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 0.9838957786560059, Test_Loss: 0.991982638835907 *\n",
      "Epoch: 16, Train_Loss: 1.0888373851776123, Test_Loss: 1.0029417276382446\n",
      "Epoch: 16, Train_Loss: 1.1371088027954102, Test_Loss: 1.1013691425323486\n",
      "Epoch: 16, Train_Loss: 0.9990763068199158, Test_Loss: 1.1928683519363403\n",
      "Epoch: 16, Train_Loss: 0.9811890125274658, Test_Loss: 1.8036129474639893\n",
      "Epoch: 16, Train_Loss: 1.008828043937683, Test_Loss: 1.6130874156951904 *\n",
      "Epoch: 16, Train_Loss: 1.0188606977462769, Test_Loss: 2.819281578063965\n",
      "Epoch: 16, Train_Loss: 0.995932936668396, Test_Loss: 1.5909218788146973 *\n",
      "Epoch: 16, Train_Loss: 0.993031919002533, Test_Loss: 1.5703895092010498 *\n",
      "Epoch: 16, Train_Loss: 2.091963291168213, Test_Loss: 0.9874849319458008 *\n",
      "Epoch: 16, Train_Loss: 2.369715690612793, Test_Loss: 0.9897854328155518\n",
      "Epoch: 16, Train_Loss: 1.244220495223999, Test_Loss: 1.5821704864501953\n",
      "Epoch: 16, Train_Loss: 1.0462379455566406, Test_Loss: 2.4890923500061035\n",
      "Epoch: 16, Train_Loss: 2.365074872970581, Test_Loss: 1.0898460149765015 *\n",
      "Epoch: 16, Train_Loss: 2.848872423171997, Test_Loss: 1.1216461658477783\n",
      "Epoch: 16, Train_Loss: 1.0528675317764282, Test_Loss: 0.9855465888977051 *\n",
      "Epoch: 16, Train_Loss: 1.00943124294281, Test_Loss: 1.1494826078414917\n",
      "Epoch: 16, Train_Loss: 1.1086750030517578, Test_Loss: 1.3028178215026855\n",
      "Epoch: 16, Train_Loss: 2.438934087753296, Test_Loss: 1.4272775650024414\n",
      "Epoch: 16, Train_Loss: 2.265951633453369, Test_Loss: 2.0388803482055664\n",
      "Epoch: 16, Train_Loss: 1.02647864818573, Test_Loss: 1.2999866008758545 *\n",
      "Epoch: 16, Train_Loss: 1.0048415660858154, Test_Loss: 0.9881197810173035 *\n",
      "Epoch: 16, Train_Loss: 1.003755807876587, Test_Loss: 0.9881969690322876\n",
      "Epoch: 16, Train_Loss: 1.7499773502349854, Test_Loss: 1.0027284622192383\n",
      "Epoch: 17, Train_Loss: 1.0421091318130493, Test_Loss: 1.017317533493042 *\n",
      "Epoch: 17, Train_Loss: 1.0709741115570068, Test_Loss: 1.1862071752548218\n",
      "Epoch: 17, Train_Loss: 1.0075100660324097, Test_Loss: 1.305060625076294\n",
      "Epoch: 17, Train_Loss: 1.1085879802703857, Test_Loss: 1.1705141067504883 *\n",
      "Epoch: 17, Train_Loss: 1.080974817276001, Test_Loss: 1.044844627380371 *\n",
      "Epoch: 17, Train_Loss: 1.177489995956421, Test_Loss: 0.9928548336029053 *\n",
      "Epoch: 17, Train_Loss: 1.3151512145996094, Test_Loss: 1.0080846548080444\n",
      "Epoch: 17, Train_Loss: 1.0318291187286377, Test_Loss: 1.112900972366333\n",
      "Epoch: 17, Train_Loss: 1.0529283285140991, Test_Loss: 1.5449339151382446\n",
      "Epoch: 17, Train_Loss: 1.076181411743164, Test_Loss: 2.0234158039093018\n",
      "Epoch: 17, Train_Loss: 1.2538729906082153, Test_Loss: 1.2227972745895386 *\n",
      "Epoch: 17, Train_Loss: 1.2066723108291626, Test_Loss: 1.0314960479736328 *\n",
      "Epoch: 17, Train_Loss: 1.0131114721298218, Test_Loss: 0.9814300537109375 *\n",
      "Epoch: 17, Train_Loss: 1.0802680253982544, Test_Loss: 0.9940177798271179\n",
      "Epoch: 17, Train_Loss: 1.1072964668273926, Test_Loss: 0.9921634197235107 *\n",
      "Epoch: 17, Train_Loss: 0.9886544346809387, Test_Loss: 1.0363993644714355\n",
      "Epoch: 17, Train_Loss: 0.9767183065414429, Test_Loss: 1.0005794763565063 *\n",
      "Epoch: 17, Train_Loss: 0.9841880798339844, Test_Loss: 1.0192128419876099\n",
      "Epoch: 17, Train_Loss: 0.9767875671386719, Test_Loss: 0.9964976906776428 *\n",
      "Epoch: 17, Train_Loss: 0.9752118587493896, Test_Loss: 1.049326777458191\n",
      "Epoch: 17, Train_Loss: 0.9804133176803589, Test_Loss: 1.326499342918396\n",
      "Epoch: 17, Train_Loss: 1.024052619934082, Test_Loss: 1.0965322256088257 *\n",
      "Epoch: 17, Train_Loss: 0.9925852417945862, Test_Loss: 1.2469370365142822\n",
      "Epoch: 17, Train_Loss: 1.0394588708877563, Test_Loss: 0.9708541631698608 *\n",
      "Epoch: 17, Train_Loss: 1.0959861278533936, Test_Loss: 0.9676750898361206 *\n",
      "Epoch: 17, Train_Loss: 1.3607869148254395, Test_Loss: 0.965743362903595 *\n",
      "Epoch: 17, Train_Loss: 0.9809890985488892, Test_Loss: 0.9658505320549011\n",
      "Epoch: 17, Train_Loss: 1.0037670135498047, Test_Loss: 0.9969329833984375\n",
      "Epoch: 17, Train_Loss: 1.1982771158218384, Test_Loss: 5.001846790313721\n",
      "Epoch: 17, Train_Loss: 1.395287275314331, Test_Loss: 2.2595953941345215 *\n",
      "Epoch: 17, Train_Loss: 1.185387134552002, Test_Loss: 0.9997092485427856 *\n",
      "Epoch: 17, Train_Loss: 0.9934560656547546, Test_Loss: 0.9958691596984863 *\n",
      "Epoch: 17, Train_Loss: 1.2018731832504272, Test_Loss: 0.9765700101852417 *\n",
      "Epoch: 17, Train_Loss: 1.4040849208831787, Test_Loss: 0.9662901163101196 *\n",
      "Epoch: 17, Train_Loss: 1.3260797262191772, Test_Loss: 1.0045926570892334\n",
      "Epoch: 17, Train_Loss: 0.9931155443191528, Test_Loss: 1.0216052532196045\n",
      "Epoch: 17, Train_Loss: 0.9820041656494141, Test_Loss: 0.9951510429382324 *\n",
      "Epoch: 17, Train_Loss: 1.000380039215088, Test_Loss: 1.0131995677947998\n",
      "Epoch: 17, Train_Loss: 2.2777161598205566, Test_Loss: 1.0198171138763428\n",
      "Epoch: 17, Train_Loss: 1.6469616889953613, Test_Loss: 1.032024621963501\n",
      "Epoch: 17, Train_Loss: 0.9832667112350464, Test_Loss: 1.0104408264160156 *\n",
      "Epoch: 17, Train_Loss: 0.9947360157966614, Test_Loss: 0.9757314920425415 *\n",
      "Epoch: 17, Train_Loss: 0.9667994379997253, Test_Loss: 0.9983851313591003\n",
      "Epoch: 17, Train_Loss: 1.0985609292984009, Test_Loss: 1.0037156343460083\n",
      "Epoch: 17, Train_Loss: 1.2850548028945923, Test_Loss: 0.9714427590370178 *\n",
      "Epoch: 17, Train_Loss: 0.9680824279785156, Test_Loss: 0.997386634349823\n",
      "Epoch: 17, Train_Loss: 0.9844372868537903, Test_Loss: 0.9798286557197571 *\n",
      "Epoch: 17, Train_Loss: 0.999045193195343, Test_Loss: 0.9878782033920288\n",
      "Epoch: 17, Train_Loss: 1.5008418560028076, Test_Loss: 0.97376549243927 *\n",
      "Epoch: 17, Train_Loss: 17.80997085571289, Test_Loss: 1.0066425800323486\n",
      "Epoch: 17, Train_Loss: 1.1238408088684082, Test_Loss: 1.0100044012069702\n",
      "Epoch: 17, Train_Loss: 3.249680519104004, Test_Loss: 1.032333254814148\n",
      "Epoch: 17, Train_Loss: 1.7088305950164795, Test_Loss: 1.0068252086639404 *\n",
      "Epoch: 17, Train_Loss: 0.9902240037918091, Test_Loss: 1.0085688829421997\n",
      "Epoch: 17, Train_Loss: 1.0760836601257324, Test_Loss: 0.9918337464332581 *\n",
      "Epoch: 17, Train_Loss: 8.603949546813965, Test_Loss: 0.9931129813194275\n",
      "Epoch: 17, Train_Loss: 4.406191825866699, Test_Loss: 0.976116955280304 *\n",
      "Epoch: 17, Train_Loss: 0.9803558588027954, Test_Loss: 0.9938504695892334\n",
      "Epoch: 17, Train_Loss: 1.864915132522583, Test_Loss: 1.0842775106430054\n",
      "Epoch: 17, Train_Loss: 5.870985984802246, Test_Loss: 1.1474173069000244\n",
      "Epoch: 17, Train_Loss: 1.1222114562988281, Test_Loss: 7.565966606140137\n",
      "Epoch: 17, Train_Loss: 0.9842383861541748, Test_Loss: 1.1300363540649414 *\n",
      "Epoch: 17, Train_Loss: 0.9741404056549072, Test_Loss: 1.003179907798767 *\n",
      "Epoch: 17, Train_Loss: 0.967901885509491, Test_Loss: 0.9979912638664246 *\n",
      "Epoch: 17, Train_Loss: 0.9755841493606567, Test_Loss: 1.0031286478042603\n",
      "Epoch: 17, Train_Loss: 0.9646363258361816, Test_Loss: 1.0134137868881226\n",
      "Epoch: 17, Train_Loss: 0.9703507423400879, Test_Loss: 1.0921028852462769\n",
      "Epoch: 17, Train_Loss: 0.9625883102416992, Test_Loss: 1.4550648927688599\n",
      "Epoch: 17, Train_Loss: 0.9710382223129272, Test_Loss: 1.1421958208084106 *\n",
      "Epoch: 17, Train_Loss: 1.0106629133224487, Test_Loss: 0.9805594086647034 *\n",
      "Epoch: 17, Train_Loss: 0.9699899554252625, Test_Loss: 1.019461989402771\n",
      "Epoch: 17, Train_Loss: 0.9877931475639343, Test_Loss: 0.9658953547477722 *\n",
      "Epoch: 17, Train_Loss: 1.052116870880127, Test_Loss: 0.9595341086387634 *\n",
      "Epoch: 17, Train_Loss: 1.053078532218933, Test_Loss: 0.9853803515434265\n",
      "Epoch: 17, Train_Loss: 0.9580066204071045, Test_Loss: 0.9813486933708191 *\n",
      "Epoch: 17, Train_Loss: 0.9711090922355652, Test_Loss: 1.0896779298782349\n",
      "Epoch: 17, Train_Loss: 0.9740551710128784, Test_Loss: 1.0214115381240845 *\n",
      "Epoch: 17, Train_Loss: 0.9567046165466309, Test_Loss: 0.9940928816795349 *\n",
      "Epoch: 17, Train_Loss: 0.9482661485671997, Test_Loss: 0.971131443977356 *\n",
      "Epoch: 17, Train_Loss: 0.9481856822967529, Test_Loss: 0.9588927030563354 *\n",
      "Epoch: 17, Train_Loss: 0.9470051527023315, Test_Loss: 0.9582476019859314 *\n",
      "Epoch: 17, Train_Loss: 0.9485602378845215, Test_Loss: 0.9589605927467346\n",
      "Epoch: 17, Train_Loss: 0.9494006633758545, Test_Loss: 0.9550020694732666 *\n",
      "Epoch: 17, Train_Loss: 0.945766031742096, Test_Loss: 0.9504653215408325 *\n",
      "Epoch: 17, Train_Loss: 0.9443916082382202, Test_Loss: 0.9515364170074463\n",
      "Epoch: 17, Train_Loss: 0.960260272026062, Test_Loss: 0.9573396444320679\n",
      "Epoch: 17, Train_Loss: 0.9573445320129395, Test_Loss: 0.952229380607605 *\n",
      "Epoch: 17, Train_Loss: 1.0327292680740356, Test_Loss: 0.9503936767578125 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.986727774143219, Test_Loss: 0.9893182516098022\n",
      "Epoch: 17, Train_Loss: 0.9543629884719849, Test_Loss: 0.9639190435409546 *\n",
      "Epoch: 17, Train_Loss: 7.427283763885498, Test_Loss: 0.9674082398414612\n",
      "Epoch: 17, Train_Loss: 3.507312059402466, Test_Loss: 1.0507487058639526\n",
      "Epoch: 17, Train_Loss: 0.9672976732254028, Test_Loss: 1.4195983409881592\n",
      "Epoch: 17, Train_Loss: 0.9905168414115906, Test_Loss: 1.0037846565246582 *\n",
      "Epoch: 17, Train_Loss: 1.0238149166107178, Test_Loss: 0.9733638167381287 *\n",
      "Epoch: 17, Train_Loss: 0.9689548015594482, Test_Loss: 0.9873107671737671\n",
      "Epoch: 17, Train_Loss: 0.9673333168029785, Test_Loss: 1.1645234823226929\n",
      "Epoch: 17, Train_Loss: 1.0215660333633423, Test_Loss: 0.973971426486969 *\n",
      "Epoch: 17, Train_Loss: 1.0772264003753662, Test_Loss: 1.1627005338668823\n",
      "Epoch: 17, Train_Loss: 1.1917190551757812, Test_Loss: 1.2603014707565308\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 1.0972787141799927, Test_Loss: 1.1763536930084229 *\n",
      "Epoch: 17, Train_Loss: 0.9699312448501587, Test_Loss: 1.065456509590149 *\n",
      "Epoch: 17, Train_Loss: 1.0154222249984741, Test_Loss: 0.9879307150840759 *\n",
      "Epoch: 17, Train_Loss: 1.0526158809661865, Test_Loss: 0.951740562915802 *\n",
      "Epoch: 17, Train_Loss: 1.1254353523254395, Test_Loss: 0.9503354430198669 *\n",
      "Epoch: 17, Train_Loss: 1.037711262702942, Test_Loss: 1.457026481628418\n",
      "Epoch: 17, Train_Loss: 0.9828588366508484, Test_Loss: 1.5426198244094849\n",
      "Epoch: 17, Train_Loss: 0.9906514286994934, Test_Loss: 1.226418137550354 *\n",
      "Epoch: 17, Train_Loss: 0.9469540119171143, Test_Loss: 1.4857704639434814\n",
      "Epoch: 17, Train_Loss: 0.9803551435470581, Test_Loss: 1.129893183708191 *\n",
      "Epoch: 17, Train_Loss: 0.9855514764785767, Test_Loss: 1.413275957107544\n",
      "Epoch: 17, Train_Loss: 0.9499056935310364, Test_Loss: 1.1218405961990356 *\n",
      "Epoch: 17, Train_Loss: 0.9395202398300171, Test_Loss: 0.975246012210846 *\n",
      "Epoch: 17, Train_Loss: 0.9364625215530396, Test_Loss: 0.9487259984016418 *\n",
      "Epoch: 17, Train_Loss: 0.985248863697052, Test_Loss: 0.9591265916824341\n",
      "Epoch: 17, Train_Loss: 5.298046588897705, Test_Loss: 1.0994460582733154\n",
      "Epoch: 17, Train_Loss: 1.6019446849822998, Test_Loss: 1.7720022201538086\n",
      "Epoch: 17, Train_Loss: 0.9696164131164551, Test_Loss: 1.2581610679626465 *\n",
      "Epoch: 17, Train_Loss: 1.0754514932632446, Test_Loss: 2.689431667327881\n",
      "Epoch: 17, Train_Loss: 0.9848228693008423, Test_Loss: 1.5139338970184326 *\n",
      "Epoch: 17, Train_Loss: 0.94681316614151, Test_Loss: 1.779397964477539\n",
      "Epoch: 17, Train_Loss: 0.9398887753486633, Test_Loss: 1.0712969303131104 *\n",
      "Epoch: 17, Train_Loss: 0.968256413936615, Test_Loss: 0.959652304649353 *\n",
      "Epoch: 17, Train_Loss: 0.9485755562782288, Test_Loss: 1.1937131881713867\n",
      "Epoch: 17, Train_Loss: 0.9489584565162659, Test_Loss: 2.152780532836914\n",
      "Epoch: 17, Train_Loss: 0.9872912168502808, Test_Loss: 1.3277194499969482 *\n",
      "Epoch: 17, Train_Loss: 0.9348929524421692, Test_Loss: 1.1240397691726685 *\n",
      "Epoch: 17, Train_Loss: 0.9295757412910461, Test_Loss: 0.9382032752037048 *\n",
      "Epoch: 17, Train_Loss: 0.9455919861793518, Test_Loss: 1.0204870700836182\n",
      "Epoch: 17, Train_Loss: 0.9331293702125549, Test_Loss: 1.307002305984497\n",
      "Epoch: 17, Train_Loss: 0.9299372434616089, Test_Loss: 1.2358437776565552 *\n",
      "Epoch: 17, Train_Loss: 0.9367548823356628, Test_Loss: 2.139652729034424\n",
      "Epoch: 17, Train_Loss: 0.9563919305801392, Test_Loss: 1.4044203758239746 *\n",
      "Epoch: 17, Train_Loss: 0.9466491341590881, Test_Loss: 0.952184796333313 *\n",
      "Epoch: 17, Train_Loss: 0.9284130334854126, Test_Loss: 0.9346663355827332 *\n",
      "Epoch: 17, Train_Loss: 0.9302495718002319, Test_Loss: 0.9415538311004639\n",
      "Epoch: 17, Train_Loss: 0.972170352935791, Test_Loss: 0.9599180221557617\n",
      "Epoch: 17, Train_Loss: 0.9724487662315369, Test_Loss: 1.050644040107727\n",
      "Epoch: 17, Train_Loss: 0.9689216017723083, Test_Loss: 1.3625872135162354\n",
      "Epoch: 17, Train_Loss: 0.9612244367599487, Test_Loss: 1.2534794807434082 *\n",
      "Epoch: 17, Train_Loss: 0.9857751131057739, Test_Loss: 1.034195899963379 *\n",
      "Epoch: 17, Train_Loss: 0.9694799780845642, Test_Loss: 0.9426965713500977 *\n",
      "Epoch: 17, Train_Loss: 0.9516046047210693, Test_Loss: 0.9457105994224548\n",
      "Epoch: 17, Train_Loss: 0.9645858407020569, Test_Loss: 0.9777498841285706\n",
      "Epoch: 17, Train_Loss: 1.0697847604751587, Test_Loss: 1.3064759969711304\n",
      "Epoch: 17, Train_Loss: 0.9603978991508484, Test_Loss: 2.1032018661499023\n",
      "Epoch: 17, Train_Loss: 0.9360969066619873, Test_Loss: 1.3846781253814697 *\n",
      "Epoch: 17, Train_Loss: 0.9232056736946106, Test_Loss: 1.0078849792480469 *\n",
      "Epoch: 17, Train_Loss: 0.9223637580871582, Test_Loss: 0.9393382668495178 *\n",
      "Epoch: 17, Train_Loss: 0.9217210412025452, Test_Loss: 0.9386191368103027 *\n",
      "Epoch: 17, Train_Loss: 0.9228265285491943, Test_Loss: 0.9320385456085205 *\n",
      "Epoch: 17, Train_Loss: 0.9234601259231567, Test_Loss: 0.940360963344574\n",
      "Epoch: 17, Train_Loss: 4.946744441986084, Test_Loss: 0.9439656734466553\n",
      "Epoch: 17, Train_Loss: 1.6461126804351807, Test_Loss: 0.9699445962905884\n",
      "Epoch: 17, Train_Loss: 0.9264733195304871, Test_Loss: 0.926144003868103 *\n",
      "Epoch: 17, Train_Loss: 0.9261400699615479, Test_Loss: 1.0024513006210327\n",
      "Epoch: 17, Train_Loss: 0.9238073229789734, Test_Loss: 1.1241365671157837\n",
      "Epoch: 17, Train_Loss: 0.9271330833435059, Test_Loss: 1.2454155683517456\n",
      "Epoch: 17, Train_Loss: 0.9251487255096436, Test_Loss: 1.1876353025436401 *\n",
      "Epoch: 17, Train_Loss: 0.9209920167922974, Test_Loss: 0.9468381404876709 *\n",
      "Epoch: 17, Train_Loss: 0.9242122173309326, Test_Loss: 0.9469661712646484\n",
      "Epoch: 17, Train_Loss: 0.9260464906692505, Test_Loss: 0.9378323554992676 *\n",
      "Epoch: 17, Train_Loss: 0.9479113817214966, Test_Loss: 0.9453940987586975\n",
      "Epoch: 17, Train_Loss: 0.9543061852455139, Test_Loss: 0.9637883901596069\n",
      "Epoch: 17, Train_Loss: 0.9712632298469543, Test_Loss: 3.237553596496582\n",
      "Epoch: 17, Train_Loss: 0.9680727124214172, Test_Loss: 3.697882890701294\n",
      "Epoch: 17, Train_Loss: 0.9254930019378662, Test_Loss: 0.9332823157310486 *\n",
      "Epoch: 17, Train_Loss: 0.976588249206543, Test_Loss: 0.9221469163894653 *\n",
      "Epoch: 17, Train_Loss: 1.162316918373108, Test_Loss: 0.919258177280426 *\n",
      "Epoch: 17, Train_Loss: 1.1498210430145264, Test_Loss: 0.9322223663330078\n",
      "Epoch: 17, Train_Loss: 1.1650300025939941, Test_Loss: 0.9203414916992188 *\n",
      "Epoch: 17, Train_Loss: 0.9392470717430115, Test_Loss: 0.9218122363090515\n",
      "Epoch: 17, Train_Loss: 0.9159688353538513, Test_Loss: 0.9170862436294556 *\n",
      "Epoch: 17, Train_Loss: 0.9165510535240173, Test_Loss: 0.915224552154541 *\n",
      "Epoch: 17, Train_Loss: 0.9249747395515442, Test_Loss: 0.9178348779678345\n",
      "Epoch: 17, Train_Loss: 0.9281108379364014, Test_Loss: 0.9189199805259705\n",
      "Epoch: 17, Train_Loss: 0.9231160283088684, Test_Loss: 0.9247140884399414\n",
      "Epoch: 17, Train_Loss: 0.9194756150245667, Test_Loss: 0.9317425489425659\n",
      "Epoch: 17, Train_Loss: 0.9132617115974426, Test_Loss: 0.9246549606323242 *\n",
      "Epoch: 17, Train_Loss: 0.9135327339172363, Test_Loss: 0.9168119430541992 *\n",
      "Epoch: 17, Train_Loss: 0.9291458129882812, Test_Loss: 0.9149550795555115 *\n",
      "Epoch: 17, Train_Loss: 1.0342274904251099, Test_Loss: 0.9199139475822449\n",
      "Epoch: 17, Train_Loss: 1.074190378189087, Test_Loss: 0.9183470606803894 *\n",
      "Epoch: 17, Train_Loss: 1.0697158575057983, Test_Loss: 0.9172791242599487 *\n",
      "Epoch: 17, Train_Loss: 0.974627673625946, Test_Loss: 0.9147245287895203 *\n",
      "Epoch: 17, Train_Loss: 1.0470844507217407, Test_Loss: 0.913727879524231 *\n",
      "Epoch: 17, Train_Loss: 1.0492435693740845, Test_Loss: 0.9191138744354248\n",
      "Epoch: 17, Train_Loss: 0.9305551648139954, Test_Loss: 0.9330309629440308\n",
      "Epoch: 17, Train_Loss: 1.1168922185897827, Test_Loss: 0.9225047826766968 *\n",
      "Epoch: 17, Train_Loss: 1.0449144840240479, Test_Loss: 0.9191365838050842 *\n",
      "Epoch: 17, Train_Loss: 1.1688745021820068, Test_Loss: 0.9166308045387268 *\n",
      "Epoch: 17, Train_Loss: 0.9182366728782654, Test_Loss: 0.9181568622589111\n",
      "Epoch: 17, Train_Loss: 1.995654582977295, Test_Loss: 0.9177764058113098 *\n",
      "Epoch: 17, Train_Loss: 2.928755044937134, Test_Loss: 0.9109787344932556 *\n",
      "Epoch: 17, Train_Loss: 0.9507415890693665, Test_Loss: 0.9649931192398071\n",
      "Epoch: 17, Train_Loss: 0.9521597027778625, Test_Loss: 0.9404380917549133 *\n",
      "Epoch: 17, Train_Loss: 0.9631786346435547, Test_Loss: 5.680568218231201\n",
      "Epoch: 17, Train_Loss: 0.9516105055809021, Test_Loss: 1.674383282661438 *\n",
      "Epoch: 17, Train_Loss: 0.9068761467933655, Test_Loss: 0.9071632027626038 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.9088306427001953, Test_Loss: 0.9246008396148682\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 1.0052062273025513, Test_Loss: 0.9606225490570068\n",
      "Epoch: 17, Train_Loss: 0.9679401516914368, Test_Loss: 0.9665324687957764\n",
      "Epoch: 17, Train_Loss: 0.9534282088279724, Test_Loss: 0.9169950485229492 *\n",
      "Epoch: 17, Train_Loss: 0.9493939280509949, Test_Loss: 0.9711468815803528\n",
      "Epoch: 17, Train_Loss: 0.9387911558151245, Test_Loss: 0.962847888469696 *\n",
      "Epoch: 17, Train_Loss: 0.9185131788253784, Test_Loss: 0.9095388650894165 *\n",
      "Epoch: 17, Train_Loss: 0.9202662110328674, Test_Loss: 0.9280925989151001\n",
      "Epoch: 17, Train_Loss: 0.9252623319625854, Test_Loss: 0.941260576248169\n",
      "Epoch: 17, Train_Loss: 0.9460386037826538, Test_Loss: 0.9144421815872192 *\n",
      "Epoch: 17, Train_Loss: 0.9165705442428589, Test_Loss: 0.9173620939254761\n",
      "Epoch: 17, Train_Loss: 0.9041364789009094, Test_Loss: 1.026429533958435\n",
      "Epoch: 17, Train_Loss: 0.9220873713493347, Test_Loss: 0.9427420496940613 *\n",
      "Epoch: 17, Train_Loss: 0.9304558038711548, Test_Loss: 0.9668488502502441\n",
      "Epoch: 17, Train_Loss: 0.9163364171981812, Test_Loss: 0.9403107166290283 *\n",
      "Epoch: 17, Train_Loss: 0.9049695730209351, Test_Loss: 0.9600920677185059\n",
      "Epoch: 17, Train_Loss: 0.9030840992927551, Test_Loss: 0.9305213689804077 *\n",
      "Epoch: 17, Train_Loss: 0.9041279554367065, Test_Loss: 0.9161986112594604 *\n",
      "Epoch: 17, Train_Loss: 0.9004424810409546, Test_Loss: 0.9287624955177307\n",
      "Epoch: 17, Train_Loss: 0.9030575752258301, Test_Loss: 0.925789475440979 *\n",
      "Epoch: 17, Train_Loss: 0.9029509425163269, Test_Loss: 0.9404130578041077\n",
      "Epoch: 17, Train_Loss: 0.9030844569206238, Test_Loss: 0.926750659942627 *\n",
      "Epoch: 17, Train_Loss: 0.8978529572486877, Test_Loss: 0.9087364077568054 *\n",
      "Epoch: 17, Train_Loss: 0.8987666368484497, Test_Loss: 0.930500328540802\n",
      "Epoch: 17, Train_Loss: 0.9091998934745789, Test_Loss: 0.9193652868270874 *\n",
      "Epoch: 17, Train_Loss: 0.9129706621170044, Test_Loss: 0.9068419933319092 *\n",
      "Epoch: 17, Train_Loss: 0.9174301624298096, Test_Loss: 0.9087247848510742\n",
      "Epoch: 17, Train_Loss: 0.9149835109710693, Test_Loss: 0.930073618888855\n",
      "Epoch: 17, Train_Loss: 0.9363116025924683, Test_Loss: 0.9210382699966431 *\n",
      "Epoch: 17, Train_Loss: 0.9040690064430237, Test_Loss: 1.2119507789611816\n",
      "Epoch: 17, Train_Loss: 0.9112066626548767, Test_Loss: 0.903968095779419 *\n",
      "Epoch: 17, Train_Loss: 0.9006830453872681, Test_Loss: 0.9463620185852051\n",
      "Epoch: 17, Train_Loss: 0.9012978672981262, Test_Loss: 0.9858716726303101\n",
      "Epoch: 17, Train_Loss: 0.9272006154060364, Test_Loss: 1.2613128423690796\n",
      "Epoch: 17, Train_Loss: 0.9091989994049072, Test_Loss: 0.9616335034370422 *\n",
      "Epoch: 17, Train_Loss: 0.8963223695755005, Test_Loss: 0.980831503868103\n",
      "Epoch: 17, Train_Loss: 0.8936936855316162, Test_Loss: 1.0484205484390259\n",
      "Epoch: 17, Train_Loss: 0.9023477435112, Test_Loss: 1.0853828191757202\n",
      "Epoch: 17, Train_Loss: 0.9565094113349915, Test_Loss: 0.9224749207496643 *\n",
      "Epoch: 17, Train_Loss: 0.9256806969642639, Test_Loss: 0.9273040890693665\n",
      "Epoch: 17, Train_Loss: 0.935272216796875, Test_Loss: 0.8989739418029785 *\n",
      "Epoch: 17, Train_Loss: 0.8910123705863953, Test_Loss: 0.9344640374183655\n",
      "Epoch: 17, Train_Loss: 0.9390383958816528, Test_Loss: 1.026015043258667\n",
      "Epoch: 17, Train_Loss: 0.9340177178382874, Test_Loss: 1.6120706796646118\n",
      "Epoch: 17, Train_Loss: 0.8928372859954834, Test_Loss: 1.112316370010376 *\n",
      "Epoch: 17, Train_Loss: 0.9003774523735046, Test_Loss: 1.6493477821350098\n",
      "Epoch: 17, Train_Loss: 0.916327178478241, Test_Loss: 1.3027377128601074 *\n",
      "Epoch: 17, Train_Loss: 0.9860969185829163, Test_Loss: 1.2931596040725708 *\n",
      "Epoch: 17, Train_Loss: 0.9685168862342834, Test_Loss: 1.1488018035888672 *\n",
      "Epoch: 17, Train_Loss: 0.9494563341140747, Test_Loss: 0.9013549089431763 *\n",
      "Epoch: 17, Train_Loss: 0.917397677898407, Test_Loss: 0.9131301641464233\n",
      "Epoch: 17, Train_Loss: 0.8940417170524597, Test_Loss: 0.9298414587974548\n",
      "Epoch: 17, Train_Loss: 0.9052613377571106, Test_Loss: 1.0321838855743408\n",
      "Epoch: 17, Train_Loss: 0.888881504535675, Test_Loss: 1.7060184478759766\n",
      "Epoch: 17, Train_Loss: 0.8944129347801208, Test_Loss: 1.1146659851074219 *\n",
      "Epoch: 17, Train_Loss: 0.8981613516807556, Test_Loss: 2.79384183883667\n",
      "Epoch: 17, Train_Loss: 0.9040694832801819, Test_Loss: 1.4929633140563965 *\n",
      "Epoch: 17, Train_Loss: 0.988109827041626, Test_Loss: 1.842693567276001\n",
      "Epoch: 17, Train_Loss: 0.886067807674408, Test_Loss: 1.0137016773223877 *\n",
      "Epoch: 17, Train_Loss: 0.9484359622001648, Test_Loss: 0.8914707899093628 *\n",
      "Epoch: 17, Train_Loss: 0.8909671902656555, Test_Loss: 1.0394892692565918\n",
      "Epoch: 17, Train_Loss: 0.9282998442649841, Test_Loss: 2.134418249130249\n",
      "Epoch: 17, Train_Loss: 0.9048561453819275, Test_Loss: 1.6297218799591064 *\n",
      "Epoch: 17, Train_Loss: 1.1789652109146118, Test_Loss: 0.9732862114906311 *\n",
      "Epoch: 17, Train_Loss: 0.919758141040802, Test_Loss: 0.9150748252868652 *\n",
      "Epoch: 17, Train_Loss: 0.9250187873840332, Test_Loss: 0.9622897505760193\n",
      "Epoch: 17, Train_Loss: 0.8862550854682922, Test_Loss: 1.3230723142623901\n",
      "Epoch: 17, Train_Loss: 0.8833509087562561, Test_Loss: 1.0597883462905884 *\n",
      "Epoch: 17, Train_Loss: 0.883354663848877, Test_Loss: 1.9657421112060547\n",
      "Epoch: 17, Train_Loss: 0.8811432123184204, Test_Loss: 1.536562442779541 *\n",
      "Epoch: 17, Train_Loss: 0.888464629650116, Test_Loss: 0.9737123847007751 *\n",
      "Epoch: 17, Train_Loss: 0.8889592885971069, Test_Loss: 0.8864725828170776 *\n",
      "Epoch: 17, Train_Loss: 0.8969180583953857, Test_Loss: 0.8915553092956543\n",
      "Epoch: 17, Train_Loss: 0.8881122469902039, Test_Loss: 0.901162326335907\n",
      "Epoch: 17, Train_Loss: 0.8879887461662292, Test_Loss: 0.9359139204025269\n",
      "Epoch: 17, Train_Loss: 0.8973466753959656, Test_Loss: 1.3906149864196777\n",
      "Epoch: 17, Train_Loss: 0.8839998841285706, Test_Loss: 1.3349510431289673 *\n",
      "Epoch: 17, Train_Loss: 0.8795609474182129, Test_Loss: 1.0256415605545044 *\n",
      "Epoch: 17, Train_Loss: 0.8935437202453613, Test_Loss: 0.9146710634231567 *\n",
      "Epoch: 17, Train_Loss: 0.8988924026489258, Test_Loss: 0.902967631816864 *\n",
      "Epoch: 17, Train_Loss: 0.9125242829322815, Test_Loss: 0.9011865258216858 *\n",
      "Epoch: 17, Train_Loss: 0.8778378367424011, Test_Loss: 1.0833207368850708\n",
      "Epoch: 17, Train_Loss: 0.9077581167221069, Test_Loss: 2.038821220397949\n",
      "Epoch: 17, Train_Loss: 0.9239230155944824, Test_Loss: 1.5374846458435059 *\n",
      "Epoch: 17, Train_Loss: 0.9215205907821655, Test_Loss: 0.9517637491226196 *\n",
      "Epoch: 17, Train_Loss: 0.8768029808998108, Test_Loss: 0.9238215088844299 *\n",
      "Epoch: 17, Train_Loss: 0.9148896932601929, Test_Loss: 0.8833250999450684 *\n",
      "Epoch: 17, Train_Loss: 0.8790962100028992, Test_Loss: 0.8804928660392761 *\n",
      "Epoch: 17, Train_Loss: 0.8915283679962158, Test_Loss: 0.8886647820472717\n",
      "Epoch: 17, Train_Loss: 0.878364622592926, Test_Loss: 0.8966312408447266\n",
      "Epoch: 17, Train_Loss: 0.8978092074394226, Test_Loss: 0.9348496794700623\n",
      "Epoch: 17, Train_Loss: 0.9421125650405884, Test_Loss: 0.8812342882156372 *\n",
      "Epoch: 17, Train_Loss: 3.733875274658203, Test_Loss: 0.9146381616592407\n",
      "Epoch: 17, Train_Loss: 3.3507542610168457, Test_Loss: 1.0077009201049805\n",
      "Epoch: 17, Train_Loss: 0.9026371836662292, Test_Loss: 1.254752278327942\n",
      "Epoch: 17, Train_Loss: 0.8769862055778503, Test_Loss: 1.129881739616394 *\n",
      "Epoch: 17, Train_Loss: 1.0302857160568237, Test_Loss: 0.8950939774513245 *\n",
      "Epoch: 17, Train_Loss: 1.0027928352355957, Test_Loss: 0.8817883133888245 *\n",
      "Epoch: 17, Train_Loss: 0.8864865899085999, Test_Loss: 0.8804879188537598 *\n",
      "Epoch: 17, Train_Loss: 0.8717063069343567, Test_Loss: 0.8827523589134216\n",
      "Epoch: 17, Train_Loss: 0.9142759442329407, Test_Loss: 0.8865564465522766\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 0.8945178389549255, Test_Loss: 1.8221237659454346\n",
      "Epoch: 17, Train_Loss: 0.8874105215072632, Test_Loss: 5.1967058181762695\n",
      "Epoch: 17, Train_Loss: 0.9752416610717773, Test_Loss: 0.8923943638801575 *\n",
      "Epoch: 17, Train_Loss: 2.0182368755340576, Test_Loss: 0.8825936317443848 *\n",
      "Epoch: 17, Train_Loss: 2.2394044399261475, Test_Loss: 0.8862247467041016\n",
      "Epoch: 17, Train_Loss: 0.9820157289505005, Test_Loss: 0.8775728344917297 *\n",
      "Epoch: 17, Train_Loss: 0.9285969734191895, Test_Loss: 0.8941832780838013\n",
      "Epoch: 17, Train_Loss: 2.582970142364502, Test_Loss: 0.8898193836212158 *\n",
      "Epoch: 17, Train_Loss: 2.2620084285736084, Test_Loss: 0.9346510171890259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.9138351082801819, Test_Loss: 0.9410187602043152\n",
      "Epoch: 17, Train_Loss: 0.8867459893226624, Test_Loss: 0.9841403961181641\n",
      "Epoch: 17, Train_Loss: 1.2466192245483398, Test_Loss: 0.9819563031196594 *\n",
      "Epoch: 17, Train_Loss: 2.3372931480407715, Test_Loss: 0.9990105032920837\n",
      "Epoch: 17, Train_Loss: 1.9740962982177734, Test_Loss: 0.9240549206733704 *\n",
      "Epoch: 17, Train_Loss: 0.891515851020813, Test_Loss: 0.9155274629592896 *\n",
      "Epoch: 17, Train_Loss: 0.8942455053329468, Test_Loss: 0.9244060516357422\n",
      "Epoch: 17, Train_Loss: 0.9797626733779907, Test_Loss: 0.8810835480690002 *\n",
      "Epoch: 17, Train_Loss: 1.5780251026153564, Test_Loss: 0.8908399343490601\n",
      "Epoch: 17, Train_Loss: 0.9031744003295898, Test_Loss: 0.8857583999633789 *\n",
      "Epoch: 17, Train_Loss: 0.9395538568496704, Test_Loss: 0.9176366329193115\n",
      "Epoch: 17, Train_Loss: 0.9324995279312134, Test_Loss: 0.8806933164596558 *\n",
      "Epoch: 17, Train_Loss: 0.9544028043746948, Test_Loss: 0.926461935043335\n",
      "Epoch: 17, Train_Loss: 1.0223017930984497, Test_Loss: 0.9113443493843079 *\n",
      "Epoch: 17, Train_Loss: 1.0554227828979492, Test_Loss: 0.9127688407897949\n",
      "Epoch: 17, Train_Loss: 1.1080952882766724, Test_Loss: 0.8934298753738403 *\n",
      "Epoch: 17, Train_Loss: 0.910530686378479, Test_Loss: 0.8889570236206055 *\n",
      "Epoch: 17, Train_Loss: 0.980808675289154, Test_Loss: 0.8734298944473267 *\n",
      "Epoch: 17, Train_Loss: 0.9999960660934448, Test_Loss: 0.8847714066505432\n",
      "Epoch: 17, Train_Loss: 1.1757804155349731, Test_Loss: 0.8790918588638306 *\n",
      "Epoch: 17, Train_Loss: 1.1122894287109375, Test_Loss: 0.8687368631362915 *\n",
      "Epoch: 17, Train_Loss: 0.8973246812820435, Test_Loss: 0.9507236480712891\n",
      "Epoch: 17, Train_Loss: 0.9710595011711121, Test_Loss: 0.9202378392219543 *\n",
      "Epoch: 17, Train_Loss: 0.9540809988975525, Test_Loss: 4.201430320739746\n",
      "Epoch: 17, Train_Loss: 0.8855305910110474, Test_Loss: 2.9941296577453613 *\n",
      "Epoch: 17, Train_Loss: 0.8686466217041016, Test_Loss: 0.8658190965652466 *\n",
      "Epoch: 17, Train_Loss: 0.8759664297103882, Test_Loss: 0.864884614944458 *\n",
      "Epoch: 17, Train_Loss: 0.870030403137207, Test_Loss: 0.900400698184967\n",
      "Epoch: 17, Train_Loss: 0.8656810522079468, Test_Loss: 0.8788188099861145 *\n",
      "Epoch: 17, Train_Loss: 0.8733391761779785, Test_Loss: 0.8832657933235168\n",
      "Epoch: 17, Train_Loss: 0.8979387283325195, Test_Loss: 0.9206519722938538\n",
      "Epoch: 17, Train_Loss: 0.8830277919769287, Test_Loss: 0.9516174793243408\n",
      "Epoch: 17, Train_Loss: 0.9297633171081543, Test_Loss: 0.8634642362594604 *\n",
      "Epoch: 17, Train_Loss: 0.9821199178695679, Test_Loss: 0.8808093070983887\n",
      "Epoch: 17, Train_Loss: 1.2056955099105835, Test_Loss: 0.8870564103126526\n",
      "Epoch: 17, Train_Loss: 0.8902155160903931, Test_Loss: 0.8851357698440552 *\n",
      "Epoch: 17, Train_Loss: 0.9029267430305481, Test_Loss: 0.8673581480979919 *\n",
      "Epoch: 17, Train_Loss: 1.1232484579086304, Test_Loss: 0.9210871458053589\n",
      "Epoch: 17, Train_Loss: 1.2335302829742432, Test_Loss: 0.9080739617347717 *\n",
      "Epoch: 17, Train_Loss: 1.0177412033081055, Test_Loss: 0.9370124340057373\n",
      "Epoch: 17, Train_Loss: 0.8857030868530273, Test_Loss: 0.9030517339706421 *\n",
      "Epoch: 17, Train_Loss: 1.1839649677276611, Test_Loss: 0.9012228846549988 *\n",
      "Epoch: 17, Train_Loss: 1.287390112876892, Test_Loss: 0.8834905624389648 *\n",
      "Epoch: 17, Train_Loss: 1.0929759740829468, Test_Loss: 0.8586620688438416 *\n",
      "Epoch: 17, Train_Loss: 0.881971538066864, Test_Loss: 0.8634247779846191\n",
      "Epoch: 17, Train_Loss: 0.8769586086273193, Test_Loss: 0.8702448010444641\n",
      "Epoch: 17, Train_Loss: 1.0032641887664795, Test_Loss: 0.8871415853500366\n",
      "Epoch: 17, Train_Loss: 2.2743072509765625, Test_Loss: 0.8886286616325378\n",
      "Epoch: 17, Train_Loss: 1.3813540935516357, Test_Loss: 0.8597501516342163 *\n",
      "Epoch: 17, Train_Loss: 0.8870643973350525, Test_Loss: 0.8668758273124695\n",
      "Epoch: 17, Train_Loss: 0.873723030090332, Test_Loss: 0.863989531993866 *\n",
      "Epoch: 17, Train_Loss: 0.8689495921134949, Test_Loss: 0.8846355676651001\n",
      "Epoch: 17, Train_Loss: 1.1354193687438965, Test_Loss: 0.9309568405151367\n",
      "Epoch: 17, Train_Loss: 1.086586594581604, Test_Loss: 0.8648451566696167 *\n",
      "Epoch: 17, Train_Loss: 0.8756410479545593, Test_Loss: 0.8806400895118713\n",
      "Epoch: 17, Train_Loss: 0.8962807059288025, Test_Loss: 1.2490792274475098\n",
      "Epoch: 17, Train_Loss: 0.9023197889328003, Test_Loss: 0.8870885968208313 *\n",
      "Epoch: 17, Train_Loss: 8.114359855651855, Test_Loss: 0.877433717250824 *\n",
      "Epoch: 17, Train_Loss: 10.953058242797852, Test_Loss: 0.9464625716209412\n",
      "Epoch: 17, Train_Loss: 1.4760669469833374, Test_Loss: 1.1006439924240112\n",
      "Epoch: 17, Train_Loss: 2.6727986335754395, Test_Loss: 0.9184876084327698 *\n",
      "Epoch: 17, Train_Loss: 1.1867345571517944, Test_Loss: 0.9152888059616089 *\n",
      "Epoch: 17, Train_Loss: 0.9183993339538574, Test_Loss: 0.9484497308731079\n",
      "Epoch: 17, Train_Loss: 1.0744761228561401, Test_Loss: 1.0116264820098877\n",
      "Epoch: 17, Train_Loss: 9.561055183410645, Test_Loss: 0.9199851751327515 *\n",
      "Epoch: 17, Train_Loss: 2.8022286891937256, Test_Loss: 0.9444229602813721\n",
      "Epoch: 17, Train_Loss: 0.9257986545562744, Test_Loss: 0.8622552156448364 *\n",
      "Epoch: 17, Train_Loss: 3.6398072242736816, Test_Loss: 0.86590576171875\n",
      "Epoch: 17, Train_Loss: 3.9919748306274414, Test_Loss: 0.9371158480644226\n",
      "Epoch: 17, Train_Loss: 0.9601077437400818, Test_Loss: 1.3708326816558838\n",
      "Epoch: 17, Train_Loss: 0.8593290448188782, Test_Loss: 1.1201919317245483 *\n",
      "Epoch: 17, Train_Loss: 0.8482406735420227, Test_Loss: 1.272982120513916\n",
      "Epoch: 17, Train_Loss: 0.8631006479263306, Test_Loss: 1.18228018283844 *\n",
      "Epoch: 17, Train_Loss: 0.890729546546936, Test_Loss: 1.2014048099517822\n",
      "Epoch: 17, Train_Loss: 0.8577214479446411, Test_Loss: 1.3201663494110107\n",
      "Epoch: 17, Train_Loss: 0.8463042974472046, Test_Loss: 0.9964975118637085 *\n",
      "Epoch: 17, Train_Loss: 0.8482683300971985, Test_Loss: 0.8794142603874207 *\n",
      "Epoch: 17, Train_Loss: 0.8627189993858337, Test_Loss: 0.8974756598472595\n",
      "Epoch: 17, Train_Loss: 0.9083613157272339, Test_Loss: 0.9134302735328674\n",
      "Epoch: 17, Train_Loss: 0.8752973675727844, Test_Loss: 1.4177794456481934\n",
      "Epoch: 17, Train_Loss: 0.8918493390083313, Test_Loss: 1.4846735000610352\n",
      "Epoch: 17, Train_Loss: 1.0145875215530396, Test_Loss: 1.867828607559204\n",
      "Epoch: 17, Train_Loss: 0.9988704919815063, Test_Loss: 1.55989670753479 *\n",
      "Epoch: 17, Train_Loss: 0.8578944802284241, Test_Loss: 1.9063969850540161\n",
      "Epoch: 17, Train_Loss: 0.8629395961761475, Test_Loss: 1.2180930376052856 *\n",
      "Epoch: 17, Train_Loss: 0.8830403089523315, Test_Loss: 0.8805423378944397 *\n",
      "Epoch: 17, Train_Loss: 0.8535093665122986, Test_Loss: 0.8943254947662354\n",
      "Epoch: 17, Train_Loss: 0.8517640829086304, Test_Loss: 1.5330719947814941\n",
      "Epoch: 17, Train_Loss: 0.8425441980361938, Test_Loss: 1.7824697494506836\n",
      "Epoch: 17, Train_Loss: 0.8472420573234558, Test_Loss: 0.9216336011886597 *\n",
      "Epoch: 17, Train_Loss: 0.8430148363113403, Test_Loss: 0.9322868585586548\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 0.8427608609199524, Test_Loss: 0.880298376083374 *\n",
      "Epoch: 17, Train_Loss: 0.8415018916130066, Test_Loss: 1.15464448928833\n",
      "Epoch: 17, Train_Loss: 0.8460668921470642, Test_Loss: 1.0401071310043335 *\n",
      "Epoch: 17, Train_Loss: 0.8537216186523438, Test_Loss: 1.704463005065918\n",
      "Epoch: 17, Train_Loss: 0.8603588342666626, Test_Loss: 1.6105329990386963 *\n",
      "Epoch: 17, Train_Loss: 0.9147845506668091, Test_Loss: 1.1559786796569824 *\n",
      "Epoch: 17, Train_Loss: 0.8558090925216675, Test_Loss: 0.8533720374107361 *\n",
      "Epoch: 17, Train_Loss: 0.8573659062385559, Test_Loss: 0.8560251593589783\n",
      "Epoch: 17, Train_Loss: 8.859333038330078, Test_Loss: 0.852853000164032 *\n",
      "Epoch: 17, Train_Loss: 1.4423699378967285, Test_Loss: 1.0048197507858276\n",
      "Epoch: 17, Train_Loss: 0.9254164695739746, Test_Loss: 1.0302839279174805\n",
      "Epoch: 17, Train_Loss: 0.9685695171356201, Test_Loss: 1.101248025894165\n",
      "Epoch: 17, Train_Loss: 0.985214352607727, Test_Loss: 0.9202577471733093 *\n",
      "Epoch: 17, Train_Loss: 0.8951044678688049, Test_Loss: 0.860519289970398 *\n",
      "Epoch: 17, Train_Loss: 0.9151849150657654, Test_Loss: 0.869216799736023\n",
      "Epoch: 17, Train_Loss: 0.9594389796257019, Test_Loss: 0.9157747030258179\n",
      "Epoch: 17, Train_Loss: 1.0606012344360352, Test_Loss: 1.0044327974319458\n",
      "Epoch: 17, Train_Loss: 1.044094204902649, Test_Loss: 1.8505072593688965\n",
      "Epoch: 17, Train_Loss: 0.9505201578140259, Test_Loss: 1.9456194639205933\n",
      "Epoch: 17, Train_Loss: 0.8449966311454773, Test_Loss: 0.9123591184616089 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.9520968198776245, Test_Loss: 0.8931336998939514 *\n",
      "Epoch: 17, Train_Loss: 0.9469664692878723, Test_Loss: 0.8514208197593689 *\n",
      "Epoch: 17, Train_Loss: 1.0628284215927124, Test_Loss: 0.8743621706962585\n",
      "Epoch: 17, Train_Loss: 0.9117107391357422, Test_Loss: 0.8837184906005859\n",
      "Epoch: 17, Train_Loss: 0.8784339427947998, Test_Loss: 0.8751312494277954 *\n",
      "Epoch: 17, Train_Loss: 0.8649101257324219, Test_Loss: 0.9378554224967957\n",
      "Epoch: 17, Train_Loss: 0.8485485315322876, Test_Loss: 0.8623653650283813 *\n",
      "Epoch: 17, Train_Loss: 0.906294047832489, Test_Loss: 0.863123893737793\n",
      "Epoch: 17, Train_Loss: 0.8570830821990967, Test_Loss: 0.909163236618042\n",
      "Epoch: 17, Train_Loss: 0.8535831570625305, Test_Loss: 1.1823077201843262\n",
      "Epoch: 17, Train_Loss: 0.8353687524795532, Test_Loss: 1.0473709106445312 *\n",
      "Epoch: 17, Train_Loss: 0.8490692973136902, Test_Loss: 0.8800651431083679 *\n",
      "Epoch: 17, Train_Loss: 0.8897552490234375, Test_Loss: 0.8343891501426697 *\n",
      "Epoch: 17, Train_Loss: 5.899881839752197, Test_Loss: 0.8338624835014343 *\n",
      "Epoch: 17, Train_Loss: 0.8738177418708801, Test_Loss: 0.834990918636322\n",
      "Epoch: 17, Train_Loss: 0.848953902721405, Test_Loss: 0.8340597152709961 *\n",
      "Epoch: 17, Train_Loss: 0.8983843326568604, Test_Loss: 1.0158236026763916\n",
      "Epoch: 17, Train_Loss: 0.8449076414108276, Test_Loss: 6.392763614654541\n",
      "Epoch: 17, Train_Loss: 0.8309471011161804, Test_Loss: 1.0153526067733765 *\n",
      "Epoch: 17, Train_Loss: 0.8338143229484558, Test_Loss: 0.87286776304245 *\n",
      "Epoch: 17, Train_Loss: 0.8489463925361633, Test_Loss: 0.8681118488311768 *\n",
      "Epoch: 17, Train_Loss: 0.8450301885604858, Test_Loss: 0.847219705581665 *\n",
      "Epoch: 17, Train_Loss: 0.8561156988143921, Test_Loss: 0.8654196262359619\n",
      "Epoch: 17, Train_Loss: 0.8842687606811523, Test_Loss: 0.8716038465499878\n",
      "Epoch: 17, Train_Loss: 0.8298439383506775, Test_Loss: 0.9296491742134094\n",
      "Epoch: 17, Train_Loss: 0.8307011127471924, Test_Loss: 0.8928623199462891 *\n",
      "Epoch: 17, Train_Loss: 0.8517811298370361, Test_Loss: 0.9354701042175293\n",
      "Epoch: 17, Train_Loss: 0.830548882484436, Test_Loss: 0.9156589508056641 *\n",
      "Epoch: 17, Train_Loss: 0.8302487730979919, Test_Loss: 0.9545336961746216\n",
      "Epoch: 17, Train_Loss: 0.8467444777488708, Test_Loss: 0.8656426072120667 *\n",
      "Epoch: 17, Train_Loss: 0.8471009135246277, Test_Loss: 0.858223021030426 *\n",
      "Epoch: 17, Train_Loss: 0.8498753905296326, Test_Loss: 0.9048064947128296\n",
      "Epoch: 17, Train_Loss: 0.8400012254714966, Test_Loss: 0.8595479130744934 *\n",
      "Epoch: 17, Train_Loss: 0.8379883766174316, Test_Loss: 0.8538789749145508 *\n",
      "Epoch: 18, Train_Loss: 0.8630231022834778, Test_Loss: 0.8618418574333191 *\n",
      "Epoch: 18, Train_Loss: 0.8569859266281128, Test_Loss: 0.88215172290802\n",
      "Epoch: 18, Train_Loss: 0.8638761639595032, Test_Loss: 0.8628404140472412 *\n",
      "Epoch: 18, Train_Loss: 0.8706979751586914, Test_Loss: 0.8877924680709839\n",
      "Epoch: 18, Train_Loss: 0.9069364666938782, Test_Loss: 0.8720697164535522 *\n",
      "Epoch: 18, Train_Loss: 0.8583043813705444, Test_Loss: 0.8695471286773682 *\n",
      "Epoch: 18, Train_Loss: 0.8499810695648193, Test_Loss: 0.891088604927063\n",
      "Epoch: 18, Train_Loss: 0.8745363354682922, Test_Loss: 0.8536128401756287 *\n",
      "Epoch: 18, Train_Loss: 1.0324690341949463, Test_Loss: 0.8535926342010498 *\n",
      "Epoch: 18, Train_Loss: 0.8624650835990906, Test_Loss: 0.8598581552505493\n",
      "Epoch: 18, Train_Loss: 0.8368174433708191, Test_Loss: 0.8510844707489014 *\n",
      "Epoch: 18, Train_Loss: 0.8235952258110046, Test_Loss: 0.8347311019897461 *\n",
      "Epoch: 18, Train_Loss: 0.8239930272102356, Test_Loss: 0.8744293451309204\n",
      "Epoch: 18, Train_Loss: 0.8242645263671875, Test_Loss: 0.9085606932640076\n",
      "Epoch: 18, Train_Loss: 0.821388304233551, Test_Loss: 2.9587631225585938\n",
      "Epoch: 18, Train_Loss: 0.83255535364151, Test_Loss: 4.393978595733643\n",
      "Epoch: 18, Train_Loss: 5.346652030944824, Test_Loss: 0.832677960395813 *\n",
      "Epoch: 18, Train_Loss: 1.0828416347503662, Test_Loss: 0.8229058384895325 *\n",
      "Epoch: 18, Train_Loss: 0.8247659802436829, Test_Loss: 0.8610838055610657\n",
      "Epoch: 18, Train_Loss: 0.8269522786140442, Test_Loss: 0.8476646542549133 *\n",
      "Epoch: 18, Train_Loss: 0.8239043354988098, Test_Loss: 0.8699703812599182\n",
      "Epoch: 18, Train_Loss: 0.8213486075401306, Test_Loss: 0.8515641093254089 *\n",
      "Epoch: 18, Train_Loss: 0.8228389024734497, Test_Loss: 0.933086633682251\n",
      "Epoch: 18, Train_Loss: 0.8186236619949341, Test_Loss: 0.8262329697608948 *\n",
      "Epoch: 18, Train_Loss: 0.8199407458305359, Test_Loss: 0.8355153799057007\n",
      "Epoch: 18, Train_Loss: 0.8202605247497559, Test_Loss: 0.8440819382667542\n",
      "Epoch: 18, Train_Loss: 0.8609976768493652, Test_Loss: 0.8368833065032959 *\n",
      "Epoch: 18, Train_Loss: 0.865202009677887, Test_Loss: 0.8265856504440308 *\n",
      "Epoch: 18, Train_Loss: 0.8835139274597168, Test_Loss: 0.8940441608428955\n",
      "Epoch: 18, Train_Loss: 0.8627169132232666, Test_Loss: 0.8988364934921265\n",
      "Epoch: 18, Train_Loss: 0.8249808549880981, Test_Loss: 0.8829194903373718 *\n",
      "Epoch: 18, Train_Loss: 0.9042522311210632, Test_Loss: 0.9226258397102356\n",
      "Epoch: 18, Train_Loss: 1.037990927696228, Test_Loss: 0.8416876792907715 *\n",
      "Epoch: 18, Train_Loss: 1.0265964269638062, Test_Loss: 0.8531510233879089\n",
      "Epoch: 18, Train_Loss: 1.0180587768554688, Test_Loss: 0.830818235874176 *\n",
      "Epoch: 18, Train_Loss: 0.8229072093963623, Test_Loss: 0.827501654624939 *\n",
      "Epoch: 18, Train_Loss: 0.8154895305633545, Test_Loss: 0.8276882767677307\n",
      "Epoch: 18, Train_Loss: 0.8170459866523743, Test_Loss: 0.826858639717102 *\n",
      "Epoch: 18, Train_Loss: 0.8179032802581787, Test_Loss: 0.8240638971328735 *\n",
      "Epoch: 18, Train_Loss: 0.8213210701942444, Test_Loss: 0.8268459439277649\n",
      "Epoch: 18, Train_Loss: 0.8206787109375, Test_Loss: 0.8271182775497437\n",
      "Epoch: 18, Train_Loss: 0.8196520209312439, Test_Loss: 0.8285183906555176\n",
      "Epoch: 18, Train_Loss: 0.8185759782791138, Test_Loss: 0.8261886239051819 *\n",
      "Epoch: 18, Train_Loss: 0.8182830214500427, Test_Loss: 0.8251765370368958 *\n",
      "Epoch: 18, Train_Loss: 0.8257654905319214, Test_Loss: 0.8302350044250488\n",
      "Epoch: 18, Train_Loss: 0.943579912185669, Test_Loss: 0.8652312159538269\n",
      "Epoch: 18, Train_Loss: 0.9202879667282104, Test_Loss: 1.1211267709732056\n",
      "Epoch: 18, Train_Loss: 0.9469007849693298, Test_Loss: 0.9532710313796997 *\n",
      "Epoch: 18, Train_Loss: 0.8676641583442688, Test_Loss: 0.8335709571838379 *\n",
      "Epoch: 18, Train_Loss: 1.0116206407546997, Test_Loss: 0.8644149899482727\n",
      "Epoch: 18, Train_Loss: 0.9979243278503418, Test_Loss: 0.9873414039611816\n",
      "Epoch: 18, Train_Loss: 0.8866720199584961, Test_Loss: 0.9937159419059753\n",
      "Epoch: 18, Train_Loss: 1.0021377801895142, Test_Loss: 0.8323729038238525 *\n",
      "Epoch: 18, Train_Loss: 1.040104627609253, Test_Loss: 1.0656921863555908\n",
      "Epoch: 18, Train_Loss: 0.998361349105835, Test_Loss: 1.1553144454956055\n",
      "Epoch: 18, Train_Loss: 0.8253982663154602, Test_Loss: 0.8533157706260681 *\n",
      "Epoch: 18, Train_Loss: 2.5983989238739014, Test_Loss: 0.8876425623893738\n",
      "Epoch: 18, Train_Loss: 2.2562785148620605, Test_Loss: 0.8159743547439575 *\n",
      "Epoch: 18, Train_Loss: 0.8421846032142639, Test_Loss: 0.8386103510856628\n",
      "Epoch: 18, Train_Loss: 0.8478431701660156, Test_Loss: 0.8251063823699951 *\n",
      "Epoch: 18, Train_Loss: 0.8576285243034363, Test_Loss: 1.511760950088501\n",
      "Epoch: 18, Train_Loss: 0.8454025983810425, Test_Loss: 1.0553176403045654 *\n",
      "Epoch: 18, Train_Loss: 0.808070182800293, Test_Loss: 1.531569242477417\n",
      "Epoch: 18, Train_Loss: 0.8231536149978638, Test_Loss: 1.5630677938461304\n",
      "Epoch: 18, Train_Loss: 0.9353477954864502, Test_Loss: 0.9860518574714661 *\n",
      "Epoch: 18, Train_Loss: 0.8739957809448242, Test_Loss: 1.2845957279205322\n",
      "Epoch: 18, Train_Loss: 0.8582921028137207, Test_Loss: 0.8789675831794739 *\n",
      "Epoch: 18, Train_Loss: 0.8480247259140015, Test_Loss: 0.824375569820404 *\n",
      "Epoch: 18, Train_Loss: 0.831885576248169, Test_Loss: 0.8327276706695557\n",
      "Epoch: 18, Train_Loss: 0.819726824760437, Test_Loss: 0.9876714944839478\n",
      "Epoch: 18, Train_Loss: 0.8224243521690369, Test_Loss: 1.071187138557434\n",
      "Epoch: 18, Train_Loss: 0.8488043546676636, Test_Loss: 1.4954478740692139\n",
      "Epoch: 18, Train_Loss: 0.8550446033477783, Test_Loss: 1.8258934020996094\n",
      "Epoch: 18, Train_Loss: 0.8238366842269897, Test_Loss: 2.501671552658081\n",
      "Epoch: 18, Train_Loss: 0.811173141002655, Test_Loss: 1.4829041957855225 *\n",
      "Epoch: 18, Train_Loss: 0.8235424757003784, Test_Loss: 1.2778406143188477 *\n",
      "Epoch: 18, Train_Loss: 0.8254532217979431, Test_Loss: 0.8061291575431824 *\n",
      "Epoch: 18, Train_Loss: 0.8199024200439453, Test_Loss: 0.8352917432785034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.8082998394966125, Test_Loss: 1.6456239223480225\n",
      "Epoch: 18, Train_Loss: 0.8085917234420776, Test_Loss: 2.362302780151367\n",
      "Epoch: 18, Train_Loss: 0.8084349036216736, Test_Loss: 0.8596762418746948 *\n",
      "Epoch: 18, Train_Loss: 0.8030767440795898, Test_Loss: 0.8615087866783142\n",
      "Epoch: 18, Train_Loss: 0.8027430772781372, Test_Loss: 0.8224624991416931 *\n",
      "Epoch: 18, Train_Loss: 0.8050528168678284, Test_Loss: 1.0896183252334595\n",
      "Epoch: 18, Train_Loss: 0.8045179843902588, Test_Loss: 1.1336145401000977\n",
      "Epoch: 18, Train_Loss: 0.8026999831199646, Test_Loss: 1.3011786937713623\n",
      "Epoch: 18, Train_Loss: 0.8055246472358704, Test_Loss: 1.7029510736465454\n",
      "Epoch: 18, Train_Loss: 0.8066498637199402, Test_Loss: 1.0702768564224243 *\n",
      "Epoch: 18, Train_Loss: 0.8155745267868042, Test_Loss: 0.8039860129356384 *\n",
      "Epoch: 18, Train_Loss: 0.8160164952278137, Test_Loss: 0.8049972057342529\n",
      "Epoch: 18, Train_Loss: 0.8190088272094727, Test_Loss: 0.8085651397705078\n",
      "Epoch: 18, Train_Loss: 0.8328341841697693, Test_Loss: 0.8276379704475403\n",
      "Epoch: 18, Train_Loss: 0.8069557547569275, Test_Loss: 1.1304726600646973\n",
      "Epoch: 18, Train_Loss: 0.8053248524665833, Test_Loss: 1.4175403118133545\n",
      "Epoch: 18, Train_Loss: 0.8038398027420044, Test_Loss: 1.0720300674438477 *\n",
      "Epoch: 18, Train_Loss: 0.8060369491577148, Test_Loss: 0.9018973708152771 *\n",
      "Epoch: 18, Train_Loss: 0.825493335723877, Test_Loss: 0.8459960222244263 *\n",
      "Epoch: 18, Train_Loss: 0.8043221831321716, Test_Loss: 0.8036983609199524 *\n",
      "Epoch: 18, Train_Loss: 0.8013824820518494, Test_Loss: 0.8642393946647644\n",
      "Epoch: 18, Train_Loss: 0.7960337400436401, Test_Loss: 1.5082629919052124\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 18\n",
      "Epoch: 18, Train_Loss: 0.8115292191505432, Test_Loss: 2.024846315383911\n",
      "Epoch: 18, Train_Loss: 0.8659986257553101, Test_Loss: 0.9245620369911194 *\n",
      "Epoch: 18, Train_Loss: 0.8417599201202393, Test_Loss: 0.9207941293716431 *\n",
      "Epoch: 18, Train_Loss: 0.8333755731582642, Test_Loss: 0.7981376647949219 *\n",
      "Epoch: 18, Train_Loss: 0.7973234057426453, Test_Loss: 0.8009274005889893\n",
      "Epoch: 18, Train_Loss: 0.8453753590583801, Test_Loss: 0.7999966144561768 *\n",
      "Epoch: 18, Train_Loss: 0.8262719511985779, Test_Loss: 0.8106666207313538\n",
      "Epoch: 18, Train_Loss: 0.7982155084609985, Test_Loss: 0.8269518613815308\n",
      "Epoch: 18, Train_Loss: 0.8116156458854675, Test_Loss: 0.8276796340942383\n",
      "Epoch: 18, Train_Loss: 0.8193753361701965, Test_Loss: 0.7996936440467834 *\n",
      "Epoch: 18, Train_Loss: 0.8974291682243347, Test_Loss: 0.9167088270187378\n",
      "Epoch: 18, Train_Loss: 0.86716228723526, Test_Loss: 1.1964566707611084\n",
      "Epoch: 18, Train_Loss: 0.8412408232688904, Test_Loss: 0.894472599029541 *\n",
      "Epoch: 18, Train_Loss: 0.8108134865760803, Test_Loss: 0.9659492373466492\n",
      "Epoch: 18, Train_Loss: 0.8001337051391602, Test_Loss: 0.8063263893127441 *\n",
      "Epoch: 18, Train_Loss: 0.8117508888244629, Test_Loss: 0.81075519323349\n",
      "Epoch: 18, Train_Loss: 0.792503833770752, Test_Loss: 0.8042717576026917 *\n",
      "Epoch: 18, Train_Loss: 0.797937273979187, Test_Loss: 0.8065454363822937\n",
      "Epoch: 18, Train_Loss: 0.8116925954818726, Test_Loss: 0.8188780546188354\n",
      "Epoch: 18, Train_Loss: 0.8195814490318298, Test_Loss: 5.673931121826172\n",
      "Epoch: 18, Train_Loss: 0.9012402892112732, Test_Loss: 1.3077945709228516 *\n",
      "Epoch: 18, Train_Loss: 0.7931854128837585, Test_Loss: 0.8008102178573608 *\n",
      "Epoch: 18, Train_Loss: 0.8607082366943359, Test_Loss: 0.7946817278862 *\n",
      "Epoch: 18, Train_Loss: 0.8010452389717102, Test_Loss: 0.7969205379486084\n",
      "Epoch: 18, Train_Loss: 0.8371670842170715, Test_Loss: 0.8020898699760437\n",
      "Epoch: 18, Train_Loss: 0.8401100039482117, Test_Loss: 0.7941502928733826 *\n",
      "Epoch: 18, Train_Loss: 1.0675508975982666, Test_Loss: 0.7936586141586304 *\n",
      "Epoch: 18, Train_Loss: 0.8079045414924622, Test_Loss: 0.790839672088623 *\n",
      "Epoch: 18, Train_Loss: 0.8325220346450806, Test_Loss: 0.7897306680679321 *\n",
      "Epoch: 18, Train_Loss: 0.7887241840362549, Test_Loss: 0.7919268608093262\n",
      "Epoch: 18, Train_Loss: 0.7888372540473938, Test_Loss: 0.7933688163757324\n",
      "Epoch: 18, Train_Loss: 0.7895689010620117, Test_Loss: 0.8020439147949219\n",
      "Epoch: 18, Train_Loss: 0.7873584032058716, Test_Loss: 0.8224809765815735\n",
      "Epoch: 18, Train_Loss: 0.7949900031089783, Test_Loss: 0.805095374584198 *\n",
      "Epoch: 18, Train_Loss: 0.7979568839073181, Test_Loss: 0.7871142625808716 *\n",
      "Epoch: 18, Train_Loss: 0.8040796518325806, Test_Loss: 0.7876145839691162\n",
      "Epoch: 18, Train_Loss: 0.7923570275306702, Test_Loss: 0.7916024327278137\n",
      "Epoch: 18, Train_Loss: 0.7989947199821472, Test_Loss: 0.7879552245140076 *\n",
      "Epoch: 18, Train_Loss: 0.7986775040626526, Test_Loss: 0.7869560718536377 *\n",
      "Epoch: 18, Train_Loss: 0.7896566390991211, Test_Loss: 0.788650393486023\n",
      "Epoch: 18, Train_Loss: 0.784703254699707, Test_Loss: 0.786864697933197 *\n",
      "Epoch: 18, Train_Loss: 0.8062663078308105, Test_Loss: 0.7887269854545593\n",
      "Epoch: 18, Train_Loss: 0.8147770762443542, Test_Loss: 0.7894544005393982\n",
      "Epoch: 18, Train_Loss: 0.8287569284439087, Test_Loss: 0.7872681617736816 *\n",
      "Epoch: 18, Train_Loss: 0.7842696905136108, Test_Loss: 0.7864469289779663 *\n",
      "Epoch: 18, Train_Loss: 0.8183629512786865, Test_Loss: 0.785172164440155 *\n",
      "Epoch: 18, Train_Loss: 0.8314791917800903, Test_Loss: 0.7851879000663757\n",
      "Epoch: 18, Train_Loss: 0.8201214671134949, Test_Loss: 0.7847617864608765 *\n",
      "Epoch: 18, Train_Loss: 0.7838836908340454, Test_Loss: 0.7875269055366516\n",
      "Epoch: 18, Train_Loss: 0.8236633539199829, Test_Loss: 0.8459271788597107\n",
      "Epoch: 18, Train_Loss: 0.7846590280532837, Test_Loss: 1.354221224784851\n",
      "Epoch: 18, Train_Loss: 0.7984167337417603, Test_Loss: 5.644573211669922\n",
      "Epoch: 18, Train_Loss: 0.7870547771453857, Test_Loss: 0.7895300388336182 *\n",
      "Epoch: 18, Train_Loss: 0.8108216524124146, Test_Loss: 0.7815965414047241 *\n",
      "Epoch: 18, Train_Loss: 1.1652882099151611, Test_Loss: 0.8197308778762817\n",
      "Epoch: 18, Train_Loss: 4.658531188964844, Test_Loss: 0.8406654000282288\n",
      "Epoch: 18, Train_Loss: 2.0368034839630127, Test_Loss: 0.8467994928359985\n",
      "Epoch: 18, Train_Loss: 0.7976312637329102, Test_Loss: 0.7876132726669312 *\n",
      "Epoch: 18, Train_Loss: 0.7823237180709839, Test_Loss: 0.9016096591949463\n",
      "Epoch: 18, Train_Loss: 0.9344096183776855, Test_Loss: 0.8132256269454956 *\n",
      "Epoch: 18, Train_Loss: 0.8708930015563965, Test_Loss: 0.7830988764762878 *\n",
      "Epoch: 18, Train_Loss: 0.7894706726074219, Test_Loss: 0.815280020236969\n",
      "Epoch: 18, Train_Loss: 0.7794235944747925, Test_Loss: 0.7952055335044861 *\n",
      "Epoch: 18, Train_Loss: 0.8344317674636841, Test_Loss: 0.7893328070640564 *\n",
      "Epoch: 18, Train_Loss: 0.7954748272895813, Test_Loss: 0.828911542892456\n",
      "Epoch: 18, Train_Loss: 0.796400785446167, Test_Loss: 0.8833960294723511\n",
      "Epoch: 18, Train_Loss: 1.0011858940124512, Test_Loss: 0.8328129649162292 *\n",
      "Epoch: 18, Train_Loss: 2.133869171142578, Test_Loss: 0.8653739094734192\n",
      "Epoch: 18, Train_Loss: 2.01725697517395, Test_Loss: 0.811116635799408 *\n",
      "Epoch: 18, Train_Loss: 0.8781901597976685, Test_Loss: 0.8324264883995056\n",
      "Epoch: 18, Train_Loss: 0.8570449948310852, Test_Loss: 0.8095425963401794 *\n",
      "Epoch: 18, Train_Loss: 3.0697546005249023, Test_Loss: 0.8066332340240479 *\n",
      "Epoch: 18, Train_Loss: 2.0182104110717773, Test_Loss: 0.8121497631072998\n",
      "Epoch: 18, Train_Loss: 0.828866720199585, Test_Loss: 0.8125544190406799\n",
      "Epoch: 18, Train_Loss: 0.809394359588623, Test_Loss: 0.806083083152771 *\n",
      "Epoch: 18, Train_Loss: 1.2825716733932495, Test_Loss: 0.7997433543205261 *\n",
      "Epoch: 18, Train_Loss: 2.3636155128479004, Test_Loss: 0.79429030418396 *\n",
      "Epoch: 18, Train_Loss: 1.6424076557159424, Test_Loss: 0.7951786518096924\n",
      "Epoch: 18, Train_Loss: 0.7818573117256165, Test_Loss: 0.8070248365402222\n",
      "Epoch: 18, Train_Loss: 0.7927147150039673, Test_Loss: 0.783809244632721 *\n",
      "Epoch: 18, Train_Loss: 1.0305951833724976, Test_Loss: 0.7847068309783936\n",
      "Epoch: 18, Train_Loss: 1.3313145637512207, Test_Loss: 0.8143452405929565\n",
      "Epoch: 18, Train_Loss: 0.798701286315918, Test_Loss: 0.9266903400421143\n",
      "Epoch: 18, Train_Loss: 0.8308461308479309, Test_Loss: 1.0218095779418945\n",
      "Epoch: 18, Train_Loss: 0.8784947395324707, Test_Loss: 0.793611466884613 *\n",
      "Epoch: 18, Train_Loss: 0.8713505268096924, Test_Loss: 0.8228293657302856\n",
      "Epoch: 18, Train_Loss: 0.9027916789054871, Test_Loss: 0.9792184233665466\n",
      "Epoch: 18, Train_Loss: 1.0329792499542236, Test_Loss: 1.0727874040603638\n",
      "Epoch: 18, Train_Loss: 0.9367545247077942, Test_Loss: 0.7977817058563232 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.8280917406082153, Test_Loss: 0.8528100252151489\n",
      "Epoch: 18, Train_Loss: 0.9378805756568909, Test_Loss: 0.9247766733169556\n",
      "Epoch: 18, Train_Loss: 0.9723162651062012, Test_Loss: 0.8565236330032349 *\n",
      "Epoch: 18, Train_Loss: 1.139628291130066, Test_Loss: 0.8565300703048706\n",
      "Epoch: 18, Train_Loss: 1.0518945455551147, Test_Loss: 0.7806790471076965 *\n",
      "Epoch: 18, Train_Loss: 0.8159651756286621, Test_Loss: 0.7799057364463806 *\n",
      "Epoch: 18, Train_Loss: 0.8971313834190369, Test_Loss: 0.7873737812042236\n",
      "Epoch: 18, Train_Loss: 0.8564402461051941, Test_Loss: 1.1832964420318604\n",
      "Epoch: 18, Train_Loss: 0.785290002822876, Test_Loss: 1.1065783500671387 *\n",
      "Epoch: 18, Train_Loss: 0.7728322148323059, Test_Loss: 1.2539345026016235\n",
      "Epoch: 18, Train_Loss: 0.7708095908164978, Test_Loss: 1.422592282295227\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 18\n",
      "Epoch: 18, Train_Loss: 0.7710012793540955, Test_Loss: 0.9790410995483398 *\n",
      "Epoch: 18, Train_Loss: 0.7776859998703003, Test_Loss: 1.1426875591278076\n",
      "Epoch: 18, Train_Loss: 0.7827190160751343, Test_Loss: 0.8808062076568604 *\n",
      "Epoch: 18, Train_Loss: 0.8114854097366333, Test_Loss: 0.7816969156265259 *\n",
      "Epoch: 18, Train_Loss: 0.7927327752113342, Test_Loss: 0.7784587740898132 *\n",
      "Epoch: 18, Train_Loss: 0.8623881936073303, Test_Loss: 0.8069178462028503\n",
      "Epoch: 18, Train_Loss: 0.9511364698410034, Test_Loss: 0.9737356305122375\n",
      "Epoch: 18, Train_Loss: 1.0604510307312012, Test_Loss: 1.4464514255523682\n",
      "Epoch: 18, Train_Loss: 0.8017968535423279, Test_Loss: 1.1928993463516235 *\n",
      "Epoch: 18, Train_Loss: 0.8214716911315918, Test_Loss: 2.5967581272125244\n",
      "Epoch: 18, Train_Loss: 1.0550100803375244, Test_Loss: 1.2246447801589966 *\n",
      "Epoch: 18, Train_Loss: 1.1155186891555786, Test_Loss: 1.5537135601043701\n",
      "Epoch: 18, Train_Loss: 0.8187918066978455, Test_Loss: 0.8113008737564087 *\n",
      "Epoch: 18, Train_Loss: 0.7834368944168091, Test_Loss: 0.7758918404579163 *\n",
      "Epoch: 18, Train_Loss: 1.164119005203247, Test_Loss: 1.215825080871582\n",
      "Epoch: 18, Train_Loss: 1.3068106174468994, Test_Loss: 2.2814035415649414\n",
      "Epoch: 18, Train_Loss: 0.9497867226600647, Test_Loss: 1.0648702383041382 *\n",
      "Epoch: 18, Train_Loss: 0.7866106629371643, Test_Loss: 0.8646111488342285 *\n",
      "Epoch: 18, Train_Loss: 0.8013972043991089, Test_Loss: 0.7984456419944763 *\n",
      "Epoch: 18, Train_Loss: 1.0655276775360107, Test_Loss: 0.9174034595489502\n",
      "Epoch: 18, Train_Loss: 2.1072237491607666, Test_Loss: 1.1793286800384521\n",
      "Epoch: 18, Train_Loss: 1.0610193014144897, Test_Loss: 1.0707495212554932 *\n",
      "Epoch: 18, Train_Loss: 0.8053798079490662, Test_Loss: 1.5594285726547241\n",
      "Epoch: 18, Train_Loss: 0.7812753319740295, Test_Loss: 1.0564420223236084 *\n",
      "Epoch: 18, Train_Loss: 0.7790645360946655, Test_Loss: 0.7907113432884216 *\n",
      "Epoch: 18, Train_Loss: 1.1550309658050537, Test_Loss: 0.7799344658851624 *\n",
      "Epoch: 18, Train_Loss: 0.8768457174301147, Test_Loss: 0.8106691241264343\n",
      "Epoch: 18, Train_Loss: 0.7874664664268494, Test_Loss: 0.8015069365501404 *\n",
      "Epoch: 18, Train_Loss: 0.7787935733795166, Test_Loss: 0.9336793422698975\n",
      "Epoch: 18, Train_Loss: 0.8026154637336731, Test_Loss: 1.0994415283203125\n",
      "Epoch: 18, Train_Loss: 15.717905044555664, Test_Loss: 0.9835278391838074 *\n",
      "Epoch: 18, Train_Loss: 2.752825975418091, Test_Loss: 0.848755955696106 *\n",
      "Epoch: 18, Train_Loss: 1.8001737594604492, Test_Loss: 0.7737863659858704 *\n",
      "Epoch: 18, Train_Loss: 2.5317084789276123, Test_Loss: 0.7824715971946716\n",
      "Epoch: 18, Train_Loss: 0.8162470459938049, Test_Loss: 0.8381983637809753\n",
      "Epoch: 18, Train_Loss: 0.8669339418411255, Test_Loss: 1.1810625791549683\n",
      "Epoch: 18, Train_Loss: 1.7013583183288574, Test_Loss: 1.648297905921936\n",
      "Epoch: 18, Train_Loss: 10.336959838867188, Test_Loss: 1.0763165950775146 *\n",
      "Epoch: 18, Train_Loss: 1.3990700244903564, Test_Loss: 0.8225917220115662 *\n",
      "Epoch: 18, Train_Loss: 0.8008903861045837, Test_Loss: 0.790626585483551 *\n",
      "Epoch: 18, Train_Loss: 5.544821262359619, Test_Loss: 0.8061909079551697\n",
      "Epoch: 18, Train_Loss: 1.891514539718628, Test_Loss: 0.8128450512886047\n",
      "Epoch: 18, Train_Loss: 0.8327108025550842, Test_Loss: 0.8428635597229004\n",
      "Epoch: 18, Train_Loss: 0.7666433453559875, Test_Loss: 0.8098308444023132 *\n",
      "Epoch: 18, Train_Loss: 0.7684156894683838, Test_Loss: 0.8928074240684509\n",
      "Epoch: 18, Train_Loss: 0.7763928771018982, Test_Loss: 0.7926521301269531 *\n",
      "Epoch: 18, Train_Loss: 0.7673460245132446, Test_Loss: 0.8254405856132507\n",
      "Epoch: 18, Train_Loss: 0.7687335014343262, Test_Loss: 1.0747865438461304\n",
      "Epoch: 18, Train_Loss: 0.756108283996582, Test_Loss: 0.8528318405151367 *\n",
      "Epoch: 18, Train_Loss: 0.7568327784538269, Test_Loss: 0.9165478944778442\n",
      "Epoch: 18, Train_Loss: 0.7642384767532349, Test_Loss: 0.7813228964805603 *\n",
      "Epoch: 18, Train_Loss: 0.7924998998641968, Test_Loss: 0.7882558703422546\n",
      "Epoch: 18, Train_Loss: 0.7854605913162231, Test_Loss: 0.7859200835227966 *\n",
      "Epoch: 18, Train_Loss: 0.8116835951805115, Test_Loss: 0.801446259021759\n",
      "Epoch: 18, Train_Loss: 0.8900166749954224, Test_Loss: 0.7720365524291992 *\n",
      "Epoch: 18, Train_Loss: 0.8766182661056519, Test_Loss: 4.781796455383301\n",
      "Epoch: 18, Train_Loss: 0.7809943556785583, Test_Loss: 3.1335315704345703 *\n",
      "Epoch: 18, Train_Loss: 0.7685547471046448, Test_Loss: 0.8052980303764343 *\n",
      "Epoch: 18, Train_Loss: 0.7902579307556152, Test_Loss: 0.823236346244812\n",
      "Epoch: 18, Train_Loss: 0.76337730884552, Test_Loss: 0.8221194744110107 *\n",
      "Epoch: 18, Train_Loss: 0.7604081630706787, Test_Loss: 0.7713609337806702 *\n",
      "Epoch: 18, Train_Loss: 0.753314197063446, Test_Loss: 0.8489887714385986\n",
      "Epoch: 18, Train_Loss: 0.754675030708313, Test_Loss: 0.8592052459716797\n",
      "Epoch: 18, Train_Loss: 0.7537330389022827, Test_Loss: 0.8642449975013733\n",
      "Epoch: 18, Train_Loss: 0.75298672914505, Test_Loss: 0.8558158278465271 *\n",
      "Epoch: 18, Train_Loss: 0.7516270279884338, Test_Loss: 0.8620271682739258\n",
      "Epoch: 18, Train_Loss: 0.7550561428070068, Test_Loss: 0.8479458689689636 *\n",
      "Epoch: 18, Train_Loss: 0.76523357629776, Test_Loss: 0.8515342473983765\n",
      "Epoch: 18, Train_Loss: 0.770352840423584, Test_Loss: 0.7776442766189575 *\n",
      "Epoch: 18, Train_Loss: 0.8115874528884888, Test_Loss: 0.782448410987854\n",
      "Epoch: 18, Train_Loss: 0.7627849578857422, Test_Loss: 0.8284066915512085\n",
      "Epoch: 18, Train_Loss: 0.7633456587791443, Test_Loss: 0.7691217064857483 *\n",
      "Epoch: 18, Train_Loss: 8.984113693237305, Test_Loss: 0.8248074054718018\n",
      "Epoch: 18, Train_Loss: 0.8986197113990784, Test_Loss: 0.7981460094451904 *\n",
      "Epoch: 18, Train_Loss: 0.7966960072517395, Test_Loss: 0.8285532593727112\n",
      "Epoch: 18, Train_Loss: 0.8367434144020081, Test_Loss: 0.8474650382995605\n",
      "Epoch: 18, Train_Loss: 0.8601087331771851, Test_Loss: 0.8655299544334412\n",
      "Epoch: 18, Train_Loss: 0.7975921630859375, Test_Loss: 0.85274338722229 *\n",
      "Epoch: 18, Train_Loss: 0.8129428625106812, Test_Loss: 0.8674190640449524\n",
      "Epoch: 18, Train_Loss: 0.8736414909362793, Test_Loss: 0.8217682838439941 *\n",
      "Epoch: 18, Train_Loss: 1.0314311981201172, Test_Loss: 0.8358906507492065\n",
      "Epoch: 18, Train_Loss: 0.9752333164215088, Test_Loss: 0.7923164367675781 *\n",
      "Epoch: 18, Train_Loss: 0.8855661749839783, Test_Loss: 0.8036093711853027\n",
      "Epoch: 18, Train_Loss: 0.7539568543434143, Test_Loss: 0.7870030999183655 *\n",
      "Epoch: 18, Train_Loss: 0.8807753324508667, Test_Loss: 0.7802016139030457 *\n",
      "Epoch: 18, Train_Loss: 0.8167146444320679, Test_Loss: 0.8894221186637878\n",
      "Epoch: 18, Train_Loss: 0.9050976634025574, Test_Loss: 0.7986151576042175 *\n",
      "Epoch: 18, Train_Loss: 0.8022598624229431, Test_Loss: 7.01844596862793\n",
      "Epoch: 18, Train_Loss: 0.7983641624450684, Test_Loss: 1.012951374053955 *\n",
      "Epoch: 18, Train_Loss: 0.7707515954971313, Test_Loss: 0.7533160448074341 *\n",
      "Epoch: 18, Train_Loss: 0.7688363790512085, Test_Loss: 0.766486644744873\n",
      "Epoch: 18, Train_Loss: 0.877498984336853, Test_Loss: 0.758720874786377 *\n",
      "Epoch: 18, Train_Loss: 0.7809979915618896, Test_Loss: 0.7670491933822632\n",
      "Epoch: 18, Train_Loss: 0.7529959082603455, Test_Loss: 0.7617071866989136 *\n",
      "Epoch: 18, Train_Loss: 0.7470521330833435, Test_Loss: 0.879128634929657\n",
      "Epoch: 18, Train_Loss: 0.7467740178108215, Test_Loss: 0.8501800894737244 *\n",
      "Epoch: 18, Train_Loss: 0.9285086989402771, Test_Loss: 0.7493896484375 *\n",
      "Epoch: 18, Train_Loss: 6.001863479614258, Test_Loss: 0.7980554699897766\n",
      "Epoch: 18, Train_Loss: 0.751603901386261, Test_Loss: 0.7532204389572144 *\n",
      "Epoch: 18, Train_Loss: 0.7554815411567688, Test_Loss: 0.7638264894485474\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.794918417930603, Test_Loss: 0.7672085165977478\n",
      "Epoch: 18, Train_Loss: 0.7715171575546265, Test_Loss: 0.7812946438789368\n",
      "Epoch: 18, Train_Loss: 0.7547787427902222, Test_Loss: 0.8402727246284485\n",
      "Epoch: 18, Train_Loss: 0.7651625871658325, Test_Loss: 0.8850436806678772\n",
      "Epoch: 18, Train_Loss: 0.7743874192237854, Test_Loss: 0.8238582015037537 *\n",
      "Epoch: 18, Train_Loss: 0.7561092972755432, Test_Loss: 0.7598873972892761 *\n",
      "Epoch: 18, Train_Loss: 0.7782195806503296, Test_Loss: 0.7506385445594788 *\n",
      "Epoch: 18, Train_Loss: 0.7940759062767029, Test_Loss: 0.7540976405143738\n",
      "Epoch: 18, Train_Loss: 0.7455803751945496, Test_Loss: 0.7530508041381836 *\n",
      "Epoch: 18, Train_Loss: 0.7540585398674011, Test_Loss: 0.756262481212616\n",
      "Epoch: 18, Train_Loss: 0.7786420583724976, Test_Loss: 0.7479868531227112 *\n",
      "Epoch: 18, Train_Loss: 0.7496628165245056, Test_Loss: 0.7470479011535645 *\n",
      "Epoch: 18, Train_Loss: 0.7469984292984009, Test_Loss: 0.7523204684257507\n",
      "Epoch: 18, Train_Loss: 0.7642556428909302, Test_Loss: 0.7467591166496277 *\n",
      "Epoch: 18, Train_Loss: 0.765506386756897, Test_Loss: 0.7468867897987366\n",
      "Epoch: 18, Train_Loss: 0.7607877254486084, Test_Loss: 0.7707257270812988\n",
      "Epoch: 18, Train_Loss: 0.7480132579803467, Test_Loss: 0.7691457271575928 *\n",
      "Epoch: 18, Train_Loss: 0.7468566298484802, Test_Loss: 0.7608028054237366 *\n",
      "Epoch: 18, Train_Loss: 0.7765091061592102, Test_Loss: 0.7804257273674011\n",
      "Epoch: 18, Train_Loss: 0.7683645486831665, Test_Loss: 1.1952003240585327\n",
      "Epoch: 18, Train_Loss: 0.781461238861084, Test_Loss: 0.7711537480354309 *\n",
      "Epoch: 18, Train_Loss: 0.7715336084365845, Test_Loss: 0.7641510963439941 *\n",
      "Epoch: 18, Train_Loss: 0.8050053715705872, Test_Loss: 0.8046886324882507\n",
      "Epoch: 18, Train_Loss: 0.7670325040817261, Test_Loss: 1.0159004926681519\n",
      "Epoch: 18, Train_Loss: 0.7799233198165894, Test_Loss: 0.765441358089447 *\n",
      "Epoch: 18, Train_Loss: 0.7638508081436157, Test_Loss: 0.8588703274726868\n",
      "Epoch: 18, Train_Loss: 0.9181572198867798, Test_Loss: 0.9451775550842285\n",
      "Epoch: 18, Train_Loss: 0.7623282670974731, Test_Loss: 0.9221773147583008 *\n",
      "Epoch: 18, Train_Loss: 0.7471868395805359, Test_Loss: 0.8053732514381409 *\n",
      "Epoch: 18, Train_Loss: 0.7372326254844666, Test_Loss: 0.7504662871360779 *\n",
      "Epoch: 18, Train_Loss: 0.7367231249809265, Test_Loss: 0.7484285831451416 *\n",
      "Epoch: 18, Train_Loss: 0.7354753017425537, Test_Loss: 0.7711516618728638\n",
      "Epoch: 18, Train_Loss: 0.735245406627655, Test_Loss: 1.000699520111084\n",
      "Epoch: 18, Train_Loss: 0.8745715618133545, Test_Loss: 1.350536823272705\n",
      "Epoch: 18, Train_Loss: 5.1166839599609375, Test_Loss: 1.057845115661621 *\n",
      "Epoch: 18, Train_Loss: 0.7870962023735046, Test_Loss: 1.4691894054412842\n",
      "Epoch: 18, Train_Loss: 0.7395707368850708, Test_Loss: 1.0163567066192627 *\n",
      "Epoch: 18, Train_Loss: 0.7378495931625366, Test_Loss: 1.1840087175369263\n",
      "Epoch: 18, Train_Loss: 0.7403545379638672, Test_Loss: 0.9518685936927795 *\n",
      "Epoch: 18, Train_Loss: 0.7366320490837097, Test_Loss: 0.7448334693908691 *\n",
      "Epoch: 18, Train_Loss: 0.7359647154808044, Test_Loss: 0.753705620765686\n",
      "Epoch: 18, Train_Loss: 0.7330514788627625, Test_Loss: 0.7931399941444397\n",
      "Epoch: 18, Train_Loss: 0.7365550398826599, Test_Loss: 0.9101333618164062\n",
      "Epoch: 18, Train_Loss: 0.7353755831718445, Test_Loss: 1.5857632160186768\n",
      "Epoch: 18, Train_Loss: 0.7810214757919312, Test_Loss: 0.9239235520362854 *\n",
      "Epoch: 18, Train_Loss: 0.7718989253044128, Test_Loss: 2.9527885913848877\n",
      "Epoch: 18, Train_Loss: 0.7918401956558228, Test_Loss: 1.285056710243225 *\n",
      "Epoch: 18, Train_Loss: 0.7736462950706482, Test_Loss: 1.6666808128356934\n",
      "Epoch: 18, Train_Loss: 0.73846036195755, Test_Loss: 0.7974560260772705 *\n",
      "Epoch: 18, Train_Loss: 0.8762799501419067, Test_Loss: 0.7374728322029114 *\n",
      "Epoch: 18, Train_Loss: 0.9803907871246338, Test_Loss: 0.9947949647903442\n",
      "Epoch: 18, Train_Loss: 0.9566981196403503, Test_Loss: 2.223378896713257\n",
      "Epoch: 18, Train_Loss: 0.9071333408355713, Test_Loss: 1.3544217348098755 *\n",
      "Epoch: 18, Train_Loss: 0.7336100935935974, Test_Loss: 0.8128881454467773 *\n",
      "Epoch: 18, Train_Loss: 0.7311673760414124, Test_Loss: 0.7486013770103455 *\n",
      "Epoch: 18, Train_Loss: 0.733401358127594, Test_Loss: 0.8511077165603638\n",
      "Epoch: 18, Train_Loss: 0.745151937007904, Test_Loss: 1.1845974922180176\n",
      "Epoch: 18, Train_Loss: 0.7417395114898682, Test_Loss: 0.9589592814445496 *\n",
      "Epoch: 18, Train_Loss: 0.7369332313537598, Test_Loss: 1.8564794063568115\n",
      "Epoch: 18, Train_Loss: 0.7311521172523499, Test_Loss: 1.2538331747055054 *\n",
      "Epoch: 18, Train_Loss: 0.7313379049301147, Test_Loss: 0.7531240582466125 *\n",
      "Epoch: 18, Train_Loss: 0.7340612411499023, Test_Loss: 0.7444906234741211 *\n",
      "Epoch: 18, Train_Loss: 0.747606635093689, Test_Loss: 0.7487360239028931\n",
      "Epoch: 18, Train_Loss: 0.8865504264831543, Test_Loss: 0.7563213109970093\n",
      "Epoch: 18, Train_Loss: 0.8678005933761597, Test_Loss: 0.8122814297676086\n",
      "Epoch: 18, Train_Loss: 0.8593323826789856, Test_Loss: 1.2230417728424072\n",
      "Epoch: 18, Train_Loss: 0.7865000367164612, Test_Loss: 1.1293562650680542 *\n",
      "Epoch: 18, Train_Loss: 0.8888769149780273, Test_Loss: 0.8541507720947266 *\n",
      "Epoch: 18, Train_Loss: 0.8593511581420898, Test_Loss: 0.750887393951416 *\n",
      "Epoch: 18, Train_Loss: 0.8358177542686462, Test_Loss: 0.7495805621147156 *\n",
      "Epoch: 18, Train_Loss: 0.9091662168502808, Test_Loss: 0.7747913002967834\n",
      "Epoch: 18, Train_Loss: 1.0754070281982422, Test_Loss: 1.0219907760620117\n",
      "Epoch: 18, Train_Loss: 0.7636460661888123, Test_Loss: 2.035721778869629\n",
      "Epoch: 18, Train_Loss: 0.7384876608848572, Test_Loss: 1.3434642553329468 *\n",
      "Epoch: 18, Train_Loss: 3.14752197265625, Test_Loss: 0.806359589099884 *\n",
      "Epoch: 18, Train_Loss: 1.5757012367248535, Test_Loss: 0.7512596845626831 *\n",
      "Epoch: 18, Train_Loss: 0.7496753931045532, Test_Loss: 0.7371661067008972 *\n",
      "Epoch: 18, Train_Loss: 0.7572640180587769, Test_Loss: 0.7316830158233643 *\n",
      "Epoch: 18, Train_Loss: 0.7735230326652527, Test_Loss: 0.736320436000824\n",
      "Epoch: 18, Train_Loss: 0.7547287940979004, Test_Loss: 0.7471104264259338\n",
      "Epoch: 18, Train_Loss: 0.7264699339866638, Test_Loss: 0.7868987321853638\n",
      "Epoch: 18, Train_Loss: 0.7581200003623962, Test_Loss: 0.7343640923500061 *\n",
      "Epoch: 18, Train_Loss: 0.8667641282081604, Test_Loss: 0.7848619222640991\n",
      "Epoch: 18, Train_Loss: 0.7894809246063232, Test_Loss: 0.8486965894699097\n",
      "Epoch: 18, Train_Loss: 0.7725867629051208, Test_Loss: 1.1070926189422607\n",
      "Epoch: 18, Train_Loss: 0.770923912525177, Test_Loss: 0.9868324995040894 *\n",
      "Epoch: 18, Train_Loss: 0.7441319823265076, Test_Loss: 0.7554416060447693 *\n",
      "Epoch: 18, Train_Loss: 0.7360321879386902, Test_Loss: 0.7412164807319641 *\n",
      "Epoch: 18, Train_Loss: 0.7336766719818115, Test_Loss: 0.7453160881996155\n",
      "Epoch: 18, Train_Loss: 0.7652876973152161, Test_Loss: 0.7434282302856445 *\n",
      "Epoch: 18, Train_Loss: 0.764384388923645, Test_Loss: 0.7562243938446045\n",
      "Epoch: 18, Train_Loss: 0.7302204966545105, Test_Loss: 2.3527402877807617\n",
      "Epoch: 18, Train_Loss: 0.7324814796447754, Test_Loss: 4.1950225830078125\n",
      "Epoch: 18, Train_Loss: 0.743354856967926, Test_Loss: 0.7406684160232544 *\n",
      "Epoch: 18, Train_Loss: 0.7441354990005493, Test_Loss: 0.7235801815986633 *\n",
      "Epoch: 18, Train_Loss: 0.7299168109893799, Test_Loss: 0.7236539125442505\n",
      "Epoch: 18, Train_Loss: 0.7232835292816162, Test_Loss: 0.732308566570282\n",
      "Epoch: 18, Train_Loss: 0.7213282585144043, Test_Loss: 0.7229387760162354 *\n",
      "Epoch: 18, Train_Loss: 0.7209731340408325, Test_Loss: 0.7264610528945923\n",
      "Epoch: 18, Train_Loss: 0.7208217978477478, Test_Loss: 0.7242680191993713 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 18\n",
      "Epoch: 18, Train_Loss: 0.7210710048675537, Test_Loss: 0.7198814153671265 *\n",
      "Epoch: 18, Train_Loss: 0.722440242767334, Test_Loss: 0.7270931601524353\n",
      "Epoch: 18, Train_Loss: 0.7227725982666016, Test_Loss: 0.7233545184135437 *\n",
      "Epoch: 18, Train_Loss: 0.7232707142829895, Test_Loss: 0.7344657182693481\n",
      "Epoch: 18, Train_Loss: 0.7229318022727966, Test_Loss: 0.7351857423782349\n",
      "Epoch: 18, Train_Loss: 0.7230427861213684, Test_Loss: 0.727314293384552 *\n",
      "Epoch: 18, Train_Loss: 0.7378010749816895, Test_Loss: 0.7347472906112671\n",
      "Epoch: 18, Train_Loss: 0.7360198497772217, Test_Loss: 0.7185578942298889 *\n",
      "Epoch: 18, Train_Loss: 0.7348839044570923, Test_Loss: 0.7239296436309814\n",
      "Epoch: 18, Train_Loss: 0.739844560623169, Test_Loss: 0.720771312713623 *\n",
      "Epoch: 18, Train_Loss: 0.7283351421356201, Test_Loss: 0.7208483219146729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.7225579023361206, Test_Loss: 0.7188001275062561 *\n",
      "Epoch: 18, Train_Loss: 0.7188988924026489, Test_Loss: 0.7186002731323242 *\n",
      "Epoch: 18, Train_Loss: 0.7268520593643188, Test_Loss: 0.7187935709953308\n",
      "Epoch: 18, Train_Loss: 0.7476879358291626, Test_Loss: 0.7221693992614746\n",
      "Epoch: 18, Train_Loss: 0.7166512608528137, Test_Loss: 0.7164294719696045 *\n",
      "Epoch: 18, Train_Loss: 0.7197386026382446, Test_Loss: 0.7176619172096252\n",
      "Epoch: 18, Train_Loss: 0.7141585946083069, Test_Loss: 0.7165291905403137 *\n",
      "Epoch: 18, Train_Loss: 0.740766167640686, Test_Loss: 0.7174534201622009\n",
      "Epoch: 18, Train_Loss: 0.779340922832489, Test_Loss: 0.7155572772026062 *\n",
      "Epoch: 18, Train_Loss: 0.7640308141708374, Test_Loss: 0.7142872214317322 *\n",
      "Epoch: 18, Train_Loss: 0.7399401068687439, Test_Loss: 0.7564657330513\n",
      "Epoch: 18, Train_Loss: 0.7136170864105225, Test_Loss: 0.749553382396698 *\n",
      "Epoch: 18, Train_Loss: 0.7715253829956055, Test_Loss: 4.713499069213867\n",
      "Epoch: 18, Train_Loss: 0.7273836135864258, Test_Loss: 2.1567840576171875 *\n",
      "Epoch: 18, Train_Loss: 0.717464029788971, Test_Loss: 0.7150975465774536 *\n",
      "Epoch: 18, Train_Loss: 0.7279787659645081, Test_Loss: 0.7243945598602295\n",
      "Epoch: 18, Train_Loss: 0.7307682037353516, Test_Loss: 0.7712165713310242\n",
      "Epoch: 18, Train_Loss: 0.8273271322250366, Test_Loss: 0.7800424695014954\n",
      "Epoch: 18, Train_Loss: 0.7883025407791138, Test_Loss: 0.7319658398628235 *\n",
      "Epoch: 18, Train_Loss: 0.7541289925575256, Test_Loss: 0.7902795672416687\n",
      "Epoch: 18, Train_Loss: 0.7241061329841614, Test_Loss: 0.7990748286247253\n",
      "Epoch: 18, Train_Loss: 0.7199960947036743, Test_Loss: 0.7125813961029053 *\n",
      "Epoch: 18, Train_Loss: 0.7270387411117554, Test_Loss: 0.7469452619552612\n",
      "Epoch: 18, Train_Loss: 0.7134279608726501, Test_Loss: 0.7304049134254456 *\n",
      "Epoch: 18, Train_Loss: 0.7189083099365234, Test_Loss: 0.7231215834617615 *\n",
      "Epoch: 18, Train_Loss: 0.724105954170227, Test_Loss: 0.7149943113327026 *\n",
      "Epoch: 18, Train_Loss: 0.7374969720840454, Test_Loss: 0.8322550654411316\n",
      "Epoch: 18, Train_Loss: 0.8181842565536499, Test_Loss: 0.7567740678787231 *\n",
      "Epoch: 18, Train_Loss: 0.7159655690193176, Test_Loss: 0.8104914426803589\n",
      "Epoch: 18, Train_Loss: 0.7721256017684937, Test_Loss: 0.7835672497749329 *\n",
      "Epoch: 18, Train_Loss: 0.7265752553939819, Test_Loss: 0.7403160333633423 *\n",
      "Epoch: 18, Train_Loss: 0.7384538650512695, Test_Loss: 0.7246782779693604 *\n",
      "Epoch: 18, Train_Loss: 0.7977549433708191, Test_Loss: 0.7223128080368042 *\n",
      "Epoch: 18, Train_Loss: 0.9382173418998718, Test_Loss: 0.7227053642272949\n",
      "Epoch: 18, Train_Loss: 0.7181703448295593, Test_Loss: 0.7232688665390015\n",
      "Epoch: 18, Train_Loss: 0.7513234615325928, Test_Loss: 0.7246332168579102\n",
      "Epoch: 18, Train_Loss: 0.7065249681472778, Test_Loss: 0.7233749628067017 *\n",
      "Epoch: 18, Train_Loss: 0.7072075605392456, Test_Loss: 0.7156251072883606 *\n",
      "Epoch: 18, Train_Loss: 0.706408679485321, Test_Loss: 0.7290263772010803\n",
      "Epoch: 18, Train_Loss: 0.7050439119338989, Test_Loss: 0.7270359992980957 *\n",
      "Epoch: 18, Train_Loss: 0.7164404988288879, Test_Loss: 0.7198845744132996 *\n",
      "Epoch: 18, Train_Loss: 0.719147264957428, Test_Loss: 0.7169672250747681 *\n",
      "Epoch: 18, Train_Loss: 0.7158522009849548, Test_Loss: 0.7430164217948914\n",
      "Epoch: 19, Train_Loss: 0.7145599722862244, Test_Loss: 0.7399204969406128 *\n",
      "Epoch: 19, Train_Loss: 0.7152162790298462, Test_Loss: 1.057715654373169\n",
      "Epoch: 19, Train_Loss: 0.7138353586196899, Test_Loss: 0.7169467806816101 *\n",
      "Epoch: 19, Train_Loss: 0.7067564725875854, Test_Loss: 0.7442730665206909\n",
      "Epoch: 19, Train_Loss: 0.7025813460350037, Test_Loss: 0.7833878397941589\n",
      "Epoch: 19, Train_Loss: 0.7253548502922058, Test_Loss: 1.0528796911239624\n",
      "Epoch: 19, Train_Loss: 0.7380309700965881, Test_Loss: 0.7883792519569397 *\n",
      "Epoch: 19, Train_Loss: 0.7327527403831482, Test_Loss: 0.7777047753334045 *\n",
      "Epoch: 19, Train_Loss: 0.7039066553115845, Test_Loss: 0.9164803624153137\n",
      "Epoch: 19, Train_Loss: 0.7554863691329956, Test_Loss: 0.9206835031509399\n",
      "Epoch: 19, Train_Loss: 0.7465775012969971, Test_Loss: 0.7207865715026855 *\n",
      "Epoch: 19, Train_Loss: 0.731837809085846, Test_Loss: 0.7539070844650269\n",
      "Epoch: 19, Train_Loss: 0.7083740234375, Test_Loss: 0.7105801701545715 *\n",
      "Epoch: 19, Train_Loss: 0.7378678321838379, Test_Loss: 0.7413954138755798\n",
      "Epoch: 19, Train_Loss: 0.7035937905311584, Test_Loss: 0.7761733531951904\n",
      "Epoch: 19, Train_Loss: 0.7141539454460144, Test_Loss: 1.4333268404006958\n",
      "Epoch: 19, Train_Loss: 0.7127633094787598, Test_Loss: 0.8657446503639221 *\n",
      "Epoch: 19, Train_Loss: 0.7277637720108032, Test_Loss: 1.4601852893829346\n",
      "Epoch: 19, Train_Loss: 2.057157039642334, Test_Loss: 1.1583950519561768 *\n",
      "Epoch: 19, Train_Loss: 4.669382572174072, Test_Loss: 1.055666446685791 *\n",
      "Epoch: 19, Train_Loss: 0.8908358812332153, Test_Loss: 1.0017699003219604 *\n",
      "Epoch: 19, Train_Loss: 0.715407133102417, Test_Loss: 0.7253077626228333 *\n",
      "Epoch: 19, Train_Loss: 0.7118139863014221, Test_Loss: 0.7114449143409729 *\n",
      "Epoch: 19, Train_Loss: 0.8752749562263489, Test_Loss: 0.723206639289856\n",
      "Epoch: 19, Train_Loss: 0.751315712928772, Test_Loss: 0.8379469513893127\n",
      "Epoch: 19, Train_Loss: 0.7055145502090454, Test_Loss: 1.4261143207550049\n",
      "Epoch: 19, Train_Loss: 0.6988742351531982, Test_Loss: 1.0060241222381592 *\n",
      "Epoch: 19, Train_Loss: 0.748384952545166, Test_Loss: 2.3823583126068115\n",
      "Epoch: 19, Train_Loss: 0.7083663940429688, Test_Loss: 1.432516098022461 *\n",
      "Epoch: 19, Train_Loss: 0.7193547487258911, Test_Loss: 1.7081552743911743\n",
      "Epoch: 19, Train_Loss: 1.087870717048645, Test_Loss: 0.8948798775672913 *\n",
      "Epoch: 19, Train_Loss: 2.0745441913604736, Test_Loss: 0.7023648619651794 *\n",
      "Epoch: 19, Train_Loss: 1.6671905517578125, Test_Loss: 0.8044187426567078\n",
      "Epoch: 19, Train_Loss: 0.8076685667037964, Test_Loss: 1.7958829402923584\n",
      "Epoch: 19, Train_Loss: 0.9171650409698486, Test_Loss: 1.6754357814788818 *\n",
      "Epoch: 19, Train_Loss: 2.9552342891693115, Test_Loss: 0.7551887035369873 *\n",
      "Epoch: 19, Train_Loss: 1.5250439643859863, Test_Loss: 0.7319360375404358 *\n",
      "Epoch: 19, Train_Loss: 0.7330505847930908, Test_Loss: 0.7663683891296387\n",
      "Epoch: 19, Train_Loss: 0.7097412943840027, Test_Loss: 1.163137674331665\n",
      "Epoch: 19, Train_Loss: 1.4924733638763428, Test_Loss: 0.8429744243621826 *\n",
      "Epoch: 19, Train_Loss: 2.106527090072632, Test_Loss: 1.5163815021514893\n",
      "Epoch: 19, Train_Loss: 1.3341197967529297, Test_Loss: 1.1504608392715454 *\n",
      "Epoch: 19, Train_Loss: 0.7041174173355103, Test_Loss: 0.8817989826202393 *\n",
      "Epoch: 19, Train_Loss: 0.7116102576255798, Test_Loss: 0.709949791431427 *\n",
      "Epoch: 19, Train_Loss: 1.0875548124313354, Test_Loss: 0.7075815200805664 *\n",
      "Epoch: 19, Train_Loss: 1.0659730434417725, Test_Loss: 0.7257582545280457\n",
      "Epoch: 19, Train_Loss: 0.713007390499115, Test_Loss: 0.8123621344566345\n",
      "Epoch: 19, Train_Loss: 0.7480342984199524, Test_Loss: 0.979004979133606\n",
      "Epoch: 19, Train_Loss: 0.8131616115570068, Test_Loss: 1.0351014137268066\n",
      "Epoch: 19, Train_Loss: 0.7561308741569519, Test_Loss: 0.8389832973480225 *\n",
      "Epoch: 19, Train_Loss: 0.7910288572311401, Test_Loss: 0.7223055958747864 *\n",
      "Epoch: 19, Train_Loss: 1.0478237867355347, Test_Loss: 0.7238569259643555\n",
      "Epoch: 19, Train_Loss: 0.8256790041923523, Test_Loss: 0.7363102436065674\n",
      "Epoch: 19, Train_Loss: 0.7770012021064758, Test_Loss: 0.827934205532074\n",
      "Epoch: 19, Train_Loss: 0.8042734861373901, Test_Loss: 1.4916692972183228\n",
      "Epoch: 19, Train_Loss: 0.8799220323562622, Test_Loss: 1.3278608322143555 *\n",
      "Epoch: 19, Train_Loss: 0.9624022245407104, Test_Loss: 0.7605700492858887 *\n",
      "Epoch: 19, Train_Loss: 0.9455643892288208, Test_Loss: 0.7365537881851196 *\n",
      "Epoch: 19, Train_Loss: 0.7534428834915161, Test_Loss: 0.7038170099258423 *\n",
      "Epoch: 19, Train_Loss: 0.847344160079956, Test_Loss: 0.711441695690155\n",
      "Epoch: 19, Train_Loss: 0.768889844417572, Test_Loss: 0.7310938239097595\n",
      "Epoch: 19, Train_Loss: 0.7058977484703064, Test_Loss: 0.7437050342559814\n",
      "Epoch: 19, Train_Loss: 0.6915178894996643, Test_Loss: 0.7588818669319153\n",
      "Epoch: 19, Train_Loss: 0.6967596411705017, Test_Loss: 0.8227925896644592\n",
      "Epoch: 19, Train_Loss: 0.6902334094047546, Test_Loss: 0.7426198124885559 *\n",
      "Epoch: 19, Train_Loss: 0.6964581608772278, Test_Loss: 0.8061316013336182\n",
      "Epoch: 19, Train_Loss: 0.721777617931366, Test_Loss: 1.0825269222259521\n",
      "Epoch: 19, Train_Loss: 0.729494571685791, Test_Loss: 1.1251226663589478\n",
      "Epoch: 19, Train_Loss: 0.723920464515686, Test_Loss: 0.7812304496765137 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 0.7769317030906677, Test_Loss: 0.6922403573989868 *\n",
      "Epoch: 19, Train_Loss: 0.9503273963928223, Test_Loss: 0.6901223659515381 *\n",
      "Epoch: 19, Train_Loss: 0.8287796974182129, Test_Loss: 0.6921223998069763\n",
      "Epoch: 19, Train_Loss: 0.7420669198036194, Test_Loss: 0.7263926267623901\n",
      "Epoch: 19, Train_Loss: 0.7704583406448364, Test_Loss: 1.176795482635498\n",
      "Epoch: 19, Train_Loss: 1.0459650754928589, Test_Loss: 5.490216255187988\n",
      "Epoch: 19, Train_Loss: 0.9094823002815247, Test_Loss: 0.7736124396324158 *\n",
      "Epoch: 19, Train_Loss: 0.7155115008354187, Test_Loss: 0.725020170211792 *\n",
      "Epoch: 19, Train_Loss: 0.7231094837188721, Test_Loss: 0.7203832268714905 *\n",
      "Epoch: 19, Train_Loss: 1.041917085647583, Test_Loss: 0.7007921934127808 *\n",
      "Epoch: 19, Train_Loss: 1.0891004800796509, Test_Loss: 0.7173888683319092\n",
      "Epoch: 19, Train_Loss: 0.8093058466911316, Test_Loss: 0.7272330522537231\n",
      "Epoch: 19, Train_Loss: 0.7140657305717468, Test_Loss: 0.7900843620300293\n",
      "Epoch: 19, Train_Loss: 0.7337870001792908, Test_Loss: 0.7133362889289856 *\n",
      "Epoch: 19, Train_Loss: 1.1598846912384033, Test_Loss: 0.7474374771118164\n",
      "Epoch: 19, Train_Loss: 1.6210508346557617, Test_Loss: 0.7289345264434814 *\n",
      "Epoch: 19, Train_Loss: 0.812089204788208, Test_Loss: 0.78526371717453\n",
      "Epoch: 19, Train_Loss: 0.7452551126480103, Test_Loss: 0.703439474105835 *\n",
      "Epoch: 19, Train_Loss: 0.703524112701416, Test_Loss: 0.7083524465560913\n",
      "Epoch: 19, Train_Loss: 0.6980642080307007, Test_Loss: 0.7209768295288086\n",
      "Epoch: 19, Train_Loss: 1.0909417867660522, Test_Loss: 0.7176768779754639 *\n",
      "Epoch: 19, Train_Loss: 0.7405689358711243, Test_Loss: 0.726212739944458\n",
      "Epoch: 19, Train_Loss: 0.7057060599327087, Test_Loss: 0.7462066411972046\n",
      "Epoch: 19, Train_Loss: 0.6976034641265869, Test_Loss: 0.7606608867645264\n",
      "Epoch: 19, Train_Loss: 0.7617654204368591, Test_Loss: 0.702934205532074 *\n",
      "Epoch: 19, Train_Loss: 17.034624099731445, Test_Loss: 0.7438603043556213\n",
      "Epoch: 19, Train_Loss: 0.7619089484214783, Test_Loss: 0.7161846160888672 *\n",
      "Epoch: 19, Train_Loss: 1.9587407112121582, Test_Loss: 0.7738460898399353\n",
      "Epoch: 19, Train_Loss: 2.1724560260772705, Test_Loss: 0.7751634120941162\n",
      "Epoch: 19, Train_Loss: 0.6976510882377625, Test_Loss: 0.7443323135375977 *\n",
      "Epoch: 19, Train_Loss: 0.7912980914115906, Test_Loss: 0.7266585230827332 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 2.735729932785034, Test_Loss: 0.7449490427970886\n",
      "Epoch: 19, Train_Loss: 9.178546905517578, Test_Loss: 0.7562309503555298\n",
      "Epoch: 19, Train_Loss: 0.9227059483528137, Test_Loss: 0.7115029096603394 *\n",
      "Epoch: 19, Train_Loss: 0.7540603280067444, Test_Loss: 0.7941045165061951\n",
      "Epoch: 19, Train_Loss: 6.219150543212891, Test_Loss: 0.7923082709312439 *\n",
      "Epoch: 19, Train_Loss: 0.9016497135162354, Test_Loss: 3.7945265769958496\n",
      "Epoch: 19, Train_Loss: 0.7467058897018433, Test_Loss: 3.739027976989746 *\n",
      "Epoch: 19, Train_Loss: 0.7157583236694336, Test_Loss: 0.7246289849281311 *\n",
      "Epoch: 19, Train_Loss: 0.7074423432350159, Test_Loss: 0.7222077250480652 *\n",
      "Epoch: 19, Train_Loss: 0.7100926637649536, Test_Loss: 0.7077958583831787 *\n",
      "Epoch: 19, Train_Loss: 0.6805102825164795, Test_Loss: 0.7228501439094543\n",
      "Epoch: 19, Train_Loss: 0.6964268088340759, Test_Loss: 0.7259385585784912\n",
      "Epoch: 19, Train_Loss: 0.6835852265357971, Test_Loss: 0.8570829629898071\n",
      "Epoch: 19, Train_Loss: 0.6857746243476868, Test_Loss: 1.072764277458191\n",
      "Epoch: 19, Train_Loss: 0.7076312899589539, Test_Loss: 0.7415302395820618 *\n",
      "Epoch: 19, Train_Loss: 0.7192767262458801, Test_Loss: 0.748138427734375\n",
      "Epoch: 19, Train_Loss: 0.703524112701416, Test_Loss: 0.7323089838027954 *\n",
      "Epoch: 19, Train_Loss: 0.7558931112289429, Test_Loss: 0.7094346284866333 *\n",
      "Epoch: 19, Train_Loss: 0.8043088912963867, Test_Loss: 0.7152318954467773\n",
      "Epoch: 19, Train_Loss: 0.757783830165863, Test_Loss: 0.7286131381988525\n",
      "Epoch: 19, Train_Loss: 0.6988872289657593, Test_Loss: 0.7374052405357361\n",
      "Epoch: 19, Train_Loss: 0.6824901700019836, Test_Loss: 0.8730674982070923\n",
      "Epoch: 19, Train_Loss: 0.7029991149902344, Test_Loss: 0.7701092958450317 *\n",
      "Epoch: 19, Train_Loss: 0.681996762752533, Test_Loss: 0.6966649293899536 *\n",
      "Epoch: 19, Train_Loss: 0.6783503293991089, Test_Loss: 0.6891434192657471 *\n",
      "Epoch: 19, Train_Loss: 0.676791787147522, Test_Loss: 0.6968722939491272\n",
      "Epoch: 19, Train_Loss: 0.6753699779510498, Test_Loss: 0.696023166179657 *\n",
      "Epoch: 19, Train_Loss: 0.6794605851173401, Test_Loss: 0.6889107823371887 *\n",
      "Epoch: 19, Train_Loss: 0.6782480478286743, Test_Loss: 0.685133159160614 *\n",
      "Epoch: 19, Train_Loss: 0.6769937872886658, Test_Loss: 0.6890218257904053\n",
      "Epoch: 19, Train_Loss: 0.6817016005516052, Test_Loss: 0.6873181462287903 *\n",
      "Epoch: 19, Train_Loss: 0.6958118081092834, Test_Loss: 0.6861140131950378 *\n",
      "Epoch: 19, Train_Loss: 0.7036529779434204, Test_Loss: 0.6896560192108154\n",
      "Epoch: 19, Train_Loss: 0.7218007445335388, Test_Loss: 0.7113836407661438\n",
      "Epoch: 19, Train_Loss: 0.693870484828949, Test_Loss: 0.7659224271774292\n",
      "Epoch: 19, Train_Loss: 0.9141911268234253, Test_Loss: 0.6920539736747742 *\n",
      "Epoch: 19, Train_Loss: 8.623210906982422, Test_Loss: 0.7079588770866394\n",
      "Epoch: 19, Train_Loss: 0.7249056100845337, Test_Loss: 1.2126905918121338\n",
      "Epoch: 19, Train_Loss: 0.7304075360298157, Test_Loss: 0.811349093914032 *\n",
      "Epoch: 19, Train_Loss: 0.8035620450973511, Test_Loss: 0.745874285697937 *\n",
      "Epoch: 19, Train_Loss: 0.7818352580070496, Test_Loss: 0.7243517637252808 *\n",
      "Epoch: 19, Train_Loss: 0.7178230881690979, Test_Loss: 0.8149346113204956\n",
      "Epoch: 19, Train_Loss: 0.7641227841377258, Test_Loss: 0.7424721121788025 *\n",
      "Epoch: 19, Train_Loss: 0.8225259184837341, Test_Loss: 0.7383570075035095 *\n",
      "Epoch: 19, Train_Loss: 0.9610561728477478, Test_Loss: 0.8659310340881348\n",
      "Epoch: 19, Train_Loss: 0.829126238822937, Test_Loss: 0.9650039076805115\n",
      "Epoch: 19, Train_Loss: 0.7490513324737549, Test_Loss: 0.780928373336792 *\n",
      "Epoch: 19, Train_Loss: 0.6780961751937866, Test_Loss: 0.7761537432670593 *\n",
      "Epoch: 19, Train_Loss: 0.7635329961776733, Test_Loss: 0.6775619387626648 *\n",
      "Epoch: 19, Train_Loss: 0.7066894173622131, Test_Loss: 0.6821801662445068\n",
      "Epoch: 19, Train_Loss: 0.7942771315574646, Test_Loss: 0.7179226875305176\n",
      "Epoch: 19, Train_Loss: 0.7037792801856995, Test_Loss: 1.3449164628982544\n",
      "Epoch: 19, Train_Loss: 0.7289779782295227, Test_Loss: 0.9393675327301025 *\n",
      "Epoch: 19, Train_Loss: 0.6863923072814941, Test_Loss: 1.2239642143249512\n",
      "Epoch: 19, Train_Loss: 0.694383978843689, Test_Loss: 1.1276345252990723 *\n",
      "Epoch: 19, Train_Loss: 0.8026796579360962, Test_Loss: 0.8838317394256592 *\n",
      "Epoch: 19, Train_Loss: 0.7110366225242615, Test_Loss: 0.9852379560470581\n",
      "Epoch: 19, Train_Loss: 0.6785092949867249, Test_Loss: 0.7297130823135376 *\n",
      "Epoch: 19, Train_Loss: 0.6718270778656006, Test_Loss: 0.6911373138427734 *\n",
      "Epoch: 19, Train_Loss: 0.6717029213905334, Test_Loss: 0.6865381598472595 *\n",
      "Epoch: 19, Train_Loss: 1.484499216079712, Test_Loss: 0.7849894762039185\n",
      "Epoch: 19, Train_Loss: 5.5084309577941895, Test_Loss: 1.0449120998382568\n",
      "Epoch: 19, Train_Loss: 0.6720077395439148, Test_Loss: 1.2397735118865967\n",
      "Epoch: 19, Train_Loss: 0.689451277256012, Test_Loss: 1.599291443824768\n",
      "Epoch: 19, Train_Loss: 0.6795126795768738, Test_Loss: 1.6766581535339355\n",
      "Epoch: 19, Train_Loss: 0.6751630902290344, Test_Loss: 1.5535238981246948 *\n",
      "Epoch: 19, Train_Loss: 0.669635534286499, Test_Loss: 1.0212130546569824 *\n",
      "Epoch: 19, Train_Loss: 0.667086660861969, Test_Loss: 0.6948034167289734 *\n",
      "Epoch: 19, Train_Loss: 0.6771494150161743, Test_Loss: 0.7049267888069153\n",
      "Epoch: 19, Train_Loss: 0.6808524131774902, Test_Loss: 1.3126262426376343\n",
      "Epoch: 19, Train_Loss: 0.7073711156845093, Test_Loss: 1.8382911682128906\n",
      "Epoch: 19, Train_Loss: 0.6868574619293213, Test_Loss: 0.7307320833206177 *\n",
      "Epoch: 19, Train_Loss: 0.6663315296173096, Test_Loss: 0.7746795415878296\n",
      "Epoch: 19, Train_Loss: 0.6663243770599365, Test_Loss: 0.6786490082740784 *\n",
      "Epoch: 19, Train_Loss: 0.6834748387336731, Test_Loss: 0.9405458569526672\n",
      "Epoch: 19, Train_Loss: 0.6671146750450134, Test_Loss: 0.8834712505340576 *\n",
      "Epoch: 19, Train_Loss: 0.6661295294761658, Test_Loss: 1.4079558849334717\n",
      "Epoch: 19, Train_Loss: 0.6863194108009338, Test_Loss: 1.5203183889389038\n",
      "Epoch: 19, Train_Loss: 0.6885138750076294, Test_Loss: 0.971372127532959 *\n",
      "Epoch: 19, Train_Loss: 0.6868062019348145, Test_Loss: 0.6744842529296875 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 0.6662927269935608, Test_Loss: 0.6767113208770752\n",
      "Epoch: 19, Train_Loss: 0.6832289099693298, Test_Loss: 0.6784908175468445\n",
      "Epoch: 19, Train_Loss: 0.6978042125701904, Test_Loss: 0.7565810084342957\n",
      "Epoch: 19, Train_Loss: 0.6974718570709229, Test_Loss: 0.8832280039787292\n",
      "Epoch: 19, Train_Loss: 0.701695442199707, Test_Loss: 1.0121418237686157\n",
      "Epoch: 19, Train_Loss: 0.720860481262207, Test_Loss: 0.8216789960861206 *\n",
      "Epoch: 19, Train_Loss: 0.7510842084884644, Test_Loss: 0.7106021642684937 *\n",
      "Epoch: 19, Train_Loss: 0.6870521903038025, Test_Loss: 0.6906329989433289 *\n",
      "Epoch: 19, Train_Loss: 0.71394282579422, Test_Loss: 0.6856092810630798 *\n",
      "Epoch: 19, Train_Loss: 0.6974075436592102, Test_Loss: 0.7947788238525391\n",
      "Epoch: 19, Train_Loss: 0.8883274793624878, Test_Loss: 1.432501196861267\n",
      "Epoch: 19, Train_Loss: 0.6845110654830933, Test_Loss: 1.7308332920074463\n",
      "Epoch: 19, Train_Loss: 0.66343092918396, Test_Loss: 0.7381977438926697 *\n",
      "Epoch: 19, Train_Loss: 0.6614864468574524, Test_Loss: 0.7177641987800598 *\n",
      "Epoch: 19, Train_Loss: 0.661233127117157, Test_Loss: 0.6648762822151184 *\n",
      "Epoch: 19, Train_Loss: 0.6593708992004395, Test_Loss: 0.680445671081543\n",
      "Epoch: 19, Train_Loss: 0.6598584651947021, Test_Loss: 0.6770867109298706 *\n",
      "Epoch: 19, Train_Loss: 1.3953852653503418, Test_Loss: 0.6952559351921082\n",
      "Epoch: 19, Train_Loss: 4.503468990325928, Test_Loss: 0.6997716426849365\n",
      "Epoch: 19, Train_Loss: 0.6657658219337463, Test_Loss: 0.6834179759025574 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 0.6648252010345459, Test_Loss: 0.664311945438385 *\n",
      "Epoch: 19, Train_Loss: 0.6639572978019714, Test_Loss: 0.7696052193641663\n",
      "Epoch: 19, Train_Loss: 0.6656416654586792, Test_Loss: 1.07170569896698\n",
      "Epoch: 19, Train_Loss: 0.6615741848945618, Test_Loss: 0.8241553902626038 *\n",
      "Epoch: 19, Train_Loss: 0.6632042527198792, Test_Loss: 0.8215471506118774 *\n",
      "Epoch: 19, Train_Loss: 0.6581894755363464, Test_Loss: 0.6910018920898438 *\n",
      "Epoch: 19, Train_Loss: 0.6618895530700684, Test_Loss: 0.691326379776001\n",
      "Epoch: 19, Train_Loss: 0.6616168022155762, Test_Loss: 0.6921941041946411\n",
      "Epoch: 19, Train_Loss: 0.7036384344100952, Test_Loss: 0.6858052611351013 *\n",
      "Epoch: 19, Train_Loss: 0.6922054290771484, Test_Loss: 0.7215575575828552\n",
      "Epoch: 19, Train_Loss: 0.7161270380020142, Test_Loss: 5.521901607513428\n",
      "Epoch: 19, Train_Loss: 0.6944088339805603, Test_Loss: 0.8321688175201416 *\n",
      "Epoch: 19, Train_Loss: 0.6612866520881653, Test_Loss: 0.6710165143013 *\n",
      "Epoch: 19, Train_Loss: 0.8389464616775513, Test_Loss: 0.6611579656600952 *\n",
      "Epoch: 19, Train_Loss: 0.8993725776672363, Test_Loss: 0.6676417589187622\n",
      "Epoch: 19, Train_Loss: 0.8704347610473633, Test_Loss: 0.6673840284347534 *\n",
      "Epoch: 19, Train_Loss: 0.8010562658309937, Test_Loss: 0.6578690409660339 *\n",
      "Epoch: 19, Train_Loss: 0.6562230587005615, Test_Loss: 0.6671247482299805\n",
      "Epoch: 19, Train_Loss: 0.6552309989929199, Test_Loss: 0.6568982005119324 *\n",
      "Epoch: 19, Train_Loss: 0.6595494151115417, Test_Loss: 0.6592316627502441\n",
      "Epoch: 19, Train_Loss: 0.6730555295944214, Test_Loss: 0.6653056144714355\n",
      "Epoch: 19, Train_Loss: 0.6723564863204956, Test_Loss: 0.6746770143508911\n",
      "Epoch: 19, Train_Loss: 0.6662909984588623, Test_Loss: 0.6639781594276428 *\n",
      "Epoch: 19, Train_Loss: 0.656563401222229, Test_Loss: 0.6667256355285645\n",
      "Epoch: 19, Train_Loss: 0.6548671722412109, Test_Loss: 0.6704517006874084\n",
      "Epoch: 19, Train_Loss: 0.663711667060852, Test_Loss: 0.664202868938446 *\n",
      "Epoch: 19, Train_Loss: 0.6927167177200317, Test_Loss: 0.6584814786911011 *\n",
      "Epoch: 19, Train_Loss: 0.7821427583694458, Test_Loss: 0.6698187589645386\n",
      "Epoch: 19, Train_Loss: 0.743918776512146, Test_Loss: 0.6654018759727478 *\n",
      "Epoch: 19, Train_Loss: 0.7498769164085388, Test_Loss: 0.6638705730438232 *\n",
      "Epoch: 19, Train_Loss: 0.7370195388793945, Test_Loss: 0.6622373461723328 *\n",
      "Epoch: 19, Train_Loss: 0.7874339818954468, Test_Loss: 0.6670336127281189\n",
      "Epoch: 19, Train_Loss: 0.7526121735572815, Test_Loss: 0.6742421984672546\n",
      "Epoch: 19, Train_Loss: 0.7840538024902344, Test_Loss: 0.6810247898101807\n",
      "Epoch: 19, Train_Loss: 0.8158543109893799, Test_Loss: 0.6618705987930298 *\n",
      "Epoch: 19, Train_Loss: 0.9854456186294556, Test_Loss: 0.663513720035553\n",
      "Epoch: 19, Train_Loss: 0.6640170812606812, Test_Loss: 0.6643862724304199\n",
      "Epoch: 19, Train_Loss: 0.6553899049758911, Test_Loss: 0.6578653454780579 *\n",
      "Epoch: 19, Train_Loss: 3.4999165534973145, Test_Loss: 0.6551934480667114 *\n",
      "Epoch: 19, Train_Loss: 1.0665669441223145, Test_Loss: 0.6633552312850952\n",
      "Epoch: 19, Train_Loss: 0.6773539781570435, Test_Loss: 0.7135939598083496\n",
      "Epoch: 19, Train_Loss: 0.6978803277015686, Test_Loss: 2.0371031761169434\n",
      "Epoch: 19, Train_Loss: 0.7034029960632324, Test_Loss: 4.7433953285217285\n",
      "Epoch: 19, Train_Loss: 0.6760019659996033, Test_Loss: 0.6577523350715637 *\n",
      "Epoch: 19, Train_Loss: 0.652457058429718, Test_Loss: 0.6501625776290894 *\n",
      "Epoch: 19, Train_Loss: 0.6923767924308777, Test_Loss: 0.6949506998062134\n",
      "Epoch: 19, Train_Loss: 0.7587727308273315, Test_Loss: 0.7019826769828796\n",
      "Epoch: 19, Train_Loss: 0.690742552280426, Test_Loss: 0.7130252122879028\n",
      "Epoch: 19, Train_Loss: 0.6948922276496887, Test_Loss: 0.6703998446464539 *\n",
      "Epoch: 19, Train_Loss: 0.6847645044326782, Test_Loss: 0.7690574526786804\n",
      "Epoch: 19, Train_Loss: 0.6621726751327515, Test_Loss: 0.6630915999412537 *\n",
      "Epoch: 19, Train_Loss: 0.6700546741485596, Test_Loss: 0.6617390513420105 *\n",
      "Epoch: 19, Train_Loss: 0.6586307287216187, Test_Loss: 0.6774541139602661\n",
      "Epoch: 19, Train_Loss: 0.6979628801345825, Test_Loss: 0.6694329977035522 *\n",
      "Epoch: 19, Train_Loss: 0.6802054047584534, Test_Loss: 0.6546247601509094 *\n",
      "Epoch: 19, Train_Loss: 0.654639720916748, Test_Loss: 0.7155208587646484\n",
      "Epoch: 19, Train_Loss: 0.659183919429779, Test_Loss: 0.7510350942611694\n",
      "Epoch: 19, Train_Loss: 0.665082573890686, Test_Loss: 0.7043644189834595 *\n",
      "Epoch: 19, Train_Loss: 0.665875256061554, Test_Loss: 0.7194884419441223\n",
      "Epoch: 19, Train_Loss: 0.6621131300926208, Test_Loss: 0.6740105152130127 *\n",
      "Epoch: 19, Train_Loss: 0.652531623840332, Test_Loss: 0.692875862121582\n",
      "Epoch: 19, Train_Loss: 0.649634838104248, Test_Loss: 0.6661106944084167 *\n",
      "Epoch: 19, Train_Loss: 0.6475257277488708, Test_Loss: 0.6675406694412231\n",
      "Epoch: 19, Train_Loss: 0.6469026803970337, Test_Loss: 0.6616531014442444 *\n",
      "Epoch: 19, Train_Loss: 0.6472357511520386, Test_Loss: 0.6696431636810303\n",
      "Epoch: 19, Train_Loss: 0.649003803730011, Test_Loss: 0.6653251051902771 *\n",
      "Epoch: 19, Train_Loss: 0.6481378078460693, Test_Loss: 0.6596697568893433 *\n",
      "Epoch: 19, Train_Loss: 0.6472209095954895, Test_Loss: 0.673038125038147\n",
      "Epoch: 19, Train_Loss: 0.6446493864059448, Test_Loss: 0.6699979901313782 *\n",
      "Epoch: 19, Train_Loss: 0.6499212980270386, Test_Loss: 0.6805227994918823\n",
      "Epoch: 19, Train_Loss: 0.6600434184074402, Test_Loss: 0.6484519243240356 *\n",
      "Epoch: 19, Train_Loss: 0.6609493494033813, Test_Loss: 0.6716335415840149\n",
      "Epoch: 19, Train_Loss: 0.6573693752288818, Test_Loss: 0.7227727770805359\n",
      "Epoch: 19, Train_Loss: 0.6650825142860413, Test_Loss: 0.830706775188446\n",
      "Epoch: 19, Train_Loss: 0.6504172086715698, Test_Loss: 0.7852264642715454 *\n",
      "Epoch: 19, Train_Loss: 0.644565761089325, Test_Loss: 0.669012725353241 *\n",
      "Epoch: 19, Train_Loss: 0.646045982837677, Test_Loss: 0.7410883903503418\n",
      "Epoch: 19, Train_Loss: 0.6558990478515625, Test_Loss: 0.8388907313346863\n",
      "Epoch: 19, Train_Loss: 0.6684513092041016, Test_Loss: 0.9437344074249268\n",
      "Epoch: 19, Train_Loss: 0.6454028487205505, Test_Loss: 0.651943027973175 *\n",
      "Epoch: 19, Train_Loss: 0.642748236656189, Test_Loss: 0.8099597096443176\n",
      "Epoch: 19, Train_Loss: 0.6424175500869751, Test_Loss: 0.905436635017395\n",
      "Epoch: 19, Train_Loss: 0.6811676025390625, Test_Loss: 0.6804395318031311 *\n",
      "Epoch: 19, Train_Loss: 0.6932395696640015, Test_Loss: 0.6948003768920898\n",
      "Epoch: 19, Train_Loss: 0.6868047714233398, Test_Loss: 0.6441510319709778 *\n",
      "Epoch: 19, Train_Loss: 0.6544179320335388, Test_Loss: 0.6798626184463501\n",
      "Epoch: 19, Train_Loss: 0.6417700052261353, Test_Loss: 0.6660768985748291 *\n",
      "Epoch: 19, Train_Loss: 0.7052928805351257, Test_Loss: 1.303955078125\n",
      "Epoch: 19, Train_Loss: 0.6466646790504456, Test_Loss: 0.925442099571228 *\n",
      "Epoch: 19, Train_Loss: 0.6499556303024292, Test_Loss: 1.292640209197998\n",
      "Epoch: 19, Train_Loss: 0.6592385172843933, Test_Loss: 1.419233798980713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 0.6528767943382263, Test_Loss: 0.7729838490486145 *\n",
      "Epoch: 19, Train_Loss: 0.7601150870323181, Test_Loss: 1.173462152481079\n",
      "Epoch: 19, Train_Loss: 0.6986434459686279, Test_Loss: 0.7533318400382996 *\n",
      "Epoch: 19, Train_Loss: 0.6732959151268005, Test_Loss: 0.6497323513031006 *\n",
      "Epoch: 19, Train_Loss: 0.6489660143852234, Test_Loss: 0.6608536243438721\n",
      "Epoch: 19, Train_Loss: 0.6579586863517761, Test_Loss: 0.7723315358161926\n",
      "Epoch: 19, Train_Loss: 0.6442244648933411, Test_Loss: 0.8656895160675049\n",
      "Epoch: 19, Train_Loss: 0.6430063843727112, Test_Loss: 1.4949955940246582\n",
      "Epoch: 19, Train_Loss: 0.650309681892395, Test_Loss: 1.3375890254974365 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 0.6563076972961426, Test_Loss: 2.4305686950683594\n",
      "Epoch: 19, Train_Loss: 0.6778872013092041, Test_Loss: 1.2510474920272827 *\n",
      "Epoch: 19, Train_Loss: 0.7284833192825317, Test_Loss: 1.1581859588623047 *\n",
      "Epoch: 19, Train_Loss: 0.6545431017875671, Test_Loss: 0.642957866191864 *\n",
      "Epoch: 19, Train_Loss: 0.6844132542610168, Test_Loss: 0.6481021642684937\n",
      "Epoch: 19, Train_Loss: 0.6622500419616699, Test_Loss: 1.2644683122634888\n",
      "Epoch: 19, Train_Loss: 0.6593208909034729, Test_Loss: 2.108997344970703\n",
      "Epoch: 19, Train_Loss: 0.7365813255310059, Test_Loss: 0.722011148929596 *\n",
      "Epoch: 19, Train_Loss: 0.866814911365509, Test_Loss: 0.7388133406639099\n",
      "Epoch: 19, Train_Loss: 0.6401403546333313, Test_Loss: 0.6419244408607483 *\n",
      "Epoch: 19, Train_Loss: 0.6812006235122681, Test_Loss: 0.8347170948982239\n",
      "Epoch: 19, Train_Loss: 0.6354458928108215, Test_Loss: 0.953464150428772\n",
      "Epoch: 19, Train_Loss: 0.6330451369285583, Test_Loss: 1.0735385417938232\n",
      "Epoch: 19, Train_Loss: 0.634850025177002, Test_Loss: 1.7327005863189697\n",
      "Epoch: 19, Train_Loss: 0.6354072093963623, Test_Loss: 0.9592474699020386 *\n",
      "Epoch: 19, Train_Loss: 0.6415067315101624, Test_Loss: 0.6386181116104126 *\n",
      "Epoch: 19, Train_Loss: 0.6492083668708801, Test_Loss: 0.6421582698822021\n",
      "Epoch: 19, Train_Loss: 0.6414586901664734, Test_Loss: 0.6527312994003296\n",
      "Epoch: 19, Train_Loss: 0.6417848467826843, Test_Loss: 0.6682190299034119\n",
      "Epoch: 19, Train_Loss: 0.6479148268699646, Test_Loss: 0.8957598805427551\n",
      "Epoch: 19, Train_Loss: 0.6372539401054382, Test_Loss: 1.2123208045959473\n",
      "Epoch: 19, Train_Loss: 0.6354743838310242, Test_Loss: 0.9070300459861755 *\n",
      "Epoch: 19, Train_Loss: 0.6326704025268555, Test_Loss: 0.7432895302772522 *\n",
      "Epoch: 19, Train_Loss: 0.6615319848060608, Test_Loss: 0.6689081192016602 *\n",
      "Epoch: 19, Train_Loss: 0.6637943387031555, Test_Loss: 0.6400611400604248 *\n",
      "Epoch: 19, Train_Loss: 0.6587778925895691, Test_Loss: 0.6993984580039978\n",
      "Epoch: 19, Train_Loss: 0.6368879675865173, Test_Loss: 1.1979098320007324\n",
      "Epoch: 19, Train_Loss: 0.6784639358520508, Test_Loss: 1.8413540124893188\n",
      "Epoch: 19, Train_Loss: 0.6649947762489319, Test_Loss: 0.8358944654464722 *\n",
      "Epoch: 19, Train_Loss: 0.6537903547286987, Test_Loss: 0.7443448305130005 *\n",
      "Epoch: 19, Train_Loss: 0.6424802541732788, Test_Loss: 0.6350126266479492 *\n",
      "Epoch: 19, Train_Loss: 0.6635512113571167, Test_Loss: 0.6403425335884094\n",
      "Epoch: 19, Train_Loss: 0.6334561109542847, Test_Loss: 0.634984016418457 *\n",
      "Epoch: 19, Train_Loss: 0.6426040530204773, Test_Loss: 0.6467886567115784\n",
      "Epoch: 19, Train_Loss: 0.6528013944625854, Test_Loss: 0.6674649119377136\n",
      "Epoch: 19, Train_Loss: 0.6824060678482056, Test_Loss: 0.6702954769134521\n",
      "Epoch: 19, Train_Loss: 2.5507893562316895, Test_Loss: 0.6346215605735779 *\n",
      "Epoch: 19, Train_Loss: 4.134439468383789, Test_Loss: 0.7340105772018433\n",
      "Epoch: 19, Train_Loss: 0.6367254853248596, Test_Loss: 0.9972509741783142\n",
      "Epoch: 19, Train_Loss: 0.6398875117301941, Test_Loss: 0.7143797278404236 *\n",
      "Epoch: 19, Train_Loss: 0.6585139036178589, Test_Loss: 0.8397471904754639\n",
      "Epoch: 19, Train_Loss: 0.7971822619438171, Test_Loss: 0.6416100263595581 *\n",
      "Epoch: 19, Train_Loss: 0.6499677300453186, Test_Loss: 0.642463743686676\n",
      "Epoch: 19, Train_Loss: 0.6316001415252686, Test_Loss: 0.64124995470047 *\n",
      "Epoch: 19, Train_Loss: 0.6290656328201294, Test_Loss: 0.6441238522529602\n",
      "Epoch: 19, Train_Loss: 0.673134982585907, Test_Loss: 0.6575618386268616\n",
      "Epoch: 19, Train_Loss: 0.637225866317749, Test_Loss: 4.705833435058594\n",
      "Epoch: 19, Train_Loss: 0.6457825899124146, Test_Loss: 1.6854140758514404 *\n",
      "Epoch: 19, Train_Loss: 1.2809816598892212, Test_Loss: 0.6357649564743042 *\n",
      "Epoch: 19, Train_Loss: 1.9579861164093018, Test_Loss: 0.6285182237625122 *\n",
      "Epoch: 19, Train_Loss: 1.3390144109725952, Test_Loss: 0.6327328085899353\n",
      "Epoch: 19, Train_Loss: 0.7246326208114624, Test_Loss: 0.641785204410553\n",
      "Epoch: 19, Train_Loss: 1.1544969081878662, Test_Loss: 0.630557656288147 *\n",
      "Epoch: 19, Train_Loss: 2.9880356788635254, Test_Loss: 0.6357988715171814\n",
      "Epoch: 19, Train_Loss: 1.2264893054962158, Test_Loss: 0.6302463412284851 *\n",
      "Epoch: 19, Train_Loss: 0.6816092729568481, Test_Loss: 0.6351162195205688\n",
      "Epoch: 19, Train_Loss: 0.6579755544662476, Test_Loss: 0.6474556922912598\n",
      "Epoch: 19, Train_Loss: 1.6137285232543945, Test_Loss: 0.6563563346862793\n",
      "Epoch: 19, Train_Loss: 1.9424865245819092, Test_Loss: 0.6474129557609558 *\n",
      "Epoch: 19, Train_Loss: 0.9294778108596802, Test_Loss: 0.6373709440231323 *\n",
      "Epoch: 19, Train_Loss: 0.6360915303230286, Test_Loss: 0.657647430896759\n",
      "Epoch: 19, Train_Loss: 0.6351832151412964, Test_Loss: 0.6671050190925598\n",
      "Epoch: 19, Train_Loss: 1.1671197414398193, Test_Loss: 0.6304229497909546 *\n",
      "Epoch: 19, Train_Loss: 0.8281441926956177, Test_Loss: 0.6775745749473572\n",
      "Epoch: 19, Train_Loss: 0.6429010033607483, Test_Loss: 0.6521326899528503 *\n",
      "Epoch: 19, Train_Loss: 0.65740966796875, Test_Loss: 0.6679180860519409\n",
      "Epoch: 19, Train_Loss: 0.7335566282272339, Test_Loss: 0.6834837794303894\n",
      "Epoch: 19, Train_Loss: 0.697676420211792, Test_Loss: 0.6978251338005066\n",
      "Epoch: 19, Train_Loss: 0.7234047055244446, Test_Loss: 0.6716053485870361 *\n",
      "Epoch: 19, Train_Loss: 1.0243169069290161, Test_Loss: 0.7003136277198792\n",
      "Epoch: 19, Train_Loss: 0.7020729780197144, Test_Loss: 0.6789407730102539 *\n",
      "Epoch: 19, Train_Loss: 0.7048338651657104, Test_Loss: 0.6682437658309937 *\n",
      "Epoch: 19, Train_Loss: 0.7262778282165527, Test_Loss: 0.6504278182983398 *\n",
      "Epoch: 19, Train_Loss: 0.8259304761886597, Test_Loss: 0.664945125579834\n",
      "Epoch: 19, Train_Loss: 0.8455309271812439, Test_Loss: 0.6553570032119751 *\n",
      "Epoch: 19, Train_Loss: 0.7853394150733948, Test_Loss: 0.6654877662658691\n",
      "Epoch: 19, Train_Loss: 0.7534366846084595, Test_Loss: 0.7610927820205688\n",
      "Epoch: 19, Train_Loss: 0.8067237138748169, Test_Loss: 0.8061771988868713\n",
      "Epoch: 19, Train_Loss: 0.6789523363113403, Test_Loss: 6.610036373138428\n",
      "Epoch: 19, Train_Loss: 0.632509171962738, Test_Loss: 0.6651502251625061 *\n",
      "Epoch: 19, Train_Loss: 0.6237397193908691, Test_Loss: 0.7024905681610107\n",
      "Epoch: 19, Train_Loss: 0.6340337991714478, Test_Loss: 0.6789230704307556 *\n",
      "Epoch: 19, Train_Loss: 0.6228622198104858, Test_Loss: 0.6349590420722961 *\n",
      "Epoch: 19, Train_Loss: 0.6306777596473694, Test_Loss: 0.6562480926513672\n",
      "Epoch: 19, Train_Loss: 0.65062016248703, Test_Loss: 0.6578565835952759\n",
      "Epoch: 19, Train_Loss: 0.6500489711761475, Test_Loss: 0.6696990132331848\n",
      "Epoch: 19, Train_Loss: 0.6502974033355713, Test_Loss: 0.66908860206604 *\n",
      "Epoch: 19, Train_Loss: 0.7139142155647278, Test_Loss: 0.6696826219558716\n",
      "Epoch: 19, Train_Loss: 0.9771625995635986, Test_Loss: 0.6763471364974976\n",
      "Epoch: 19, Train_Loss: 0.6472179293632507, Test_Loss: 0.7402396202087402\n",
      "Epoch: 19, Train_Loss: 0.6757823824882507, Test_Loss: 0.6911588311195374 *\n",
      "Epoch: 19, Train_Loss: 0.7114658951759338, Test_Loss: 0.7205383777618408\n",
      "Epoch: 19, Train_Loss: 0.9780408143997192, Test_Loss: 0.6471549868583679 *\n",
      "Epoch: 19, Train_Loss: 0.841876745223999, Test_Loss: 0.7042123675346375\n",
      "Epoch: 19, Train_Loss: 0.6495091319084167, Test_Loss: 0.6649062037467957 *\n",
      "Epoch: 19, Train_Loss: 0.6471841931343079, Test_Loss: 0.7167066335678101\n",
      "Epoch: 19, Train_Loss: 0.9957982897758484, Test_Loss: 0.7373185157775879\n",
      "Epoch: 19, Train_Loss: 0.9969419240951538, Test_Loss: 0.6286592483520508 *\n",
      "Epoch: 19, Train_Loss: 0.6988510489463806, Test_Loss: 0.6292287111282349\n",
      "Epoch: 19, Train_Loss: 0.6376867294311523, Test_Loss: 0.650072455406189\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 0.6509923338890076, Test_Loss: 0.6598144173622131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 1.3307368755340576, Test_Loss: 0.659615695476532 *\n",
      "Epoch: 19, Train_Loss: 1.480849266052246, Test_Loss: 0.6549800038337708 *\n",
      "Epoch: 19, Train_Loss: 0.6690980195999146, Test_Loss: 0.630369246006012 *\n",
      "Epoch: 19, Train_Loss: 0.6566975116729736, Test_Loss: 0.6235811114311218 *\n",
      "Epoch: 19, Train_Loss: 0.627240777015686, Test_Loss: 0.630608081817627\n",
      "Epoch: 19, Train_Loss: 0.6312967538833618, Test_Loss: 0.6704522967338562\n",
      "Epoch: 19, Train_Loss: 1.0577499866485596, Test_Loss: 0.6616418361663818 *\n",
      "Epoch: 19, Train_Loss: 0.6539263129234314, Test_Loss: 0.6378995180130005 *\n",
      "Epoch: 19, Train_Loss: 0.6583491563796997, Test_Loss: 0.7544570565223694\n",
      "Epoch: 19, Train_Loss: 0.6311728954315186, Test_Loss: 0.9126105904579163\n",
      "Epoch: 19, Train_Loss: 0.7406593561172485, Test_Loss: 0.6730414628982544 *\n",
      "Epoch: 19, Train_Loss: 17.22260856628418, Test_Loss: 0.6438401341438293 *\n",
      "Epoch: 19, Train_Loss: 0.6338676810264587, Test_Loss: 0.8295315504074097\n",
      "Epoch: 19, Train_Loss: 2.250330924987793, Test_Loss: 0.7885335683822632 *\n",
      "Epoch: 19, Train_Loss: 1.876404047012329, Test_Loss: 0.643661379814148 *\n",
      "Epoch: 19, Train_Loss: 0.6253907084465027, Test_Loss: 0.7270877361297607\n",
      "Epoch: 19, Train_Loss: 0.7250535488128662, Test_Loss: 0.7226174473762512 *\n",
      "Epoch: 19, Train_Loss: 3.858001232147217, Test_Loss: 0.8009505271911621\n",
      "Epoch: 19, Train_Loss: 7.990920543670654, Test_Loss: 0.7168520092964172 *\n",
      "Epoch: 19, Train_Loss: 0.7373272180557251, Test_Loss: 0.6818285584449768 *\n",
      "Epoch: 19, Train_Loss: 0.659184992313385, Test_Loss: 0.6296010613441467 *\n",
      "Epoch: 19, Train_Loss: 6.287608623504639, Test_Loss: 0.6434621810913086\n",
      "Epoch: 19, Train_Loss: 0.7307140231132507, Test_Loss: 1.0089747905731201\n",
      "Epoch: 19, Train_Loss: 0.6613193154335022, Test_Loss: 1.140573263168335\n",
      "Epoch: 19, Train_Loss: 0.6947113275527954, Test_Loss: 0.8989558219909668 *\n",
      "Epoch: 19, Train_Loss: 0.6632649302482605, Test_Loss: 1.0311923027038574\n",
      "Epoch: 19, Train_Loss: 0.67592453956604, Test_Loss: 0.8492773771286011 *\n",
      "Epoch: 19, Train_Loss: 0.6324761509895325, Test_Loss: 0.9027682542800903\n",
      "Epoch: 19, Train_Loss: 0.6249227523803711, Test_Loss: 0.7488852143287659 *\n",
      "Epoch: 19, Train_Loss: 0.6131305694580078, Test_Loss: 0.6634488701820374 *\n",
      "Epoch: 19, Train_Loss: 0.6119642853736877, Test_Loss: 0.6555042862892151 *\n",
      "Epoch: 19, Train_Loss: 0.660447359085083, Test_Loss: 0.6540952920913696 *\n",
      "Epoch: 19, Train_Loss: 0.6587864756584167, Test_Loss: 0.8047832250595093\n",
      "Epoch: 19, Train_Loss: 0.6558768153190613, Test_Loss: 1.359487771987915\n",
      "Epoch: 19, Train_Loss: 0.7835507392883301, Test_Loss: 0.9920932650566101 *\n",
      "Epoch: 19, Train_Loss: 0.7725431323051453, Test_Loss: 2.0125513076782227\n",
      "Epoch: 19, Train_Loss: 0.6825379729270935, Test_Loss: 1.2505598068237305 *\n",
      "Epoch: 19, Train_Loss: 0.6440516710281372, Test_Loss: 1.3801047801971436\n",
      "Epoch: 19, Train_Loss: 0.6229642033576965, Test_Loss: 0.7246415615081787 *\n",
      "Epoch: 19, Train_Loss: 0.6487118601799011, Test_Loss: 0.6419147253036499 *\n",
      "Epoch: 19, Train_Loss: 0.6134205460548401, Test_Loss: 0.857779860496521\n",
      "Epoch: 19, Train_Loss: 0.6131585836410522, Test_Loss: 1.5671119689941406\n",
      "Epoch: 19, Train_Loss: 0.6078602075576782, Test_Loss: 0.9347058534622192 *\n",
      "Epoch: 19, Train_Loss: 0.6096470355987549, Test_Loss: 0.9395692944526672\n",
      "Epoch: 19, Train_Loss: 0.6083219051361084, Test_Loss: 0.6240894198417664 *\n",
      "Epoch: 19, Train_Loss: 0.6078760027885437, Test_Loss: 0.6929455995559692\n",
      "Epoch: 19, Train_Loss: 0.6082807779312134, Test_Loss: 0.9349021315574646\n",
      "Epoch: 19, Train_Loss: 0.6160879135131836, Test_Loss: 0.9927603006362915\n",
      "Epoch: 19, Train_Loss: 0.6290609240531921, Test_Loss: 1.9748187065124512\n",
      "Epoch: 19, Train_Loss: 0.6488396525382996, Test_Loss: 1.036560297012329 *\n",
      "Epoch: 19, Train_Loss: 0.6303960680961609, Test_Loss: 0.6697131991386414 *\n",
      "Epoch: 19, Train_Loss: 0.6676463484764099, Test_Loss: 0.620384156703949 *\n",
      "Epoch: 19, Train_Loss: 1.8490302562713623, Test_Loss: 0.6155790090560913 *\n",
      "Epoch: 20, Train_Loss: 7.211377143859863, Test_Loss: 0.668960452079773 *\n",
      "Epoch: 20, Train_Loss: 0.6411601305007935, Test_Loss: 0.7656494975090027\n",
      "Epoch: 20, Train_Loss: 0.7126473188400269, Test_Loss: 0.8351626396179199\n",
      "Epoch: 20, Train_Loss: 0.8000800013542175, Test_Loss: 0.7616168856620789 *\n",
      "Epoch: 20, Train_Loss: 0.7275060415267944, Test_Loss: 0.6707552671432495 *\n",
      "Epoch: 20, Train_Loss: 0.659968376159668, Test_Loss: 0.6560146808624268 *\n",
      "Epoch: 20, Train_Loss: 0.6830043792724609, Test_Loss: 0.7015226483345032\n",
      "Epoch: 20, Train_Loss: 0.775587797164917, Test_Loss: 0.7731202244758606\n",
      "Epoch: 20, Train_Loss: 0.83389812707901, Test_Loss: 1.0684700012207031\n",
      "Epoch: 20, Train_Loss: 0.728003978729248, Test_Loss: 1.7114019393920898\n",
      "Epoch: 20, Train_Loss: 0.6581050753593445, Test_Loss: 1.083390474319458 *\n",
      "Epoch: 20, Train_Loss: 0.6202785968780518, Test_Loss: 0.692761242389679 *\n",
      "Epoch: 20, Train_Loss: 0.6767653226852417, Test_Loss: 0.6294237375259399 *\n",
      "Epoch: 20, Train_Loss: 0.6793116927146912, Test_Loss: 0.635735809803009\n",
      "Epoch: 20, Train_Loss: 0.7381618618965149, Test_Loss: 0.631791353225708 *\n",
      "Epoch: 20, Train_Loss: 0.651891827583313, Test_Loss: 0.6413323283195496\n",
      "Epoch: 20, Train_Loss: 0.6609616279602051, Test_Loss: 0.639415979385376 *\n",
      "Epoch: 20, Train_Loss: 0.6080769300460815, Test_Loss: 0.7053220868110657\n",
      "Epoch: 20, Train_Loss: 0.6344162821769714, Test_Loss: 0.6110875010490417 *\n",
      "Epoch: 20, Train_Loss: 0.7003655433654785, Test_Loss: 0.6645718216896057\n",
      "Epoch: 20, Train_Loss: 0.6329259276390076, Test_Loss: 0.8283622860908508\n",
      "Epoch: 20, Train_Loss: 0.6029464602470398, Test_Loss: 0.8106483221054077 *\n",
      "Epoch: 20, Train_Loss: 0.6018353700637817, Test_Loss: 0.8416805267333984\n",
      "Epoch: 20, Train_Loss: 0.6064286828041077, Test_Loss: 0.6155983805656433 *\n",
      "Epoch: 20, Train_Loss: 2.19972825050354, Test_Loss: 0.6040964126586914 *\n",
      "Epoch: 20, Train_Loss: 4.6663498878479, Test_Loss: 0.6071776151657104\n",
      "Epoch: 20, Train_Loss: 0.6072602868080139, Test_Loss: 0.6054731011390686 *\n",
      "Epoch: 20, Train_Loss: 0.6185814142227173, Test_Loss: 0.6158857345581055\n",
      "Epoch: 20, Train_Loss: 0.6069448590278625, Test_Loss: 3.2486624717712402\n",
      "Epoch: 20, Train_Loss: 0.604512095451355, Test_Loss: 3.5860626697540283\n",
      "Epoch: 20, Train_Loss: 0.6031556129455566, Test_Loss: 0.6426256895065308 *\n",
      "Epoch: 20, Train_Loss: 0.601874053478241, Test_Loss: 0.6317538619041443 *\n",
      "Epoch: 20, Train_Loss: 0.6155434250831604, Test_Loss: 0.6484335064888\n",
      "Epoch: 20, Train_Loss: 0.6150831580162048, Test_Loss: 0.6061047315597534 *\n",
      "Epoch: 20, Train_Loss: 0.6445642709732056, Test_Loss: 0.654880702495575\n",
      "Epoch: 20, Train_Loss: 0.6063323020935059, Test_Loss: 0.6587201356887817\n",
      "Epoch: 20, Train_Loss: 0.5989362597465515, Test_Loss: 0.6708938479423523\n",
      "Epoch: 20, Train_Loss: 0.6000474691390991, Test_Loss: 0.6681879162788391 *\n",
      "Epoch: 20, Train_Loss: 0.617415189743042, Test_Loss: 0.6870644092559814\n",
      "Epoch: 20, Train_Loss: 0.6002091765403748, Test_Loss: 0.6772969961166382 *\n",
      "Epoch: 20, Train_Loss: 0.5996133089065552, Test_Loss: 0.6810057163238525\n",
      "Epoch: 20, Train_Loss: 0.6178252696990967, Test_Loss: 0.6292726397514343 *\n",
      "Epoch: 20, Train_Loss: 0.6166934967041016, Test_Loss: 0.6266599893569946 *\n",
      "Epoch: 20, Train_Loss: 0.6144025325775146, Test_Loss: 0.6600682735443115\n",
      "Epoch: 20, Train_Loss: 0.6004475951194763, Test_Loss: 0.6053345203399658 *\n",
      "Epoch: 20, Train_Loss: 0.6330516934394836, Test_Loss: 0.6474642753601074\n",
      "Epoch: 20, Train_Loss: 0.6295039653778076, Test_Loss: 0.6222985982894897 *\n",
      "Epoch: 20, Train_Loss: 0.625404417514801, Test_Loss: 0.6525929570198059\n",
      "Epoch: 20, Train_Loss: 0.6239092350006104, Test_Loss: 0.6271948218345642 *\n",
      "Epoch: 20, Train_Loss: 0.6774088740348816, Test_Loss: 0.6463256478309631\n",
      "Epoch: 20, Train_Loss: 0.6809211373329163, Test_Loss: 0.6314916014671326 *\n",
      "Epoch: 20, Train_Loss: 0.6156120300292969, Test_Loss: 0.6597130298614502\n",
      "Epoch: 20, Train_Loss: 0.6399667263031006, Test_Loss: 0.6417311429977417 *\n",
      "Epoch: 20, Train_Loss: 0.6426071524620056, Test_Loss: 0.637545108795166 *\n",
      "Epoch: 20, Train_Loss: 0.7557631134986877, Test_Loss: 0.6163007020950317 *\n",
      "Epoch: 20, Train_Loss: 0.6134901642799377, Test_Loss: 0.6200528144836426\n",
      "Epoch: 20, Train_Loss: 0.5978552103042603, Test_Loss: 0.6116020679473877 *\n",
      "Epoch: 20, Train_Loss: 0.5959579348564148, Test_Loss: 0.6053498983383179 *\n",
      "Epoch: 20, Train_Loss: 0.5984362959861755, Test_Loss: 0.7068642377853394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 0.5946159958839417, Test_Loss: 0.6294508576393127 *\n",
      "Epoch: 20, Train_Loss: 0.5979616641998291, Test_Loss: 5.887028217315674\n",
      "Epoch: 20, Train_Loss: 2.3434324264526367, Test_Loss: 1.2923957109451294 *\n",
      "Epoch: 20, Train_Loss: 3.527367115020752, Test_Loss: 0.5962064266204834 *\n",
      "Epoch: 20, Train_Loss: 0.6012657880783081, Test_Loss: 0.6107622385025024\n",
      "Epoch: 20, Train_Loss: 0.5989207625389099, Test_Loss: 0.6196922063827515\n",
      "Epoch: 20, Train_Loss: 0.5966706275939941, Test_Loss: 0.6221694350242615\n",
      "Epoch: 20, Train_Loss: 0.5996866822242737, Test_Loss: 0.6064161658287048 *\n",
      "Epoch: 20, Train_Loss: 0.5952274799346924, Test_Loss: 0.7081210613250732\n",
      "Epoch: 20, Train_Loss: 0.598999559879303, Test_Loss: 0.7029393911361694 *\n",
      "Epoch: 20, Train_Loss: 0.5968855023384094, Test_Loss: 0.5948871970176697 *\n",
      "Epoch: 20, Train_Loss: 0.594820499420166, Test_Loss: 0.637376606464386\n",
      "Epoch: 20, Train_Loss: 0.6025328040122986, Test_Loss: 0.604529619216919 *\n",
      "Epoch: 20, Train_Loss: 0.6456754803657532, Test_Loss: 0.6061736345291138\n",
      "Epoch: 20, Train_Loss: 0.6257971525192261, Test_Loss: 0.6007694005966187 *\n",
      "Epoch: 20, Train_Loss: 0.6552667617797852, Test_Loss: 0.6572888493537903\n",
      "Epoch: 20, Train_Loss: 0.6223677396774292, Test_Loss: 0.6407469511032104 *\n",
      "Epoch: 20, Train_Loss: 0.5923807621002197, Test_Loss: 0.7192733287811279\n",
      "Epoch: 20, Train_Loss: 0.7182919979095459, Test_Loss: 0.6669115424156189 *\n",
      "Epoch: 20, Train_Loss: 0.7554872035980225, Test_Loss: 0.6184757351875305 *\n",
      "Epoch: 20, Train_Loss: 0.7317569255828857, Test_Loss: 0.5981956124305725 *\n",
      "Epoch: 20, Train_Loss: 0.667118489742279, Test_Loss: 0.5964091420173645 *\n",
      "Epoch: 20, Train_Loss: 0.6024795770645142, Test_Loss: 0.5957416296005249 *\n",
      "Epoch: 20, Train_Loss: 0.6009660959243774, Test_Loss: 0.5983629822731018\n",
      "Epoch: 20, Train_Loss: 0.5922062993049622, Test_Loss: 0.5967292189598083 *\n",
      "Epoch: 20, Train_Loss: 0.5910335183143616, Test_Loss: 0.595594048500061 *\n",
      "Epoch: 20, Train_Loss: 0.5981186628341675, Test_Loss: 0.5952040553092957 *\n",
      "Epoch: 20, Train_Loss: 0.5924999713897705, Test_Loss: 0.5983678102493286\n",
      "Epoch: 20, Train_Loss: 0.6057530641555786, Test_Loss: 0.5970028638839722 *\n",
      "Epoch: 20, Train_Loss: 0.6025754809379578, Test_Loss: 0.6162917613983154\n",
      "Epoch: 20, Train_Loss: 0.5936403274536133, Test_Loss: 0.6269510388374329\n",
      "Epoch: 20, Train_Loss: 0.6276230216026306, Test_Loss: 0.6097936630249023 *\n",
      "Epoch: 20, Train_Loss: 0.6705993413925171, Test_Loss: 0.6135499477386475\n",
      "Epoch: 20, Train_Loss: 0.623430073261261, Test_Loss: 1.0615345239639282\n",
      "Epoch: 20, Train_Loss: 0.6638435125350952, Test_Loss: 0.6453461647033691 *\n",
      "Epoch: 20, Train_Loss: 0.7205126285552979, Test_Loss: 0.6100742220878601 *\n",
      "Epoch: 20, Train_Loss: 0.7420780062675476, Test_Loss: 0.6259791851043701\n",
      "Epoch: 20, Train_Loss: 0.6905832886695862, Test_Loss: 0.8520675897598267\n",
      "Epoch: 20, Train_Loss: 0.7838565111160278, Test_Loss: 0.6167664527893066 *\n",
      "Epoch: 20, Train_Loss: 0.7856855392456055, Test_Loss: 0.6994859576225281\n",
      "Epoch: 20, Train_Loss: 0.9143551588058472, Test_Loss: 0.7445726990699768\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 0.5977262258529663, Test_Loss: 0.7865190505981445\n",
      "Epoch: 20, Train_Loss: 0.6275744438171387, Test_Loss: 0.6275286078453064 *\n",
      "Epoch: 20, Train_Loss: 3.258537530899048, Test_Loss: 0.6383160352706909\n",
      "Epoch: 20, Train_Loss: 0.7532039284706116, Test_Loss: 0.5911189913749695 *\n",
      "Epoch: 20, Train_Loss: 0.6726531386375427, Test_Loss: 0.6632780432701111\n",
      "Epoch: 20, Train_Loss: 0.7081941962242126, Test_Loss: 0.7124757170677185\n",
      "Epoch: 20, Train_Loss: 0.7103326916694641, Test_Loss: 1.148781180381775\n",
      "Epoch: 20, Train_Loss: 0.6191611886024475, Test_Loss: 0.9207596778869629 *\n",
      "Epoch: 20, Train_Loss: 0.5975168943405151, Test_Loss: 1.581789493560791\n",
      "Epoch: 20, Train_Loss: 0.6221731305122375, Test_Loss: 1.078588843345642 *\n",
      "Epoch: 20, Train_Loss: 0.644350528717041, Test_Loss: 0.8860782384872437 *\n",
      "Epoch: 20, Train_Loss: 0.6241658329963684, Test_Loss: 0.7617322206497192 *\n",
      "Epoch: 20, Train_Loss: 0.6210458874702454, Test_Loss: 0.6016960740089417 *\n",
      "Epoch: 20, Train_Loss: 0.6185316443443298, Test_Loss: 0.630540668964386\n",
      "Epoch: 20, Train_Loss: 0.6013204455375671, Test_Loss: 0.6427067518234253\n",
      "Epoch: 20, Train_Loss: 0.6021506190299988, Test_Loss: 0.8057457208633423\n",
      "Epoch: 20, Train_Loss: 0.6011449694633484, Test_Loss: 1.2524888515472412\n",
      "Epoch: 20, Train_Loss: 0.6281633377075195, Test_Loss: 0.7701082825660706 *\n",
      "Epoch: 20, Train_Loss: 0.6079206466674805, Test_Loss: 2.823890447616577\n",
      "Epoch: 20, Train_Loss: 0.5886626243591309, Test_Loss: 1.1950815916061401 *\n",
      "Epoch: 20, Train_Loss: 0.5995259881019592, Test_Loss: 1.3142406940460205\n",
      "Epoch: 20, Train_Loss: 0.6035982370376587, Test_Loss: 0.6906555891036987 *\n",
      "Epoch: 20, Train_Loss: 0.5985472798347473, Test_Loss: 0.5965970754623413 *\n",
      "Epoch: 20, Train_Loss: 0.5933266878128052, Test_Loss: 0.7683426141738892\n",
      "Epoch: 20, Train_Loss: 0.5922769904136658, Test_Loss: 1.9725186824798584\n",
      "Epoch: 20, Train_Loss: 0.5870556235313416, Test_Loss: 1.3593263626098633 *\n",
      "Epoch: 20, Train_Loss: 0.5844058990478516, Test_Loss: 0.6739400029182434 *\n",
      "Epoch: 20, Train_Loss: 0.5821528434753418, Test_Loss: 0.6095330119132996 *\n",
      "Epoch: 20, Train_Loss: 0.5852187871932983, Test_Loss: 0.6730236411094666\n",
      "Epoch: 20, Train_Loss: 0.5834310054779053, Test_Loss: 1.0433028936386108\n",
      "Epoch: 20, Train_Loss: 0.5823427438735962, Test_Loss: 0.7559728622436523 *\n",
      "Epoch: 20, Train_Loss: 0.5818321704864502, Test_Loss: 1.5519859790802002\n",
      "Epoch: 20, Train_Loss: 0.5824228525161743, Test_Loss: 1.174448013305664 *\n",
      "Epoch: 20, Train_Loss: 0.587664783000946, Test_Loss: 0.6415392160415649 *\n",
      "Epoch: 20, Train_Loss: 0.5913298726081848, Test_Loss: 0.5892318487167358 *\n",
      "Epoch: 20, Train_Loss: 0.5902226567268372, Test_Loss: 0.586705207824707 *\n",
      "Epoch: 20, Train_Loss: 0.5982187390327454, Test_Loss: 0.5935097932815552\n",
      "Epoch: 20, Train_Loss: 0.5900355577468872, Test_Loss: 0.6334835290908813\n",
      "Epoch: 20, Train_Loss: 0.5905019044876099, Test_Loss: 1.1343390941619873\n",
      "Epoch: 20, Train_Loss: 0.5808788537979126, Test_Loss: 1.0509891510009766 *\n",
      "Epoch: 20, Train_Loss: 0.5795958042144775, Test_Loss: 0.719021201133728 *\n",
      "Epoch: 20, Train_Loss: 0.5977073907852173, Test_Loss: 0.61282879114151 *\n",
      "Epoch: 20, Train_Loss: 0.5983389019966125, Test_Loss: 0.6001580953598022 *\n",
      "Epoch: 20, Train_Loss: 0.5795034766197205, Test_Loss: 0.6203603148460388\n",
      "Epoch: 20, Train_Loss: 0.579131543636322, Test_Loss: 0.854722797870636\n",
      "Epoch: 20, Train_Loss: 0.5813454985618591, Test_Loss: 1.8145430088043213\n",
      "Epoch: 20, Train_Loss: 0.6424887180328369, Test_Loss: 1.2989596128463745 *\n",
      "Epoch: 20, Train_Loss: 0.6155505776405334, Test_Loss: 0.6466709971427917 *\n",
      "Epoch: 20, Train_Loss: 0.6147966384887695, Test_Loss: 0.6293249726295471 *\n",
      "Epoch: 20, Train_Loss: 0.5866264700889587, Test_Loss: 0.5839564800262451 *\n",
      "Epoch: 20, Train_Loss: 0.5829494595527649, Test_Loss: 0.5840024352073669\n",
      "Epoch: 20, Train_Loss: 0.6492062211036682, Test_Loss: 0.5870400071144104\n",
      "Epoch: 20, Train_Loss: 0.5797314643859863, Test_Loss: 0.6022298336029053\n",
      "Epoch: 20, Train_Loss: 0.5930765271186829, Test_Loss: 0.6504243612289429\n",
      "Epoch: 20, Train_Loss: 0.6063343286514282, Test_Loss: 0.5786809325218201 *\n",
      "Epoch: 20, Train_Loss: 0.6109805107116699, Test_Loss: 0.6225246787071228\n",
      "Epoch: 20, Train_Loss: 0.6946573853492737, Test_Loss: 0.7053369283676147\n",
      "Epoch: 20, Train_Loss: 0.6333290934562683, Test_Loss: 0.9365599751472473\n",
      "Epoch: 20, Train_Loss: 0.6140933036804199, Test_Loss: 0.7870742082595825 *\n",
      "Epoch: 20, Train_Loss: 0.5852059721946716, Test_Loss: 0.5922804474830627 *\n",
      "Epoch: 20, Train_Loss: 0.6021546125411987, Test_Loss: 0.583982527256012 *\n",
      "Epoch: 20, Train_Loss: 0.5791596174240112, Test_Loss: 0.581790566444397 *\n",
      "Epoch: 20, Train_Loss: 0.5823503732681274, Test_Loss: 0.5791648626327515 *\n",
      "Epoch: 20, Train_Loss: 0.5866450071334839, Test_Loss: 0.5874481797218323\n",
      "Epoch: 20, Train_Loss: 0.5916247963905334, Test_Loss: 1.6637455224990845\n",
      "Epoch: 20, Train_Loss: 0.6358745694160461, Test_Loss: 4.796030521392822\n",
      "Epoch: 20, Train_Loss: 0.6401280760765076, Test_Loss: 0.593023955821991 *\n",
      "Epoch: 20, Train_Loss: 0.6060501337051392, Test_Loss: 0.5784140229225159 *\n",
      "Epoch: 20, Train_Loss: 0.6037602424621582, Test_Loss: 0.5791192650794983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 0.6017839312553406, Test_Loss: 0.5842449069023132\n",
      "Epoch: 20, Train_Loss: 0.5888904929161072, Test_Loss: 0.5786899328231812 *\n",
      "Epoch: 20, Train_Loss: 0.7117141485214233, Test_Loss: 0.5778857469558716 *\n",
      "Epoch: 20, Train_Loss: 0.7481251955032349, Test_Loss: 0.5790272951126099\n",
      "Epoch: 20, Train_Loss: 0.5756264328956604, Test_Loss: 0.575588047504425 *\n",
      "Epoch: 20, Train_Loss: 0.6129821538925171, Test_Loss: 0.5781623721122742\n",
      "Epoch: 20, Train_Loss: 0.5738444924354553, Test_Loss: 0.5777223706245422 *\n",
      "Epoch: 20, Train_Loss: 0.5729867219924927, Test_Loss: 0.5796607732772827\n",
      "Epoch: 20, Train_Loss: 0.5742274522781372, Test_Loss: 0.6055089235305786\n",
      "Epoch: 20, Train_Loss: 0.5734925866127014, Test_Loss: 0.605793833732605\n",
      "Epoch: 20, Train_Loss: 0.5784837603569031, Test_Loss: 0.5854650139808655 *\n",
      "Epoch: 20, Train_Loss: 0.589500904083252, Test_Loss: 0.5731413960456848 *\n",
      "Epoch: 20, Train_Loss: 0.582310676574707, Test_Loss: 0.5742782354354858\n",
      "Epoch: 20, Train_Loss: 0.5868144035339355, Test_Loss: 0.5841702818870544\n",
      "Epoch: 20, Train_Loss: 0.5887705087661743, Test_Loss: 0.5741374492645264 *\n",
      "Epoch: 20, Train_Loss: 0.5716986656188965, Test_Loss: 0.5741264820098877 *\n",
      "Epoch: 20, Train_Loss: 0.5730265974998474, Test_Loss: 0.5723884105682373 *\n",
      "Epoch: 20, Train_Loss: 0.5713024139404297, Test_Loss: 0.5703065395355225 *\n",
      "Epoch: 20, Train_Loss: 0.5960671305656433, Test_Loss: 0.5740405321121216\n",
      "Epoch: 20, Train_Loss: 0.5979341864585876, Test_Loss: 0.5726360082626343 *\n",
      "Epoch: 20, Train_Loss: 0.5858101844787598, Test_Loss: 0.5723707675933838 *\n",
      "Epoch: 20, Train_Loss: 0.5803694725036621, Test_Loss: 0.5705763101577759 *\n",
      "Epoch: 20, Train_Loss: 0.6071664094924927, Test_Loss: 0.5705190300941467 *\n",
      "Epoch: 20, Train_Loss: 0.6050817966461182, Test_Loss: 0.5727567076683044\n",
      "Epoch: 20, Train_Loss: 0.5841478705406189, Test_Loss: 0.5691418051719666 *\n",
      "Epoch: 20, Train_Loss: 0.5884884595870972, Test_Loss: 0.5999312400817871\n",
      "Epoch: 20, Train_Loss: 0.5883371233940125, Test_Loss: 0.6167601943016052\n",
      "Epoch: 20, Train_Loss: 0.5764138698577881, Test_Loss: 3.999549388885498\n",
      "Epoch: 20, Train_Loss: 0.5774057507514954, Test_Loss: 2.6887123584747314 *\n",
      "Epoch: 20, Train_Loss: 0.6004058718681335, Test_Loss: 0.5704619884490967 *\n",
      "Epoch: 20, Train_Loss: 0.6267482042312622, Test_Loss: 0.5737293362617493\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 2.692685842514038, Test_Loss: 0.6380428671836853\n",
      "Epoch: 20, Train_Loss: 3.7337088584899902, Test_Loss: 0.6199257373809814 *\n",
      "Epoch: 20, Train_Loss: 0.5875009894371033, Test_Loss: 0.5955432653427124 *\n",
      "Epoch: 20, Train_Loss: 0.5740779638290405, Test_Loss: 0.6325042843818665\n",
      "Epoch: 20, Train_Loss: 0.6168493628501892, Test_Loss: 0.6611095666885376\n",
      "Epoch: 20, Train_Loss: 0.7354229688644409, Test_Loss: 0.5711595416069031 *\n",
      "Epoch: 20, Train_Loss: 0.5931038856506348, Test_Loss: 0.5915789008140564\n",
      "Epoch: 20, Train_Loss: 0.5698943138122559, Test_Loss: 0.5908613801002502 *\n",
      "Epoch: 20, Train_Loss: 0.5765389204025269, Test_Loss: 0.5966492891311646\n",
      "Epoch: 20, Train_Loss: 0.6160686612129211, Test_Loss: 0.5742700695991516 *\n",
      "Epoch: 20, Train_Loss: 0.5754133462905884, Test_Loss: 0.6638139486312866\n",
      "Epoch: 20, Train_Loss: 0.5800477266311646, Test_Loss: 0.6277779340744019 *\n",
      "Epoch: 20, Train_Loss: 1.3073971271514893, Test_Loss: 0.6188799738883972 *\n",
      "Epoch: 20, Train_Loss: 1.6758482456207275, Test_Loss: 0.5999221801757812 *\n",
      "Epoch: 20, Train_Loss: 0.9738953113555908, Test_Loss: 0.6791059374809265\n",
      "Epoch: 20, Train_Loss: 0.6246318221092224, Test_Loss: 0.6619716882705688 *\n",
      "Epoch: 20, Train_Loss: 1.6093010902404785, Test_Loss: 0.6325817108154297 *\n",
      "Epoch: 20, Train_Loss: 2.935330390930176, Test_Loss: 0.6427646279335022\n",
      "Epoch: 20, Train_Loss: 0.8898751735687256, Test_Loss: 0.6311637759208679 *\n",
      "Epoch: 20, Train_Loss: 0.605196475982666, Test_Loss: 0.6363128423690796\n",
      "Epoch: 20, Train_Loss: 0.6021192669868469, Test_Loss: 0.6128444075584412 *\n",
      "Epoch: 20, Train_Loss: 1.5918562412261963, Test_Loss: 0.5742456316947937 *\n",
      "Epoch: 20, Train_Loss: 1.6607446670532227, Test_Loss: 0.5803948640823364\n",
      "Epoch: 20, Train_Loss: 0.6440116763114929, Test_Loss: 0.5768439769744873 *\n",
      "Epoch: 20, Train_Loss: 0.5831767916679382, Test_Loss: 0.5713710784912109 *\n",
      "Epoch: 20, Train_Loss: 0.5659496188163757, Test_Loss: 0.5903297662734985\n",
      "Epoch: 20, Train_Loss: 1.209604024887085, Test_Loss: 0.5767674446105957 *\n",
      "Epoch: 20, Train_Loss: 0.7253387570381165, Test_Loss: 0.5949790477752686\n",
      "Epoch: 20, Train_Loss: 0.6216569542884827, Test_Loss: 0.9015017151832581\n",
      "Epoch: 20, Train_Loss: 0.5875623226165771, Test_Loss: 0.6067359447479248 *\n",
      "Epoch: 20, Train_Loss: 0.6829373240470886, Test_Loss: 0.591354250907898 *\n",
      "Epoch: 20, Train_Loss: 0.6534152030944824, Test_Loss: 0.7283368110656738\n",
      "Epoch: 20, Train_Loss: 0.7523945569992065, Test_Loss: 0.8442986011505127\n",
      "Epoch: 20, Train_Loss: 0.9598492383956909, Test_Loss: 0.6268605589866638 *\n",
      "Epoch: 20, Train_Loss: 0.6605574488639832, Test_Loss: 0.6264228224754333 *\n",
      "Epoch: 20, Train_Loss: 0.6242658495903015, Test_Loss: 0.6302992105484009\n",
      "Epoch: 20, Train_Loss: 0.6867573261260986, Test_Loss: 0.7313072085380554\n",
      "Epoch: 20, Train_Loss: 0.7829413414001465, Test_Loss: 0.6154963970184326 *\n",
      "Epoch: 20, Train_Loss: 0.7641474008560181, Test_Loss: 0.6025593876838684 *\n",
      "Epoch: 20, Train_Loss: 0.6182863116264343, Test_Loss: 0.5709381103515625 *\n",
      "Epoch: 20, Train_Loss: 0.7392791509628296, Test_Loss: 0.5882899165153503\n",
      "Epoch: 20, Train_Loss: 0.7207228541374207, Test_Loss: 0.7349478602409363\n",
      "Epoch: 20, Train_Loss: 0.6211807727813721, Test_Loss: 0.9854962825775146\n",
      "Epoch: 20, Train_Loss: 0.5732182264328003, Test_Loss: 0.7638862729072571 *\n",
      "Epoch: 20, Train_Loss: 0.5685032606124878, Test_Loss: 1.0820764303207397\n",
      "Epoch: 20, Train_Loss: 0.6027339696884155, Test_Loss: 0.9633122682571411 *\n",
      "Epoch: 20, Train_Loss: 0.5663163065910339, Test_Loss: 0.788588285446167 *\n",
      "Epoch: 20, Train_Loss: 0.5713738203048706, Test_Loss: 0.7507510185241699 *\n",
      "Epoch: 20, Train_Loss: 0.6021020412445068, Test_Loss: 0.6177271604537964 *\n",
      "Epoch: 20, Train_Loss: 0.5781272649765015, Test_Loss: 0.5867862701416016 *\n",
      "Epoch: 20, Train_Loss: 0.5959683060646057, Test_Loss: 0.5741764307022095 *\n",
      "Epoch: 20, Train_Loss: 0.6831368207931519, Test_Loss: 0.6618808507919312\n",
      "Epoch: 20, Train_Loss: 0.9752784967422485, Test_Loss: 0.9666616320610046\n",
      "Epoch: 20, Train_Loss: 0.5702821016311646, Test_Loss: 0.9670881628990173\n",
      "Epoch: 20, Train_Loss: 0.6068556308746338, Test_Loss: 1.9522078037261963\n",
      "Epoch: 20, Train_Loss: 0.6891188621520996, Test_Loss: 1.5747843980789185 *\n",
      "Epoch: 20, Train_Loss: 0.8644744157791138, Test_Loss: 1.134117841720581 *\n",
      "Epoch: 20, Train_Loss: 0.7780765295028687, Test_Loss: 0.8385800123214722 *\n",
      "Epoch: 20, Train_Loss: 0.5798271298408508, Test_Loss: 0.5664142370223999 *\n",
      "Epoch: 20, Train_Loss: 0.6772983074188232, Test_Loss: 0.6288458704948425\n",
      "Epoch: 20, Train_Loss: 0.9849014282226562, Test_Loss: 1.394104242324829\n",
      "Epoch: 20, Train_Loss: 0.9306514263153076, Test_Loss: 1.6500179767608643\n",
      "Epoch: 20, Train_Loss: 0.6097642779350281, Test_Loss: 0.6360887289047241 *\n",
      "Epoch: 20, Train_Loss: 0.5719902515411377, Test_Loss: 0.6275420188903809 *\n",
      "Epoch: 20, Train_Loss: 0.5716395378112793, Test_Loss: 0.656764030456543\n",
      "Epoch: 20, Train_Loss: 1.4330224990844727, Test_Loss: 1.0843913555145264\n",
      "Epoch: 20, Train_Loss: 1.3140453100204468, Test_Loss: 0.7439730167388916 *\n",
      "Epoch: 20, Train_Loss: 0.5751200318336487, Test_Loss: 1.2050278186798096\n",
      "Epoch: 20, Train_Loss: 0.5967353582382202, Test_Loss: 0.9833945035934448 *\n",
      "Epoch: 20, Train_Loss: 0.5699906945228577, Test_Loss: 0.8095266222953796 *\n",
      "Epoch: 20, Train_Loss: 0.5990555286407471, Test_Loss: 0.5866116285324097 *\n",
      "Epoch: 20, Train_Loss: 0.9316922426223755, Test_Loss: 0.6473327875137329\n",
      "Epoch: 20, Train_Loss: 0.5752565264701843, Test_Loss: 0.6787247657775879\n",
      "Epoch: 20, Train_Loss: 0.5851125717163086, Test_Loss: 0.664560079574585 *\n",
      "Epoch: 20, Train_Loss: 0.5831316709518433, Test_Loss: 0.8123886585235596\n",
      "Epoch: 20, Train_Loss: 0.637087345123291, Test_Loss: 0.8497540950775146\n",
      "Epoch: 20, Train_Loss: 17.40726089477539, Test_Loss: 0.680489182472229 *\n",
      "Epoch: 20, Train_Loss: 0.5740999579429626, Test_Loss: 0.5995758771896362 *\n",
      "Epoch: 20, Train_Loss: 2.105264186859131, Test_Loss: 0.6087415218353271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 1.6711740493774414, Test_Loss: 0.5780220627784729 *\n",
      "Epoch: 20, Train_Loss: 0.5721622705459595, Test_Loss: 0.604763925075531\n",
      "Epoch: 20, Train_Loss: 0.6736485958099365, Test_Loss: 1.0846588611602783\n",
      "Epoch: 20, Train_Loss: 5.5348687171936035, Test_Loss: 1.1357812881469727\n",
      "Epoch: 20, Train_Loss: 6.2321672439575195, Test_Loss: 0.6239416599273682 *\n",
      "Epoch: 20, Train_Loss: 0.6495604515075684, Test_Loss: 0.6197023987770081 *\n",
      "Epoch: 20, Train_Loss: 0.6988010406494141, Test_Loss: 0.5830676555633545 *\n",
      "Epoch: 20, Train_Loss: 6.1719865798950195, Test_Loss: 0.6103958487510681\n",
      "Epoch: 20, Train_Loss: 0.669670820236206, Test_Loss: 0.6477345824241638\n",
      "Epoch: 20, Train_Loss: 0.5728555917739868, Test_Loss: 0.6514610052108765\n",
      "Epoch: 20, Train_Loss: 0.5647482872009277, Test_Loss: 0.7500272989273071\n",
      "Epoch: 20, Train_Loss: 0.5664759278297424, Test_Loss: 0.7119990587234497 *\n",
      "Epoch: 20, Train_Loss: 0.5958158373832703, Test_Loss: 0.6611144542694092 *\n",
      "Epoch: 20, Train_Loss: 0.5662049651145935, Test_Loss: 0.6323950290679932 *\n",
      "Epoch: 20, Train_Loss: 0.5796008110046387, Test_Loss: 0.8740216493606567\n",
      "Epoch: 20, Train_Loss: 0.5667266845703125, Test_Loss: 0.6836122274398804 *\n",
      "Epoch: 20, Train_Loss: 0.5648476481437683, Test_Loss: 0.5858491659164429 *\n",
      "Epoch: 20, Train_Loss: 0.6044319868087769, Test_Loss: 0.5875623226165771\n",
      "Epoch: 20, Train_Loss: 0.5942835211753845, Test_Loss: 0.5963517427444458\n",
      "Epoch: 20, Train_Loss: 0.6141141653060913, Test_Loss: 0.5852833986282349 *\n",
      "Epoch: 20, Train_Loss: 0.664986789226532, Test_Loss: 0.5826954245567322 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 0.6454535126686096, Test_Loss: 0.8291863203048706\n",
      "Epoch: 20, Train_Loss: 0.5711683630943298, Test_Loss: 6.264305114746094\n",
      "Epoch: 20, Train_Loss: 0.5795484185218811, Test_Loss: 0.7298539876937866 *\n",
      "Epoch: 20, Train_Loss: 0.565999448299408, Test_Loss: 0.6624825596809387 *\n",
      "Epoch: 20, Train_Loss: 0.5581494569778442, Test_Loss: 0.6321747899055481 *\n",
      "Epoch: 20, Train_Loss: 0.5542604327201843, Test_Loss: 0.582719624042511 *\n",
      "Epoch: 20, Train_Loss: 0.5532457828521729, Test_Loss: 0.5874174237251282\n",
      "Epoch: 20, Train_Loss: 0.5549366474151611, Test_Loss: 0.638237476348877\n",
      "Epoch: 20, Train_Loss: 0.5542962551116943, Test_Loss: 0.6924569010734558\n",
      "Epoch: 20, Train_Loss: 0.5546143651008606, Test_Loss: 0.6227445602416992 *\n",
      "Epoch: 20, Train_Loss: 0.5542741417884827, Test_Loss: 0.6533972024917603\n",
      "Epoch: 20, Train_Loss: 0.5526654720306396, Test_Loss: 0.6433022618293762 *\n",
      "Epoch: 20, Train_Loss: 0.557361364364624, Test_Loss: 0.689837634563446\n",
      "Epoch: 20, Train_Loss: 0.5664494037628174, Test_Loss: 0.5818204879760742 *\n",
      "Epoch: 20, Train_Loss: 0.5925151705741882, Test_Loss: 0.601983904838562\n",
      "Epoch: 20, Train_Loss: 0.6417730450630188, Test_Loss: 0.641941249370575\n",
      "Epoch: 20, Train_Loss: 0.5571662187576294, Test_Loss: 0.5821999907493591 *\n",
      "Epoch: 20, Train_Loss: 3.5529112815856934, Test_Loss: 0.5627164244651794 *\n",
      "Epoch: 20, Train_Loss: 6.6529860496521, Test_Loss: 0.5747007131576538\n",
      "Epoch: 20, Train_Loss: 0.5726953744888306, Test_Loss: 0.5966126918792725\n",
      "Epoch: 20, Train_Loss: 0.5666563510894775, Test_Loss: 0.597038984298706\n",
      "Epoch: 20, Train_Loss: 0.5941128730773926, Test_Loss: 0.611362099647522\n",
      "Epoch: 20, Train_Loss: 0.5776758790016174, Test_Loss: 0.6137570738792419\n",
      "Epoch: 20, Train_Loss: 0.5652190446853638, Test_Loss: 0.6178936958312988\n",
      "Epoch: 20, Train_Loss: 0.6156287789344788, Test_Loss: 0.6327910423278809\n",
      "Epoch: 20, Train_Loss: 0.6811530590057373, Test_Loss: 0.6101999282836914 *\n",
      "Epoch: 20, Train_Loss: 0.8067854642868042, Test_Loss: 0.600193202495575 *\n",
      "Epoch: 20, Train_Loss: 0.7339781522750854, Test_Loss: 0.6053012013435364\n",
      "Epoch: 20, Train_Loss: 0.6041709184646606, Test_Loss: 0.5870671272277832 *\n",
      "Epoch: 20, Train_Loss: 0.6020958423614502, Test_Loss: 0.5769043564796448 *\n",
      "Epoch: 20, Train_Loss: 0.6698309779167175, Test_Loss: 0.6220458745956421\n",
      "Epoch: 20, Train_Loss: 0.6746864914894104, Test_Loss: 0.639433741569519\n",
      "Epoch: 20, Train_Loss: 0.6461775898933411, Test_Loss: 2.9120607376098633\n",
      "Epoch: 20, Train_Loss: 0.600131630897522, Test_Loss: 4.191225051879883\n",
      "Epoch: 20, Train_Loss: 0.5829168558120728, Test_Loss: 0.5786539912223816 *\n",
      "Epoch: 20, Train_Loss: 0.5539961457252502, Test_Loss: 0.5566075444221497 *\n",
      "Epoch: 20, Train_Loss: 0.581463634967804, Test_Loss: 0.5690740942955017\n",
      "Epoch: 20, Train_Loss: 0.6366660594940186, Test_Loss: 0.5644230842590332 *\n",
      "Epoch: 20, Train_Loss: 0.5896100401878357, Test_Loss: 0.5709453821182251\n",
      "Epoch: 20, Train_Loss: 0.5646125078201294, Test_Loss: 0.6051833033561707\n",
      "Epoch: 20, Train_Loss: 0.5647568106651306, Test_Loss: 0.7233722805976868\n",
      "Epoch: 20, Train_Loss: 0.5660637617111206, Test_Loss: 0.5558997988700867 *\n",
      "Epoch: 20, Train_Loss: 2.999750852584839, Test_Loss: 0.5668692588806152\n",
      "Epoch: 20, Train_Loss: 3.535407066345215, Test_Loss: 0.5786670446395874\n",
      "Epoch: 20, Train_Loss: 0.5600893497467041, Test_Loss: 0.5609208345413208 *\n",
      "Epoch: 20, Train_Loss: 0.5903869867324829, Test_Loss: 0.5587172508239746 *\n",
      "Epoch: 20, Train_Loss: 0.5563536286354065, Test_Loss: 0.5952700972557068\n",
      "Epoch: 20, Train_Loss: 0.5452134013175964, Test_Loss: 0.5960176587104797\n",
      "Epoch: 20, Train_Loss: 0.54840487241745, Test_Loss: 0.6463538408279419\n",
      "Epoch: 20, Train_Loss: 0.5486839413642883, Test_Loss: 0.6501307487487793\n",
      "Epoch: 20, Train_Loss: 0.5692777633666992, Test_Loss: 0.561374306678772 *\n",
      "Epoch: 20, Train_Loss: 0.5612316131591797, Test_Loss: 0.5642035603523254\n",
      "Epoch: 20, Train_Loss: 0.6126900911331177, Test_Loss: 0.5489409565925598 *\n",
      "Epoch: 20, Train_Loss: 0.5465442538261414, Test_Loss: 0.551702082157135\n",
      "Epoch: 20, Train_Loss: 0.5442618131637573, Test_Loss: 0.5498855710029602 *\n",
      "Epoch: 20, Train_Loss: 0.550977349281311, Test_Loss: 0.5485703945159912 *\n",
      "Epoch: 20, Train_Loss: 0.5568983554840088, Test_Loss: 0.548897922039032\n",
      "Epoch: 20, Train_Loss: 0.5473908185958862, Test_Loss: 0.5492585301399231\n",
      "Epoch: 20, Train_Loss: 0.5485294461250305, Test_Loss: 0.5523523092269897\n",
      "Epoch: 20, Train_Loss: 0.5682728886604309, Test_Loss: 0.547613263130188 *\n",
      "Epoch: 20, Train_Loss: 0.5647221803665161, Test_Loss: 0.5484139323234558\n",
      "Epoch: 20, Train_Loss: 0.5455886125564575, Test_Loss: 0.5710855722427368\n",
      "Epoch: 20, Train_Loss: 0.5461106896400452, Test_Loss: 0.5501611232757568 *\n",
      "Epoch: 20, Train_Loss: 0.5767830014228821, Test_Loss: 0.5722513198852539\n",
      "Epoch: 20, Train_Loss: 0.5758967399597168, Test_Loss: 0.8557246923446655\n",
      "Epoch: 20, Train_Loss: 0.5711333155632019, Test_Loss: 0.6418962478637695 *\n",
      "Epoch: 20, Train_Loss: 0.5784388184547424, Test_Loss: 0.566104531288147 *\n",
      "Epoch: 20, Train_Loss: 0.6600158214569092, Test_Loss: 0.5969215035438538\n",
      "Epoch: 20, Train_Loss: 0.5948446989059448, Test_Loss: 0.7135632634162903\n",
      "Epoch: 20, Train_Loss: 0.5590197443962097, Test_Loss: 0.7042016983032227 *\n",
      "Epoch: 20, Train_Loss: 0.5984009504318237, Test_Loss: 0.5587593913078308 *\n",
      "Epoch: 20, Train_Loss: 0.6645410060882568, Test_Loss: 0.7072388529777527\n",
      "Epoch: 20, Train_Loss: 0.6522588133811951, Test_Loss: 0.7719683647155762\n",
      "Epoch: 20, Train_Loss: 0.5568855404853821, Test_Loss: 0.5874552130699158 *\n",
      "Epoch: 20, Train_Loss: 0.5400952100753784, Test_Loss: 0.6099237203598022\n",
      "Epoch: 20, Train_Loss: 0.5409044623374939, Test_Loss: 0.5467953085899353 *\n",
      "Epoch: 20, Train_Loss: 0.5394106507301331, Test_Loss: 0.5600199699401855\n",
      "Epoch: 20, Train_Loss: 0.5412624478340149, Test_Loss: 0.5548714399337769 *\n",
      "Epoch: 20, Train_Loss: 0.541654109954834, Test_Loss: 1.2414286136627197\n",
      "Epoch: 20, Train_Loss: 3.3858351707458496, Test_Loss: 0.8147866725921631 *\n",
      "Epoch: 20, Train_Loss: 2.4138548374176025, Test_Loss: 1.1785765886306763\n",
      "Epoch: 20, Train_Loss: 0.5400237441062927, Test_Loss: 1.1185760498046875 *\n",
      "Epoch: 20, Train_Loss: 0.5435873866081238, Test_Loss: 0.731020450592041 *\n",
      "Epoch: 20, Train_Loss: 0.5414178371429443, Test_Loss: 1.001884937286377\n",
      "Epoch: 20, Train_Loss: 0.5421220064163208, Test_Loss: 0.6072494983673096 *\n",
      "Epoch: 20, Train_Loss: 0.5395690202713013, Test_Loss: 0.5565761923789978 *\n",
      "Epoch: 20, Train_Loss: 0.5416095852851868, Test_Loss: 0.5553349852561951 *\n",
      "Epoch: 20, Train_Loss: 0.5462546348571777, Test_Loss: 0.6760873198509216\n",
      "Epoch: 20, Train_Loss: 0.5423634648323059, Test_Loss: 0.8346284031867981\n",
      "Epoch: 20, Train_Loss: 0.5551136136054993, Test_Loss: 1.225799322128296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 0.5854101181030273, Test_Loss: 1.470998764038086\n",
      "Epoch: 20, Train_Loss: 0.5814076662063599, Test_Loss: 2.04563045501709\n",
      "Epoch: 20, Train_Loss: 0.5897809863090515, Test_Loss: 1.1269431114196777 *\n",
      "Epoch: 20, Train_Loss: 0.5601138472557068, Test_Loss: 0.9138298034667969 *\n",
      "Epoch: 20, Train_Loss: 0.5498856902122498, Test_Loss: 0.5425899624824524 *\n",
      "Epoch: 20, Train_Loss: 0.7209862470626831, Test_Loss: 0.5676076412200928\n",
      "Epoch: 20, Train_Loss: 0.7391645312309265, Test_Loss: 1.2694628238677979\n",
      "Epoch: 20, Train_Loss: 0.734229564666748, Test_Loss: 1.900948405265808\n",
      "Epoch: 20, Train_Loss: 0.6040931344032288, Test_Loss: 0.5898252129554749 *\n",
      "Epoch: 20, Train_Loss: 0.5368366837501526, Test_Loss: 0.6221179962158203\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 0.5369967818260193, Test_Loss: 0.54362952709198 *\n",
      "Epoch: 20, Train_Loss: 0.5429468154907227, Test_Loss: 0.7766199111938477\n",
      "Epoch: 20, Train_Loss: 0.5463903546333313, Test_Loss: 0.8233382701873779\n",
      "Epoch: 20, Train_Loss: 0.5507986545562744, Test_Loss: 1.1115752458572388\n",
      "Epoch: 20, Train_Loss: 0.5391150116920471, Test_Loss: 1.4054522514343262\n",
      "Epoch: 20, Train_Loss: 0.5371989607810974, Test_Loss: 0.8276057839393616 *\n",
      "Epoch: 20, Train_Loss: 0.5370534062385559, Test_Loss: 0.5415695309638977 *\n",
      "Epoch: 20, Train_Loss: 0.5477238893508911, Test_Loss: 0.5441865921020508\n",
      "Epoch: 20, Train_Loss: 0.6076495051383972, Test_Loss: 0.5468907356262207\n",
      "Epoch: 20, Train_Loss: 0.6945933103561401, Test_Loss: 0.5820683240890503\n",
      "Epoch: 20, Train_Loss: 0.6557265520095825, Test_Loss: 0.7949188947677612\n",
      "Epoch: 20, Train_Loss: 0.6124640107154846, Test_Loss: 0.9562273025512695\n",
      "Epoch: 20, Train_Loss: 0.6409979462623596, Test_Loss: 0.7027873992919922 *\n",
      "Epoch: 20, Train_Loss: 0.6667932271957397, Test_Loss: 0.6018481254577637 *\n",
      "Epoch: 20, Train_Loss: 0.5673173666000366, Test_Loss: 0.547317385673523 *\n",
      "Epoch: 20, Train_Loss: 0.7233399152755737, Test_Loss: 0.5527964234352112\n",
      "Epoch: 20, Train_Loss: 0.6644695401191711, Test_Loss: 0.6433113813400269\n",
      "Epoch: 20, Train_Loss: 0.8150018453598022, Test_Loss: 1.017266035079956\n",
      "Epoch: 20, Train_Loss: 0.5433955788612366, Test_Loss: 1.36399507522583\n",
      "Epoch: 20, Train_Loss: 0.867253303527832, Test_Loss: 0.6555818319320679 *\n",
      "Epoch: 20, Train_Loss: 3.1460797786712646, Test_Loss: 0.6199066042900085 *\n",
      "Epoch: 20, Train_Loss: 0.6148104071617126, Test_Loss: 0.5388807058334351 *\n",
      "Epoch: 20, Train_Loss: 0.5564842820167542, Test_Loss: 0.5480098724365234\n",
      "Epoch: 20, Train_Loss: 0.5613121390342712, Test_Loss: 0.5454127192497253 *\n",
      "Epoch: 20, Train_Loss: 0.5766135454177856, Test_Loss: 0.5542744398117065\n",
      "Epoch: 20, Train_Loss: 0.5378164052963257, Test_Loss: 0.5792860388755798\n",
      "Epoch: 20, Train_Loss: 0.5350568890571594, Test_Loss: 0.5649245977401733 *\n",
      "Epoch: 20, Train_Loss: 0.5893624424934387, Test_Loss: 0.5357757806777954 *\n",
      "Epoch: 20, Train_Loss: 0.5942795276641846, Test_Loss: 0.6290326714515686\n",
      "Epoch: 20, Train_Loss: 0.5683176517486572, Test_Loss: 0.9230756759643555\n",
      "Epoch: 20, Train_Loss: 0.5633001923561096, Test_Loss: 0.6234023571014404 *\n",
      "Epoch: 20, Train_Loss: 0.564151406288147, Test_Loss: 0.7170282006263733\n",
      "Epoch: 20, Train_Loss: 0.5415415167808533, Test_Loss: 0.5450973510742188 *\n",
      "Epoch: 20, Train_Loss: 0.5582292675971985, Test_Loss: 0.5489175915718079\n",
      "Epoch: 20, Train_Loss: 0.5465114116668701, Test_Loss: 0.5495178699493408\n",
      "Epoch: 20, Train_Loss: 0.5637772679328918, Test_Loss: 0.5461937785148621 *\n",
      "Epoch: 20, Train_Loss: 0.5428873896598816, Test_Loss: 0.5679965615272522\n",
      "Epoch: 20, Train_Loss: 0.5345611572265625, Test_Loss: 5.260708332061768\n",
      "Epoch: 20, Train_Loss: 0.5469759702682495, Test_Loss: 0.9758777618408203 *\n",
      "Epoch: 20, Train_Loss: 0.5475212335586548, Test_Loss: 0.5396089553833008 *\n",
      "Epoch: 20, Train_Loss: 0.5481364727020264, Test_Loss: 0.5356504917144775 *\n",
      "Epoch: 20, Train_Loss: 0.5425538420677185, Test_Loss: 0.5333063006401062 *\n",
      "Epoch: 20, Train_Loss: 0.5328336954116821, Test_Loss: 0.5377618670463562\n",
      "Epoch: 20, Train_Loss: 0.5311800241470337, Test_Loss: 0.5355737805366516 *\n",
      "Epoch: 20, Train_Loss: 0.5309135317802429, Test_Loss: 0.5513217449188232\n",
      "Epoch: 20, Train_Loss: 0.531073808670044, Test_Loss: 0.5368044376373291 *\n",
      "Epoch: 20, Train_Loss: 0.5316968560218811, Test_Loss: 0.533599317073822 *\n",
      "Epoch: 20, Train_Loss: 0.532478392124176, Test_Loss: 0.5469402074813843\n",
      "Epoch: 20, Train_Loss: 0.5317156314849854, Test_Loss: 0.5586713552474976\n",
      "Epoch: 20, Train_Loss: 0.528785765171051, Test_Loss: 0.5363465547561646 *\n",
      "Epoch: 20, Train_Loss: 0.5301491618156433, Test_Loss: 0.5472018718719482\n",
      "Epoch: 20, Train_Loss: 0.5347769260406494, Test_Loss: 0.5571958422660828\n",
      "Epoch: 20, Train_Loss: 0.539405107498169, Test_Loss: 0.539738655090332 *\n",
      "Epoch: 20, Train_Loss: 0.5358182787895203, Test_Loss: 0.53199702501297 *\n",
      "Epoch: 21, Train_Loss: 0.5511230826377869, Test_Loss: 0.5450628399848938 *\n",
      "Epoch: 21, Train_Loss: 0.5308431386947632, Test_Loss: 0.5393060445785522 *\n",
      "Epoch: 21, Train_Loss: 0.53492671251297, Test_Loss: 0.5322058796882629 *\n",
      "Epoch: 21, Train_Loss: 0.5281349420547485, Test_Loss: 0.5310630202293396 *\n",
      "Epoch: 21, Train_Loss: 0.5268704891204834, Test_Loss: 0.5319582223892212\n",
      "Epoch: 21, Train_Loss: 0.5349092483520508, Test_Loss: 0.5381001830101013\n",
      "Epoch: 21, Train_Loss: 0.5366788506507874, Test_Loss: 0.5479174256324768\n",
      "Epoch: 21, Train_Loss: 0.5290875434875488, Test_Loss: 0.5376739501953125 *\n",
      "Epoch: 21, Train_Loss: 0.5278794765472412, Test_Loss: 0.5362486839294434 *\n",
      "Epoch: 21, Train_Loss: 0.5345188975334167, Test_Loss: 0.5346060395240784 *\n",
      "Epoch: 21, Train_Loss: 0.6229231953620911, Test_Loss: 0.5345764756202698 *\n",
      "Epoch: 21, Train_Loss: 0.5459455251693726, Test_Loss: 0.5363383293151855\n",
      "Epoch: 21, Train_Loss: 0.5557296872138977, Test_Loss: 0.5437126755714417\n",
      "Epoch: 21, Train_Loss: 0.5333601832389832, Test_Loss: 0.5998095273971558\n",
      "Epoch: 21, Train_Loss: 0.5417942404747009, Test_Loss: 1.259186029434204\n",
      "Epoch: 21, Train_Loss: 0.5702829957008362, Test_Loss: 5.638725757598877\n",
      "Epoch: 21, Train_Loss: 0.5277219414710999, Test_Loss: 0.5533993244171143 *\n",
      "Epoch: 21, Train_Loss: 0.5370524525642395, Test_Loss: 0.5288630723953247 *\n",
      "Epoch: 21, Train_Loss: 0.5537086129188538, Test_Loss: 0.5556291341781616\n",
      "Epoch: 21, Train_Loss: 0.5997055768966675, Test_Loss: 0.5597697496414185\n",
      "Epoch: 21, Train_Loss: 0.6020564436912537, Test_Loss: 0.567581295967102\n",
      "Epoch: 21, Train_Loss: 0.5711987018585205, Test_Loss: 0.5403627157211304 *\n",
      "Epoch: 21, Train_Loss: 0.5585513710975647, Test_Loss: 0.6822319626808167\n",
      "Epoch: 21, Train_Loss: 0.529133677482605, Test_Loss: 0.5635836720466614 *\n",
      "Epoch: 21, Train_Loss: 0.5521284937858582, Test_Loss: 0.5296069383621216 *\n",
      "Epoch: 21, Train_Loss: 0.5289955735206604, Test_Loss: 0.5687975287437439\n",
      "Epoch: 21, Train_Loss: 0.5277982950210571, Test_Loss: 0.535557210445404 *\n",
      "Epoch: 21, Train_Loss: 0.5324611663818359, Test_Loss: 0.5368369817733765\n",
      "Epoch: 21, Train_Loss: 0.538693904876709, Test_Loss: 0.5685172080993652\n",
      "Epoch: 21, Train_Loss: 0.5963347554206848, Test_Loss: 0.6106469631195068\n",
      "Epoch: 21, Train_Loss: 0.5596709251403809, Test_Loss: 0.5905675888061523 *\n",
      "Epoch: 21, Train_Loss: 0.5713402032852173, Test_Loss: 0.6032085418701172\n",
      "Epoch: 21, Train_Loss: 0.5364456176757812, Test_Loss: 0.5571610927581787 *\n",
      "Epoch: 21, Train_Loss: 0.5606640577316284, Test_Loss: 0.5574567914009094\n",
      "Epoch: 21, Train_Loss: 0.5367313623428345, Test_Loss: 0.5287132263183594 *\n",
      "Epoch: 21, Train_Loss: 0.7520194053649902, Test_Loss: 0.5289676785469055\n",
      "Epoch: 21, Train_Loss: 0.630500078201294, Test_Loss: 0.5313208103179932\n",
      "Epoch: 21, Train_Loss: 0.5335550904273987, Test_Loss: 0.5292875170707703 *\n",
      "Epoch: 21, Train_Loss: 0.5543594360351562, Test_Loss: 0.5314854979515076\n",
      "Epoch: 21, Train_Loss: 0.5232459902763367, Test_Loss: 0.5301218628883362 *\n",
      "Epoch: 21, Train_Loss: 0.5238838195800781, Test_Loss: 0.5281261205673218 *\n",
      "Epoch: 21, Train_Loss: 0.5267829895019531, Test_Loss: 0.5270658731460571 *\n",
      "Epoch: 21, Train_Loss: 0.5329119563102722, Test_Loss: 0.5361458659172058\n",
      "Epoch: 21, Train_Loss: 0.5257353186607361, Test_Loss: 0.5316561460494995 *\n",
      "Epoch: 21, Train_Loss: 0.541892945766449, Test_Loss: 0.5296187400817871 *\n",
      "Epoch: 21, Train_Loss: 0.5273453593254089, Test_Loss: 0.5660277605056763\n",
      "Epoch: 21, Train_Loss: 0.525234043598175, Test_Loss: 0.6584712266921997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 0.5413948893547058, Test_Loss: 0.7335792183876038\n",
      "Epoch: 21, Train_Loss: 0.5222866535186768, Test_Loss: 0.5368991494178772 *\n",
      "Epoch: 21, Train_Loss: 0.5234897136688232, Test_Loss: 0.5825474262237549\n",
      "Epoch: 21, Train_Loss: 0.5241680145263672, Test_Loss: 0.6489654779434204\n",
      "Epoch: 21, Train_Loss: 0.5431003570556641, Test_Loss: 0.7766166925430298\n",
      "Epoch: 21, Train_Loss: 0.5413029193878174, Test_Loss: 0.5406917929649353 *\n",
      "Epoch: 21, Train_Loss: 0.5295841097831726, Test_Loss: 0.6471477150917053\n",
      "Epoch: 21, Train_Loss: 0.532998263835907, Test_Loss: 0.7271127104759216\n",
      "Epoch: 21, Train_Loss: 0.5530346035957336, Test_Loss: 0.6141888499259949 *\n",
      "Epoch: 21, Train_Loss: 0.5504698753356934, Test_Loss: 0.5816711187362671 *\n",
      "Epoch: 21, Train_Loss: 0.522705078125, Test_Loss: 0.5270697474479675 *\n",
      "Epoch: 21, Train_Loss: 0.5404430627822876, Test_Loss: 0.5323848724365234\n",
      "Epoch: 21, Train_Loss: 0.532927393913269, Test_Loss: 0.5364266633987427\n",
      "Epoch: 21, Train_Loss: 0.5289103984832764, Test_Loss: 1.063324213027954\n",
      "Epoch: 21, Train_Loss: 0.5237230658531189, Test_Loss: 0.9004976749420166 *\n",
      "Epoch: 21, Train_Loss: 0.5423899292945862, Test_Loss: 1.0259109735488892\n",
      "Epoch: 21, Train_Loss: 0.5780322551727295, Test_Loss: 1.2596385478973389\n",
      "Epoch: 21, Train_Loss: 2.8147637844085693, Test_Loss: 0.6672717332839966 *\n",
      "Epoch: 21, Train_Loss: 3.556114673614502, Test_Loss: 0.9645732045173645\n",
      "Epoch: 21, Train_Loss: 0.5576228499412537, Test_Loss: 0.6224247813224792 *\n",
      "Epoch: 21, Train_Loss: 0.5215230584144592, Test_Loss: 0.5268825888633728 *\n",
      "Epoch: 21, Train_Loss: 0.6134539246559143, Test_Loss: 0.5304175615310669\n",
      "Epoch: 21, Train_Loss: 0.6880365610122681, Test_Loss: 0.5972485542297363\n",
      "Epoch: 21, Train_Loss: 0.5436962246894836, Test_Loss: 0.708554744720459\n",
      "Epoch: 21, Train_Loss: 0.5199815034866333, Test_Loss: 1.3117601871490479\n",
      "Epoch: 21, Train_Loss: 0.5378897190093994, Test_Loss: 0.9814107418060303 *\n",
      "Epoch: 21, Train_Loss: 0.5732806921005249, Test_Loss: 2.3841049671173096\n",
      "Epoch: 21, Train_Loss: 0.5284150242805481, Test_Loss: 0.9396308660507202 *\n",
      "Epoch: 21, Train_Loss: 0.5280773043632507, Test_Loss: 1.0442862510681152\n",
      "Epoch: 21, Train_Loss: 1.5429883003234863, Test_Loss: 0.5285145044326782 *\n",
      "Epoch: 21, Train_Loss: 1.7719619274139404, Test_Loss: 0.5315288305282593\n",
      "Epoch: 21, Train_Loss: 0.8364362716674805, Test_Loss: 1.049527883529663\n",
      "Epoch: 21, Train_Loss: 0.5948859453201294, Test_Loss: 2.0519046783447266\n",
      "Epoch: 21, Train_Loss: 1.862788200378418, Test_Loss: 0.7526177167892456 *\n",
      "Epoch: 21, Train_Loss: 2.5606579780578613, Test_Loss: 0.6286869645118713 *\n",
      "Epoch: 21, Train_Loss: 0.6158429384231567, Test_Loss: 0.5445451140403748 *\n",
      "Epoch: 21, Train_Loss: 0.5324780344963074, Test_Loss: 0.6859762072563171\n",
      "Epoch: 21, Train_Loss: 0.6258809566497803, Test_Loss: 0.9134833812713623\n",
      "Epoch: 21, Train_Loss: 1.739349365234375, Test_Loss: 0.7668499946594238 *\n",
      "Epoch: 21, Train_Loss: 1.558213472366333, Test_Loss: 1.3191118240356445\n",
      "Epoch: 21, Train_Loss: 0.5308456420898438, Test_Loss: 0.7826248407363892 *\n",
      "Epoch: 21, Train_Loss: 0.5371583104133606, Test_Loss: 0.5340498685836792 *\n",
      "Epoch: 21, Train_Loss: 0.5354985594749451, Test_Loss: 0.5811022520065308\n",
      "Epoch: 21, Train_Loss: 1.2382664680480957, Test_Loss: 0.5886452198028564\n",
      "Epoch: 21, Train_Loss: 0.5663713216781616, Test_Loss: 0.5513691306114197 *\n",
      "Epoch: 21, Train_Loss: 0.5827404856681824, Test_Loss: 0.8117227554321289\n",
      "Epoch: 21, Train_Loss: 0.5280230641365051, Test_Loss: 0.8303772211074829\n",
      "Epoch: 21, Train_Loss: 0.6289253234863281, Test_Loss: 0.6765164136886597 *\n",
      "Epoch: 21, Train_Loss: 0.6173883676528931, Test_Loss: 0.589099109172821 *\n",
      "Epoch: 21, Train_Loss: 0.7558757066726685, Test_Loss: 0.5315696597099304 *\n",
      "Epoch: 21, Train_Loss: 0.7925763726234436, Test_Loss: 0.6059212684631348\n",
      "Epoch: 21, Train_Loss: 0.6017879843711853, Test_Loss: 0.7405072450637817\n",
      "Epoch: 21, Train_Loss: 0.615687906742096, Test_Loss: 0.9656487703323364\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.6348391175270081, Test_Loss: 1.2004010677337646\n",
      "Epoch: 21, Train_Loss: 0.7262301445007324, Test_Loss: 0.8110222220420837 *\n",
      "Epoch: 21, Train_Loss: 0.7308946847915649, Test_Loss: 0.5823028087615967 *\n",
      "Epoch: 21, Train_Loss: 0.547231137752533, Test_Loss: 0.570445716381073 *\n",
      "Epoch: 21, Train_Loss: 0.6423764228820801, Test_Loss: 0.5336599946022034 *\n",
      "Epoch: 21, Train_Loss: 0.6600372195243835, Test_Loss: 0.5301512479782104 *\n",
      "Epoch: 21, Train_Loss: 0.5449665188789368, Test_Loss: 0.5897964835166931\n",
      "Epoch: 21, Train_Loss: 0.5173868536949158, Test_Loss: 0.5353121757507324 *\n",
      "Epoch: 21, Train_Loss: 0.5345082879066467, Test_Loss: 0.5754038691520691\n",
      "Epoch: 21, Train_Loss: 0.5733899474143982, Test_Loss: 0.5239766836166382 *\n",
      "Epoch: 21, Train_Loss: 0.5197632312774658, Test_Loss: 0.6010158658027649\n",
      "Epoch: 21, Train_Loss: 0.5305135250091553, Test_Loss: 0.8494329452514648\n",
      "Epoch: 21, Train_Loss: 0.5460354089736938, Test_Loss: 0.7270551323890686 *\n",
      "Epoch: 21, Train_Loss: 0.538636326789856, Test_Loss: 0.8426141738891602\n",
      "Epoch: 21, Train_Loss: 0.5480453372001648, Test_Loss: 0.5226574540138245 *\n",
      "Epoch: 21, Train_Loss: 0.6776190996170044, Test_Loss: 0.5117170214653015 *\n",
      "Epoch: 21, Train_Loss: 0.9140611886978149, Test_Loss: 0.5114920735359192 *\n",
      "Epoch: 21, Train_Loss: 0.5371763110160828, Test_Loss: 0.5153899192810059\n",
      "Epoch: 21, Train_Loss: 0.5612931251525879, Test_Loss: 0.5894311666488647\n",
      "Epoch: 21, Train_Loss: 0.6637760996818542, Test_Loss: 3.9067223072052\n",
      "Epoch: 21, Train_Loss: 0.7017198801040649, Test_Loss: 2.196810722351074 *\n",
      "Epoch: 21, Train_Loss: 0.7636020183563232, Test_Loss: 0.5320189595222473 *\n",
      "Epoch: 21, Train_Loss: 0.5520933866500854, Test_Loss: 0.5399622321128845\n",
      "Epoch: 21, Train_Loss: 0.7091307640075684, Test_Loss: 0.5274169445037842 *\n",
      "Epoch: 21, Train_Loss: 0.9149107933044434, Test_Loss: 0.5196035504341125 *\n",
      "Epoch: 21, Train_Loss: 0.8348579406738281, Test_Loss: 0.5452960133552551\n",
      "Epoch: 21, Train_Loss: 0.548566997051239, Test_Loss: 0.562752902507782\n",
      "Epoch: 21, Train_Loss: 0.5294867157936096, Test_Loss: 0.5434691905975342 *\n",
      "Epoch: 21, Train_Loss: 0.5382594466209412, Test_Loss: 0.5421401858329773 *\n",
      "Epoch: 21, Train_Loss: 1.4387540817260742, Test_Loss: 0.5479611754417419\n",
      "Epoch: 21, Train_Loss: 1.0557975769042969, Test_Loss: 0.5689546465873718\n",
      "Epoch: 21, Train_Loss: 0.5249384641647339, Test_Loss: 0.5882442593574524\n",
      "Epoch: 21, Train_Loss: 0.5493897795677185, Test_Loss: 0.525254487991333 *\n",
      "Epoch: 21, Train_Loss: 0.5134930610656738, Test_Loss: 0.5431079864501953\n",
      "Epoch: 21, Train_Loss: 0.6401644945144653, Test_Loss: 0.5313504934310913 *\n",
      "Epoch: 21, Train_Loss: 0.883914589881897, Test_Loss: 0.5204282999038696 *\n",
      "Epoch: 21, Train_Loss: 0.5240377187728882, Test_Loss: 0.5750287175178528\n",
      "Epoch: 21, Train_Loss: 0.5600219964981079, Test_Loss: 0.5672444701194763 *\n",
      "Epoch: 21, Train_Loss: 0.5513591170310974, Test_Loss: 0.5592399835586548 *\n",
      "Epoch: 21, Train_Loss: 0.6952052116394043, Test_Loss: 0.5255805253982544 *\n",
      "Epoch: 21, Train_Loss: 17.349584579467773, Test_Loss: 0.5663729906082153\n",
      "Epoch: 21, Train_Loss: 0.6273155212402344, Test_Loss: 0.5612640380859375 *\n",
      "Epoch: 21, Train_Loss: 2.4276070594787598, Test_Loss: 0.6169145107269287\n",
      "Epoch: 21, Train_Loss: 1.3508780002593994, Test_Loss: 0.5821950435638428 *\n",
      "Epoch: 21, Train_Loss: 0.532524824142456, Test_Loss: 0.575896143913269 *\n",
      "Epoch: 21, Train_Loss: 0.6549168825149536, Test_Loss: 0.5468483567237854 *\n",
      "Epoch: 21, Train_Loss: 7.3146820068359375, Test_Loss: 0.5500926971435547\n",
      "Epoch: 21, Train_Loss: 4.235273361206055, Test_Loss: 0.5581138134002686\n",
      "Epoch: 21, Train_Loss: 0.5371795892715454, Test_Loss: 0.557837188243866 *\n",
      "Epoch: 21, Train_Loss: 1.1144399642944336, Test_Loss: 0.683539867401123\n",
      "Epoch: 21, Train_Loss: 5.535068988800049, Test_Loss: 0.619020938873291 *\n",
      "Epoch: 21, Train_Loss: 0.6604938507080078, Test_Loss: 7.307730197906494\n",
      "Epoch: 21, Train_Loss: 0.5341845750808716, Test_Loss: 0.840552806854248 *\n",
      "Epoch: 21, Train_Loss: 0.5255109071731567, Test_Loss: 0.5390502214431763 *\n",
      "Epoch: 21, Train_Loss: 0.5235636234283447, Test_Loss: 0.5581282377243042\n",
      "Epoch: 21, Train_Loss: 0.5406051278114319, Test_Loss: 0.5758909583091736\n",
      "Epoch: 21, Train_Loss: 0.5157413482666016, Test_Loss: 0.5684283971786499 *\n",
      "Epoch: 21, Train_Loss: 0.5101787447929382, Test_Loss: 0.5960442423820496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 0.5087144374847412, Test_Loss: 0.8499815464019775\n",
      "Epoch: 21, Train_Loss: 0.5091806650161743, Test_Loss: 0.7241847515106201 *\n",
      "Epoch: 21, Train_Loss: 0.548682451248169, Test_Loss: 0.5530858039855957 *\n",
      "Epoch: 21, Train_Loss: 0.5378543138504028, Test_Loss: 0.5747528672218323\n",
      "Epoch: 21, Train_Loss: 0.5449121594429016, Test_Loss: 0.5145662426948547 *\n",
      "Epoch: 21, Train_Loss: 0.6565735340118408, Test_Loss: 0.5508384108543396\n",
      "Epoch: 21, Train_Loss: 0.6692166328430176, Test_Loss: 0.5552698969841003\n",
      "Epoch: 21, Train_Loss: 0.5364593863487244, Test_Loss: 0.5468716025352478 *\n",
      "Epoch: 21, Train_Loss: 0.539429783821106, Test_Loss: 0.6425728797912598\n",
      "Epoch: 21, Train_Loss: 0.5406653881072998, Test_Loss: 0.627730131149292 *\n",
      "Epoch: 21, Train_Loss: 0.5154893398284912, Test_Loss: 0.5709618330001831 *\n",
      "Epoch: 21, Train_Loss: 0.5107513666152954, Test_Loss: 0.5221394896507263 *\n",
      "Epoch: 21, Train_Loss: 0.5087058544158936, Test_Loss: 0.530274510383606\n",
      "Epoch: 21, Train_Loss: 0.5079926252365112, Test_Loss: 0.5279470086097717 *\n",
      "Epoch: 21, Train_Loss: 0.5070559978485107, Test_Loss: 0.51957768201828 *\n",
      "Epoch: 21, Train_Loss: 0.5057826042175293, Test_Loss: 0.5190277099609375 *\n",
      "Epoch: 21, Train_Loss: 0.5072085857391357, Test_Loss: 0.5161679983139038 *\n",
      "Epoch: 21, Train_Loss: 0.5058342218399048, Test_Loss: 0.5152145624160767 *\n",
      "Epoch: 21, Train_Loss: 0.5143558382987976, Test_Loss: 0.5161874890327454\n",
      "Epoch: 21, Train_Loss: 0.5130358934402466, Test_Loss: 0.5136080980300903 *\n",
      "Epoch: 21, Train_Loss: 0.5852834582328796, Test_Loss: 0.5099353194236755 *\n",
      "Epoch: 21, Train_Loss: 0.5640223622322083, Test_Loss: 0.5446938276290894\n",
      "Epoch: 21, Train_Loss: 0.5178711414337158, Test_Loss: 0.5404380559921265 *\n",
      "Epoch: 21, Train_Loss: 5.958103179931641, Test_Loss: 0.5179169774055481 *\n",
      "Epoch: 21, Train_Loss: 3.677295446395874, Test_Loss: 0.58992999792099\n",
      "Epoch: 21, Train_Loss: 0.5303853750228882, Test_Loss: 0.9664735794067383\n",
      "Epoch: 21, Train_Loss: 0.54697185754776, Test_Loss: 0.5671980381011963 *\n",
      "Epoch: 21, Train_Loss: 0.6100444793701172, Test_Loss: 0.5315940976142883 *\n",
      "Epoch: 21, Train_Loss: 0.542787492275238, Test_Loss: 0.539551317691803\n",
      "Epoch: 21, Train_Loss: 0.539033830165863, Test_Loss: 0.6632170677185059\n",
      "Epoch: 21, Train_Loss: 0.5906316041946411, Test_Loss: 0.542839527130127 *\n",
      "Epoch: 21, Train_Loss: 0.6853575706481934, Test_Loss: 0.6740155816078186\n",
      "Epoch: 21, Train_Loss: 0.7611700892448425, Test_Loss: 0.7260180115699768\n",
      "Epoch: 21, Train_Loss: 0.6590902805328369, Test_Loss: 0.8149057626724243\n",
      "Epoch: 21, Train_Loss: 0.5318857431411743, Test_Loss: 0.6281173825263977 *\n",
      "Epoch: 21, Train_Loss: 0.5509629249572754, Test_Loss: 0.5477489233016968 *\n",
      "Epoch: 21, Train_Loss: 0.5361554622650146, Test_Loss: 0.5139431953430176 *\n",
      "Epoch: 21, Train_Loss: 0.6536387205123901, Test_Loss: 0.5132265090942383 *\n",
      "Epoch: 21, Train_Loss: 0.5458900928497314, Test_Loss: 0.8784906268119812\n",
      "Epoch: 21, Train_Loss: 0.5461364984512329, Test_Loss: 1.1733529567718506\n",
      "Epoch: 21, Train_Loss: 0.5554361939430237, Test_Loss: 0.7128646373748779 *\n",
      "Epoch: 21, Train_Loss: 0.5282849073410034, Test_Loss: 1.0373612642288208\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.5592641830444336, Test_Loss: 0.7279314398765564 *\n",
      "Epoch: 21, Train_Loss: 0.5561039447784424, Test_Loss: 0.9395030736923218\n",
      "Epoch: 21, Train_Loss: 0.5470041036605835, Test_Loss: 0.669755220413208 *\n",
      "Epoch: 21, Train_Loss: 0.5098889470100403, Test_Loss: 0.526312530040741 *\n",
      "Epoch: 21, Train_Loss: 0.5021580457687378, Test_Loss: 0.510368824005127 *\n",
      "Epoch: 21, Train_Loss: 0.5566210150718689, Test_Loss: 0.5227899551391602\n",
      "Epoch: 21, Train_Loss: 4.447118282318115, Test_Loss: 0.6273891925811768\n",
      "Epoch: 21, Train_Loss: 1.5922399759292603, Test_Loss: 1.33124840259552\n",
      "Epoch: 21, Train_Loss: 0.5003383755683899, Test_Loss: 0.7756730318069458 *\n",
      "Epoch: 21, Train_Loss: 0.5595517158508301, Test_Loss: 2.1498820781707764\n",
      "Epoch: 21, Train_Loss: 0.5129557847976685, Test_Loss: 1.1027435064315796 *\n",
      "Epoch: 21, Train_Loss: 0.5004447102546692, Test_Loss: 1.0562517642974854 *\n",
      "Epoch: 21, Train_Loss: 0.5040726661682129, Test_Loss: 0.6272762417793274 *\n",
      "Epoch: 21, Train_Loss: 0.5126395225524902, Test_Loss: 0.527217447757721 *\n",
      "Epoch: 21, Train_Loss: 0.5216938257217407, Test_Loss: 0.6812739372253418\n",
      "Epoch: 21, Train_Loss: 0.5215602517127991, Test_Loss: 1.4946739673614502\n",
      "Epoch: 21, Train_Loss: 0.5712141990661621, Test_Loss: 1.00739586353302 *\n",
      "Epoch: 21, Train_Loss: 0.5005432367324829, Test_Loss: 0.7285115122795105 *\n",
      "Epoch: 21, Train_Loss: 0.4984322786331177, Test_Loss: 0.5170530676841736 *\n",
      "Epoch: 21, Train_Loss: 0.5096280574798584, Test_Loss: 0.5634340047836304\n",
      "Epoch: 21, Train_Loss: 0.5106672048568726, Test_Loss: 0.8349020481109619\n",
      "Epoch: 21, Train_Loss: 0.5017279982566833, Test_Loss: 0.7576829195022583 *\n",
      "Epoch: 21, Train_Loss: 0.5069745182991028, Test_Loss: 1.680973768234253\n",
      "Epoch: 21, Train_Loss: 0.518937885761261, Test_Loss: 0.9591625928878784 *\n",
      "Epoch: 21, Train_Loss: 0.5119989514350891, Test_Loss: 0.5345029830932617 *\n",
      "Epoch: 21, Train_Loss: 0.4996020495891571, Test_Loss: 0.5080596804618835 *\n",
      "Epoch: 21, Train_Loss: 0.498595654964447, Test_Loss: 0.5081778168678284\n",
      "Epoch: 21, Train_Loss: 0.546167254447937, Test_Loss: 0.5370765924453735\n",
      "Epoch: 21, Train_Loss: 0.5235906839370728, Test_Loss: 0.5988352298736572\n",
      "Epoch: 21, Train_Loss: 0.5264727473258972, Test_Loss: 0.8476663827896118\n",
      "Epoch: 21, Train_Loss: 0.5343090891838074, Test_Loss: 0.7610989212989807 *\n",
      "Epoch: 21, Train_Loss: 0.6144718527793884, Test_Loss: 0.5902576446533203 *\n",
      "Epoch: 21, Train_Loss: 0.5325693488121033, Test_Loss: 0.5186662673950195 *\n",
      "Epoch: 21, Train_Loss: 0.5136289000511169, Test_Loss: 0.5547676682472229\n",
      "Epoch: 21, Train_Loss: 0.5564422607421875, Test_Loss: 0.606296181678772\n",
      "Epoch: 21, Train_Loss: 0.6822868585586548, Test_Loss: 0.7510833740234375\n",
      "Epoch: 21, Train_Loss: 0.5519758462905884, Test_Loss: 1.2439029216766357\n",
      "Epoch: 21, Train_Loss: 0.5107961297035217, Test_Loss: 0.945315420627594 *\n",
      "Epoch: 21, Train_Loss: 0.49560004472732544, Test_Loss: 0.577773928642273 *\n",
      "Epoch: 21, Train_Loss: 0.494005024433136, Test_Loss: 0.5257163047790527 *\n",
      "Epoch: 21, Train_Loss: 0.49563828110694885, Test_Loss: 0.5121670365333557 *\n",
      "Epoch: 21, Train_Loss: 0.49447101354599, Test_Loss: 0.5137559771537781\n",
      "Epoch: 21, Train_Loss: 0.49501436948776245, Test_Loss: 0.5157473087310791\n",
      "Epoch: 21, Train_Loss: 4.368577480316162, Test_Loss: 0.5251573324203491\n",
      "Epoch: 21, Train_Loss: 1.4024633169174194, Test_Loss: 0.5757049918174744\n",
      "Epoch: 21, Train_Loss: 0.49520406126976013, Test_Loss: 0.49797937273979187 *\n",
      "Epoch: 21, Train_Loss: 0.4991508722305298, Test_Loss: 0.5487809181213379\n",
      "Epoch: 21, Train_Loss: 0.4981287121772766, Test_Loss: 0.6269428133964539\n",
      "Epoch: 21, Train_Loss: 0.4945439398288727, Test_Loss: 0.830127477645874\n",
      "Epoch: 21, Train_Loss: 0.4966983497142792, Test_Loss: 0.7229328155517578 *\n",
      "Epoch: 21, Train_Loss: 0.49544671177864075, Test_Loss: 0.511957049369812 *\n",
      "Epoch: 21, Train_Loss: 0.49524399638175964, Test_Loss: 0.5016681551933289 *\n",
      "Epoch: 21, Train_Loss: 0.49820536375045776, Test_Loss: 0.5060672760009766\n",
      "Epoch: 21, Train_Loss: 0.5168757438659668, Test_Loss: 0.5035597085952759 *\n",
      "Epoch: 21, Train_Loss: 0.5289620161056519, Test_Loss: 0.5090305209159851\n",
      "Epoch: 21, Train_Loss: 0.5360928177833557, Test_Loss: 2.3092215061187744\n",
      "Epoch: 21, Train_Loss: 0.5313089489936829, Test_Loss: 4.103778839111328\n",
      "Epoch: 21, Train_Loss: 0.5037438273429871, Test_Loss: 0.5060617923736572 *\n",
      "Epoch: 21, Train_Loss: 0.5228533148765564, Test_Loss: 0.49873799085617065 *\n",
      "Epoch: 21, Train_Loss: 0.6484966278076172, Test_Loss: 0.4998977780342102\n",
      "Epoch: 21, Train_Loss: 0.6583290696144104, Test_Loss: 0.4984479248523712 *\n",
      "Epoch: 21, Train_Loss: 0.6574938297271729, Test_Loss: 0.4990004599094391\n",
      "Epoch: 21, Train_Loss: 0.5253525376319885, Test_Loss: 0.5095358490943909\n",
      "Epoch: 21, Train_Loss: 0.49455124139785767, Test_Loss: 0.5055036544799805 *\n",
      "Epoch: 21, Train_Loss: 0.49093127250671387, Test_Loss: 0.49978286027908325 *\n",
      "Epoch: 21, Train_Loss: 0.4963822364807129, Test_Loss: 0.5089715719223022\n",
      "Epoch: 21, Train_Loss: 0.49544066190719604, Test_Loss: 0.5204856395721436\n",
      "Epoch: 21, Train_Loss: 0.4972743093967438, Test_Loss: 0.5179803371429443 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 0.49262380599975586, Test_Loss: 0.5001817941665649 *\n",
      "Epoch: 21, Train_Loss: 0.49512171745300293, Test_Loss: 0.5025824904441833\n",
      "Epoch: 21, Train_Loss: 0.4964561462402344, Test_Loss: 0.5168042182922363\n",
      "Epoch: 21, Train_Loss: 0.499322772026062, Test_Loss: 0.4975649416446686 *\n",
      "Epoch: 21, Train_Loss: 0.5578433275222778, Test_Loss: 0.5078850984573364\n",
      "Epoch: 21, Train_Loss: 0.5715254545211792, Test_Loss: 0.5027675032615662 *\n",
      "Epoch: 21, Train_Loss: 0.5680637359619141, Test_Loss: 0.5150882601737976\n",
      "Epoch: 21, Train_Loss: 0.5597713589668274, Test_Loss: 0.5023545622825623 *\n",
      "Epoch: 21, Train_Loss: 0.6034632325172424, Test_Loss: 0.5068262815475464\n",
      "Epoch: 21, Train_Loss: 0.6258528232574463, Test_Loss: 0.5132225751876831\n",
      "Epoch: 21, Train_Loss: 0.5229343175888062, Test_Loss: 0.5245628356933594\n",
      "Epoch: 21, Train_Loss: 0.7053852677345276, Test_Loss: 0.5184262990951538 *\n",
      "Epoch: 21, Train_Loss: 0.596341073513031, Test_Loss: 0.5096564888954163 *\n",
      "Epoch: 21, Train_Loss: 0.7503728866577148, Test_Loss: 0.4989641606807709 *\n",
      "Epoch: 21, Train_Loss: 0.49611803889274597, Test_Loss: 0.5058432221412659\n",
      "Epoch: 21, Train_Loss: 1.2799735069274902, Test_Loss: 0.4976675510406494 *\n",
      "Epoch: 21, Train_Loss: 2.175313711166382, Test_Loss: 0.49172186851501465 *\n",
      "Epoch: 21, Train_Loss: 0.5584300756454468, Test_Loss: 0.5571141839027405\n",
      "Epoch: 21, Train_Loss: 0.5198464393615723, Test_Loss: 0.5282732844352722 *\n",
      "Epoch: 21, Train_Loss: 0.5364229679107666, Test_Loss: 4.845453262329102\n",
      "Epoch: 21, Train_Loss: 0.5679622888565063, Test_Loss: 1.853822946548462 *\n",
      "Epoch: 21, Train_Loss: 0.5162667036056519, Test_Loss: 0.498123437166214 *\n",
      "Epoch: 21, Train_Loss: 0.5156270861625671, Test_Loss: 0.5122452974319458\n",
      "Epoch: 21, Train_Loss: 0.5183271765708923, Test_Loss: 0.5572155714035034\n",
      "Epoch: 21, Train_Loss: 0.5099985599517822, Test_Loss: 0.5372039675712585 *\n",
      "Epoch: 21, Train_Loss: 0.5158814191818237, Test_Loss: 0.504021406173706 *\n",
      "Epoch: 21, Train_Loss: 0.5193771719932556, Test_Loss: 0.5394514799118042\n",
      "Epoch: 21, Train_Loss: 0.5105592608451843, Test_Loss: 0.5395110845565796\n",
      "Epoch: 21, Train_Loss: 0.4960766136646271, Test_Loss: 0.5106751918792725 *\n",
      "Epoch: 21, Train_Loss: 0.5039196610450745, Test_Loss: 0.520431637763977\n",
      "Epoch: 21, Train_Loss: 0.5046792030334473, Test_Loss: 0.5465623140335083\n",
      "Epoch: 21, Train_Loss: 0.5025854706764221, Test_Loss: 0.5380479097366333 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.49537748098373413, Test_Loss: 0.5149063467979431 *\n",
      "Epoch: 21, Train_Loss: 0.4883474111557007, Test_Loss: 0.5855327844619751\n",
      "Epoch: 21, Train_Loss: 0.499248206615448, Test_Loss: 0.5380134582519531 *\n",
      "Epoch: 21, Train_Loss: 0.5231276154518127, Test_Loss: 0.5300439596176147 *\n",
      "Epoch: 21, Train_Loss: 0.5138611793518066, Test_Loss: 0.5182654857635498 *\n",
      "Epoch: 21, Train_Loss: 0.5134300589561462, Test_Loss: 0.5683667659759521\n",
      "Epoch: 21, Train_Loss: 0.4886398911476135, Test_Loss: 0.5158608555793762 *\n",
      "Epoch: 21, Train_Loss: 0.4919431805610657, Test_Loss: 0.48842528462409973 *\n",
      "Epoch: 21, Train_Loss: 0.4884588420391083, Test_Loss: 0.495759516954422\n",
      "Epoch: 21, Train_Loss: 0.48736926913261414, Test_Loss: 0.49581557512283325\n",
      "Epoch: 21, Train_Loss: 0.4882635474205017, Test_Loss: 0.49703869223594666\n",
      "Epoch: 21, Train_Loss: 0.487810879945755, Test_Loss: 0.49783486127853394\n",
      "Epoch: 21, Train_Loss: 0.4872129559516907, Test_Loss: 0.48866018652915955 *\n",
      "Epoch: 21, Train_Loss: 0.4845355749130249, Test_Loss: 0.49497556686401367\n",
      "Epoch: 21, Train_Loss: 0.4900575876235962, Test_Loss: 0.4931257367134094 *\n",
      "Epoch: 21, Train_Loss: 0.4954279959201813, Test_Loss: 0.49413982033729553\n",
      "Epoch: 21, Train_Loss: 0.49025505781173706, Test_Loss: 0.49801138043403625\n",
      "Epoch: 21, Train_Loss: 0.48976197838783264, Test_Loss: 0.513820469379425\n",
      "Epoch: 21, Train_Loss: 0.5079637169837952, Test_Loss: 0.5235645771026611\n",
      "Epoch: 21, Train_Loss: 0.4922918379306793, Test_Loss: 0.7643495798110962\n",
      "Epoch: 21, Train_Loss: 0.4984772205352783, Test_Loss: 0.4981861710548401 *\n",
      "Epoch: 21, Train_Loss: 0.4856022894382477, Test_Loss: 0.5251275897026062\n",
      "Epoch: 21, Train_Loss: 0.48859158158302307, Test_Loss: 0.6397677063941956\n",
      "Epoch: 21, Train_Loss: 0.503026008605957, Test_Loss: 0.7550222873687744\n",
      "Epoch: 21, Train_Loss: 0.49764835834503174, Test_Loss: 0.5526271462440491 *\n",
      "Epoch: 21, Train_Loss: 0.4886835217475891, Test_Loss: 0.5547887086868286\n",
      "Epoch: 21, Train_Loss: 0.4836954176425934, Test_Loss: 0.5909271240234375\n",
      "Epoch: 21, Train_Loss: 0.4907366633415222, Test_Loss: 0.7135887145996094\n",
      "Epoch: 21, Train_Loss: 0.5546709299087524, Test_Loss: 0.5047523379325867 *\n",
      "Epoch: 21, Train_Loss: 0.5005174875259399, Test_Loss: 0.5222309827804565\n",
      "Epoch: 21, Train_Loss: 0.5149411559104919, Test_Loss: 0.48755964636802673 *\n",
      "Epoch: 21, Train_Loss: 0.48479926586151123, Test_Loss: 0.5219982266426086\n",
      "Epoch: 21, Train_Loss: 0.516452431678772, Test_Loss: 0.5634912848472595\n",
      "Epoch: 21, Train_Loss: 0.5156242847442627, Test_Loss: 1.0983914136886597\n",
      "Epoch: 21, Train_Loss: 0.48777881264686584, Test_Loss: 0.6370234489440918 *\n",
      "Epoch: 21, Train_Loss: 0.4874173402786255, Test_Loss: 1.1474143266677856\n",
      "Epoch: 21, Train_Loss: 0.5199856162071228, Test_Loss: 0.9749325513839722 *\n",
      "Epoch: 21, Train_Loss: 0.5759771466255188, Test_Loss: 0.7343941926956177 *\n",
      "Epoch: 21, Train_Loss: 0.5384104251861572, Test_Loss: 0.6968786716461182 *\n",
      "Epoch: 21, Train_Loss: 0.5114489793777466, Test_Loss: 0.5001013278961182 *\n",
      "Epoch: 21, Train_Loss: 0.5206741094589233, Test_Loss: 0.5092485547065735\n",
      "Epoch: 21, Train_Loss: 0.48535656929016113, Test_Loss: 0.5142500996589661\n",
      "Epoch: 21, Train_Loss: 0.523352861404419, Test_Loss: 0.6755263209342957\n",
      "Epoch: 21, Train_Loss: 0.48383739590644836, Test_Loss: 1.0453155040740967\n",
      "Epoch: 21, Train_Loss: 0.48966923356056213, Test_Loss: 0.7493356466293335 *\n",
      "Epoch: 21, Train_Loss: 0.48773086071014404, Test_Loss: 2.2060234546661377\n",
      "Epoch: 21, Train_Loss: 0.48857325315475464, Test_Loss: 1.2363612651824951 *\n",
      "Epoch: 21, Train_Loss: 0.5681489706039429, Test_Loss: 1.0339058637619019 *\n",
      "Epoch: 21, Train_Loss: 0.487801194190979, Test_Loss: 0.6361420154571533 *\n",
      "Epoch: 21, Train_Loss: 0.5262706279754639, Test_Loss: 0.5024663805961609 *\n",
      "Epoch: 21, Train_Loss: 0.491628497838974, Test_Loss: 0.5849420428276062\n",
      "Epoch: 21, Train_Loss: 0.4962422847747803, Test_Loss: 1.4688442945480347\n",
      "Epoch: 21, Train_Loss: 0.4959542453289032, Test_Loss: 1.231177806854248 *\n",
      "Epoch: 21, Train_Loss: 0.7630919218063354, Test_Loss: 0.6182717084884644 *\n",
      "Epoch: 21, Train_Loss: 0.5099334716796875, Test_Loss: 0.5391346216201782 *\n",
      "Epoch: 21, Train_Loss: 0.4992358386516571, Test_Loss: 0.5235429406166077 *\n",
      "Epoch: 21, Train_Loss: 0.4837729036808014, Test_Loss: 0.832839846611023\n",
      "Epoch: 21, Train_Loss: 0.48411890864372253, Test_Loss: 0.6329458951950073 *\n",
      "Epoch: 21, Train_Loss: 0.48301830887794495, Test_Loss: 1.434085726737976\n",
      "Epoch: 21, Train_Loss: 0.48258301615715027, Test_Loss: 1.030864953994751 *\n",
      "Epoch: 21, Train_Loss: 0.48840150237083435, Test_Loss: 0.6316250562667847 *\n",
      "Epoch: 21, Train_Loss: 0.48239901661872864, Test_Loss: 0.4867483973503113 *\n",
      "Epoch: 21, Train_Loss: 0.4914783835411072, Test_Loss: 0.4835027754306793 *\n",
      "Epoch: 21, Train_Loss: 0.4848858118057251, Test_Loss: 0.48622581362724304\n",
      "Epoch: 21, Train_Loss: 0.4847251772880554, Test_Loss: 0.5225647687911987\n",
      "Epoch: 21, Train_Loss: 0.49472713470458984, Test_Loss: 0.93424391746521\n",
      "Epoch: 21, Train_Loss: 0.48157835006713867, Test_Loss: 0.8425220847129822 *\n",
      "Epoch: 21, Train_Loss: 0.4771457612514496, Test_Loss: 0.5654529929161072 *\n",
      "Epoch: 21, Train_Loss: 0.4911227524280548, Test_Loss: 0.5038499236106873 *\n",
      "Epoch: 21, Train_Loss: 0.48685479164123535, Test_Loss: 0.5223629474639893\n",
      "Epoch: 21, Train_Loss: 0.4981350898742676, Test_Loss: 0.5701761841773987\n",
      "Epoch: 21, Train_Loss: 0.47913697361946106, Test_Loss: 0.6188318729400635\n",
      "Epoch: 21, Train_Loss: 0.4921486973762512, Test_Loss: 1.1884832382202148\n",
      "Epoch: 21, Train_Loss: 0.5113317966461182, Test_Loss: 1.0304597616195679 *\n",
      "Epoch: 21, Train_Loss: 0.510831356048584, Test_Loss: 0.5712555646896362 *\n",
      "Epoch: 21, Train_Loss: 0.47660261392593384, Test_Loss: 0.5670928955078125 *\n",
      "Epoch: 21, Train_Loss: 0.4940677285194397, Test_Loss: 0.48447805643081665 *\n",
      "Epoch: 21, Train_Loss: 0.47933757305145264, Test_Loss: 0.4801124036312103 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 0.4943252503871918, Test_Loss: 0.4876074492931366\n",
      "Epoch: 21, Train_Loss: 0.4831370711326599, Test_Loss: 0.4858418405056 *\n",
      "Epoch: 21, Train_Loss: 0.4934949278831482, Test_Loss: 0.5406216979026794\n",
      "Epoch: 21, Train_Loss: 0.5380080938339233, Test_Loss: 0.48919135332107544 *\n",
      "Epoch: 21, Train_Loss: 3.119067668914795, Test_Loss: 0.5093067288398743\n",
      "Epoch: 21, Train_Loss: 3.0582737922668457, Test_Loss: 0.5913227200508118\n",
      "Epoch: 21, Train_Loss: 0.5309464335441589, Test_Loss: 0.8798179626464844\n",
      "Epoch: 21, Train_Loss: 0.48567885160446167, Test_Loss: 0.7160820960998535 *\n",
      "Epoch: 21, Train_Loss: 0.5948619842529297, Test_Loss: 0.4953984320163727 *\n",
      "Epoch: 21, Train_Loss: 0.6426392197608948, Test_Loss: 0.481918066740036 *\n",
      "Epoch: 21, Train_Loss: 0.5106342434883118, Test_Loss: 0.48110929131507874 *\n",
      "Epoch: 21, Train_Loss: 0.4768776297569275, Test_Loss: 0.482911080121994\n",
      "Epoch: 21, Train_Loss: 0.5161439180374146, Test_Loss: 0.47910961508750916 *\n",
      "Epoch: 21, Train_Loss: 0.5238565802574158, Test_Loss: 1.0384818315505981\n",
      "Epoch: 21, Train_Loss: 0.48364198207855225, Test_Loss: 5.052222728729248\n",
      "Epoch: 21, Train_Loss: 0.5590717196464539, Test_Loss: 0.5120274424552917 *\n",
      "Epoch: 21, Train_Loss: 1.4605752229690552, Test_Loss: 0.48612985014915466 *\n",
      "Epoch: 21, Train_Loss: 1.5994218587875366, Test_Loss: 0.49355971813201904\n",
      "Epoch: 21, Train_Loss: 0.6108356714248657, Test_Loss: 0.48400092124938965 *\n",
      "Epoch: 21, Train_Loss: 0.5359863042831421, Test_Loss: 0.48565128445625305\n",
      "Epoch: 21, Train_Loss: 2.0176496505737305, Test_Loss: 0.5011873841285706\n",
      "Epoch: 21, Train_Loss: 1.832911491394043, Test_Loss: 0.5914695262908936\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.49610742926597595, Test_Loss: 0.5444084405899048 *\n",
      "Epoch: 21, Train_Loss: 0.4789636731147766, Test_Loss: 0.6005944013595581\n",
      "Epoch: 21, Train_Loss: 0.8035041093826294, Test_Loss: 0.5916234850883484 *\n",
      "Epoch: 21, Train_Loss: 1.5858289003372192, Test_Loss: 0.6126456260681152\n",
      "Epoch: 21, Train_Loss: 1.3723942041397095, Test_Loss: 0.5184768438339233 *\n",
      "Epoch: 21, Train_Loss: 0.48618778586387634, Test_Loss: 0.5400606393814087\n",
      "Epoch: 21, Train_Loss: 0.5069692730903625, Test_Loss: 0.5538712739944458\n",
      "Epoch: 21, Train_Loss: 0.5477072596549988, Test_Loss: 0.5360159873962402 *\n",
      "Epoch: 21, Train_Loss: 0.9177799224853516, Test_Loss: 0.49859243631362915 *\n",
      "Epoch: 21, Train_Loss: 0.4812925159931183, Test_Loss: 0.5355436205863953\n",
      "Epoch: 21, Train_Loss: 0.5092588067054749, Test_Loss: 0.5117926597595215 *\n",
      "Epoch: 21, Train_Loss: 0.4989722669124603, Test_Loss: 0.4885212182998657 *\n",
      "Epoch: 21, Train_Loss: 0.5349853038787842, Test_Loss: 0.504197895526886\n",
      "Epoch: 21, Train_Loss: 0.7318440675735474, Test_Loss: 0.4782795310020447 *\n",
      "Epoch: 21, Train_Loss: 0.6364263892173767, Test_Loss: 0.4944073557853699\n",
      "Epoch: 21, Train_Loss: 0.6312822103500366, Test_Loss: 0.49780699610710144\n",
      "Epoch: 21, Train_Loss: 0.521128237247467, Test_Loss: 0.4819335639476776 *\n",
      "Epoch: 21, Train_Loss: 0.6213967800140381, Test_Loss: 0.48340216279029846\n",
      "Epoch: 21, Train_Loss: 0.5626617670059204, Test_Loss: 0.4927724301815033\n",
      "Epoch: 21, Train_Loss: 0.6976828575134277, Test_Loss: 0.4900319576263428 *\n",
      "Epoch: 21, Train_Loss: 0.7168794274330139, Test_Loss: 0.4961833953857422\n",
      "Epoch: 21, Train_Loss: 0.5102163553237915, Test_Loss: 0.5082859396934509\n",
      "Epoch: 21, Train_Loss: 0.6598243117332458, Test_Loss: 0.5625322461128235\n",
      "Epoch: 21, Train_Loss: 0.5561157464981079, Test_Loss: 3.398345470428467\n",
      "Epoch: 21, Train_Loss: 0.4998076260089874, Test_Loss: 2.873138666152954 *\n",
      "Epoch: 21, Train_Loss: 0.4810737669467926, Test_Loss: 0.49291202425956726 *\n",
      "Epoch: 21, Train_Loss: 0.47583508491516113, Test_Loss: 0.4872526526451111 *\n",
      "Epoch: 21, Train_Loss: 0.47529223561286926, Test_Loss: 0.530798614025116\n",
      "Epoch: 21, Train_Loss: 0.4756077826023102, Test_Loss: 0.48443761467933655 *\n",
      "Epoch: 21, Train_Loss: 0.47851595282554626, Test_Loss: 0.508880615234375\n",
      "Epoch: 21, Train_Loss: 0.5009850263595581, Test_Loss: 0.519385814666748\n",
      "Epoch: 21, Train_Loss: 0.48848873376846313, Test_Loss: 0.5370295643806458\n",
      "Epoch: 21, Train_Loss: 0.4976692497730255, Test_Loss: 0.4885370433330536 *\n",
      "Epoch: 21, Train_Loss: 0.6030083894729614, Test_Loss: 0.5163301825523376\n",
      "Epoch: 21, Train_Loss: 0.8001275062561035, Test_Loss: 0.5145382881164551 *\n",
      "Epoch: 21, Train_Loss: 0.49645909667015076, Test_Loss: 0.5791023969650269\n",
      "Epoch: 21, Train_Loss: 0.5124392509460449, Test_Loss: 0.5008362531661987 *\n",
      "Epoch: 21, Train_Loss: 0.6730043888092041, Test_Loss: 0.5267840027809143\n",
      "Epoch: 21, Train_Loss: 0.6556645631790161, Test_Loss: 0.5094265341758728 *\n",
      "Epoch: 21, Train_Loss: 0.683781623840332, Test_Loss: 0.5275310277938843\n",
      "Epoch: 21, Train_Loss: 0.5108916163444519, Test_Loss: 0.5018019080162048 *\n",
      "Epoch: 21, Train_Loss: 0.8171156048774719, Test_Loss: 0.5308886766433716\n",
      "Epoch: 21, Train_Loss: 0.8469343781471252, Test_Loss: 0.5160074830055237 *\n",
      "Epoch: 21, Train_Loss: 0.7045761346817017, Test_Loss: 0.47464871406555176 *\n",
      "Epoch: 21, Train_Loss: 0.4932720363140106, Test_Loss: 0.4823972284793854\n",
      "Epoch: 21, Train_Loss: 0.48823606967926025, Test_Loss: 0.4810602366924286 *\n",
      "Epoch: 21, Train_Loss: 0.5830605626106262, Test_Loss: 0.5152946710586548\n",
      "Epoch: 21, Train_Loss: 1.5168380737304688, Test_Loss: 0.49950242042541504 *\n",
      "Epoch: 21, Train_Loss: 0.9422875046730042, Test_Loss: 0.4701891541481018 *\n",
      "Epoch: 21, Train_Loss: 0.505677342414856, Test_Loss: 0.47512611746788025\n",
      "Epoch: 21, Train_Loss: 0.4904477000236511, Test_Loss: 0.4713706076145172 *\n",
      "Epoch: 21, Train_Loss: 0.4791207015514374, Test_Loss: 0.4779704511165619\n",
      "Epoch: 21, Train_Loss: 0.703700065612793, Test_Loss: 0.510886549949646\n",
      "Epoch: 21, Train_Loss: 0.7669385671615601, Test_Loss: 0.47675102949142456 *\n",
      "Epoch: 22, Train_Loss: 0.4781360328197479, Test_Loss: 0.4895741641521454 *\n",
      "Epoch: 22, Train_Loss: 0.5071028470993042, Test_Loss: 0.8275221586227417\n",
      "Epoch: 22, Train_Loss: 0.5495887398719788, Test_Loss: 0.5090675950050354 *\n",
      "Epoch: 22, Train_Loss: 5.3162522315979, Test_Loss: 0.49483171105384827 *\n",
      "Epoch: 22, Train_Loss: 12.416013717651367, Test_Loss: 0.573174774646759\n",
      "Epoch: 22, Train_Loss: 0.9789637923240662, Test_Loss: 0.703800618648529\n",
      "Epoch: 22, Train_Loss: 1.61366605758667, Test_Loss: 0.5535379648208618 *\n",
      "Epoch: 22, Train_Loss: 0.8608983159065247, Test_Loss: 0.5173686742782593 *\n",
      "Epoch: 22, Train_Loss: 0.5375176072120667, Test_Loss: 0.5213461518287659\n",
      "Epoch: 22, Train_Loss: 0.6855484247207642, Test_Loss: 0.5979427695274353\n",
      "Epoch: 22, Train_Loss: 7.972777843475342, Test_Loss: 0.5460931062698364 *\n",
      "Epoch: 22, Train_Loss: 2.54455828666687, Test_Loss: 0.5407280921936035 *\n",
      "Epoch: 22, Train_Loss: 0.49443328380584717, Test_Loss: 0.49811121821403503 *\n",
      "Epoch: 22, Train_Loss: 2.6138830184936523, Test_Loss: 0.4858334958553314 *\n",
      "Epoch: 22, Train_Loss: 3.8642797470092773, Test_Loss: 0.5887953639030457\n",
      "Epoch: 22, Train_Loss: 0.6596280932426453, Test_Loss: 0.8627735376358032\n",
      "Epoch: 22, Train_Loss: 0.49103954434394836, Test_Loss: 0.7726988792419434 *\n",
      "Epoch: 22, Train_Loss: 0.5006305575370789, Test_Loss: 0.9133496880531311\n",
      "Epoch: 22, Train_Loss: 0.4904874563217163, Test_Loss: 0.7376614809036255 *\n",
      "Epoch: 22, Train_Loss: 0.5099719166755676, Test_Loss: 0.9254311323165894\n",
      "Epoch: 22, Train_Loss: 0.4814131259918213, Test_Loss: 0.7635332345962524 *\n",
      "Epoch: 22, Train_Loss: 0.4847993850708008, Test_Loss: 0.5911019444465637 *\n",
      "Epoch: 22, Train_Loss: 0.48169416189193726, Test_Loss: 0.5006436705589294 *\n",
      "Epoch: 22, Train_Loss: 0.48752549290657043, Test_Loss: 0.5365517735481262\n",
      "Epoch: 22, Train_Loss: 0.5379970669746399, Test_Loss: 0.5191546082496643 *\n",
      "Epoch: 22, Train_Loss: 0.4882453680038452, Test_Loss: 0.9755662083625793\n",
      "Epoch: 22, Train_Loss: 0.4980510175228119, Test_Loss: 1.0288504362106323\n",
      "Epoch: 22, Train_Loss: 0.6249367594718933, Test_Loss: 1.390264630317688\n",
      "Epoch: 22, Train_Loss: 0.6365358829498291, Test_Loss: 1.1986109018325806 *\n",
      "Epoch: 22, Train_Loss: 0.5080806016921997, Test_Loss: 1.9538308382034302\n",
      "Epoch: 22, Train_Loss: 0.5022032856941223, Test_Loss: 0.8994715213775635 *\n",
      "Epoch: 22, Train_Loss: 0.5135114192962646, Test_Loss: 0.6075538992881775 *\n",
      "Epoch: 22, Train_Loss: 0.4686446785926819, Test_Loss: 0.5264822244644165 *\n",
      "Epoch: 22, Train_Loss: 0.4740160405635834, Test_Loss: 0.8735263347625732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.4654938876628876, Test_Loss: 1.4283502101898193\n",
      "Epoch: 22, Train_Loss: 0.4665448069572449, Test_Loss: 0.533720076084137 *\n",
      "Epoch: 22, Train_Loss: 0.4657890200614929, Test_Loss: 0.5318179130554199 *\n",
      "Epoch: 22, Train_Loss: 0.4653181731700897, Test_Loss: 0.49794694781303406 *\n",
      "Epoch: 22, Train_Loss: 0.4630160629749298, Test_Loss: 0.7720075845718384\n",
      "Epoch: 22, Train_Loss: 0.46614399552345276, Test_Loss: 0.686813235282898 *\n",
      "Epoch: 22, Train_Loss: 0.47664839029312134, Test_Loss: 1.0119622945785522\n",
      "Epoch: 22, Train_Loss: 0.47851330041885376, Test_Loss: 1.0215892791748047\n",
      "Epoch: 22, Train_Loss: 0.6637437343597412, Test_Loss: 0.8294743299484253 *\n",
      "Epoch: 22, Train_Loss: 0.5086569786071777, Test_Loss: 0.4785476326942444 *\n",
      "Epoch: 22, Train_Loss: 0.47451743483543396, Test_Loss: 0.4786294102668762\n",
      "Epoch: 22, Train_Loss: 8.019743919372559, Test_Loss: 0.4772403836250305 *\n",
      "Epoch: 22, Train_Loss: 1.4105710983276367, Test_Loss: 0.5550206303596497\n",
      "Epoch: 22, Train_Loss: 0.4866953194141388, Test_Loss: 0.6538764238357544\n",
      "Epoch: 22, Train_Loss: 0.5172698497772217, Test_Loss: 0.6491846442222595 *\n",
      "Epoch: 22, Train_Loss: 0.5533725023269653, Test_Loss: 0.5117882490158081 *\n",
      "Epoch: 22, Train_Loss: 0.4899055063724518, Test_Loss: 0.511535108089447 *\n",
      "Epoch: 22, Train_Loss: 0.5210052728652954, Test_Loss: 0.5944886207580566\n",
      "Epoch: 22, Train_Loss: 0.6370236277580261, Test_Loss: 0.6166707277297974\n",
      "Epoch: 22, Train_Loss: 0.7109826803207397, Test_Loss: 0.6607195138931274\n",
      "Epoch: 22, Train_Loss: 0.6834379434585571, Test_Loss: 1.022844672203064\n",
      "Epoch: 22, Train_Loss: 0.6089373826980591, Test_Loss: 1.2832751274108887\n",
      "Epoch: 22, Train_Loss: 0.47149693965911865, Test_Loss: 0.5529296398162842 *\n",
      "Epoch: 22, Train_Loss: 0.5385072231292725, Test_Loss: 0.5451375246047974 *\n",
      "Epoch: 22, Train_Loss: 0.5099526047706604, Test_Loss: 0.47243842482566833 *\n",
      "Epoch: 22, Train_Loss: 0.6887820959091187, Test_Loss: 0.4984665811061859\n",
      "Epoch: 22, Train_Loss: 0.49090418219566345, Test_Loss: 0.4958920180797577 *\n",
      "Epoch: 22, Train_Loss: 0.49981194734573364, Test_Loss: 0.5038534998893738\n",
      "Epoch: 22, Train_Loss: 0.501709520816803, Test_Loss: 0.5370371341705322\n",
      "Epoch: 22, Train_Loss: 0.4872848391532898, Test_Loss: 0.4915095567703247 *\n",
      "Epoch: 22, Train_Loss: 0.5360147953033447, Test_Loss: 0.4867083430290222 *\n",
      "Epoch: 22, Train_Loss: 0.4966680705547333, Test_Loss: 0.514226496219635\n",
      "Epoch: 22, Train_Loss: 0.48778119683265686, Test_Loss: 0.8179646730422974\n",
      "Epoch: 22, Train_Loss: 0.46419548988342285, Test_Loss: 0.5615992546081543 *\n",
      "Epoch: 22, Train_Loss: 0.4619293212890625, Test_Loss: 0.559328556060791 *\n",
      "Epoch: 22, Train_Loss: 0.5343614816665649, Test_Loss: 0.4640309810638428 *\n",
      "Epoch: 22, Train_Loss: 5.31667423248291, Test_Loss: 0.46709296107292175\n",
      "Epoch: 22, Train_Loss: 0.5567482709884644, Test_Loss: 0.46152442693710327 *\n",
      "Epoch: 22, Train_Loss: 0.4597402811050415, Test_Loss: 0.4648558795452118\n",
      "Epoch: 22, Train_Loss: 0.48306262493133545, Test_Loss: 0.5316253900527954\n",
      "Epoch: 22, Train_Loss: 0.4694196879863739, Test_Loss: 6.078135967254639\n",
      "Epoch: 22, Train_Loss: 0.465064138174057, Test_Loss: 0.6957260966300964 *\n",
      "Epoch: 22, Train_Loss: 0.4629096984863281, Test_Loss: 0.5040101408958435 *\n",
      "Epoch: 22, Train_Loss: 0.46217218041419983, Test_Loss: 0.49826425313949585 *\n",
      "Epoch: 22, Train_Loss: 0.48222851753234863, Test_Loss: 0.48001158237457275 *\n",
      "Epoch: 22, Train_Loss: 0.4755789339542389, Test_Loss: 0.4877837002277374\n",
      "Epoch: 22, Train_Loss: 0.5134773254394531, Test_Loss: 0.5143524408340454\n",
      "Epoch: 22, Train_Loss: 0.4581993818283081, Test_Loss: 0.64152592420578\n",
      "Epoch: 22, Train_Loss: 0.4589096009731293, Test_Loss: 0.5636709928512573 *\n",
      "Epoch: 22, Train_Loss: 0.4718741774559021, Test_Loss: 0.5648770332336426\n",
      "Epoch: 22, Train_Loss: 0.4740625321865082, Test_Loss: 0.6292800903320312\n",
      "Epoch: 22, Train_Loss: 0.46673718094825745, Test_Loss: 0.6389580368995667\n",
      "Epoch: 22, Train_Loss: 0.4751899838447571, Test_Loss: 0.5494373440742493 *\n",
      "Epoch: 22, Train_Loss: 0.4712800681591034, Test_Loss: 0.5362732410430908 *\n",
      "Epoch: 22, Train_Loss: 0.47776806354522705, Test_Loss: 0.5675249695777893\n",
      "Epoch: 22, Train_Loss: 0.4643753468990326, Test_Loss: 0.5047876834869385 *\n",
      "Epoch: 22, Train_Loss: 0.46122506260871887, Test_Loss: 0.49279507994651794 *\n",
      "Epoch: 22, Train_Loss: 0.502616286277771, Test_Loss: 0.4964964687824249\n",
      "Epoch: 22, Train_Loss: 0.5002893805503845, Test_Loss: 0.5092947483062744\n",
      "Epoch: 22, Train_Loss: 0.4925501048564911, Test_Loss: 0.5136129856109619\n",
      "Epoch: 22, Train_Loss: 0.5463683009147644, Test_Loss: 0.5431694984436035\n",
      "Epoch: 22, Train_Loss: 0.5429220199584961, Test_Loss: 0.49407193064689636 *\n",
      "Epoch: 22, Train_Loss: 0.4810720682144165, Test_Loss: 0.5097447037696838\n",
      "Epoch: 22, Train_Loss: 0.47094884514808655, Test_Loss: 0.5162653923034668\n",
      "Epoch: 22, Train_Loss: 0.5202561616897583, Test_Loss: 0.491466760635376 *\n",
      "Epoch: 22, Train_Loss: 0.6629397869110107, Test_Loss: 0.4792036712169647 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.5124251842498779, Test_Loss: 0.49952536821365356\n",
      "Epoch: 22, Train_Loss: 0.46500709652900696, Test_Loss: 0.47526243329048157 *\n",
      "Epoch: 22, Train_Loss: 0.45532655715942383, Test_Loss: 0.47172245383262634 *\n",
      "Epoch: 22, Train_Loss: 0.46073874831199646, Test_Loss: 0.49056190252304077\n",
      "Epoch: 22, Train_Loss: 0.4614694118499756, Test_Loss: 0.5446681380271912\n",
      "Epoch: 22, Train_Loss: 0.4587218761444092, Test_Loss: 2.16383695602417\n",
      "Epoch: 22, Train_Loss: 0.46000099182128906, Test_Loss: 4.886777877807617\n",
      "Epoch: 22, Train_Loss: 4.662985801696777, Test_Loss: 0.4834044873714447 *\n",
      "Epoch: 22, Train_Loss: 0.8175203800201416, Test_Loss: 0.4680268466472626 *\n",
      "Epoch: 22, Train_Loss: 0.4659360647201538, Test_Loss: 0.4776904881000519\n",
      "Epoch: 22, Train_Loss: 0.46159520745277405, Test_Loss: 0.46939602494239807 *\n",
      "Epoch: 22, Train_Loss: 0.4582984447479248, Test_Loss: 0.4794577658176422\n",
      "Epoch: 22, Train_Loss: 0.46348586678504944, Test_Loss: 0.507851243019104\n",
      "Epoch: 22, Train_Loss: 0.4612324833869934, Test_Loss: 0.6452606916427612\n",
      "Epoch: 22, Train_Loss: 0.4555429518222809, Test_Loss: 0.478252112865448 *\n",
      "Epoch: 22, Train_Loss: 0.4592851996421814, Test_Loss: 0.4688217341899872 *\n",
      "Epoch: 22, Train_Loss: 0.4573315382003784, Test_Loss: 0.4975844621658325\n",
      "Epoch: 22, Train_Loss: 0.4904116690158844, Test_Loss: 0.47230926156044006 *\n",
      "Epoch: 22, Train_Loss: 0.4907207190990448, Test_Loss: 0.47624021768569946\n",
      "Epoch: 22, Train_Loss: 0.4986315965652466, Test_Loss: 0.5028964281082153\n",
      "Epoch: 22, Train_Loss: 0.4827251732349396, Test_Loss: 0.49477678537368774 *\n",
      "Epoch: 22, Train_Loss: 0.4585511088371277, Test_Loss: 0.5609316229820251\n",
      "Epoch: 22, Train_Loss: 0.5025448203086853, Test_Loss: 0.5568204522132874 *\n",
      "Epoch: 22, Train_Loss: 0.5840400457382202, Test_Loss: 0.4801885187625885 *\n",
      "Epoch: 22, Train_Loss: 0.5854448080062866, Test_Loss: 0.4781939685344696 *\n",
      "Epoch: 22, Train_Loss: 0.5783836841583252, Test_Loss: 0.45762506127357483 *\n",
      "Epoch: 22, Train_Loss: 0.4818907380104065, Test_Loss: 0.4582180380821228\n",
      "Epoch: 22, Train_Loss: 0.46099796891212463, Test_Loss: 0.4592280983924866\n",
      "Epoch: 22, Train_Loss: 0.46113622188568115, Test_Loss: 0.4577350318431854 *\n",
      "Epoch: 22, Train_Loss: 0.45248711109161377, Test_Loss: 0.4566552937030792 *\n",
      "Epoch: 22, Train_Loss: 0.45426318049430847, Test_Loss: 0.458439439535141\n",
      "Epoch: 22, Train_Loss: 0.4541323781013489, Test_Loss: 0.46109095215797424\n",
      "Epoch: 22, Train_Loss: 0.4557499289512634, Test_Loss: 0.4579235017299652 *\n",
      "Epoch: 22, Train_Loss: 0.46051767468452454, Test_Loss: 0.4588303864002228\n",
      "Epoch: 22, Train_Loss: 0.45725017786026, Test_Loss: 0.48615482449531555\n",
      "Epoch: 22, Train_Loss: 0.46316760778427124, Test_Loss: 0.45753341913223267 *\n",
      "Epoch: 22, Train_Loss: 0.5189487338066101, Test_Loss: 0.4759369492530823\n",
      "Epoch: 22, Train_Loss: 0.4920211136341095, Test_Loss: 0.7254330515861511\n",
      "Epoch: 22, Train_Loss: 0.516974925994873, Test_Loss: 0.609096884727478 *\n",
      "Epoch: 22, Train_Loss: 0.5194258689880371, Test_Loss: 0.47863733768463135 *\n",
      "Epoch: 22, Train_Loss: 0.5532407760620117, Test_Loss: 0.4837495684623718\n",
      "Epoch: 22, Train_Loss: 0.5863211154937744, Test_Loss: 0.5930459499359131\n",
      "Epoch: 22, Train_Loss: 0.5000806450843811, Test_Loss: 0.6244575381278992\n",
      "Epoch: 22, Train_Loss: 0.6509775519371033, Test_Loss: 0.46084141731262207 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.5779037475585938, Test_Loss: 0.5334631204605103\n",
      "Epoch: 22, Train_Loss: 0.6668639183044434, Test_Loss: 0.6079892516136169\n",
      "Epoch: 22, Train_Loss: 0.45581528544425964, Test_Loss: 0.5271750688552856 *\n",
      "Epoch: 22, Train_Loss: 1.7373842000961304, Test_Loss: 0.5219985246658325 *\n",
      "Epoch: 22, Train_Loss: 1.5628557205200195, Test_Loss: 0.45902329683303833 *\n",
      "Epoch: 22, Train_Loss: 0.5247821807861328, Test_Loss: 0.47407251596450806\n",
      "Epoch: 22, Train_Loss: 0.5271561145782471, Test_Loss: 0.5205941200256348\n",
      "Epoch: 22, Train_Loss: 0.5115422010421753, Test_Loss: 0.8138415813446045\n",
      "Epoch: 22, Train_Loss: 0.5080445408821106, Test_Loss: 0.7646725177764893 *\n",
      "Epoch: 22, Train_Loss: 0.47876664996147156, Test_Loss: 1.0434236526489258\n",
      "Epoch: 22, Train_Loss: 0.460555762052536, Test_Loss: 1.0094326734542847 *\n",
      "Epoch: 22, Train_Loss: 0.5000903010368347, Test_Loss: 0.6554405093193054 *\n",
      "Epoch: 22, Train_Loss: 0.48086827993392944, Test_Loss: 0.6876006722450256\n",
      "Epoch: 22, Train_Loss: 0.4842292070388794, Test_Loss: 0.5458686351776123 *\n",
      "Epoch: 22, Train_Loss: 0.4798261821269989, Test_Loss: 0.47696495056152344 *\n",
      "Epoch: 22, Train_Loss: 0.48038074374198914, Test_Loss: 0.46807244420051575 *\n",
      "Epoch: 22, Train_Loss: 0.4644966125488281, Test_Loss: 0.5798748731613159\n",
      "Epoch: 22, Train_Loss: 0.4654421806335449, Test_Loss: 0.6818233728408813\n",
      "Epoch: 22, Train_Loss: 0.46175459027290344, Test_Loss: 0.9760945439338684\n",
      "Epoch: 22, Train_Loss: 0.4603263735771179, Test_Loss: 1.2161749601364136\n",
      "Epoch: 22, Train_Loss: 0.456569105386734, Test_Loss: 2.0485401153564453\n",
      "Epoch: 22, Train_Loss: 0.4530089497566223, Test_Loss: 0.8172848224639893 *\n",
      "Epoch: 22, Train_Loss: 0.462943434715271, Test_Loss: 0.8463025093078613\n",
      "Epoch: 22, Train_Loss: 0.4600141644477844, Test_Loss: 0.46254321932792664 *\n",
      "Epoch: 22, Train_Loss: 0.47137898206710815, Test_Loss: 0.46595194935798645\n",
      "Epoch: 22, Train_Loss: 0.4510015547275543, Test_Loss: 1.0139856338500977\n",
      "Epoch: 22, Train_Loss: 0.44803500175476074, Test_Loss: 1.6722757816314697\n",
      "Epoch: 22, Train_Loss: 0.4468335509300232, Test_Loss: 0.5255663394927979 *\n",
      "Epoch: 22, Train_Loss: 0.4495307505130768, Test_Loss: 0.5867205262184143\n",
      "Epoch: 22, Train_Loss: 0.4518446624279022, Test_Loss: 0.45911696553230286 *\n",
      "Epoch: 22, Train_Loss: 0.4481620788574219, Test_Loss: 0.6294006109237671\n",
      "Epoch: 22, Train_Loss: 0.44870758056640625, Test_Loss: 0.7600424885749817\n",
      "Epoch: 22, Train_Loss: 0.45110732316970825, Test_Loss: 0.8181033134460449\n",
      "Epoch: 22, Train_Loss: 0.4469841420650482, Test_Loss: 1.2017937898635864\n",
      "Epoch: 22, Train_Loss: 0.4494114816188812, Test_Loss: 0.7320942878723145 *\n",
      "Epoch: 22, Train_Loss: 0.45387378334999084, Test_Loss: 0.45566484332084656 *\n",
      "Epoch: 22, Train_Loss: 0.4499800503253937, Test_Loss: 0.4562299847602844\n",
      "Epoch: 22, Train_Loss: 0.448313444852829, Test_Loss: 0.4723765254020691\n",
      "Epoch: 22, Train_Loss: 0.46631625294685364, Test_Loss: 0.48028382658958435\n",
      "Epoch: 22, Train_Loss: 0.45418781042099, Test_Loss: 0.6917896270751953\n",
      "Epoch: 22, Train_Loss: 0.4543713927268982, Test_Loss: 0.8376095294952393\n",
      "Epoch: 22, Train_Loss: 0.4486880600452423, Test_Loss: 0.5959745049476624 *\n",
      "Epoch: 22, Train_Loss: 0.4489348828792572, Test_Loss: 0.5201283693313599 *\n",
      "Epoch: 22, Train_Loss: 0.46379509568214417, Test_Loss: 0.45989617705345154 *\n",
      "Epoch: 22, Train_Loss: 0.451722651720047, Test_Loss: 0.48217347264289856\n",
      "Epoch: 22, Train_Loss: 0.45215922594070435, Test_Loss: 0.5698086619377136\n",
      "Epoch: 22, Train_Loss: 0.44914403557777405, Test_Loss: 0.8207112550735474\n",
      "Epoch: 22, Train_Loss: 0.45836207270622253, Test_Loss: 0.9745550155639648\n",
      "Epoch: 22, Train_Loss: 0.520426869392395, Test_Loss: 0.6279568076133728 *\n",
      "Epoch: 22, Train_Loss: 0.46619921922683716, Test_Loss: 0.5541467070579529 *\n",
      "Epoch: 22, Train_Loss: 0.46573135256767273, Test_Loss: 0.4537537395954132 *\n",
      "Epoch: 22, Train_Loss: 0.44762346148490906, Test_Loss: 0.4555497169494629\n",
      "Epoch: 22, Train_Loss: 0.48338583111763, Test_Loss: 0.4536144435405731 *\n",
      "Epoch: 22, Train_Loss: 0.47062790393829346, Test_Loss: 0.4737578332424164\n",
      "Epoch: 22, Train_Loss: 0.45298245549201965, Test_Loss: 0.47470739483833313\n",
      "Epoch: 22, Train_Loss: 0.4564884603023529, Test_Loss: 0.4841097891330719\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.49056220054626465, Test_Loss: 0.4580157995223999 *\n",
      "Epoch: 22, Train_Loss: 0.5416959524154663, Test_Loss: 0.5048951506614685\n",
      "Epoch: 22, Train_Loss: 0.49176251888275146, Test_Loss: 0.8020776510238647\n",
      "Epoch: 22, Train_Loss: 0.47485145926475525, Test_Loss: 0.529329776763916 *\n",
      "Epoch: 22, Train_Loss: 0.4809831976890564, Test_Loss: 0.6581487655639648\n",
      "Epoch: 22, Train_Loss: 0.4482570290565491, Test_Loss: 0.4529359042644501 *\n",
      "Epoch: 22, Train_Loss: 0.47109124064445496, Test_Loss: 0.45111244916915894 *\n",
      "Epoch: 22, Train_Loss: 0.4454736113548279, Test_Loss: 0.45469748973846436\n",
      "Epoch: 22, Train_Loss: 0.4582219421863556, Test_Loss: 0.4521251916885376 *\n",
      "Epoch: 22, Train_Loss: 0.4493784010410309, Test_Loss: 0.46133971214294434\n",
      "Epoch: 22, Train_Loss: 0.45011115074157715, Test_Loss: 4.69332218170166\n",
      "Epoch: 22, Train_Loss: 0.5322390794754028, Test_Loss: 1.4483582973480225 *\n",
      "Epoch: 22, Train_Loss: 0.44754695892333984, Test_Loss: 0.462481826543808 *\n",
      "Epoch: 22, Train_Loss: 0.4931502938270569, Test_Loss: 0.45408815145492554 *\n",
      "Epoch: 22, Train_Loss: 0.4560581147670746, Test_Loss: 0.45635315775871277\n",
      "Epoch: 22, Train_Loss: 0.47555094957351685, Test_Loss: 0.4465218782424927 *\n",
      "Epoch: 22, Train_Loss: 0.487560898065567, Test_Loss: 0.46107667684555054\n",
      "Epoch: 22, Train_Loss: 0.701890230178833, Test_Loss: 0.4828915297985077\n",
      "Epoch: 22, Train_Loss: 0.45443469285964966, Test_Loss: 0.46415975689888 *\n",
      "Epoch: 22, Train_Loss: 0.4724242091178894, Test_Loss: 0.4569447934627533 *\n",
      "Epoch: 22, Train_Loss: 0.4424607455730438, Test_Loss: 0.4785534143447876\n",
      "Epoch: 22, Train_Loss: 0.44346004724502563, Test_Loss: 0.48569992184638977\n",
      "Epoch: 22, Train_Loss: 0.4489102065563202, Test_Loss: 0.46173885464668274 *\n",
      "Epoch: 22, Train_Loss: 0.45475268363952637, Test_Loss: 0.4950251579284668\n",
      "Epoch: 22, Train_Loss: 0.4512501657009125, Test_Loss: 0.4960719645023346\n",
      "Epoch: 22, Train_Loss: 0.44945353269577026, Test_Loss: 0.456585556268692 *\n",
      "Epoch: 22, Train_Loss: 0.45298802852630615, Test_Loss: 0.44599467515945435 *\n",
      "Epoch: 22, Train_Loss: 0.4480999708175659, Test_Loss: 0.45433300733566284\n",
      "Epoch: 22, Train_Loss: 0.4460936486721039, Test_Loss: 0.45744261145591736\n",
      "Epoch: 22, Train_Loss: 0.45399680733680725, Test_Loss: 0.454612672328949 *\n",
      "Epoch: 22, Train_Loss: 0.4428974688053131, Test_Loss: 0.44728341698646545 *\n",
      "Epoch: 22, Train_Loss: 0.44002920389175415, Test_Loss: 0.44856762886047363\n",
      "Epoch: 22, Train_Loss: 0.45778557658195496, Test_Loss: 0.4506729245185852\n",
      "Epoch: 22, Train_Loss: 0.4494985044002533, Test_Loss: 0.46571171283721924\n",
      "Epoch: 22, Train_Loss: 0.4655349850654602, Test_Loss: 0.449361652135849 *\n",
      "Epoch: 22, Train_Loss: 0.4408446252346039, Test_Loss: 0.44330379366874695 *\n",
      "Epoch: 22, Train_Loss: 0.4524586498737335, Test_Loss: 0.444981187582016\n",
      "Epoch: 22, Train_Loss: 0.4655071496963501, Test_Loss: 0.4448833763599396 *\n",
      "Epoch: 22, Train_Loss: 0.4718852639198303, Test_Loss: 0.4414691925048828 *\n",
      "Epoch: 22, Train_Loss: 0.44355714321136475, Test_Loss: 0.4437386393547058\n",
      "Epoch: 22, Train_Loss: 0.45603951811790466, Test_Loss: 0.5152781009674072\n",
      "Epoch: 22, Train_Loss: 0.44140076637268066, Test_Loss: 0.6906082630157471\n",
      "Epoch: 22, Train_Loss: 0.462678462266922, Test_Loss: 6.064357280731201\n",
      "Epoch: 22, Train_Loss: 0.4489293694496155, Test_Loss: 0.4842372238636017 *\n",
      "Epoch: 22, Train_Loss: 0.45558586716651917, Test_Loss: 0.4413798451423645 *\n",
      "Epoch: 22, Train_Loss: 0.6716027855873108, Test_Loss: 0.4592212438583374\n",
      "Epoch: 22, Train_Loss: 4.187149524688721, Test_Loss: 0.4696090817451477\n",
      "Epoch: 22, Train_Loss: 1.9544998407363892, Test_Loss: 0.4728822708129883\n",
      "Epoch: 22, Train_Loss: 0.48106294870376587, Test_Loss: 0.4466175138950348 *\n",
      "Epoch: 22, Train_Loss: 0.4495261311531067, Test_Loss: 0.5889513492584229\n",
      "Epoch: 22, Train_Loss: 0.588287889957428, Test_Loss: 0.4879026412963867 *\n",
      "Epoch: 22, Train_Loss: 0.5545040369033813, Test_Loss: 0.4400142729282379 *\n",
      "Epoch: 22, Train_Loss: 0.4631631374359131, Test_Loss: 0.4742208421230316\n",
      "Epoch: 22, Train_Loss: 0.43985000252723694, Test_Loss: 0.45288923382759094 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.4889143407344818, Test_Loss: 0.44802725315093994 *\n",
      "Epoch: 22, Train_Loss: 0.47207579016685486, Test_Loss: 0.47262850403785706\n",
      "Epoch: 22, Train_Loss: 0.44650089740753174, Test_Loss: 0.5096539855003357\n",
      "Epoch: 22, Train_Loss: 0.6339318752288818, Test_Loss: 0.49140819907188416 *\n",
      "Epoch: 22, Train_Loss: 1.5140445232391357, Test_Loss: 0.4928126931190491\n",
      "Epoch: 22, Train_Loss: 1.4740477800369263, Test_Loss: 0.47270238399505615 *\n",
      "Epoch: 22, Train_Loss: 0.5259969234466553, Test_Loss: 0.5188801884651184\n",
      "Epoch: 22, Train_Loss: 0.521151065826416, Test_Loss: 0.452342689037323 *\n",
      "Epoch: 22, Train_Loss: 2.339895725250244, Test_Loss: 0.44839417934417725 *\n",
      "Epoch: 22, Train_Loss: 1.5423154830932617, Test_Loss: 0.459700345993042\n",
      "Epoch: 22, Train_Loss: 0.45553654432296753, Test_Loss: 0.4604002833366394\n",
      "Epoch: 22, Train_Loss: 0.44299495220184326, Test_Loss: 0.4603610336780548 *\n",
      "Epoch: 22, Train_Loss: 0.8218275904655457, Test_Loss: 0.4469674527645111 *\n",
      "Epoch: 22, Train_Loss: 1.395085096359253, Test_Loss: 0.4429907202720642 *\n",
      "Epoch: 22, Train_Loss: 1.158087968826294, Test_Loss: 0.4452205002307892\n",
      "Epoch: 22, Train_Loss: 0.44484570622444153, Test_Loss: 0.4487963020801544\n",
      "Epoch: 22, Train_Loss: 0.4795806109905243, Test_Loss: 0.4891323447227478\n",
      "Epoch: 22, Train_Loss: 0.6229406595230103, Test_Loss: 0.45669111609458923 *\n",
      "Epoch: 22, Train_Loss: 0.8886468410491943, Test_Loss: 0.46725890040397644\n",
      "Epoch: 22, Train_Loss: 0.4537373483181, Test_Loss: 0.5581057071685791\n",
      "Epoch: 22, Train_Loss: 0.4687524139881134, Test_Loss: 0.6354264616966248\n",
      "Epoch: 22, Train_Loss: 0.4842800498008728, Test_Loss: 0.5355461239814758 *\n",
      "Epoch: 22, Train_Loss: 0.49242454767227173, Test_Loss: 0.4984048008918762 *\n",
      "Epoch: 22, Train_Loss: 0.5644189119338989, Test_Loss: 0.9273940324783325\n",
      "Epoch: 22, Train_Loss: 0.6669008731842041, Test_Loss: 0.580051064491272 *\n",
      "Epoch: 22, Train_Loss: 0.5866382122039795, Test_Loss: 0.4714803695678711 *\n",
      "Epoch: 22, Train_Loss: 0.5008081197738647, Test_Loss: 0.5548065900802612\n",
      "Epoch: 22, Train_Loss: 0.5519260168075562, Test_Loss: 0.5335673093795776 *\n",
      "Epoch: 22, Train_Loss: 0.5737794637680054, Test_Loss: 0.6031783819198608\n",
      "Epoch: 22, Train_Loss: 0.6797138452529907, Test_Loss: 0.5247650146484375 *\n",
      "Epoch: 22, Train_Loss: 0.6836510896682739, Test_Loss: 0.46048209071159363 *\n",
      "Epoch: 22, Train_Loss: 0.48782891035079956, Test_Loss: 0.452510267496109 *\n",
      "Epoch: 22, Train_Loss: 0.5653748512268066, Test_Loss: 0.5343629121780396\n",
      "Epoch: 22, Train_Loss: 0.533165454864502, Test_Loss: 0.8476129770278931\n",
      "Epoch: 22, Train_Loss: 0.45655202865600586, Test_Loss: 0.8677867650985718\n",
      "Epoch: 22, Train_Loss: 0.44204410910606384, Test_Loss: 0.8823047280311584\n",
      "Epoch: 22, Train_Loss: 0.45880812406539917, Test_Loss: 1.0229599475860596\n",
      "Epoch: 22, Train_Loss: 0.4434024691581726, Test_Loss: 0.8726222515106201 *\n",
      "Epoch: 22, Train_Loss: 0.4405853748321533, Test_Loss: 0.63761305809021 *\n",
      "Epoch: 22, Train_Loss: 0.47039496898651123, Test_Loss: 0.5710598826408386 *\n",
      "Epoch: 22, Train_Loss: 0.47142931818962097, Test_Loss: 0.5011160373687744 *\n",
      "Epoch: 22, Train_Loss: 0.4613620340824127, Test_Loss: 0.45735740661621094 *\n",
      "Epoch: 22, Train_Loss: 0.48200923204421997, Test_Loss: 0.4797016382217407\n",
      "Epoch: 22, Train_Loss: 0.601561427116394, Test_Loss: 0.7991195917129517\n",
      "Epoch: 22, Train_Loss: 0.7135462760925293, Test_Loss: 0.7815618515014648 *\n",
      "Epoch: 22, Train_Loss: 0.4643741250038147, Test_Loss: 0.7847151756286621\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.4829639494419098, Test_Loss: 2.4127819538116455\n",
      "Epoch: 22, Train_Loss: 0.6319690346717834, Test_Loss: 0.8648279905319214 *\n",
      "Epoch: 22, Train_Loss: 0.5720254778862, Test_Loss: 1.1370152235031128\n",
      "Epoch: 22, Train_Loss: 0.5342741012573242, Test_Loss: 0.4968613386154175 *\n",
      "Epoch: 22, Train_Loss: 0.4616592526435852, Test_Loss: 0.4508020877838135 *\n",
      "Epoch: 22, Train_Loss: 0.777371346950531, Test_Loss: 0.7963235378265381\n",
      "Epoch: 22, Train_Loss: 0.776535153388977, Test_Loss: 1.7846894264221191\n",
      "Epoch: 22, Train_Loss: 0.5910213589668274, Test_Loss: 0.8822187781333923 *\n",
      "Epoch: 22, Train_Loss: 0.459719181060791, Test_Loss: 0.5130359530448914 *\n",
      "Epoch: 22, Train_Loss: 0.455573707818985, Test_Loss: 0.5024539232254028 *\n",
      "Epoch: 22, Train_Loss: 0.6852508783340454, Test_Loss: 0.5731937885284424\n",
      "Epoch: 22, Train_Loss: 1.3253531455993652, Test_Loss: 0.9409440755844116\n",
      "Epoch: 22, Train_Loss: 0.7605535984039307, Test_Loss: 0.6244247555732727 *\n",
      "Epoch: 22, Train_Loss: 0.4873347878456116, Test_Loss: 0.8616288304328918\n",
      "Epoch: 22, Train_Loss: 0.4546542763710022, Test_Loss: 0.6799887418746948 *\n",
      "Epoch: 22, Train_Loss: 0.44457319378852844, Test_Loss: 0.45890939235687256 *\n",
      "Epoch: 22, Train_Loss: 0.7700494527816772, Test_Loss: 0.47611990571022034\n",
      "Epoch: 22, Train_Loss: 0.5898790955543518, Test_Loss: 0.5992310047149658\n",
      "Epoch: 22, Train_Loss: 0.4571794867515564, Test_Loss: 0.5248668789863586 *\n",
      "Epoch: 22, Train_Loss: 0.4540766179561615, Test_Loss: 0.5980785489082336\n",
      "Epoch: 22, Train_Loss: 0.5215890407562256, Test_Loss: 0.7067473530769348\n",
      "Epoch: 22, Train_Loss: 13.867669105529785, Test_Loss: 0.6337944269180298 *\n",
      "Epoch: 22, Train_Loss: 3.584862232208252, Test_Loss: 0.5289586782455444 *\n",
      "Epoch: 22, Train_Loss: 1.2635095119476318, Test_Loss: 0.44668224453926086 *\n",
      "Epoch: 22, Train_Loss: 1.7370445728302002, Test_Loss: 0.45030292868614197\n",
      "Epoch: 22, Train_Loss: 0.5248824954032898, Test_Loss: 0.47978225350379944\n",
      "Epoch: 22, Train_Loss: 0.5459139943122864, Test_Loss: 0.7595760822296143\n",
      "Epoch: 22, Train_Loss: 1.1501450538635254, Test_Loss: 0.9958294630050659\n",
      "Epoch: 22, Train_Loss: 9.331817626953125, Test_Loss: 0.8315780162811279 *\n",
      "Epoch: 22, Train_Loss: 1.3363609313964844, Test_Loss: 0.49724483489990234 *\n",
      "Epoch: 22, Train_Loss: 0.4575555920600891, Test_Loss: 0.46238645911216736 *\n",
      "Epoch: 22, Train_Loss: 4.671105861663818, Test_Loss: 0.45146289467811584 *\n",
      "Epoch: 22, Train_Loss: 1.9952807426452637, Test_Loss: 0.45453396439552307\n",
      "Epoch: 22, Train_Loss: 0.5475472807884216, Test_Loss: 0.5230548977851868\n",
      "Epoch: 22, Train_Loss: 0.45630186796188354, Test_Loss: 0.46061551570892334 *\n",
      "Epoch: 22, Train_Loss: 0.44276899099349976, Test_Loss: 0.5445040464401245\n",
      "Epoch: 22, Train_Loss: 0.46942272782325745, Test_Loss: 0.4433106482028961 *\n",
      "Epoch: 22, Train_Loss: 0.44108840823173523, Test_Loss: 0.5736031532287598\n",
      "Epoch: 22, Train_Loss: 0.4356515109539032, Test_Loss: 0.7456710338592529\n",
      "Epoch: 22, Train_Loss: 0.4295528829097748, Test_Loss: 0.562667191028595 *\n",
      "Epoch: 22, Train_Loss: 0.4303896725177765, Test_Loss: 0.5809220671653748\n",
      "Epoch: 22, Train_Loss: 0.43683698773384094, Test_Loss: 0.43564924597740173 *\n",
      "Epoch: 22, Train_Loss: 0.47938570380210876, Test_Loss: 0.4444432258605957\n",
      "Epoch: 22, Train_Loss: 0.4697263240814209, Test_Loss: 0.4590320289134979\n",
      "Epoch: 22, Train_Loss: 0.496379017829895, Test_Loss: 0.4594154357910156\n",
      "Epoch: 22, Train_Loss: 0.6252323389053345, Test_Loss: 0.46436890959739685\n",
      "Epoch: 22, Train_Loss: 0.5925465822219849, Test_Loss: 3.7494430541992188\n",
      "Epoch: 22, Train_Loss: 0.4455740749835968, Test_Loss: 3.3109610080718994 *\n",
      "Epoch: 22, Train_Loss: 0.44234487414360046, Test_Loss: 0.5378356575965881 *\n",
      "Epoch: 22, Train_Loss: 0.4901632070541382, Test_Loss: 0.5778151750564575\n",
      "Epoch: 22, Train_Loss: 0.43724197149276733, Test_Loss: 0.5431634783744812 *\n",
      "Epoch: 22, Train_Loss: 0.43136343359947205, Test_Loss: 0.45180922746658325 *\n",
      "Epoch: 22, Train_Loss: 0.4292145073413849, Test_Loss: 0.6129451990127563\n",
      "Epoch: 22, Train_Loss: 0.4298204183578491, Test_Loss: 0.6382050514221191\n",
      "Epoch: 22, Train_Loss: 0.4289610683917999, Test_Loss: 0.593579888343811 *\n",
      "Epoch: 22, Train_Loss: 0.4303756058216095, Test_Loss: 0.5704926252365112 *\n",
      "Epoch: 22, Train_Loss: 0.42800456285476685, Test_Loss: 0.5687589049339294 *\n",
      "Epoch: 22, Train_Loss: 0.4305945932865143, Test_Loss: 0.5842575430870056\n",
      "Epoch: 22, Train_Loss: 0.4416390359401703, Test_Loss: 0.5776233077049255 *\n",
      "Epoch: 22, Train_Loss: 0.4454466700553894, Test_Loss: 0.4838225245475769 *\n",
      "Epoch: 22, Train_Loss: 0.4688205122947693, Test_Loss: 0.5598340034484863\n",
      "Epoch: 22, Train_Loss: 0.4656476378440857, Test_Loss: 0.5143500566482544 *\n",
      "Epoch: 22, Train_Loss: 0.4473147392272949, Test_Loss: 0.4640901982784271 *\n",
      "Epoch: 22, Train_Loss: 8.004727363586426, Test_Loss: 0.5264266729354858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.6037315130233765, Test_Loss: 0.48402923345565796 *\n",
      "Epoch: 22, Train_Loss: 0.48375141620635986, Test_Loss: 0.5851472616195679\n",
      "Epoch: 22, Train_Loss: 0.5517390966415405, Test_Loss: 0.5459734201431274 *\n",
      "Epoch: 22, Train_Loss: 0.5638167858123779, Test_Loss: 0.5890810489654541\n",
      "Epoch: 22, Train_Loss: 0.5031342506408691, Test_Loss: 0.5270587801933289 *\n",
      "Epoch: 22, Train_Loss: 0.5368683934211731, Test_Loss: 0.5949981808662415\n",
      "Epoch: 22, Train_Loss: 0.5735492706298828, Test_Loss: 0.5177497267723083 *\n",
      "Epoch: 22, Train_Loss: 0.6524203419685364, Test_Loss: 0.5038993954658508 *\n",
      "Epoch: 22, Train_Loss: 0.5937873125076294, Test_Loss: 0.45177003741264343 *\n",
      "Epoch: 22, Train_Loss: 0.537565290927887, Test_Loss: 0.46803852915763855\n",
      "Epoch: 22, Train_Loss: 0.42945459485054016, Test_Loss: 0.450746089220047 *\n",
      "Epoch: 22, Train_Loss: 0.5117617249488831, Test_Loss: 0.44328781962394714 *\n",
      "Epoch: 22, Train_Loss: 0.45911863446235657, Test_Loss: 0.5710635185241699\n",
      "Epoch: 22, Train_Loss: 0.6520102620124817, Test_Loss: 0.4688633680343628 *\n",
      "Epoch: 22, Train_Loss: 0.45983821153640747, Test_Loss: 5.795199871063232\n",
      "Epoch: 22, Train_Loss: 0.4841027557849884, Test_Loss: 1.0001530647277832 *\n",
      "Epoch: 22, Train_Loss: 0.45857715606689453, Test_Loss: 0.43103834986686707 *\n",
      "Epoch: 22, Train_Loss: 0.44261229038238525, Test_Loss: 0.44515669345855713\n",
      "Epoch: 22, Train_Loss: 0.5067567229270935, Test_Loss: 0.4436414837837219 *\n",
      "Epoch: 22, Train_Loss: 0.4527316093444824, Test_Loss: 0.4534986913204193\n",
      "Epoch: 22, Train_Loss: 0.4363551437854767, Test_Loss: 0.4381817579269409 *\n",
      "Epoch: 22, Train_Loss: 0.4300791323184967, Test_Loss: 0.5391297340393066\n",
      "Epoch: 22, Train_Loss: 0.427489310503006, Test_Loss: 0.5323710441589355 *\n",
      "Epoch: 22, Train_Loss: 0.5478419065475464, Test_Loss: 0.43026113510131836 *\n",
      "Epoch: 22, Train_Loss: 5.63987922668457, Test_Loss: 0.46764880418777466\n",
      "Epoch: 22, Train_Loss: 0.43484434485435486, Test_Loss: 0.4363781213760376 *\n",
      "Epoch: 22, Train_Loss: 0.4296252131462097, Test_Loss: 0.4406835436820984\n",
      "Epoch: 22, Train_Loss: 0.44744473695755005, Test_Loss: 0.4422769248485565\n",
      "Epoch: 22, Train_Loss: 0.43921568989753723, Test_Loss: 0.46721363067626953\n",
      "Epoch: 22, Train_Loss: 0.433784544467926, Test_Loss: 0.4897986650466919\n",
      "Epoch: 22, Train_Loss: 0.4275285303592682, Test_Loss: 0.5387597680091858\n",
      "Epoch: 22, Train_Loss: 0.435606986284256, Test_Loss: 0.4737233817577362 *\n",
      "Epoch: 22, Train_Loss: 0.4371030628681183, Test_Loss: 0.45077380537986755 *\n",
      "Epoch: 22, Train_Loss: 0.4471414089202881, Test_Loss: 0.42991459369659424 *\n",
      "Epoch: 22, Train_Loss: 0.4501287043094635, Test_Loss: 0.4341277778148651\n",
      "Epoch: 22, Train_Loss: 0.42528489232063293, Test_Loss: 0.4299229085445404 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.4309627413749695, Test_Loss: 0.43146249651908875\n",
      "Epoch: 22, Train_Loss: 0.445600688457489, Test_Loss: 0.4332204759120941\n",
      "Epoch: 22, Train_Loss: 0.43165382742881775, Test_Loss: 0.4309982359409332 *\n",
      "Epoch: 22, Train_Loss: 0.43738728761672974, Test_Loss: 0.4362868070602417\n",
      "Epoch: 22, Train_Loss: 0.44459831714630127, Test_Loss: 0.4296119511127472 *\n",
      "Epoch: 22, Train_Loss: 0.4406198263168335, Test_Loss: 0.42903637886047363 *\n",
      "Epoch: 22, Train_Loss: 0.45225873589515686, Test_Loss: 0.44431447982788086\n",
      "Epoch: 22, Train_Loss: 0.43778499960899353, Test_Loss: 0.4616284966468811\n",
      "Epoch: 22, Train_Loss: 0.4441341161727905, Test_Loss: 0.44251736998558044 *\n",
      "Epoch: 22, Train_Loss: 0.4557918608188629, Test_Loss: 0.4586072862148285\n",
      "Epoch: 22, Train_Loss: 0.45477524399757385, Test_Loss: 0.8332040309906006\n",
      "Epoch: 22, Train_Loss: 0.46607154607772827, Test_Loss: 0.45256713032722473 *\n",
      "Epoch: 22, Train_Loss: 0.4534044563770294, Test_Loss: 0.44819456338882446 *\n",
      "Epoch: 22, Train_Loss: 0.4850177764892578, Test_Loss: 0.47345882654190063\n",
      "Epoch: 22, Train_Loss: 0.445000022649765, Test_Loss: 0.6617218255996704\n",
      "Epoch: 22, Train_Loss: 0.4451698064804077, Test_Loss: 0.4563824534416199 *\n",
      "Epoch: 22, Train_Loss: 0.44947782158851624, Test_Loss: 0.5190168023109436\n",
      "Epoch: 22, Train_Loss: 0.6124940514564514, Test_Loss: 0.48457151651382446 *\n",
      "Epoch: 22, Train_Loss: 0.4587883949279785, Test_Loss: 0.567284882068634\n",
      "Epoch: 22, Train_Loss: 0.4322894811630249, Test_Loss: 0.48937568068504333 *\n",
      "Epoch: 22, Train_Loss: 0.4227421283721924, Test_Loss: 0.45634621381759644 *\n",
      "Epoch: 22, Train_Loss: 0.4332348704338074, Test_Loss: 0.43183669447898865 *\n",
      "Epoch: 22, Train_Loss: 0.43662023544311523, Test_Loss: 0.4369714558124542\n",
      "Epoch: 22, Train_Loss: 0.42990246415138245, Test_Loss: 0.5879172682762146\n",
      "Epoch: 22, Train_Loss: 0.5206601023674011, Test_Loss: 0.9738864302635193\n",
      "Epoch: 22, Train_Loss: 3.8032875061035156, Test_Loss: 0.6333290934562683 *\n",
      "Epoch: 22, Train_Loss: 0.5027745962142944, Test_Loss: 0.9718886017799377\n",
      "Epoch: 22, Train_Loss: 0.4471715986728668, Test_Loss: 0.7235616445541382 *\n",
      "Epoch: 22, Train_Loss: 0.4316595196723938, Test_Loss: 0.7117615938186646 *\n",
      "Epoch: 22, Train_Loss: 0.4325162470340729, Test_Loss: 0.5648572444915771 *\n",
      "Epoch: 22, Train_Loss: 0.43712297081947327, Test_Loss: 0.454785019159317 *\n",
      "Epoch: 22, Train_Loss: 0.43448251485824585, Test_Loss: 0.42717689275741577 *\n",
      "Epoch: 22, Train_Loss: 0.42289409041404724, Test_Loss: 0.44002196192741394\n",
      "Epoch: 22, Train_Loss: 0.4229736030101776, Test_Loss: 0.5368243455886841\n",
      "Epoch: 22, Train_Loss: 0.42194032669067383, Test_Loss: 1.1457828283309937\n",
      "Epoch: 22, Train_Loss: 0.45820558071136475, Test_Loss: 0.6669356822967529 *\n",
      "Epoch: 22, Train_Loss: 0.4728805720806122, Test_Loss: 2.167285442352295\n",
      "Epoch: 22, Train_Loss: 0.4706498086452484, Test_Loss: 1.206042766571045 *\n",
      "Epoch: 22, Train_Loss: 0.45061594247817993, Test_Loss: 1.384225845336914\n",
      "Epoch: 22, Train_Loss: 0.4247667193412781, Test_Loss: 0.5578458309173584 *\n",
      "Epoch: 22, Train_Loss: 0.4937419891357422, Test_Loss: 0.4404594302177429 *\n",
      "Epoch: 22, Train_Loss: 0.5367223620414734, Test_Loss: 0.5475583672523499\n",
      "Epoch: 22, Train_Loss: 0.5347102880477905, Test_Loss: 1.497962236404419\n",
      "Epoch: 22, Train_Loss: 0.5175764560699463, Test_Loss: 1.1290210485458374 *\n",
      "Epoch: 22, Train_Loss: 0.46085256338119507, Test_Loss: 0.49519482254981995 *\n",
      "Epoch: 22, Train_Loss: 0.4381175935268402, Test_Loss: 0.44871649146080017 *\n",
      "Epoch: 22, Train_Loss: 0.43319374322891235, Test_Loss: 0.5114273428916931\n",
      "Epoch: 22, Train_Loss: 0.42071500420570374, Test_Loss: 0.883620023727417\n",
      "Epoch: 22, Train_Loss: 0.4240921437740326, Test_Loss: 0.6122955083847046 *\n",
      "Epoch: 22, Train_Loss: 0.42265284061431885, Test_Loss: 1.2004035711288452\n",
      "Epoch: 22, Train_Loss: 0.42406532168388367, Test_Loss: 0.8722989559173584 *\n",
      "Epoch: 22, Train_Loss: 0.42612898349761963, Test_Loss: 0.5159079432487488 *\n",
      "Epoch: 22, Train_Loss: 0.42606398463249207, Test_Loss: 0.43338918685913086 *\n",
      "Epoch: 22, Train_Loss: 0.43628016114234924, Test_Loss: 0.42827755212783813 *\n",
      "Epoch: 23, Train_Loss: 0.5033022165298462, Test_Loss: 0.4425990581512451 *\n",
      "Epoch: 23, Train_Loss: 0.45698535442352295, Test_Loss: 0.5047793388366699\n",
      "Epoch: 23, Train_Loss: 0.4565969705581665, Test_Loss: 0.7330409288406372\n",
      "Epoch: 23, Train_Loss: 0.48499155044555664, Test_Loss: 0.7094879150390625 *\n",
      "Epoch: 23, Train_Loss: 0.5474875569343567, Test_Loss: 0.515013575553894 *\n",
      "Epoch: 23, Train_Loss: 0.5710415840148926, Test_Loss: 0.44341790676116943 *\n",
      "Epoch: 23, Train_Loss: 0.5224717855453491, Test_Loss: 0.45190951228141785\n",
      "Epoch: 23, Train_Loss: 0.6283697485923767, Test_Loss: 0.45653095841407776\n",
      "Epoch: 23, Train_Loss: 0.7325407266616821, Test_Loss: 0.600197434425354\n",
      "Epoch: 23, Train_Loss: 0.4988425672054291, Test_Loss: 0.8678843975067139\n",
      "Epoch: 23, Train_Loss: 0.42920753359794617, Test_Loss: 0.9134315252304077\n",
      "Epoch: 23, Train_Loss: 2.170327663421631, Test_Loss: 0.503114640712738 *\n",
      "Epoch: 23, Train_Loss: 1.2361390590667725, Test_Loss: 0.4604954719543457 *\n",
      "Epoch: 23, Train_Loss: 0.4424498975276947, Test_Loss: 0.42663803696632385 *\n",
      "Epoch: 23, Train_Loss: 0.4362781345844269, Test_Loss: 0.42352086305618286 *\n",
      "Epoch: 23, Train_Loss: 0.4289762079715729, Test_Loss: 0.4361266791820526\n",
      "Epoch: 23, Train_Loss: 0.44788408279418945, Test_Loss: 0.4327981770038605 *\n",
      "Epoch: 23, Train_Loss: 0.42761364579200745, Test_Loss: 0.4530090391635895\n",
      "Epoch: 23, Train_Loss: 0.4370603561401367, Test_Loss: 0.5050356388092041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.4900483787059784, Test_Loss: 0.4705334007740021 *\n",
      "Epoch: 23, Train_Loss: 0.4471336305141449, Test_Loss: 0.5467242002487183\n",
      "Epoch: 23, Train_Loss: 0.44742366671562195, Test_Loss: 0.8631219863891602\n",
      "Epoch: 23, Train_Loss: 0.45728838443756104, Test_Loss: 0.766919732093811 *\n",
      "Epoch: 23, Train_Loss: 0.4440496861934662, Test_Loss: 0.5003337264060974 *\n",
      "Epoch: 23, Train_Loss: 0.4332747757434845, Test_Loss: 0.4672631323337555 *\n",
      "Epoch: 23, Train_Loss: 0.43495824933052063, Test_Loss: 0.46256276965141296 *\n",
      "Epoch: 23, Train_Loss: 0.4627764821052551, Test_Loss: 0.4643422067165375\n",
      "Epoch: 23, Train_Loss: 0.44475746154785156, Test_Loss: 0.4831055700778961\n",
      "Epoch: 23, Train_Loss: 0.426637202501297, Test_Loss: 1.5107104778289795\n",
      "Epoch: 23, Train_Loss: 0.43118810653686523, Test_Loss: 4.141876220703125\n",
      "Epoch: 23, Train_Loss: 0.4420253038406372, Test_Loss: 0.44449204206466675 *\n",
      "Epoch: 23, Train_Loss: 0.44828635454177856, Test_Loss: 0.4265585243701935 *\n",
      "Epoch: 23, Train_Loss: 0.4454202651977539, Test_Loss: 0.4307279586791992\n",
      "Epoch: 23, Train_Loss: 0.42681580781936646, Test_Loss: 0.43219971656799316\n",
      "Epoch: 23, Train_Loss: 0.42308130860328674, Test_Loss: 0.4265572130680084 *\n",
      "Epoch: 23, Train_Loss: 0.4185662567615509, Test_Loss: 0.43999096751213074\n",
      "Epoch: 23, Train_Loss: 0.4157091975212097, Test_Loss: 0.446517676115036\n",
      "Epoch: 23, Train_Loss: 0.4167919456958771, Test_Loss: 0.43393030762672424 *\n",
      "Epoch: 23, Train_Loss: 0.4173640012741089, Test_Loss: 0.4455229938030243\n",
      "Epoch: 23, Train_Loss: 0.42124801874160767, Test_Loss: 0.4617142677307129\n",
      "Epoch: 23, Train_Loss: 0.4167026877403259, Test_Loss: 0.47545352578163147\n",
      "Epoch: 23, Train_Loss: 0.4143104553222656, Test_Loss: 0.44474613666534424 *\n",
      "Epoch: 23, Train_Loss: 0.41773825883865356, Test_Loss: 0.4476813077926636\n",
      "Epoch: 23, Train_Loss: 0.42450830340385437, Test_Loss: 0.452082097530365\n",
      "Epoch: 23, Train_Loss: 0.4244605004787445, Test_Loss: 0.4369935691356659 *\n",
      "Epoch: 23, Train_Loss: 0.419881135225296, Test_Loss: 0.4223640263080597 *\n",
      "Epoch: 23, Train_Loss: 0.43089187145233154, Test_Loss: 0.4250323176383972\n",
      "Epoch: 23, Train_Loss: 0.4244905114173889, Test_Loss: 0.4262976348400116\n",
      "Epoch: 23, Train_Loss: 0.4143235385417938, Test_Loss: 0.4174281358718872 *\n",
      "Epoch: 23, Train_Loss: 0.41626542806625366, Test_Loss: 0.4365665316581726\n",
      "Epoch: 23, Train_Loss: 0.42206698656082153, Test_Loss: 0.41491296887397766 *\n",
      "Epoch: 23, Train_Loss: 0.4334721565246582, Test_Loss: 0.42054593563079834\n",
      "Epoch: 23, Train_Loss: 0.41676193475723267, Test_Loss: 0.4196232557296753 *\n",
      "Epoch: 23, Train_Loss: 0.4168131351470947, Test_Loss: 0.41955628991127014 *\n",
      "Epoch: 23, Train_Loss: 0.4153623878955841, Test_Loss: 0.41495344042778015 *\n",
      "Epoch: 23, Train_Loss: 0.4348524808883667, Test_Loss: 0.4180668294429779\n",
      "Epoch: 23, Train_Loss: 0.47972333431243896, Test_Loss: 0.42123255133628845\n",
      "Epoch: 23, Train_Loss: 0.4449767768383026, Test_Loss: 0.4190160036087036 *\n",
      "Epoch: 23, Train_Loss: 0.4314713180065155, Test_Loss: 0.4463541805744171\n",
      "Epoch: 23, Train_Loss: 0.413878470659256, Test_Loss: 0.46817880868911743\n",
      "Epoch: 23, Train_Loss: 0.4589556157588959, Test_Loss: 4.051634788513184\n",
      "Epoch: 23, Train_Loss: 0.43094873428344727, Test_Loss: 2.5115976333618164 *\n",
      "Epoch: 23, Train_Loss: 0.41695964336395264, Test_Loss: 0.4175606667995453 *\n",
      "Epoch: 23, Train_Loss: 0.42069128155708313, Test_Loss: 0.4205779433250427\n",
      "Epoch: 23, Train_Loss: 0.43641480803489685, Test_Loss: 0.4646048843860626\n",
      "Epoch: 23, Train_Loss: 0.5156670808792114, Test_Loss: 0.4550887942314148 *\n",
      "Epoch: 23, Train_Loss: 0.48126327991485596, Test_Loss: 0.43429023027420044 *\n",
      "Epoch: 23, Train_Loss: 0.4569566249847412, Test_Loss: 0.48277121782302856\n",
      "Epoch: 23, Train_Loss: 0.431172251701355, Test_Loss: 0.513665497303009\n",
      "Epoch: 23, Train_Loss: 0.42129313945770264, Test_Loss: 0.4180678129196167 *\n",
      "Epoch: 23, Train_Loss: 0.44007739424705505, Test_Loss: 0.4381033778190613\n",
      "Epoch: 23, Train_Loss: 0.4153040647506714, Test_Loss: 0.4329925775527954 *\n",
      "Epoch: 23, Train_Loss: 0.4275922477245331, Test_Loss: 0.4301837384700775 *\n",
      "Epoch: 23, Train_Loss: 0.42029064893722534, Test_Loss: 0.41820278763771057 *\n",
      "Epoch: 23, Train_Loss: 0.42458900809288025, Test_Loss: 0.5118744373321533\n",
      "Epoch: 23, Train_Loss: 0.5140710473060608, Test_Loss: 0.4620513617992401 *\n",
      "Epoch: 23, Train_Loss: 0.41715335845947266, Test_Loss: 0.4937434494495392\n",
      "Epoch: 23, Train_Loss: 0.46012812852859497, Test_Loss: 0.467205673456192 *\n",
      "Epoch: 23, Train_Loss: 0.4292224049568176, Test_Loss: 0.44942355155944824 *\n",
      "Epoch: 23, Train_Loss: 0.43141233921051025, Test_Loss: 0.4288505017757416 *\n",
      "Epoch: 23, Train_Loss: 0.4986070394515991, Test_Loss: 0.4172027111053467 *\n",
      "Epoch: 23, Train_Loss: 0.6068308353424072, Test_Loss: 0.42284438014030457\n",
      "Epoch: 23, Train_Loss: 0.41874995827674866, Test_Loss: 0.4224238395690918 *\n",
      "Epoch: 23, Train_Loss: 0.4432794749736786, Test_Loss: 0.42530232667922974\n",
      "Epoch: 23, Train_Loss: 0.41213250160217285, Test_Loss: 0.42238399386405945 *\n",
      "Epoch: 23, Train_Loss: 0.4121493995189667, Test_Loss: 0.41784143447875977 *\n",
      "Epoch: 23, Train_Loss: 0.416824072599411, Test_Loss: 0.42387834191322327\n",
      "Epoch: 23, Train_Loss: 0.42075809836387634, Test_Loss: 0.42149218916893005 *\n",
      "Epoch: 23, Train_Loss: 0.41852492094039917, Test_Loss: 0.42293643951416016\n",
      "Epoch: 23, Train_Loss: 0.4222656190395355, Test_Loss: 0.42276760935783386 *\n",
      "Epoch: 23, Train_Loss: 0.42181864380836487, Test_Loss: 0.43720510601997375\n",
      "Epoch: 23, Train_Loss: 0.4204215407371521, Test_Loss: 0.4511895775794983\n",
      "Epoch: 23, Train_Loss: 0.4202921688556671, Test_Loss: 0.7177188396453857\n",
      "Epoch: 23, Train_Loss: 0.4198601245880127, Test_Loss: 0.42512083053588867 *\n",
      "Epoch: 23, Train_Loss: 0.412706196308136, Test_Loss: 0.45478156208992004\n",
      "Epoch: 23, Train_Loss: 0.409829318523407, Test_Loss: 0.5784083008766174\n",
      "Epoch: 23, Train_Loss: 0.4259926378726959, Test_Loss: 0.6773310899734497\n",
      "Epoch: 23, Train_Loss: 0.4270751178264618, Test_Loss: 0.5106906890869141 *\n",
      "Epoch: 23, Train_Loss: 0.4397251009941101, Test_Loss: 0.4583955705165863 *\n",
      "Epoch: 23, Train_Loss: 0.41098496317863464, Test_Loss: 0.5059229135513306\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.43854960799217224, Test_Loss: 0.5777067542076111\n",
      "Epoch: 23, Train_Loss: 0.4383487105369568, Test_Loss: 0.426777184009552 *\n",
      "Epoch: 23, Train_Loss: 0.4368758797645569, Test_Loss: 0.4516583979129791\n",
      "Epoch: 23, Train_Loss: 0.41812819242477417, Test_Loss: 0.41341432929039 *\n",
      "Epoch: 23, Train_Loss: 0.4320186674594879, Test_Loss: 0.4726027250289917\n",
      "Epoch: 23, Train_Loss: 0.40901628136634827, Test_Loss: 0.502436637878418\n",
      "Epoch: 23, Train_Loss: 0.42354023456573486, Test_Loss: 0.9715688228607178\n",
      "Epoch: 23, Train_Loss: 0.4201221764087677, Test_Loss: 0.5735112428665161 *\n",
      "Epoch: 23, Train_Loss: 0.4294735789299011, Test_Loss: 1.0830004215240479\n",
      "Epoch: 23, Train_Loss: 1.4934090375900269, Test_Loss: 0.9030404686927795 *\n",
      "Epoch: 23, Train_Loss: 4.522406101226807, Test_Loss: 0.6328896284103394 *\n",
      "Epoch: 23, Train_Loss: 0.7669130563735962, Test_Loss: 0.5967867374420166 *\n",
      "Epoch: 23, Train_Loss: 0.4389832317829132, Test_Loss: 0.4397318363189697 *\n",
      "Epoch: 23, Train_Loss: 0.4218914210796356, Test_Loss: 0.43134167790412903 *\n",
      "Epoch: 23, Train_Loss: 0.5441702604293823, Test_Loss: 0.43232429027557373\n",
      "Epoch: 23, Train_Loss: 0.49048468470573425, Test_Loss: 0.6118682622909546\n",
      "Epoch: 23, Train_Loss: 0.4245082139968872, Test_Loss: 0.8498954772949219\n",
      "Epoch: 23, Train_Loss: 0.40974557399749756, Test_Loss: 0.782448410987854 *\n",
      "Epoch: 23, Train_Loss: 0.4741355776786804, Test_Loss: 2.011626958847046\n",
      "Epoch: 23, Train_Loss: 0.43164756894111633, Test_Loss: 1.4116015434265137 *\n",
      "Epoch: 23, Train_Loss: 0.41633668541908264, Test_Loss: 0.9792252779006958 *\n",
      "Epoch: 23, Train_Loss: 0.7269091606140137, Test_Loss: 0.6466319561004639 *\n",
      "Epoch: 23, Train_Loss: 1.379237174987793, Test_Loss: 0.42721179127693176 *\n",
      "Epoch: 23, Train_Loss: 1.2184261083602905, Test_Loss: 0.4764149487018585\n",
      "Epoch: 23, Train_Loss: 0.5008402466773987, Test_Loss: 1.3392759561538696\n",
      "Epoch: 23, Train_Loss: 0.5733815431594849, Test_Loss: 1.3093894720077515 *\n",
      "Epoch: 23, Train_Loss: 2.3814375400543213, Test_Loss: 0.47408148646354675 *\n",
      "Epoch: 23, Train_Loss: 1.1887047290802002, Test_Loss: 0.4788285791873932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.41331353783607483, Test_Loss: 0.45000243186950684 *\n",
      "Epoch: 23, Train_Loss: 0.4240514934062958, Test_Loss: 0.7384381294250488\n",
      "Epoch: 23, Train_Loss: 0.8830633759498596, Test_Loss: 0.5877968072891235 *\n",
      "Epoch: 23, Train_Loss: 1.1722793579101562, Test_Loss: 1.0266835689544678\n",
      "Epoch: 23, Train_Loss: 1.0240839719772339, Test_Loss: 0.7999213337898254 *\n",
      "Epoch: 23, Train_Loss: 0.41883963346481323, Test_Loss: 0.6133063435554504 *\n",
      "Epoch: 23, Train_Loss: 0.4596680998802185, Test_Loss: 0.5889806151390076 *\n",
      "Epoch: 23, Train_Loss: 0.670707106590271, Test_Loss: 0.702014684677124\n",
      "Epoch: 23, Train_Loss: 0.684424638748169, Test_Loss: 0.7182379364967346\n",
      "Epoch: 23, Train_Loss: 0.4211452603340149, Test_Loss: 0.4997790455818176 *\n",
      "Epoch: 23, Train_Loss: 0.441861093044281, Test_Loss: 0.8458337187767029\n",
      "Epoch: 23, Train_Loss: 0.4556032717227936, Test_Loss: 0.6720453500747681 *\n",
      "Epoch: 23, Train_Loss: 0.45971792936325073, Test_Loss: 0.4748477637767792 *\n",
      "Epoch: 23, Train_Loss: 0.50431227684021, Test_Loss: 0.4390754997730255 *\n",
      "Epoch: 23, Train_Loss: 0.639758825302124, Test_Loss: 0.4311826825141907 *\n",
      "Epoch: 23, Train_Loss: 0.5105087757110596, Test_Loss: 0.430202841758728 *\n",
      "Epoch: 23, Train_Loss: 0.4861055016517639, Test_Loss: 0.4998859763145447\n",
      "Epoch: 23, Train_Loss: 0.5380187034606934, Test_Loss: 0.7399023771286011\n",
      "Epoch: 23, Train_Loss: 0.5513648390769958, Test_Loss: 0.8699080348014832\n",
      "Epoch: 23, Train_Loss: 0.6204253435134888, Test_Loss: 0.47615063190460205 *\n",
      "Epoch: 23, Train_Loss: 0.7107830047607422, Test_Loss: 0.6755472421646118\n",
      "Epoch: 23, Train_Loss: 0.45654430985450745, Test_Loss: 0.4376319646835327 *\n",
      "Epoch: 23, Train_Loss: 0.4928531348705292, Test_Loss: 0.4566211700439453\n",
      "Epoch: 23, Train_Loss: 0.46683651208877563, Test_Loss: 0.4512321650981903 *\n",
      "Epoch: 23, Train_Loss: 0.42933472990989685, Test_Loss: 0.5398083329200745\n",
      "Epoch: 23, Train_Loss: 0.40936538577079773, Test_Loss: 0.4436852037906647 *\n",
      "Epoch: 23, Train_Loss: 0.4123491644859314, Test_Loss: 0.745952844619751\n",
      "Epoch: 23, Train_Loss: 0.40910282731056213, Test_Loss: 0.521956741809845 *\n",
      "Epoch: 23, Train_Loss: 0.4193617105484009, Test_Loss: 0.5002327561378479 *\n",
      "Epoch: 23, Train_Loss: 0.4264972507953644, Test_Loss: 0.8545564413070679\n",
      "Epoch: 23, Train_Loss: 0.43644508719444275, Test_Loss: 1.013076663017273\n",
      "Epoch: 23, Train_Loss: 0.4559539556503296, Test_Loss: 0.6212130784988403 *\n",
      "Epoch: 23, Train_Loss: 0.4885757565498352, Test_Loss: 0.5060311555862427 *\n",
      "Epoch: 23, Train_Loss: 0.6395741701126099, Test_Loss: 0.4967629313468933 *\n",
      "Epoch: 23, Train_Loss: 0.5689730048179626, Test_Loss: 0.49675628542900085 *\n",
      "Epoch: 23, Train_Loss: 0.4422110617160797, Test_Loss: 0.5621327757835388\n",
      "Epoch: 23, Train_Loss: 0.46153250336647034, Test_Loss: 0.7454349994659424\n",
      "Epoch: 23, Train_Loss: 0.6778371334075928, Test_Loss: 4.865942001342773\n",
      "Epoch: 23, Train_Loss: 0.5434262156486511, Test_Loss: 0.512786328792572 *\n",
      "Epoch: 23, Train_Loss: 0.47151124477386475, Test_Loss: 0.45516517758369446 *\n",
      "Epoch: 23, Train_Loss: 0.4521177113056183, Test_Loss: 0.429042786359787 *\n",
      "Epoch: 23, Train_Loss: 0.7718815803527832, Test_Loss: 0.4168057143688202 *\n",
      "Epoch: 23, Train_Loss: 0.7256388664245605, Test_Loss: 0.43131259083747864\n",
      "Epoch: 23, Train_Loss: 0.5229433178901672, Test_Loss: 0.4420669972896576\n",
      "Epoch: 23, Train_Loss: 0.43312013149261475, Test_Loss: 0.52591472864151\n",
      "Epoch: 23, Train_Loss: 0.42816174030303955, Test_Loss: 0.4306586682796478 *\n",
      "Epoch: 23, Train_Loss: 0.8091264367103577, Test_Loss: 0.45111358165740967\n",
      "Epoch: 23, Train_Loss: 1.0859601497650146, Test_Loss: 0.4616636037826538\n",
      "Epoch: 23, Train_Loss: 0.5500366687774658, Test_Loss: 0.5028075575828552\n",
      "Epoch: 23, Train_Loss: 0.46087437868118286, Test_Loss: 0.41632726788520813 *\n",
      "Epoch: 23, Train_Loss: 0.42860326170921326, Test_Loss: 0.4206998944282532\n",
      "Epoch: 23, Train_Loss: 0.4159338176250458, Test_Loss: 0.4362795948982239\n",
      "Epoch: 23, Train_Loss: 0.7401171922683716, Test_Loss: 0.4158822000026703 *\n",
      "Epoch: 23, Train_Loss: 0.46073636412620544, Test_Loss: 0.44129258394241333\n",
      "Epoch: 23, Train_Loss: 0.4282335042953491, Test_Loss: 0.42961373925209045 *\n",
      "Epoch: 23, Train_Loss: 0.4165053367614746, Test_Loss: 0.46721312403678894\n",
      "Epoch: 23, Train_Loss: 0.5225059986114502, Test_Loss: 0.4155585467815399 *\n",
      "Epoch: 23, Train_Loss: 16.26099395751953, Test_Loss: 0.4519573748111725\n",
      "Epoch: 23, Train_Loss: 0.5932016968727112, Test_Loss: 0.4083431363105774 *\n",
      "Epoch: 23, Train_Loss: 1.4302760362625122, Test_Loss: 0.44535982608795166\n",
      "Epoch: 23, Train_Loss: 1.7484030723571777, Test_Loss: 0.4779723286628723\n",
      "Epoch: 23, Train_Loss: 0.42635899782180786, Test_Loss: 0.44016796350479126 *\n",
      "Epoch: 23, Train_Loss: 0.4848152697086334, Test_Loss: 0.412103533744812 *\n",
      "Epoch: 23, Train_Loss: 2.0852341651916504, Test_Loss: 0.45507416129112244\n",
      "Epoch: 23, Train_Loss: 8.609711647033691, Test_Loss: 0.4551229476928711\n",
      "Epoch: 23, Train_Loss: 0.6935199499130249, Test_Loss: 0.4377419948577881 *\n",
      "Epoch: 23, Train_Loss: 0.46636420488357544, Test_Loss: 0.43391966819763184 *\n",
      "Epoch: 23, Train_Loss: 5.667305946350098, Test_Loss: 0.5800018310546875\n",
      "Epoch: 23, Train_Loss: 0.7638512253761292, Test_Loss: 3.1575653553009033\n",
      "Epoch: 23, Train_Loss: 0.47834619879722595, Test_Loss: 4.382585525512695\n",
      "Epoch: 23, Train_Loss: 0.4254598617553711, Test_Loss: 0.5221070647239685 *\n",
      "Epoch: 23, Train_Loss: 0.42743536829948425, Test_Loss: 0.49643465876579285 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.5221724510192871, Test_Loss: 0.5393041968345642\n",
      "Epoch: 23, Train_Loss: 0.4046279788017273, Test_Loss: 0.5849893093109131\n",
      "Epoch: 23, Train_Loss: 0.4183850884437561, Test_Loss: 0.5162914395332336 *\n",
      "Epoch: 23, Train_Loss: 0.4073649048805237, Test_Loss: 0.6596862077713013\n",
      "Epoch: 23, Train_Loss: 0.4197552800178528, Test_Loss: 1.0743772983551025\n",
      "Epoch: 23, Train_Loss: 0.47446054220199585, Test_Loss: 0.523233950138092 *\n",
      "Epoch: 23, Train_Loss: 0.46516311168670654, Test_Loss: 0.5126748085021973 *\n",
      "Epoch: 23, Train_Loss: 0.4332748055458069, Test_Loss: 0.509547233581543 *\n",
      "Epoch: 23, Train_Loss: 0.5285393595695496, Test_Loss: 0.45942944288253784 *\n",
      "Epoch: 23, Train_Loss: 0.6191985607147217, Test_Loss: 0.48384222388267517\n",
      "Epoch: 23, Train_Loss: 0.530380129814148, Test_Loss: 0.5150331854820251\n",
      "Epoch: 23, Train_Loss: 0.430420845746994, Test_Loss: 0.5172939300537109\n",
      "Epoch: 23, Train_Loss: 0.407089501619339, Test_Loss: 0.609656035900116\n",
      "Epoch: 23, Train_Loss: 0.46474146842956543, Test_Loss: 0.4708462655544281 *\n",
      "Epoch: 23, Train_Loss: 0.40462082624435425, Test_Loss: 0.43581053614616394 *\n",
      "Epoch: 23, Train_Loss: 0.4029250741004944, Test_Loss: 0.4115796983242035 *\n",
      "Epoch: 23, Train_Loss: 0.40446901321411133, Test_Loss: 0.4359392523765564\n",
      "Epoch: 23, Train_Loss: 0.4027692973613739, Test_Loss: 0.44327330589294434\n",
      "Epoch: 23, Train_Loss: 0.40238842368125916, Test_Loss: 0.4255805015563965 *\n",
      "Epoch: 23, Train_Loss: 0.4088456630706787, Test_Loss: 0.4173157215118408 *\n",
      "Epoch: 23, Train_Loss: 0.4034934937953949, Test_Loss: 0.4108698070049286 *\n",
      "Epoch: 23, Train_Loss: 0.4064020812511444, Test_Loss: 0.42140257358551025\n",
      "Epoch: 23, Train_Loss: 0.4201348125934601, Test_Loss: 0.41183197498321533 *\n",
      "Epoch: 23, Train_Loss: 0.43735527992248535, Test_Loss: 0.4051976203918457 *\n",
      "Epoch: 23, Train_Loss: 0.4355333745479584, Test_Loss: 0.40924280881881714\n",
      "Epoch: 23, Train_Loss: 0.4167340397834778, Test_Loss: 0.44366222620010376\n",
      "Epoch: 23, Train_Loss: 0.5492258667945862, Test_Loss: 0.40476691722869873 *\n",
      "Epoch: 23, Train_Loss: 8.425483703613281, Test_Loss: 0.42018452286720276\n",
      "Epoch: 23, Train_Loss: 0.4661322236061096, Test_Loss: 0.7665657997131348\n",
      "Epoch: 23, Train_Loss: 0.4140692949295044, Test_Loss: 0.4867842197418213 *\n",
      "Epoch: 23, Train_Loss: 0.44319823384284973, Test_Loss: 0.4716525673866272 *\n",
      "Epoch: 23, Train_Loss: 0.469402015209198, Test_Loss: 0.4429526925086975 *\n",
      "Epoch: 23, Train_Loss: 0.43062758445739746, Test_Loss: 0.5343644618988037\n",
      "Epoch: 23, Train_Loss: 0.4644438624382019, Test_Loss: 0.4886232614517212 *\n",
      "Epoch: 23, Train_Loss: 0.4925450086593628, Test_Loss: 0.43124932050704956 *\n",
      "Epoch: 23, Train_Loss: 0.6593879461288452, Test_Loss: 0.4841991364955902\n",
      "Epoch: 23, Train_Loss: 0.5424990653991699, Test_Loss: 0.634806215763092\n",
      "Epoch: 23, Train_Loss: 0.48276492953300476, Test_Loss: 0.5150141716003418 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.40322256088256836, Test_Loss: 0.4759965240955353 *\n",
      "Epoch: 23, Train_Loss: 0.47360238432884216, Test_Loss: 0.4156205356121063 *\n",
      "Epoch: 23, Train_Loss: 0.4299193024635315, Test_Loss: 0.4185212254524231\n",
      "Epoch: 23, Train_Loss: 0.6084471940994263, Test_Loss: 0.46398860216140747\n",
      "Epoch: 23, Train_Loss: 0.43212205171585083, Test_Loss: 0.7785031795501709\n",
      "Epoch: 23, Train_Loss: 0.4524359405040741, Test_Loss: 0.6880316734313965 *\n",
      "Epoch: 23, Train_Loss: 0.42394593358039856, Test_Loss: 0.9720332622528076\n",
      "Epoch: 23, Train_Loss: 0.4255702793598175, Test_Loss: 0.7653931379318237 *\n",
      "Epoch: 23, Train_Loss: 0.48177558183670044, Test_Loss: 0.6132985949516296 *\n",
      "Epoch: 23, Train_Loss: 0.44806158542633057, Test_Loss: 0.6258900761604309\n",
      "Epoch: 23, Train_Loss: 0.41254696249961853, Test_Loss: 0.4600217938423157 *\n",
      "Epoch: 23, Train_Loss: 0.42194026708602905, Test_Loss: 0.41107767820358276 *\n",
      "Epoch: 23, Train_Loss: 0.4250820279121399, Test_Loss: 0.40927672386169434 *\n",
      "Epoch: 23, Train_Loss: 0.8920911550521851, Test_Loss: 0.4971727728843689\n",
      "Epoch: 23, Train_Loss: 5.222484111785889, Test_Loss: 0.7124139666557312\n",
      "Epoch: 23, Train_Loss: 0.40244221687316895, Test_Loss: 0.9750853776931763\n",
      "Epoch: 23, Train_Loss: 0.40561047196388245, Test_Loss: 1.3186973333358765\n",
      "Epoch: 23, Train_Loss: 0.40339580178260803, Test_Loss: 1.6499300003051758\n",
      "Epoch: 23, Train_Loss: 0.40342721343040466, Test_Loss: 0.9746894240379333 *\n",
      "Epoch: 23, Train_Loss: 0.399571031332016, Test_Loss: 0.8335227966308594 *\n",
      "Epoch: 23, Train_Loss: 0.3983524739742279, Test_Loss: 0.4124131500720978 *\n",
      "Epoch: 23, Train_Loss: 0.4054913818836212, Test_Loss: 0.41982147097587585\n",
      "Epoch: 23, Train_Loss: 0.4133954346179962, Test_Loss: 0.944380521774292\n",
      "Epoch: 23, Train_Loss: 0.4263535439968109, Test_Loss: 1.6014118194580078\n",
      "Epoch: 23, Train_Loss: 0.40754857659339905, Test_Loss: 0.45033347606658936 *\n",
      "Epoch: 23, Train_Loss: 0.39869943261146545, Test_Loss: 0.4707789421081543\n",
      "Epoch: 23, Train_Loss: 0.40004774928092957, Test_Loss: 0.41761744022369385 *\n",
      "Epoch: 23, Train_Loss: 0.4172506630420685, Test_Loss: 0.675281286239624\n",
      "Epoch: 23, Train_Loss: 0.4013301432132721, Test_Loss: 0.6699703931808472 *\n",
      "Epoch: 23, Train_Loss: 0.3990665376186371, Test_Loss: 0.8546053171157837\n",
      "Epoch: 23, Train_Loss: 0.41597503423690796, Test_Loss: 0.9687095880508423\n",
      "Epoch: 23, Train_Loss: 0.4158717691898346, Test_Loss: 0.7192549705505371 *\n",
      "Epoch: 23, Train_Loss: 0.4284220039844513, Test_Loss: 0.4086230993270874 *\n",
      "Epoch: 23, Train_Loss: 0.398068368434906, Test_Loss: 0.40212082862854004 *\n",
      "Epoch: 23, Train_Loss: 0.41207653284072876, Test_Loss: 0.4121547043323517\n",
      "Epoch: 23, Train_Loss: 0.4238430857658386, Test_Loss: 0.4800946116447449\n",
      "Epoch: 23, Train_Loss: 0.43194371461868286, Test_Loss: 0.6104209423065186\n",
      "Epoch: 23, Train_Loss: 0.43582189083099365, Test_Loss: 0.6893891096115112\n",
      "Epoch: 23, Train_Loss: 0.4392988085746765, Test_Loss: 0.5281312465667725 *\n",
      "Epoch: 23, Train_Loss: 0.4590872824192047, Test_Loss: 0.45873332023620605 *\n",
      "Epoch: 23, Train_Loss: 0.4194711446762085, Test_Loss: 0.4137364327907562 *\n",
      "Epoch: 23, Train_Loss: 0.43336087465286255, Test_Loss: 0.40310677886009216 *\n",
      "Epoch: 23, Train_Loss: 0.40952542424201965, Test_Loss: 0.48446905612945557\n",
      "Epoch: 23, Train_Loss: 0.6051723957061768, Test_Loss: 0.8731005191802979\n",
      "Epoch: 23, Train_Loss: 0.42173469066619873, Test_Loss: 0.9509772658348083\n",
      "Epoch: 23, Train_Loss: 0.39756619930267334, Test_Loss: 0.5154194235801697 *\n",
      "Epoch: 23, Train_Loss: 0.39537712931632996, Test_Loss: 0.4641883373260498 *\n",
      "Epoch: 23, Train_Loss: 0.39416995644569397, Test_Loss: 0.40844425559043884 *\n",
      "Epoch: 23, Train_Loss: 0.3963087797164917, Test_Loss: 0.4200078845024109\n",
      "Epoch: 23, Train_Loss: 0.3963821828365326, Test_Loss: 0.416731059551239 *\n",
      "Epoch: 23, Train_Loss: 0.9295821189880371, Test_Loss: 0.4329693615436554\n",
      "Epoch: 23, Train_Loss: 3.6267919540405273, Test_Loss: 0.4321483075618744 *\n",
      "Epoch: 23, Train_Loss: 0.4102381467819214, Test_Loss: 0.4415338635444641\n",
      "Epoch: 23, Train_Loss: 0.40395715832710266, Test_Loss: 0.4099026322364807 *\n",
      "Epoch: 23, Train_Loss: 0.3984920382499695, Test_Loss: 0.45291420817375183\n",
      "Epoch: 23, Train_Loss: 0.3963950574398041, Test_Loss: 0.7510960102081299\n",
      "Epoch: 23, Train_Loss: 0.3982257843017578, Test_Loss: 0.5008484125137329 *\n",
      "Epoch: 23, Train_Loss: 0.3980976641178131, Test_Loss: 0.6382613778114319\n",
      "Epoch: 23, Train_Loss: 0.3943343460559845, Test_Loss: 0.43537309765815735 *\n",
      "Epoch: 23, Train_Loss: 0.3955223560333252, Test_Loss: 0.43925440311431885\n",
      "Epoch: 23, Train_Loss: 0.39633697271347046, Test_Loss: 0.428926944732666 *\n",
      "Epoch: 23, Train_Loss: 0.4261149764060974, Test_Loss: 0.44257986545562744\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.4260481297969818, Test_Loss: 0.4801045358181\n",
      "Epoch: 23, Train_Loss: 0.4360333979129791, Test_Loss: 4.593149185180664\n",
      "Epoch: 23, Train_Loss: 0.42548397183418274, Test_Loss: 0.784783124923706 *\n",
      "Epoch: 23, Train_Loss: 0.40010708570480347, Test_Loss: 0.4214133620262146 *\n",
      "Epoch: 23, Train_Loss: 0.5031068921089172, Test_Loss: 0.410218745470047 *\n",
      "Epoch: 23, Train_Loss: 0.5110476016998291, Test_Loss: 0.40269994735717773 *\n",
      "Epoch: 23, Train_Loss: 0.5162902474403381, Test_Loss: 0.40279778838157654\n",
      "Epoch: 23, Train_Loss: 0.47110050916671753, Test_Loss: 0.4259488582611084\n",
      "Epoch: 23, Train_Loss: 0.41954344511032104, Test_Loss: 0.48802345991134644\n",
      "Epoch: 23, Train_Loss: 0.40292638540267944, Test_Loss: 0.41848570108413696 *\n",
      "Epoch: 23, Train_Loss: 0.3957883417606354, Test_Loss: 0.4661078155040741\n",
      "Epoch: 23, Train_Loss: 0.39605623483657837, Test_Loss: 0.44802412390708923 *\n",
      "Epoch: 23, Train_Loss: 0.4054000675678253, Test_Loss: 0.5109245181083679\n",
      "Epoch: 23, Train_Loss: 0.400073766708374, Test_Loss: 0.4346229135990143 *\n",
      "Epoch: 23, Train_Loss: 0.3953550159931183, Test_Loss: 0.42726340889930725 *\n",
      "Epoch: 23, Train_Loss: 0.39249759912490845, Test_Loss: 0.46826353669166565\n",
      "Epoch: 23, Train_Loss: 0.4005395174026489, Test_Loss: 0.4257888197898865 *\n",
      "Epoch: 23, Train_Loss: 0.4344879388809204, Test_Loss: 0.4057908058166504 *\n",
      "Epoch: 23, Train_Loss: 0.4806176424026489, Test_Loss: 0.4202263355255127\n",
      "Epoch: 23, Train_Loss: 0.4246090054512024, Test_Loss: 0.4441002607345581\n",
      "Epoch: 23, Train_Loss: 0.44607505202293396, Test_Loss: 0.4396004378795624 *\n",
      "Epoch: 23, Train_Loss: 0.4598717987537384, Test_Loss: 0.4427371919155121\n",
      "Epoch: 23, Train_Loss: 0.4777258634567261, Test_Loss: 0.4451937675476074\n",
      "Epoch: 23, Train_Loss: 0.5289928317070007, Test_Loss: 0.4745252728462219\n",
      "Epoch: 23, Train_Loss: 0.4912460446357727, Test_Loss: 0.46347683668136597 *\n",
      "Epoch: 23, Train_Loss: 0.5176650285720825, Test_Loss: 0.45289963483810425 *\n",
      "Epoch: 23, Train_Loss: 0.6883347630500793, Test_Loss: 0.40841391682624817 *\n",
      "Epoch: 23, Train_Loss: 0.40310588479042053, Test_Loss: 0.4291228652000427\n",
      "Epoch: 23, Train_Loss: 0.43763065338134766, Test_Loss: 0.4255937337875366 *\n",
      "Epoch: 23, Train_Loss: 2.1676766872406006, Test_Loss: 0.40622979402542114 *\n",
      "Epoch: 23, Train_Loss: 0.7681920528411865, Test_Loss: 0.4131607413291931\n",
      "Epoch: 23, Train_Loss: 0.41015616059303284, Test_Loss: 0.5005567073822021\n",
      "Epoch: 23, Train_Loss: 0.40559977293014526, Test_Loss: 1.199030876159668\n",
      "Epoch: 23, Train_Loss: 0.403560072183609, Test_Loss: 5.1905646324157715\n",
      "Epoch: 23, Train_Loss: 0.437718003988266, Test_Loss: 0.4078123867511749 *\n",
      "Epoch: 23, Train_Loss: 0.4180973470211029, Test_Loss: 0.39998745918273926 *\n",
      "Epoch: 23, Train_Loss: 0.4194042980670929, Test_Loss: 0.41608914732933044\n",
      "Epoch: 23, Train_Loss: 0.4702426791191101, Test_Loss: 0.41232138872146606 *\n",
      "Epoch: 23, Train_Loss: 0.4222927987575531, Test_Loss: 0.4400860369205475\n",
      "Epoch: 23, Train_Loss: 0.42295151948928833, Test_Loss: 0.4042549729347229 *\n",
      "Epoch: 23, Train_Loss: 0.40900105237960815, Test_Loss: 0.49939844012260437\n",
      "Epoch: 23, Train_Loss: 0.4089851975440979, Test_Loss: 0.4159999191761017 *\n",
      "Epoch: 23, Train_Loss: 0.4188612103462219, Test_Loss: 0.40610194206237793 *\n",
      "Epoch: 23, Train_Loss: 0.41272303462028503, Test_Loss: 0.41743654012680054\n",
      "Epoch: 23, Train_Loss: 0.4376956820487976, Test_Loss: 0.4318615794181824\n",
      "Epoch: 23, Train_Loss: 0.40770798921585083, Test_Loss: 0.3975808024406433 *\n",
      "Epoch: 23, Train_Loss: 0.3920937776565552, Test_Loss: 0.43894681334495544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.4033065438270569, Test_Loss: 0.4619203209877014\n",
      "Epoch: 23, Train_Loss: 0.4109833836555481, Test_Loss: 0.4485630393028259 *\n",
      "Epoch: 23, Train_Loss: 0.40263456106185913, Test_Loss: 0.4290492534637451 *\n",
      "Epoch: 23, Train_Loss: 0.3960587978363037, Test_Loss: 0.4221004545688629 *\n",
      "Epoch: 23, Train_Loss: 0.38914021849632263, Test_Loss: 0.4269119203090668\n",
      "Epoch: 23, Train_Loss: 0.39002373814582825, Test_Loss: 0.397109717130661 *\n",
      "Epoch: 23, Train_Loss: 0.3900560140609741, Test_Loss: 0.39569219946861267 *\n",
      "Epoch: 23, Train_Loss: 0.3909304738044739, Test_Loss: 0.3999640941619873\n",
      "Epoch: 23, Train_Loss: 0.39121517539024353, Test_Loss: 0.4051557779312134\n",
      "Epoch: 23, Train_Loss: 0.39464712142944336, Test_Loss: 0.40016821026802063 *\n",
      "Epoch: 23, Train_Loss: 0.3909819722175598, Test_Loss: 0.39283832907676697 *\n",
      "Epoch: 23, Train_Loss: 0.3902356028556824, Test_Loss: 0.3984726369380951\n",
      "Epoch: 23, Train_Loss: 0.39048051834106445, Test_Loss: 0.39297622442245483 *\n",
      "Epoch: 23, Train_Loss: 0.3907451033592224, Test_Loss: 0.41271859407424927\n",
      "Epoch: 23, Train_Loss: 0.39514482021331787, Test_Loss: 0.3988278806209564 *\n",
      "Epoch: 23, Train_Loss: 0.39381298422813416, Test_Loss: 0.3968594968318939 *\n",
      "Epoch: 23, Train_Loss: 0.3942475914955139, Test_Loss: 0.44785797595977783\n",
      "Epoch: 23, Train_Loss: 0.4039338529109955, Test_Loss: 0.5347033739089966\n",
      "Epoch: 23, Train_Loss: 0.39221277832984924, Test_Loss: 0.5634492635726929\n",
      "Epoch: 23, Train_Loss: 0.3893221616744995, Test_Loss: 0.409828245639801 *\n",
      "Epoch: 23, Train_Loss: 0.39003053307533264, Test_Loss: 0.4514453411102295\n",
      "Epoch: 23, Train_Loss: 0.3942413032054901, Test_Loss: 0.5273526310920715\n",
      "Epoch: 23, Train_Loss: 0.40491876006126404, Test_Loss: 0.61509108543396\n",
      "Epoch: 23, Train_Loss: 0.3918052613735199, Test_Loss: 0.4081602692604065 *\n",
      "Epoch: 23, Train_Loss: 0.3886493444442749, Test_Loss: 0.4801403880119324\n",
      "Epoch: 23, Train_Loss: 0.3885861337184906, Test_Loss: 0.519690990447998\n",
      "Epoch: 23, Train_Loss: 0.4376008212566376, Test_Loss: 0.4620829224586487 *\n",
      "Epoch: 23, Train_Loss: 0.45373615622520447, Test_Loss: 0.4434065520763397 *\n",
      "Epoch: 23, Train_Loss: 0.4074438512325287, Test_Loss: 0.39840930700302124 *\n",
      "Epoch: 23, Train_Loss: 0.3974980115890503, Test_Loss: 0.3975995182991028 *\n",
      "Epoch: 23, Train_Loss: 0.3901575207710266, Test_Loss: 0.4127786457538605\n",
      "Epoch: 23, Train_Loss: 0.42703303694725037, Test_Loss: 0.8060250878334045\n",
      "Epoch: 23, Train_Loss: 0.3956299424171448, Test_Loss: 0.7448396682739258 *\n",
      "Epoch: 23, Train_Loss: 0.3968506157398224, Test_Loss: 0.9325979948043823\n",
      "Epoch: 23, Train_Loss: 0.4035675823688507, Test_Loss: 1.1238964796066284\n",
      "Epoch: 23, Train_Loss: 0.4003213047981262, Test_Loss: 0.5802466869354248 *\n",
      "Epoch: 23, Train_Loss: 0.48649612069129944, Test_Loss: 0.6883165836334229\n",
      "Epoch: 23, Train_Loss: 0.4370844066143036, Test_Loss: 0.4824172258377075 *\n",
      "Epoch: 23, Train_Loss: 0.4213049113750458, Test_Loss: 0.40215831995010376 *\n",
      "Epoch: 23, Train_Loss: 0.39944902062416077, Test_Loss: 0.4018395245075226 *\n",
      "Epoch: 23, Train_Loss: 0.4124753773212433, Test_Loss: 0.4925321340560913\n",
      "Epoch: 23, Train_Loss: 0.4054572582244873, Test_Loss: 0.6146096587181091\n",
      "Epoch: 23, Train_Loss: 0.39254486560821533, Test_Loss: 0.9727283120155334\n",
      "Epoch: 23, Train_Loss: 0.40263816714286804, Test_Loss: 0.9260433912277222 *\n",
      "Epoch: 23, Train_Loss: 0.3943832218647003, Test_Loss: 2.227177858352661\n",
      "Epoch: 23, Train_Loss: 0.4191291332244873, Test_Loss: 0.7562795877456665 *\n",
      "Epoch: 23, Train_Loss: 0.4996940493583679, Test_Loss: 0.8500046730041504\n",
      "Epoch: 23, Train_Loss: 0.39841100573539734, Test_Loss: 0.42404234409332275 *\n",
      "Epoch: 23, Train_Loss: 0.4191291928291321, Test_Loss: 0.41001880168914795 *\n",
      "Epoch: 23, Train_Loss: 0.41478681564331055, Test_Loss: 0.8808296322822571\n",
      "Epoch: 23, Train_Loss: 0.41049420833587646, Test_Loss: 1.745888352394104\n",
      "Epoch: 23, Train_Loss: 0.4811856150627136, Test_Loss: 0.5382213592529297 *\n",
      "Epoch: 23, Train_Loss: 0.5819734334945679, Test_Loss: 0.5171530246734619 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.3902566134929657, Test_Loss: 0.3909280300140381 *\n",
      "Epoch: 23, Train_Loss: 0.4224804639816284, Test_Loss: 0.5138760209083557\n",
      "Epoch: 23, Train_Loss: 0.386088490486145, Test_Loss: 0.6888913512229919\n",
      "Epoch: 23, Train_Loss: 0.3898737132549286, Test_Loss: 0.7536724805831909\n",
      "Epoch: 23, Train_Loss: 0.3882793188095093, Test_Loss: 1.4458204507827759\n",
      "Epoch: 23, Train_Loss: 0.39418622851371765, Test_Loss: 0.7435157895088196 *\n",
      "Epoch: 23, Train_Loss: 0.3884160816669464, Test_Loss: 0.3992728888988495 *\n",
      "Epoch: 23, Train_Loss: 0.4086288511753082, Test_Loss: 0.389750599861145 *\n",
      "Epoch: 23, Train_Loss: 0.3935316503047943, Test_Loss: 0.39919695258140564\n",
      "Epoch: 23, Train_Loss: 0.39601022005081177, Test_Loss: 0.41764533519744873\n",
      "Epoch: 23, Train_Loss: 0.3966981768608093, Test_Loss: 0.5889581441879272\n",
      "Epoch: 23, Train_Loss: 0.39031192660331726, Test_Loss: 0.8678117394447327\n",
      "Epoch: 23, Train_Loss: 0.3864849805831909, Test_Loss: 0.613042414188385 *\n",
      "Epoch: 23, Train_Loss: 0.38598474860191345, Test_Loss: 0.4515892267227173 *\n",
      "Epoch: 23, Train_Loss: 0.3977872431278229, Test_Loss: 0.39686959981918335 *\n",
      "Epoch: 23, Train_Loss: 0.39427798986434937, Test_Loss: 0.3993813693523407\n",
      "Epoch: 23, Train_Loss: 0.40608125925064087, Test_Loss: 0.47298628091812134\n",
      "Epoch: 23, Train_Loss: 0.3897726833820343, Test_Loss: 0.8100209832191467\n",
      "Epoch: 23, Train_Loss: 0.41873157024383545, Test_Loss: 0.9643079042434692\n",
      "Epoch: 23, Train_Loss: 0.40649667382240295, Test_Loss: 0.6728485226631165 *\n",
      "Epoch: 23, Train_Loss: 0.41129952669143677, Test_Loss: 0.4947415888309479 *\n",
      "Epoch: 23, Train_Loss: 0.3911757469177246, Test_Loss: 0.3924286663532257 *\n",
      "Epoch: 23, Train_Loss: 0.41455474495887756, Test_Loss: 0.40600478649139404\n",
      "Epoch: 23, Train_Loss: 0.38634777069091797, Test_Loss: 0.3980613946914673 *\n",
      "Epoch: 23, Train_Loss: 0.39945971965789795, Test_Loss: 0.4096347689628601\n",
      "Epoch: 23, Train_Loss: 0.40207505226135254, Test_Loss: 0.4261294901371002\n",
      "Epoch: 23, Train_Loss: 0.41496896743774414, Test_Loss: 0.44219332933425903\n",
      "Epoch: 23, Train_Loss: 2.2597012519836426, Test_Loss: 0.3888515830039978 *\n",
      "Epoch: 23, Train_Loss: 3.9636919498443604, Test_Loss: 0.4463716745376587\n",
      "Epoch: 23, Train_Loss: 0.40160825848579407, Test_Loss: 0.7092304229736328\n",
      "Epoch: 23, Train_Loss: 0.41146913170814514, Test_Loss: 0.4587208926677704 *\n",
      "Epoch: 23, Train_Loss: 0.40966764092445374, Test_Loss: 0.5864194631576538\n",
      "Epoch: 23, Train_Loss: 0.539865255355835, Test_Loss: 0.39753827452659607 *\n",
      "Epoch: 23, Train_Loss: 0.4234107434749603, Test_Loss: 0.39619752764701843 *\n",
      "Epoch: 23, Train_Loss: 0.39305004477500916, Test_Loss: 0.3996310830116272\n",
      "Epoch: 23, Train_Loss: 0.38188979029655457, Test_Loss: 0.4078601598739624\n",
      "Epoch: 23, Train_Loss: 0.4504324793815613, Test_Loss: 0.4022674262523651 *\n",
      "Epoch: 23, Train_Loss: 0.40336230397224426, Test_Loss: 4.080951690673828\n",
      "Epoch: 23, Train_Loss: 0.39633527398109436, Test_Loss: 2.0155417919158936 *\n",
      "Epoch: 23, Train_Loss: 0.9569509625434875, Test_Loss: 0.3922538161277771 *\n",
      "Epoch: 23, Train_Loss: 1.5520695447921753, Test_Loss: 0.38597485423088074 *\n",
      "Epoch: 23, Train_Loss: 1.1030596494674683, Test_Loss: 0.3873867690563202\n",
      "Epoch: 23, Train_Loss: 0.4594905972480774, Test_Loss: 0.41406089067459106\n",
      "Epoch: 23, Train_Loss: 0.8621443510055542, Test_Loss: 0.3898356258869171 *\n",
      "Epoch: 23, Train_Loss: 2.934675693511963, Test_Loss: 0.38648101687431335 *\n",
      "Epoch: 23, Train_Loss: 1.039548635482788, Test_Loss: 0.3872702121734619\n",
      "Epoch: 23, Train_Loss: 0.4088437855243683, Test_Loss: 0.3875148594379425\n",
      "Epoch: 23, Train_Loss: 0.40771356225013733, Test_Loss: 0.3972785770893097\n",
      "Epoch: 23, Train_Loss: 1.2198829650878906, Test_Loss: 0.4035964906215668\n",
      "Epoch: 23, Train_Loss: 1.2140839099884033, Test_Loss: 0.40805456042289734\n",
      "Epoch: 23, Train_Loss: 0.7697335481643677, Test_Loss: 0.4212915003299713\n",
      "Epoch: 23, Train_Loss: 0.39412689208984375, Test_Loss: 0.4892011284828186\n",
      "Epoch: 23, Train_Loss: 0.39308345317840576, Test_Loss: 0.3979542851448059 *\n",
      "Epoch: 23, Train_Loss: 0.9553192257881165, Test_Loss: 0.4217381477355957\n",
      "Epoch: 24, Train_Loss: 0.6137539148330688, Test_Loss: 0.3997325599193573 *\n",
      "Epoch: 24, Train_Loss: 0.3997851610183716, Test_Loss: 0.4508105218410492\n",
      "Epoch: 24, Train_Loss: 0.4139286279678345, Test_Loss: 0.41575613617897034 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.4438861310482025, Test_Loss: 0.43908530473709106\n",
      "Epoch: 24, Train_Loss: 0.4783608317375183, Test_Loss: 0.47942841053009033\n",
      "Epoch: 24, Train_Loss: 0.4851890206336975, Test_Loss: 0.4034973680973053 *\n",
      "Epoch: 24, Train_Loss: 0.6585712432861328, Test_Loss: 0.43738698959350586\n",
      "Epoch: 24, Train_Loss: 0.41971832513809204, Test_Loss: 0.43035459518432617 *\n",
      "Epoch: 24, Train_Loss: 0.4646971523761749, Test_Loss: 0.426193505525589 *\n",
      "Epoch: 24, Train_Loss: 0.5285923480987549, Test_Loss: 0.4428417384624481\n",
      "Epoch: 24, Train_Loss: 0.6025416851043701, Test_Loss: 0.5652658939361572\n",
      "Epoch: 24, Train_Loss: 0.5878145098686218, Test_Loss: 0.5048917531967163 *\n",
      "Epoch: 24, Train_Loss: 0.5536881685256958, Test_Loss: 0.4582620859146118 *\n",
      "Epoch: 24, Train_Loss: 0.46355655789375305, Test_Loss: 0.5039408206939697\n",
      "Epoch: 24, Train_Loss: 0.46325206756591797, Test_Loss: 0.4746456742286682 *\n",
      "Epoch: 24, Train_Loss: 0.43218010663986206, Test_Loss: 6.505617141723633\n",
      "Epoch: 24, Train_Loss: 0.3933827579021454, Test_Loss: 0.5204707384109497 *\n",
      "Epoch: 24, Train_Loss: 0.3815068006515503, Test_Loss: 0.4759543240070343 *\n",
      "Epoch: 24, Train_Loss: 0.40762433409690857, Test_Loss: 0.4362357556819916 *\n",
      "Epoch: 24, Train_Loss: 0.39804700016975403, Test_Loss: 0.3960297107696533 *\n",
      "Epoch: 24, Train_Loss: 0.389121413230896, Test_Loss: 0.4398820400238037\n",
      "Epoch: 24, Train_Loss: 0.41174647212028503, Test_Loss: 0.40915030241012573 *\n",
      "Epoch: 24, Train_Loss: 0.40566015243530273, Test_Loss: 0.4421416223049164\n",
      "Epoch: 24, Train_Loss: 0.42054110765457153, Test_Loss: 0.43518152832984924 *\n",
      "Epoch: 24, Train_Loss: 0.4888349175453186, Test_Loss: 0.4727901220321655\n",
      "Epoch: 24, Train_Loss: 0.643641471862793, Test_Loss: 0.5191331505775452\n",
      "Epoch: 24, Train_Loss: 0.42580533027648926, Test_Loss: 0.5889772176742554\n",
      "Epoch: 24, Train_Loss: 0.41230180859565735, Test_Loss: 0.5344696640968323 *\n",
      "Epoch: 24, Train_Loss: 0.44227781891822815, Test_Loss: 0.483215868473053 *\n",
      "Epoch: 24, Train_Loss: 0.5786849856376648, Test_Loss: 0.3996296226978302 *\n",
      "Epoch: 24, Train_Loss: 0.6022108197212219, Test_Loss: 0.4599378705024719\n",
      "Epoch: 24, Train_Loss: 0.444031298160553, Test_Loss: 0.497724324464798\n",
      "Epoch: 24, Train_Loss: 0.41684648394584656, Test_Loss: 0.5701981782913208\n",
      "Epoch: 24, Train_Loss: 0.6601642370223999, Test_Loss: 0.5809179544448853\n",
      "Epoch: 24, Train_Loss: 0.654100775718689, Test_Loss: 0.4047197699546814 *\n",
      "Epoch: 24, Train_Loss: 0.4495355486869812, Test_Loss: 0.3970598876476288 *\n",
      "Epoch: 24, Train_Loss: 0.40597110986709595, Test_Loss: 0.4077000916004181\n",
      "Epoch: 24, Train_Loss: 0.4236702620983124, Test_Loss: 0.4188670516014099\n",
      "Epoch: 24, Train_Loss: 0.8680340647697449, Test_Loss: 0.4004025459289551 *\n",
      "Epoch: 24, Train_Loss: 0.9148693084716797, Test_Loss: 0.410876601934433\n",
      "Epoch: 24, Train_Loss: 0.4647068381309509, Test_Loss: 0.39785417914390564 *\n",
      "Epoch: 24, Train_Loss: 0.41927579045295715, Test_Loss: 0.38950058817863464 *\n",
      "Epoch: 24, Train_Loss: 0.3940238952636719, Test_Loss: 0.39405184984207153\n",
      "Epoch: 24, Train_Loss: 0.3866751790046692, Test_Loss: 0.44398999214172363\n",
      "Epoch: 24, Train_Loss: 0.6995621919631958, Test_Loss: 0.4477676749229431\n",
      "Epoch: 24, Train_Loss: 0.40650689601898193, Test_Loss: 0.3929331302642822 *\n",
      "Epoch: 24, Train_Loss: 0.41261374950408936, Test_Loss: 0.4321894347667694\n",
      "Epoch: 24, Train_Loss: 0.39717772603034973, Test_Loss: 0.6886128187179565\n",
      "Epoch: 24, Train_Loss: 0.5708701014518738, Test_Loss: 0.42523595690727234 *\n",
      "Epoch: 24, Train_Loss: 16.372854232788086, Test_Loss: 0.4013712704181671 *\n",
      "Epoch: 24, Train_Loss: 0.46956512331962585, Test_Loss: 0.5144280195236206\n",
      "Epoch: 24, Train_Loss: 1.3908424377441406, Test_Loss: 0.5217687487602234\n",
      "Epoch: 24, Train_Loss: 1.5632240772247314, Test_Loss: 0.42157498002052307 *\n",
      "Epoch: 24, Train_Loss: 0.43006211519241333, Test_Loss: 0.4794495403766632\n",
      "Epoch: 24, Train_Loss: 0.45969158411026, Test_Loss: 0.4826924204826355\n",
      "Epoch: 24, Train_Loss: 3.20439076423645, Test_Loss: 0.6076427698135376\n",
      "Epoch: 24, Train_Loss: 7.279222011566162, Test_Loss: 0.4710542857646942 *\n",
      "Epoch: 24, Train_Loss: 0.4336174726486206, Test_Loss: 0.43034911155700684 *\n",
      "Epoch: 24, Train_Loss: 0.47570857405662537, Test_Loss: 0.4328823983669281\n",
      "Epoch: 24, Train_Loss: 5.696248531341553, Test_Loss: 0.42903172969818115 *\n",
      "Epoch: 24, Train_Loss: 0.5385180115699768, Test_Loss: 0.753284752368927\n",
      "Epoch: 24, Train_Loss: 0.4550946354866028, Test_Loss: 0.9310294389724731\n",
      "Epoch: 24, Train_Loss: 0.4220319092273712, Test_Loss: 0.5708545446395874 *\n",
      "Epoch: 24, Train_Loss: 0.41758230328559875, Test_Loss: 0.6781691908836365\n",
      "Epoch: 24, Train_Loss: 0.4780201017856598, Test_Loss: 0.6548664569854736 *\n",
      "Epoch: 24, Train_Loss: 0.3884139358997345, Test_Loss: 0.9293310642242432\n",
      "Epoch: 24, Train_Loss: 0.3880482017993927, Test_Loss: 0.7425452470779419 *\n",
      "Epoch: 24, Train_Loss: 0.37973055243492126, Test_Loss: 0.50045245885849 *\n",
      "Epoch: 24, Train_Loss: 0.3829500675201416, Test_Loss: 0.4692195653915405 *\n",
      "Epoch: 24, Train_Loss: 0.5108860731124878, Test_Loss: 0.584497332572937\n",
      "Epoch: 24, Train_Loss: 0.4466155469417572, Test_Loss: 0.5490702390670776 *\n",
      "Epoch: 24, Train_Loss: 0.4406970739364624, Test_Loss: 1.4541083574295044\n",
      "Epoch: 24, Train_Loss: 0.6333817839622498, Test_Loss: 0.8510168790817261 *\n",
      "Epoch: 24, Train_Loss: 0.6925790309906006, Test_Loss: 1.543010950088501\n",
      "Epoch: 24, Train_Loss: 0.41713249683380127, Test_Loss: 1.0380334854125977 *\n",
      "Epoch: 24, Train_Loss: 0.4053887724876404, Test_Loss: 1.6410167217254639\n",
      "Epoch: 24, Train_Loss: 0.39789214730262756, Test_Loss: 0.6260267496109009 *\n",
      "Epoch: 24, Train_Loss: 0.45968571305274963, Test_Loss: 0.4203336536884308 *\n",
      "Epoch: 24, Train_Loss: 0.3827707767486572, Test_Loss: 0.5398678183555603\n",
      "Epoch: 24, Train_Loss: 0.37795448303222656, Test_Loss: 1.099909782409668\n",
      "Epoch: 24, Train_Loss: 0.3827569782733917, Test_Loss: 0.8569735288619995 *\n",
      "Epoch: 24, Train_Loss: 0.3768116235733032, Test_Loss: 0.5478965044021606 *\n",
      "Epoch: 24, Train_Loss: 0.3793736398220062, Test_Loss: 0.3962044417858124 *\n",
      "Epoch: 24, Train_Loss: 0.37989211082458496, Test_Loss: 0.4535927176475525\n",
      "Epoch: 24, Train_Loss: 0.37899571657180786, Test_Loss: 0.7527775764465332\n",
      "Epoch: 24, Train_Loss: 0.3803567588329315, Test_Loss: 0.6205223202705383 *\n",
      "Epoch: 24, Train_Loss: 0.3938019275665283, Test_Loss: 1.3466135263442993\n",
      "Epoch: 24, Train_Loss: 0.40235990285873413, Test_Loss: 0.7573117017745972 *\n",
      "Epoch: 24, Train_Loss: 0.44065114855766296, Test_Loss: 0.45193910598754883 *\n",
      "Epoch: 24, Train_Loss: 0.4312480092048645, Test_Loss: 0.4107821583747864 *\n",
      "Epoch: 24, Train_Loss: 1.3358216285705566, Test_Loss: 0.40087565779685974 *\n",
      "Epoch: 24, Train_Loss: 6.877019882202148, Test_Loss: 0.46122193336486816\n",
      "Epoch: 24, Train_Loss: 0.437996506690979, Test_Loss: 0.5064495801925659\n",
      "Epoch: 24, Train_Loss: 0.48185276985168457, Test_Loss: 0.5761497020721436\n",
      "Epoch: 24, Train_Loss: 0.5514912009239197, Test_Loss: 0.5678595304489136 *\n",
      "Epoch: 24, Train_Loss: 0.5415807366371155, Test_Loss: 0.4432940185070038 *\n",
      "Epoch: 24, Train_Loss: 0.5102702379226685, Test_Loss: 0.39157071709632874 *\n",
      "Epoch: 24, Train_Loss: 0.5085676312446594, Test_Loss: 0.4023815989494324\n",
      "Epoch: 24, Train_Loss: 0.526336669921875, Test_Loss: 0.4343622326850891\n",
      "Epoch: 24, Train_Loss: 0.675940752029419, Test_Loss: 0.69842529296875\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 0.5265782475471497, Test_Loss: 1.273007869720459\n",
      "Epoch: 24, Train_Loss: 0.43839767575263977, Test_Loss: 0.9634945392608643 *\n",
      "Epoch: 24, Train_Loss: 0.3905431628227234, Test_Loss: 0.4533028304576874 *\n",
      "Epoch: 24, Train_Loss: 0.4530850052833557, Test_Loss: 0.40993455052375793 *\n",
      "Epoch: 24, Train_Loss: 0.458954781293869, Test_Loss: 0.3996845483779907 *\n",
      "Epoch: 24, Train_Loss: 0.5492247343063354, Test_Loss: 0.4118303656578064\n",
      "Epoch: 24, Train_Loss: 0.409121572971344, Test_Loss: 0.4258568286895752\n",
      "Epoch: 24, Train_Loss: 0.42371824383735657, Test_Loss: 0.3862265348434448 *\n",
      "Epoch: 24, Train_Loss: 0.3895425796508789, Test_Loss: 0.4415629208087921\n",
      "Epoch: 24, Train_Loss: 0.3976278603076935, Test_Loss: 0.39086443185806274 *\n",
      "Epoch: 24, Train_Loss: 0.4669432044029236, Test_Loss: 0.46004271507263184\n",
      "Epoch: 24, Train_Loss: 0.4123718738555908, Test_Loss: 0.5680035352706909\n",
      "Epoch: 24, Train_Loss: 0.3774834871292114, Test_Loss: 0.6574686765670776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.37690141797065735, Test_Loss: 0.6688576936721802\n",
      "Epoch: 24, Train_Loss: 0.3765944838523865, Test_Loss: 0.4395217001438141 *\n",
      "Epoch: 24, Train_Loss: 1.7610756158828735, Test_Loss: 0.41202065348625183 *\n",
      "Epoch: 24, Train_Loss: 4.661156177520752, Test_Loss: 0.41859400272369385\n",
      "Epoch: 24, Train_Loss: 0.38344013690948486, Test_Loss: 0.42356303334236145\n",
      "Epoch: 24, Train_Loss: 0.38788580894470215, Test_Loss: 0.43988773226737976\n",
      "Epoch: 24, Train_Loss: 0.3824480175971985, Test_Loss: 2.377199172973633\n",
      "Epoch: 24, Train_Loss: 0.3792661428451538, Test_Loss: 3.7435615062713623\n",
      "Epoch: 24, Train_Loss: 0.376390665769577, Test_Loss: 0.4005902409553528 *\n",
      "Epoch: 24, Train_Loss: 0.37600651383399963, Test_Loss: 0.4022983908653259\n",
      "Epoch: 24, Train_Loss: 0.38630568981170654, Test_Loss: 0.40163928270339966 *\n",
      "Epoch: 24, Train_Loss: 0.3887362480163574, Test_Loss: 0.3798231780529022 *\n",
      "Epoch: 24, Train_Loss: 0.4127095341682434, Test_Loss: 0.40933385491371155\n",
      "Epoch: 24, Train_Loss: 0.37848153710365295, Test_Loss: 0.45811763405799866\n",
      "Epoch: 24, Train_Loss: 0.37458673119544983, Test_Loss: 0.46394845843315125\n",
      "Epoch: 24, Train_Loss: 0.37354135513305664, Test_Loss: 0.41725635528564453 *\n",
      "Epoch: 24, Train_Loss: 0.3957226872444153, Test_Loss: 0.46781012415885925\n",
      "Epoch: 24, Train_Loss: 0.3802812993526459, Test_Loss: 0.42971646785736084 *\n",
      "Epoch: 24, Train_Loss: 0.37836363911628723, Test_Loss: 0.47951555252075195\n",
      "Epoch: 24, Train_Loss: 0.3911878168582916, Test_Loss: 0.38497239351272583 *\n",
      "Epoch: 24, Train_Loss: 0.38283616304397583, Test_Loss: 0.40445175766944885\n",
      "Epoch: 24, Train_Loss: 0.3821805715560913, Test_Loss: 0.42063334584236145\n",
      "Epoch: 24, Train_Loss: 0.3747124969959259, Test_Loss: 0.3831426203250885 *\n",
      "Epoch: 24, Train_Loss: 0.3907874822616577, Test_Loss: 0.4245847761631012\n",
      "Epoch: 24, Train_Loss: 0.40214040875434875, Test_Loss: 0.39388155937194824 *\n",
      "Epoch: 24, Train_Loss: 0.39769554138183594, Test_Loss: 0.4807073473930359\n",
      "Epoch: 24, Train_Loss: 0.4105820655822754, Test_Loss: 0.39336642622947693 *\n",
      "Epoch: 24, Train_Loss: 0.4314330220222473, Test_Loss: 0.41800370812416077\n",
      "Epoch: 24, Train_Loss: 0.43114691972732544, Test_Loss: 0.4082919955253601 *\n",
      "Epoch: 24, Train_Loss: 0.3942332863807678, Test_Loss: 0.4694501459598541\n",
      "Epoch: 24, Train_Loss: 0.41113826632499695, Test_Loss: 0.46655845642089844 *\n",
      "Epoch: 24, Train_Loss: 0.38756173849105835, Test_Loss: 0.425139456987381 *\n",
      "Epoch: 24, Train_Loss: 0.5745829343795776, Test_Loss: 0.38574519753456116 *\n",
      "Epoch: 24, Train_Loss: 0.38654056191444397, Test_Loss: 0.4153628349304199\n",
      "Epoch: 24, Train_Loss: 0.3728049397468567, Test_Loss: 0.4095369279384613 *\n",
      "Epoch: 24, Train_Loss: 0.37049537897109985, Test_Loss: 0.38236817717552185 *\n",
      "Epoch: 24, Train_Loss: 0.3727453649044037, Test_Loss: 0.5261258482933044\n",
      "Epoch: 24, Train_Loss: 0.3778837323188782, Test_Loss: 0.44570064544677734 *\n",
      "Epoch: 24, Train_Loss: 0.3734853267669678, Test_Loss: 4.963705062866211\n",
      "Epoch: 24, Train_Loss: 1.8509516716003418, Test_Loss: 1.5565226078033447 *\n",
      "Epoch: 24, Train_Loss: 2.9846465587615967, Test_Loss: 0.39113399386405945 *\n",
      "Epoch: 24, Train_Loss: 0.3907082974910736, Test_Loss: 0.39535239338874817\n",
      "Epoch: 24, Train_Loss: 0.3787136971950531, Test_Loss: 0.3804531693458557 *\n",
      "Epoch: 24, Train_Loss: 0.37555068731307983, Test_Loss: 0.38524770736694336\n",
      "Epoch: 24, Train_Loss: 0.3796793520450592, Test_Loss: 0.39333462715148926\n",
      "Epoch: 24, Train_Loss: 0.37547847628593445, Test_Loss: 0.4489041864871979\n",
      "Epoch: 24, Train_Loss: 0.37666916847229004, Test_Loss: 0.47363346815109253\n",
      "Epoch: 24, Train_Loss: 0.371473491191864, Test_Loss: 0.37686291337013245 *\n",
      "Epoch: 24, Train_Loss: 0.3757273554801941, Test_Loss: 0.40135443210601807\n",
      "Epoch: 24, Train_Loss: 0.39118102192878723, Test_Loss: 0.41411617398262024\n",
      "Epoch: 24, Train_Loss: 0.39768746495246887, Test_Loss: 0.3976271450519562 *\n",
      "Epoch: 24, Train_Loss: 0.3959798514842987, Test_Loss: 0.38416939973831177 *\n",
      "Epoch: 24, Train_Loss: 0.4118345081806183, Test_Loss: 0.3891313076019287\n",
      "Epoch: 24, Train_Loss: 0.40640920400619507, Test_Loss: 0.4261077344417572\n",
      "Epoch: 24, Train_Loss: 0.3733041286468506, Test_Loss: 0.4609243869781494\n",
      "Epoch: 24, Train_Loss: 0.45000365376472473, Test_Loss: 0.4057559072971344 *\n",
      "Epoch: 24, Train_Loss: 0.467404305934906, Test_Loss: 0.4199526309967041\n",
      "Epoch: 24, Train_Loss: 0.4357767701148987, Test_Loss: 0.3795023560523987 *\n",
      "Epoch: 24, Train_Loss: 0.44120165705680847, Test_Loss: 0.3788960874080658 *\n",
      "Epoch: 24, Train_Loss: 0.41401174664497375, Test_Loss: 0.39164435863494873\n",
      "Epoch: 24, Train_Loss: 0.3912453055381775, Test_Loss: 0.3811614215373993 *\n",
      "Epoch: 24, Train_Loss: 0.3717479407787323, Test_Loss: 0.38586241006851196\n",
      "Epoch: 24, Train_Loss: 0.3693534731864929, Test_Loss: 0.3841686248779297 *\n",
      "Epoch: 24, Train_Loss: 0.3829714357852936, Test_Loss: 0.39751267433166504\n",
      "Epoch: 24, Train_Loss: 0.3786385953426361, Test_Loss: 0.38577306270599365 *\n",
      "Epoch: 24, Train_Loss: 0.36912041902542114, Test_Loss: 0.3776227533817291 *\n",
      "Epoch: 24, Train_Loss: 0.3719441890716553, Test_Loss: 0.39470672607421875\n",
      "Epoch: 24, Train_Loss: 0.37969332933425903, Test_Loss: 0.4338071942329407\n",
      "Epoch: 24, Train_Loss: 0.41254734992980957, Test_Loss: 0.38471561670303345 *\n",
      "Epoch: 24, Train_Loss: 0.4305741786956787, Test_Loss: 0.39661598205566406\n",
      "Epoch: 24, Train_Loss: 0.3982465863227844, Test_Loss: 0.7637395858764648\n",
      "Epoch: 24, Train_Loss: 0.45463767647743225, Test_Loss: 0.3980819880962372 *\n",
      "Epoch: 24, Train_Loss: 0.43323004245758057, Test_Loss: 0.40125349164009094\n",
      "Epoch: 24, Train_Loss: 0.44202202558517456, Test_Loss: 0.4049726128578186\n",
      "Epoch: 24, Train_Loss: 0.4484925866127014, Test_Loss: 0.5403546094894409\n",
      "Epoch: 24, Train_Loss: 0.4460282325744629, Test_Loss: 0.41414570808410645 *\n",
      "Epoch: 24, Train_Loss: 0.4303430914878845, Test_Loss: 0.4554826021194458\n",
      "Epoch: 24, Train_Loss: 0.6367161273956299, Test_Loss: 0.4049828350543976 *\n",
      "Epoch: 24, Train_Loss: 0.3819286525249481, Test_Loss: 0.48107969760894775\n",
      "Epoch: 24, Train_Loss: 0.46108168363571167, Test_Loss: 0.4131294786930084 *\n",
      "Epoch: 24, Train_Loss: 2.5781147480010986, Test_Loss: 0.41798147559165955\n",
      "Epoch: 24, Train_Loss: 0.58376145362854, Test_Loss: 0.38527533411979675 *\n",
      "Epoch: 24, Train_Loss: 0.39403238892555237, Test_Loss: 0.3858414888381958\n",
      "Epoch: 24, Train_Loss: 0.38622406125068665, Test_Loss: 0.5755597352981567\n",
      "Epoch: 24, Train_Loss: 0.37786224484443665, Test_Loss: 0.7613307237625122\n",
      "Epoch: 24, Train_Loss: 0.388367623090744, Test_Loss: 0.5163909196853638 *\n",
      "Epoch: 24, Train_Loss: 0.3722255527973175, Test_Loss: 0.8509476780891418\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 0.3986475169658661, Test_Loss: 0.7320360541343689 *\n",
      "Epoch: 24, Train_Loss: 0.4232970178127289, Test_Loss: 0.5965144634246826 *\n",
      "Epoch: 24, Train_Loss: 0.3938479423522949, Test_Loss: 0.5204243659973145 *\n",
      "Epoch: 24, Train_Loss: 0.39567455649375916, Test_Loss: 0.4215741753578186 *\n",
      "Epoch: 24, Train_Loss: 0.40547189116477966, Test_Loss: 0.38177332282066345 *\n",
      "Epoch: 24, Train_Loss: 0.37869587540626526, Test_Loss: 0.3947926163673401\n",
      "Epoch: 24, Train_Loss: 0.38559475541114807, Test_Loss: 0.5522183179855347\n",
      "Epoch: 24, Train_Loss: 0.4313420057296753, Test_Loss: 0.7896009087562561\n",
      "Epoch: 24, Train_Loss: 0.41248220205307007, Test_Loss: 0.647757887840271 *\n",
      "Epoch: 24, Train_Loss: 0.37890422344207764, Test_Loss: 1.9109190702438354\n",
      "Epoch: 24, Train_Loss: 0.37147924304008484, Test_Loss: 1.3865463733673096 *\n",
      "Epoch: 24, Train_Loss: 0.3807586133480072, Test_Loss: 0.8056134581565857 *\n",
      "Epoch: 24, Train_Loss: 0.3942529559135437, Test_Loss: 0.527808427810669 *\n",
      "Epoch: 24, Train_Loss: 0.39644482731819153, Test_Loss: 0.37665805220603943 *\n",
      "Epoch: 24, Train_Loss: 0.38479334115982056, Test_Loss: 0.4710957407951355\n",
      "Epoch: 24, Train_Loss: 0.3705819249153137, Test_Loss: 1.1940311193466187\n",
      "Epoch: 24, Train_Loss: 0.36822307109832764, Test_Loss: 1.0883631706237793 *\n",
      "Epoch: 24, Train_Loss: 0.3709203898906708, Test_Loss: 0.44204312562942505 *\n",
      "Epoch: 24, Train_Loss: 0.3710631728172302, Test_Loss: 0.4033396542072296 *\n",
      "Epoch: 24, Train_Loss: 0.3686671257019043, Test_Loss: 0.43062490224838257\n",
      "Epoch: 24, Train_Loss: 0.3676682710647583, Test_Loss: 0.7583897113800049\n",
      "Epoch: 24, Train_Loss: 0.36870062351226807, Test_Loss: 0.5076369643211365 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.36831510066986084, Test_Loss: 0.8812057375907898\n",
      "Epoch: 24, Train_Loss: 0.36770766973495483, Test_Loss: 0.7810887694358826 *\n",
      "Epoch: 24, Train_Loss: 0.3696364760398865, Test_Loss: 0.512519121170044 *\n",
      "Epoch: 24, Train_Loss: 0.3696281909942627, Test_Loss: 0.39361658692359924 *\n",
      "Epoch: 24, Train_Loss: 0.3709339499473572, Test_Loss: 0.42340099811553955\n",
      "Epoch: 24, Train_Loss: 0.37633848190307617, Test_Loss: 0.39328858256340027 *\n",
      "Epoch: 24, Train_Loss: 0.3790547847747803, Test_Loss: 0.4156566560268402\n",
      "Epoch: 24, Train_Loss: 0.3798102140426636, Test_Loss: 0.7297189235687256\n",
      "Epoch: 24, Train_Loss: 0.3667832314968109, Test_Loss: 0.6928306818008423 *\n",
      "Epoch: 24, Train_Loss: 0.3653580844402313, Test_Loss: 0.4541310966014862 *\n",
      "Epoch: 24, Train_Loss: 0.38150930404663086, Test_Loss: 0.38402608036994934 *\n",
      "Epoch: 24, Train_Loss: 0.3839721977710724, Test_Loss: 0.3854183256626129\n",
      "Epoch: 24, Train_Loss: 0.36720743775367737, Test_Loss: 0.3959713876247406\n",
      "Epoch: 24, Train_Loss: 0.3686027228832245, Test_Loss: 0.5257319211959839\n",
      "Epoch: 24, Train_Loss: 0.36889296770095825, Test_Loss: 0.8778154253959656\n",
      "Epoch: 24, Train_Loss: 0.3990360200405121, Test_Loss: 0.8862937092781067\n",
      "Epoch: 24, Train_Loss: 0.41234442591667175, Test_Loss: 0.4278712272644043 *\n",
      "Epoch: 24, Train_Loss: 0.3840683698654175, Test_Loss: 0.4965090751647949\n",
      "Epoch: 24, Train_Loss: 0.37387579679489136, Test_Loss: 0.3719290494918823 *\n",
      "Epoch: 24, Train_Loss: 0.37047532200813293, Test_Loss: 0.3825989067554474\n",
      "Epoch: 24, Train_Loss: 0.39805367588996887, Test_Loss: 0.4018583297729492\n",
      "Epoch: 24, Train_Loss: 0.37011370062828064, Test_Loss: 0.3833666741847992 *\n",
      "Epoch: 24, Train_Loss: 0.3768177926540375, Test_Loss: 0.45563340187072754\n",
      "Epoch: 24, Train_Loss: 0.3800930678844452, Test_Loss: 0.38042640686035156 *\n",
      "Epoch: 24, Train_Loss: 0.3896033763885498, Test_Loss: 0.4120962619781494\n",
      "Epoch: 24, Train_Loss: 0.4730430543422699, Test_Loss: 0.4851599931716919\n",
      "Epoch: 24, Train_Loss: 0.40038740634918213, Test_Loss: 0.6463955640792847\n",
      "Epoch: 24, Train_Loss: 0.4012448489665985, Test_Loss: 0.566962480545044 *\n",
      "Epoch: 24, Train_Loss: 0.37998417019844055, Test_Loss: 0.3855990171432495 *\n",
      "Epoch: 24, Train_Loss: 0.40634575486183167, Test_Loss: 0.37247905135154724 *\n",
      "Epoch: 24, Train_Loss: 0.38024967908859253, Test_Loss: 0.37557923793792725\n",
      "Epoch: 24, Train_Loss: 0.3687780201435089, Test_Loss: 0.376250684261322\n",
      "Epoch: 24, Train_Loss: 0.3917064368724823, Test_Loss: 0.37606149911880493 *\n",
      "Epoch: 24, Train_Loss: 0.3774626851081848, Test_Loss: 1.0291274785995483\n",
      "Epoch: 24, Train_Loss: 0.39609184861183167, Test_Loss: 4.808365821838379\n",
      "Epoch: 24, Train_Loss: 0.4426552951335907, Test_Loss: 0.4049968421459198 *\n",
      "Epoch: 24, Train_Loss: 0.3817307949066162, Test_Loss: 0.37330853939056396 *\n",
      "Epoch: 24, Train_Loss: 0.39427128434181213, Test_Loss: 0.373406320810318\n",
      "Epoch: 24, Train_Loss: 0.38397908210754395, Test_Loss: 0.3667340576648712 *\n",
      "Epoch: 24, Train_Loss: 0.370039701461792, Test_Loss: 0.37487274408340454\n",
      "Epoch: 24, Train_Loss: 0.49563539028167725, Test_Loss: 0.3869297206401825\n",
      "Epoch: 24, Train_Loss: 0.5288031101226807, Test_Loss: 0.42736300826072693\n",
      "Epoch: 24, Train_Loss: 0.3664741516113281, Test_Loss: 0.37841856479644775 *\n",
      "Epoch: 24, Train_Loss: 0.39652127027511597, Test_Loss: 0.3955632746219635\n",
      "Epoch: 24, Train_Loss: 0.3633190095424652, Test_Loss: 0.39171379804611206 *\n",
      "Epoch: 24, Train_Loss: 0.3649083375930786, Test_Loss: 0.45653676986694336\n",
      "Epoch: 24, Train_Loss: 0.3687584102153778, Test_Loss: 0.374691367149353 *\n",
      "Epoch: 24, Train_Loss: 0.3697803020477295, Test_Loss: 0.37683361768722534\n",
      "Epoch: 24, Train_Loss: 0.3656298816204071, Test_Loss: 0.39894264936447144\n",
      "Epoch: 24, Train_Loss: 0.3778448700904846, Test_Loss: 0.3716815710067749 *\n",
      "Epoch: 24, Train_Loss: 0.3667660355567932, Test_Loss: 0.36785072088241577 *\n",
      "Epoch: 24, Train_Loss: 0.37355607748031616, Test_Loss: 0.3708396553993225\n",
      "Epoch: 24, Train_Loss: 0.37425583600997925, Test_Loss: 0.3987705111503601\n",
      "Epoch: 24, Train_Loss: 0.36354729533195496, Test_Loss: 0.36428627371788025 *\n",
      "Epoch: 24, Train_Loss: 0.36286163330078125, Test_Loss: 0.3776426911354065\n",
      "Epoch: 24, Train_Loss: 0.3658745586872101, Test_Loss: 0.36876440048217773 *\n",
      "Epoch: 24, Train_Loss: 0.3724398910999298, Test_Loss: 0.37891507148742676\n",
      "Epoch: 24, Train_Loss: 0.3734774589538574, Test_Loss: 0.3809069097042084\n",
      "Epoch: 24, Train_Loss: 0.37803930044174194, Test_Loss: 0.3694106936454773 *\n",
      "Epoch: 24, Train_Loss: 0.37170901894569397, Test_Loss: 0.3642566204071045 *\n",
      "Epoch: 24, Train_Loss: 0.38442474603652954, Test_Loss: 0.3827936053276062\n",
      "Epoch: 24, Train_Loss: 0.3850058913230896, Test_Loss: 0.38359567523002625\n",
      "Epoch: 24, Train_Loss: 0.3781750798225403, Test_Loss: 0.37169432640075684 *\n",
      "Epoch: 24, Train_Loss: 0.3714599013328552, Test_Loss: 0.39125365018844604\n",
      "Epoch: 24, Train_Loss: 0.3855311870574951, Test_Loss: 0.44300419092178345\n",
      "Epoch: 24, Train_Loss: 0.3659283518791199, Test_Loss: 3.691506862640381\n",
      "Epoch: 24, Train_Loss: 0.3712756931781769, Test_Loss: 3.250749111175537 *\n",
      "Epoch: 24, Train_Loss: 0.38488873839378357, Test_Loss: 0.3866671323776245 *\n",
      "Epoch: 24, Train_Loss: 0.4036269187927246, Test_Loss: 0.3738452196121216 *\n",
      "Epoch: 24, Train_Loss: 2.5323469638824463, Test_Loss: 0.3684556782245636 *\n",
      "Epoch: 24, Train_Loss: 3.4189224243164062, Test_Loss: 0.3687756657600403\n",
      "Epoch: 24, Train_Loss: 0.3792068064212799, Test_Loss: 0.3830960690975189\n",
      "Epoch: 24, Train_Loss: 0.37500742077827454, Test_Loss: 0.449556827545166\n",
      "Epoch: 24, Train_Loss: 0.40106528997421265, Test_Loss: 0.5465788841247559\n",
      "Epoch: 24, Train_Loss: 0.5114960670471191, Test_Loss: 0.3628838062286377 *\n",
      "Epoch: 24, Train_Loss: 0.3913280665874481, Test_Loss: 0.38666629791259766\n",
      "Epoch: 24, Train_Loss: 0.37133705615997314, Test_Loss: 0.3833773732185364 *\n",
      "Epoch: 24, Train_Loss: 0.36617812514305115, Test_Loss: 0.37985002994537354 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 0.47029268741607666, Test_Loss: 0.3714149594306946 *\n",
      "Epoch: 24, Train_Loss: 0.38959404826164246, Test_Loss: 0.3940410315990448\n",
      "Epoch: 24, Train_Loss: 0.37749484181404114, Test_Loss: 0.4046892523765564\n",
      "Epoch: 24, Train_Loss: 0.9289110898971558, Test_Loss: 0.4467182755470276\n",
      "Epoch: 24, Train_Loss: 1.0396316051483154, Test_Loss: 0.4013822376728058 *\n",
      "Epoch: 24, Train_Loss: 0.904172420501709, Test_Loss: 0.406087726354599\n",
      "Epoch: 24, Train_Loss: 0.4966716468334198, Test_Loss: 0.3893112242221832 *\n",
      "Epoch: 24, Train_Loss: 1.036935567855835, Test_Loss: 0.3694879114627838 *\n",
      "Epoch: 24, Train_Loss: 1.9241995811462402, Test_Loss: 0.37407949566841125\n",
      "Epoch: 24, Train_Loss: 0.5755254030227661, Test_Loss: 0.3752196431159973\n",
      "Epoch: 24, Train_Loss: 0.36447083950042725, Test_Loss: 0.37738871574401855\n",
      "Epoch: 24, Train_Loss: 0.45773613452911377, Test_Loss: 0.3755955398082733 *\n",
      "Epoch: 24, Train_Loss: 0.8665314316749573, Test_Loss: 0.3797069489955902\n",
      "Epoch: 24, Train_Loss: 0.8483679890632629, Test_Loss: 0.37279537320137024 *\n",
      "Epoch: 24, Train_Loss: 0.49445539712905884, Test_Loss: 0.37097272276878357 *\n",
      "Epoch: 24, Train_Loss: 0.3822786509990692, Test_Loss: 0.3765490651130676\n",
      "Epoch: 24, Train_Loss: 0.3649628162384033, Test_Loss: 0.4170600473880768\n",
      "Epoch: 24, Train_Loss: 0.7400987148284912, Test_Loss: 0.374320387840271 *\n",
      "Epoch: 24, Train_Loss: 0.4320421814918518, Test_Loss: 0.399311900138855\n",
      "Epoch: 24, Train_Loss: 0.4076958894729614, Test_Loss: 0.6534044742584229\n",
      "Epoch: 24, Train_Loss: 0.41057565808296204, Test_Loss: 0.3878759443759918 *\n",
      "Epoch: 24, Train_Loss: 0.40472638607025146, Test_Loss: 0.4189644157886505\n",
      "Epoch: 24, Train_Loss: 0.4571119248867035, Test_Loss: 0.5619470477104187\n",
      "Epoch: 24, Train_Loss: 0.4974050521850586, Test_Loss: 0.6287975311279297\n",
      "Epoch: 24, Train_Loss: 0.578964352607727, Test_Loss: 0.46814388036727905 *\n",
      "Epoch: 24, Train_Loss: 0.41183143854141235, Test_Loss: 0.4248393177986145 *\n",
      "Epoch: 24, Train_Loss: 0.46581554412841797, Test_Loss: 0.49889618158340454\n",
      "Epoch: 24, Train_Loss: 0.4476095736026764, Test_Loss: 0.5162314772605896\n",
      "Epoch: 24, Train_Loss: 0.5928137302398682, Test_Loss: 0.4105851948261261 *\n",
      "Epoch: 24, Train_Loss: 0.5669438242912292, Test_Loss: 0.4090063273906708 *\n",
      "Epoch: 24, Train_Loss: 0.4907141923904419, Test_Loss: 0.3978532552719116 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.4316410422325134, Test_Loss: 0.4072473347187042\n",
      "Epoch: 24, Train_Loss: 0.4084983766078949, Test_Loss: 0.7172781825065613\n",
      "Epoch: 24, Train_Loss: 0.3929145038127899, Test_Loss: 0.6259573698043823 *\n",
      "Epoch: 24, Train_Loss: 0.37171879410743713, Test_Loss: 0.6493233442306519\n",
      "Epoch: 24, Train_Loss: 0.36190539598464966, Test_Loss: 0.8666566610336304\n",
      "Epoch: 24, Train_Loss: 0.36172032356262207, Test_Loss: 0.6715617775917053 *\n",
      "Epoch: 24, Train_Loss: 0.3608088791370392, Test_Loss: 0.6358656883239746 *\n",
      "Epoch: 24, Train_Loss: 0.3659147322177887, Test_Loss: 0.5669203996658325 *\n",
      "Epoch: 24, Train_Loss: 0.4038301706314087, Test_Loss: 0.4767855107784271 *\n",
      "Epoch: 24, Train_Loss: 0.3822251856327057, Test_Loss: 0.4008120000362396 *\n",
      "Epoch: 24, Train_Loss: 0.40178847312927246, Test_Loss: 0.3832697570323944 *\n",
      "Epoch: 24, Train_Loss: 0.5622926354408264, Test_Loss: 0.4592770040035248\n",
      "Epoch: 24, Train_Loss: 0.5782450437545776, Test_Loss: 0.7962704300880432\n",
      "Epoch: 24, Train_Loss: 0.38302743434906006, Test_Loss: 0.854438841342926\n",
      "Epoch: 24, Train_Loss: 0.4048331677913666, Test_Loss: 1.4612531661987305\n",
      "Epoch: 24, Train_Loss: 0.4608379304409027, Test_Loss: 1.70164954662323\n",
      "Epoch: 24, Train_Loss: 0.5793064832687378, Test_Loss: 0.7491729259490967 *\n",
      "Epoch: 24, Train_Loss: 0.7539442181587219, Test_Loss: 0.6759971976280212 *\n",
      "Epoch: 24, Train_Loss: 0.41345322132110596, Test_Loss: 0.3847932815551758 *\n",
      "Epoch: 24, Train_Loss: 0.44378861784935, Test_Loss: 0.418907105922699\n",
      "Epoch: 24, Train_Loss: 0.6172896027565002, Test_Loss: 0.9894312620162964\n",
      "Epoch: 24, Train_Loss: 0.5584217309951782, Test_Loss: 1.3875372409820557\n",
      "Epoch: 24, Train_Loss: 0.4025210738182068, Test_Loss: 0.411482036113739 *\n",
      "Epoch: 24, Train_Loss: 0.3756794035434723, Test_Loss: 0.4105551838874817 *\n",
      "Epoch: 24, Train_Loss: 0.40245321393013, Test_Loss: 0.42980748414993286\n",
      "Epoch: 24, Train_Loss: 0.8477783203125, Test_Loss: 0.7205308675765991\n",
      "Epoch: 24, Train_Loss: 0.9048816561698914, Test_Loss: 0.6601678729057312 *\n",
      "Epoch: 24, Train_Loss: 0.4044873118400574, Test_Loss: 0.6850574016571045\n",
      "Epoch: 24, Train_Loss: 0.4083245098590851, Test_Loss: 0.7036097645759583\n",
      "Epoch: 24, Train_Loss: 0.3737713694572449, Test_Loss: 0.6891834139823914 *\n",
      "Epoch: 24, Train_Loss: 0.3907625675201416, Test_Loss: 0.37339574098587036 *\n",
      "Epoch: 24, Train_Loss: 0.6694344282150269, Test_Loss: 0.4633961319923401\n",
      "Epoch: 24, Train_Loss: 0.3660878539085388, Test_Loss: 0.4466543197631836 *\n",
      "Epoch: 24, Train_Loss: 0.40760552883148193, Test_Loss: 0.4543114900588989\n",
      "Epoch: 24, Train_Loss: 0.4290659427642822, Test_Loss: 0.5782174468040466\n",
      "Epoch: 24, Train_Loss: 0.5506001114845276, Test_Loss: 0.5711655616760254 *\n",
      "Epoch: 24, Train_Loss: 16.363622665405273, Test_Loss: 0.4196659028530121 *\n",
      "Epoch: 24, Train_Loss: 0.4451950192451477, Test_Loss: 0.39675021171569824 *\n",
      "Epoch: 24, Train_Loss: 1.48093581199646, Test_Loss: 0.38410142064094543 *\n",
      "Epoch: 24, Train_Loss: 1.4481900930404663, Test_Loss: 0.3659391701221466 *\n",
      "Epoch: 24, Train_Loss: 0.4385666251182556, Test_Loss: 0.39822500944137573\n",
      "Epoch: 24, Train_Loss: 0.46905195713043213, Test_Loss: 0.6816496849060059\n",
      "Epoch: 24, Train_Loss: 4.324921131134033, Test_Loss: 0.8181865215301514\n",
      "Epoch: 24, Train_Loss: 6.142691612243652, Test_Loss: 0.42848026752471924 *\n",
      "Epoch: 24, Train_Loss: 0.4055047631263733, Test_Loss: 0.4238215982913971 *\n",
      "Epoch: 24, Train_Loss: 0.48375147581100464, Test_Loss: 0.3832308351993561 *\n",
      "Epoch: 24, Train_Loss: 5.506162166595459, Test_Loss: 0.4823329746723175\n",
      "Epoch: 24, Train_Loss: 0.5192978382110596, Test_Loss: 0.5335603356361389\n",
      "Epoch: 24, Train_Loss: 0.41026371717453003, Test_Loss: 0.48753058910369873 *\n",
      "Epoch: 24, Train_Loss: 0.4046207070350647, Test_Loss: 0.6031538248062134\n",
      "Epoch: 24, Train_Loss: 0.451401948928833, Test_Loss: 0.4979848861694336 *\n",
      "Epoch: 24, Train_Loss: 0.49974536895751953, Test_Loss: 0.5193381309509277\n",
      "Epoch: 24, Train_Loss: 0.3923306465148926, Test_Loss: 0.5012246370315552 *\n",
      "Epoch: 24, Train_Loss: 0.3956531286239624, Test_Loss: 0.8179867267608643\n",
      "Epoch: 24, Train_Loss: 0.37068650126457214, Test_Loss: 0.4565986394882202 *\n",
      "Epoch: 24, Train_Loss: 0.36487290263175964, Test_Loss: 0.4643906056880951\n",
      "Epoch: 24, Train_Loss: 0.41882020235061646, Test_Loss: 0.3614383935928345 *\n",
      "Epoch: 24, Train_Loss: 0.38958391547203064, Test_Loss: 0.3592747449874878 *\n",
      "Epoch: 24, Train_Loss: 0.3782276213169098, Test_Loss: 0.3559507131576538 *\n",
      "Epoch: 24, Train_Loss: 0.46597713232040405, Test_Loss: 0.3593384921550751\n",
      "Epoch: 24, Train_Loss: 0.4304499626159668, Test_Loss: 0.4881974458694458\n",
      "Epoch: 24, Train_Loss: 0.37595731019973755, Test_Loss: 5.5522050857543945\n",
      "Epoch: 24, Train_Loss: 0.3694419860839844, Test_Loss: 0.5990244150161743 *\n",
      "Epoch: 24, Train_Loss: 0.3839890658855438, Test_Loss: 0.4671468138694763 *\n",
      "Epoch: 24, Train_Loss: 0.39815735816955566, Test_Loss: 0.45919549465179443 *\n",
      "Epoch: 24, Train_Loss: 0.37334632873535156, Test_Loss: 0.43560469150543213 *\n",
      "Epoch: 24, Train_Loss: 0.36577108502388, Test_Loss: 0.4173218309879303 *\n",
      "Epoch: 24, Train_Loss: 0.3609541356563568, Test_Loss: 0.5307883620262146\n",
      "Epoch: 24, Train_Loss: 0.3593328893184662, Test_Loss: 0.6884600520133972\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 0.3664497137069702, Test_Loss: 0.4632570445537567 *\n",
      "Epoch: 24, Train_Loss: 0.3596731722354889, Test_Loss: 0.5596824288368225\n",
      "Epoch: 24, Train_Loss: 0.35793742537498474, Test_Loss: 0.5820891261100769\n",
      "Epoch: 24, Train_Loss: 0.37183064222335815, Test_Loss: 0.6952749490737915\n",
      "Epoch: 24, Train_Loss: 0.3841497004032135, Test_Loss: 0.5133215188980103 *\n",
      "Epoch: 24, Train_Loss: 0.412712961435318, Test_Loss: 0.487290620803833 *\n",
      "Epoch: 24, Train_Loss: 0.3924148678779602, Test_Loss: 0.4988126754760742\n",
      "Epoch: 24, Train_Loss: 0.37208181619644165, Test_Loss: 0.3965519666671753 *\n",
      "Epoch: 24, Train_Loss: 2.7769734859466553, Test_Loss: 0.39805951714515686\n",
      "Epoch: 24, Train_Loss: 6.1029863357543945, Test_Loss: 0.3961254358291626 *\n",
      "Epoch: 24, Train_Loss: 0.37835827469825745, Test_Loss: 0.5547827482223511\n",
      "Epoch: 24, Train_Loss: 0.39092934131622314, Test_Loss: 0.46054476499557495 *\n",
      "Epoch: 24, Train_Loss: 0.4546879827976227, Test_Loss: 0.529621958732605\n",
      "Epoch: 24, Train_Loss: 0.4254298210144043, Test_Loss: 0.4461022615432739 *\n",
      "Epoch: 24, Train_Loss: 0.4673173129558563, Test_Loss: 0.4900502562522888\n",
      "Epoch: 24, Train_Loss: 0.42957639694213867, Test_Loss: 0.5464580059051514\n",
      "Epoch: 24, Train_Loss: 0.4617374837398529, Test_Loss: 0.4849507510662079 *\n",
      "Epoch: 24, Train_Loss: 0.588991641998291, Test_Loss: 0.4218350946903229 *\n",
      "Epoch: 24, Train_Loss: 0.4741106629371643, Test_Loss: 0.4542037546634674\n",
      "Epoch: 24, Train_Loss: 0.3918057084083557, Test_Loss: 0.4374711513519287 *\n",
      "Epoch: 24, Train_Loss: 0.3716740012168884, Test_Loss: 0.4002491533756256 *\n",
      "Epoch: 24, Train_Loss: 0.37127792835235596, Test_Loss: 0.4316117763519287\n",
      "Epoch: 24, Train_Loss: 0.464694082736969, Test_Loss: 0.511713981628418\n",
      "Epoch: 24, Train_Loss: 0.43792515993118286, Test_Loss: 2.4359805583953857\n",
      "Epoch: 24, Train_Loss: 0.38176751136779785, Test_Loss: 4.8332839012146\n",
      "Epoch: 24, Train_Loss: 0.3989572525024414, Test_Loss: 0.416346937417984 *\n",
      "Epoch: 24, Train_Loss: 0.39972496032714844, Test_Loss: 0.3639957308769226 *\n",
      "Epoch: 24, Train_Loss: 0.39588600397109985, Test_Loss: 0.3709634840488434\n",
      "Epoch: 24, Train_Loss: 0.4393109381198883, Test_Loss: 0.3810928463935852\n",
      "Epoch: 24, Train_Loss: 0.43167057633399963, Test_Loss: 0.37699636816978455 *\n",
      "Epoch: 24, Train_Loss: 0.36184361577033997, Test_Loss: 0.41159531474113464\n",
      "Epoch: 24, Train_Loss: 0.3615676462650299, Test_Loss: 0.5541397333145142\n",
      "Epoch: 24, Train_Loss: 0.3683929741382599, Test_Loss: 0.3673159182071686 *\n",
      "Epoch: 24, Train_Loss: 2.5730719566345215, Test_Loss: 0.36861652135849\n",
      "Epoch: 24, Train_Loss: 3.4638917446136475, Test_Loss: 0.3865261673927307\n",
      "Epoch: 24, Train_Loss: 0.3565600514411926, Test_Loss: 0.36799338459968567 *\n",
      "Epoch: 24, Train_Loss: 0.38347211480140686, Test_Loss: 0.36988940834999084\n",
      "Epoch: 24, Train_Loss: 0.36130407452583313, Test_Loss: 0.398494154214859\n",
      "Epoch: 24, Train_Loss: 0.3549623191356659, Test_Loss: 0.3796524405479431 *\n",
      "Epoch: 24, Train_Loss: 0.3559558689594269, Test_Loss: 0.4671068787574768\n",
      "Epoch: 24, Train_Loss: 0.35773441195487976, Test_Loss: 0.42955294251441956 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.373613178730011, Test_Loss: 0.379256010055542 *\n",
      "Epoch: 24, Train_Loss: 0.3729994297027588, Test_Loss: 0.36505457758903503 *\n",
      "Epoch: 24, Train_Loss: 0.39385363459587097, Test_Loss: 0.367005318403244\n",
      "Epoch: 24, Train_Loss: 0.3545885682106018, Test_Loss: 0.3722074329853058\n",
      "Epoch: 24, Train_Loss: 0.3546392023563385, Test_Loss: 0.3664381206035614 *\n",
      "Epoch: 24, Train_Loss: 0.3578663766384125, Test_Loss: 0.3568325340747833 *\n",
      "Epoch: 24, Train_Loss: 0.40547460317611694, Test_Loss: 0.363516241312027\n",
      "Epoch: 24, Train_Loss: 0.3650802969932556, Test_Loss: 0.36692702770233154\n",
      "Epoch: 24, Train_Loss: 0.3593742549419403, Test_Loss: 0.3616125285625458 *\n",
      "Epoch: 24, Train_Loss: 0.36927783489227295, Test_Loss: 0.36378514766693115\n",
      "Epoch: 24, Train_Loss: 0.37524017691612244, Test_Loss: 0.3597507178783417 *\n",
      "Epoch: 24, Train_Loss: 0.35635897517204285, Test_Loss: 0.39810609817504883\n",
      "Epoch: 24, Train_Loss: 0.3603280484676361, Test_Loss: 0.3581082224845886 *\n",
      "Epoch: 25, Train_Loss: 0.4026604890823364, Test_Loss: 0.37676456570625305 *\n",
      "Epoch: 25, Train_Loss: 0.38352838158607483, Test_Loss: 0.6182900071144104\n",
      "Epoch: 25, Train_Loss: 0.39652252197265625, Test_Loss: 0.46476083993911743 *\n",
      "Epoch: 25, Train_Loss: 0.3891121447086334, Test_Loss: 0.396729975938797 *\n",
      "Epoch: 25, Train_Loss: 0.45124876499176025, Test_Loss: 0.393870085477829 *\n",
      "Epoch: 25, Train_Loss: 0.39891839027404785, Test_Loss: 0.4664245843887329\n",
      "Epoch: 25, Train_Loss: 0.3674686551094055, Test_Loss: 0.48363572359085083\n",
      "Epoch: 25, Train_Loss: 0.3938145637512207, Test_Loss: 0.35863399505615234 *\n",
      "Epoch: 25, Train_Loss: 0.3899869918823242, Test_Loss: 0.45989111065864563\n",
      "Epoch: 25, Train_Loss: 0.46310973167419434, Test_Loss: 0.5617475509643555\n",
      "Epoch: 25, Train_Loss: 0.36988967657089233, Test_Loss: 0.4530879259109497 *\n",
      "Epoch: 25, Train_Loss: 0.3529466390609741, Test_Loss: 0.42130425572395325 *\n",
      "Epoch: 25, Train_Loss: 0.3511689007282257, Test_Loss: 0.36658403277397156 *\n",
      "Epoch: 25, Train_Loss: 0.3526690602302551, Test_Loss: 0.373094767332077\n",
      "Epoch: 25, Train_Loss: 0.35504183173179626, Test_Loss: 0.3777211904525757\n",
      "Epoch: 25, Train_Loss: 0.3523208498954773, Test_Loss: 0.8759143352508545\n",
      "Epoch: 25, Train_Loss: 2.800393581390381, Test_Loss: 0.7047539949417114 *\n",
      "Epoch: 25, Train_Loss: 2.316101312637329, Test_Loss: 0.8221230506896973\n",
      "Epoch: 25, Train_Loss: 0.3569409251213074, Test_Loss: 0.6911953687667847 *\n",
      "Epoch: 25, Train_Loss: 0.35500025749206543, Test_Loss: 0.5201113224029541 *\n",
      "Epoch: 25, Train_Loss: 0.35463017225265503, Test_Loss: 0.6671340465545654\n",
      "Epoch: 25, Train_Loss: 0.3544982075691223, Test_Loss: 0.43362584710121155 *\n",
      "Epoch: 25, Train_Loss: 0.3524895906448364, Test_Loss: 0.35853055119514465 *\n",
      "Epoch: 25, Train_Loss: 0.3518854081630707, Test_Loss: 0.36239612102508545\n",
      "Epoch: 25, Train_Loss: 0.3504185378551483, Test_Loss: 0.45173269510269165\n",
      "Epoch: 25, Train_Loss: 0.3526248633861542, Test_Loss: 0.6032027006149292\n",
      "Epoch: 25, Train_Loss: 0.3646546006202698, Test_Loss: 1.0110251903533936\n",
      "Epoch: 25, Train_Loss: 0.38163474202156067, Test_Loss: 1.0278642177581787\n",
      "Epoch: 25, Train_Loss: 0.38915136456489563, Test_Loss: 1.7414361238479614\n",
      "Epoch: 25, Train_Loss: 0.40016669034957886, Test_Loss: 0.7629807591438293 *\n",
      "Epoch: 25, Train_Loss: 0.37541529536247253, Test_Loss: 0.7685030698776245\n",
      "Epoch: 25, Train_Loss: 0.3604136109352112, Test_Loss: 0.3564595580101013 *\n",
      "Epoch: 25, Train_Loss: 0.4554120898246765, Test_Loss: 0.36714693903923035\n",
      "Epoch: 25, Train_Loss: 0.4434428811073303, Test_Loss: 0.7895095944404602\n",
      "Epoch: 25, Train_Loss: 0.4162074625492096, Test_Loss: 1.426966667175293\n",
      "Epoch: 25, Train_Loss: 0.41581660509109497, Test_Loss: 0.4160257875919342 *\n",
      "Epoch: 25, Train_Loss: 0.36912256479263306, Test_Loss: 0.43906140327453613\n",
      "Epoch: 25, Train_Loss: 0.3512132465839386, Test_Loss: 0.356717050075531 *\n",
      "Epoch: 25, Train_Loss: 0.3537016212940216, Test_Loss: 0.5014855265617371\n",
      "Epoch: 25, Train_Loss: 0.3479238450527191, Test_Loss: 0.650689423084259\n",
      "Epoch: 25, Train_Loss: 0.3599248230457306, Test_Loss: 0.739892840385437\n",
      "Epoch: 25, Train_Loss: 0.3537466824054718, Test_Loss: 1.0818592309951782\n",
      "Epoch: 25, Train_Loss: 0.34857532382011414, Test_Loss: 0.6489430069923401 *\n",
      "Epoch: 25, Train_Loss: 0.35048791766166687, Test_Loss: 0.3609859347343445 *\n",
      "Epoch: 25, Train_Loss: 0.3580895662307739, Test_Loss: 0.35628780722618103 *\n",
      "Epoch: 25, Train_Loss: 0.39511993527412415, Test_Loss: 0.3593592047691345\n",
      "Epoch: 25, Train_Loss: 0.4306577146053314, Test_Loss: 0.4025668799877167\n",
      "Epoch: 25, Train_Loss: 0.40107378363609314, Test_Loss: 0.558247983455658\n",
      "Epoch: 25, Train_Loss: 0.4358624219894409, Test_Loss: 0.5836652517318726\n",
      "Epoch: 25, Train_Loss: 0.40933850407600403, Test_Loss: 0.479560911655426 *\n",
      "Epoch: 25, Train_Loss: 0.44057944416999817, Test_Loss: 0.42879214882850647 *\n",
      "Epoch: 25, Train_Loss: 0.38065633177757263, Test_Loss: 0.36289897561073303 *\n",
      "Epoch: 25, Train_Loss: 0.43010589480400085, Test_Loss: 0.36163389682769775 *\n",
      "Epoch: 25, Train_Loss: 0.37744784355163574, Test_Loss: 0.3992323875427246\n",
      "Epoch: 25, Train_Loss: 0.6055516004562378, Test_Loss: 0.6925061345100403\n",
      "Epoch: 25, Train_Loss: 0.3628239929676056, Test_Loss: 0.8386046886444092\n",
      "Epoch: 25, Train_Loss: 0.5692709684371948, Test_Loss: 0.5322599411010742 *\n",
      "Epoch: 25, Train_Loss: 2.098372220993042, Test_Loss: 0.4152188301086426 *\n",
      "Epoch: 25, Train_Loss: 0.4963949918746948, Test_Loss: 0.35493072867393494 *\n",
      "Epoch: 25, Train_Loss: 0.3851943910121918, Test_Loss: 0.38612794876098633\n",
      "Epoch: 25, Train_Loss: 0.381364643573761, Test_Loss: 0.3767332434654236 *\n",
      "Epoch: 25, Train_Loss: 0.37511470913887024, Test_Loss: 0.4106263518333435\n",
      "Epoch: 25, Train_Loss: 0.378317654132843, Test_Loss: 0.38296881318092346 *\n",
      "Epoch: 25, Train_Loss: 0.36684486269950867, Test_Loss: 0.3902589976787567\n",
      "Epoch: 25, Train_Loss: 0.3855536878108978, Test_Loss: 0.3702700734138489 *\n",
      "Epoch: 25, Train_Loss: 0.3828907310962677, Test_Loss: 0.4034932255744934\n",
      "Epoch: 25, Train_Loss: 0.37379980087280273, Test_Loss: 0.6940115690231323\n",
      "Epoch: 25, Train_Loss: 0.3683513402938843, Test_Loss: 0.40738075971603394 *\n",
      "Epoch: 25, Train_Loss: 0.3733970522880554, Test_Loss: 0.5683151483535767\n",
      "Epoch: 25, Train_Loss: 0.35541340708732605, Test_Loss: 0.3632083237171173 *\n",
      "Epoch: 25, Train_Loss: 0.3708938956260681, Test_Loss: 0.3645821809768677\n",
      "Epoch: 25, Train_Loss: 0.37414735555648804, Test_Loss: 0.36657217144966125\n",
      "Epoch: 25, Train_Loss: 0.369131475687027, Test_Loss: 0.3683214783668518\n",
      "Epoch: 25, Train_Loss: 0.35643669962882996, Test_Loss: 0.370179146528244\n",
      "Epoch: 25, Train_Loss: 0.34956735372543335, Test_Loss: 4.707045078277588\n",
      "Epoch: 25, Train_Loss: 0.359323114156723, Test_Loss: 1.223175287246704 *\n",
      "Epoch: 25, Train_Loss: 0.3622431755065918, Test_Loss: 0.3733079135417938 *\n",
      "Epoch: 25, Train_Loss: 0.3658509850502014, Test_Loss: 0.37152835726737976 *\n",
      "Epoch: 25, Train_Loss: 0.3578275442123413, Test_Loss: 0.3855310380458832\n",
      "Epoch: 25, Train_Loss: 0.348002552986145, Test_Loss: 0.3572814464569092 *\n",
      "Epoch: 25, Train_Loss: 0.3482517600059509, Test_Loss: 0.38763681054115295\n",
      "Epoch: 25, Train_Loss: 0.34746333956718445, Test_Loss: 0.4766019582748413\n",
      "Epoch: 25, Train_Loss: 0.35130247473716736, Test_Loss: 0.43891724944114685 *\n",
      "Epoch: 25, Train_Loss: 0.34628838300704956, Test_Loss: 0.40489131212234497 *\n",
      "Epoch: 25, Train_Loss: 0.34827059507369995, Test_Loss: 0.43975287675857544\n",
      "Epoch: 25, Train_Loss: 0.35047057271003723, Test_Loss: 0.45493242144584656\n",
      "Epoch: 25, Train_Loss: 0.3476659953594208, Test_Loss: 0.3999347686767578 *\n",
      "Epoch: 25, Train_Loss: 0.3467044234275818, Test_Loss: 0.39947083592414856 *\n",
      "Epoch: 25, Train_Loss: 0.34980082511901855, Test_Loss: 0.40031829476356506\n",
      "Epoch: 25, Train_Loss: 0.3492688536643982, Test_Loss: 0.3854999542236328 *\n",
      "Epoch: 25, Train_Loss: 0.35120701789855957, Test_Loss: 0.3763410151004791 *\n",
      "Epoch: 25, Train_Loss: 0.3642396926879883, Test_Loss: 0.39691537618637085\n",
      "Epoch: 25, Train_Loss: 0.3509158790111542, Test_Loss: 0.4511412978172302\n",
      "Epoch: 25, Train_Loss: 0.3577418327331543, Test_Loss: 0.4384757876396179 *\n",
      "Epoch: 25, Train_Loss: 0.3466518521308899, Test_Loss: 0.3817374110221863 *\n",
      "Epoch: 25, Train_Loss: 0.3471485376358032, Test_Loss: 0.3998175263404846\n",
      "Epoch: 25, Train_Loss: 0.3575497567653656, Test_Loss: 0.3710399866104126 *\n",
      "Epoch: 25, Train_Loss: 0.35796159505844116, Test_Loss: 0.39595404267311096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.34940603375434875, Test_Loss: 0.3908611238002777 *\n",
      "Epoch: 25, Train_Loss: 0.347957044839859, Test_Loss: 0.3634772300720215 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.350603312253952, Test_Loss: 0.3783670663833618\n",
      "Epoch: 25, Train_Loss: 0.3963918387889862, Test_Loss: 0.3692256808280945 *\n",
      "Epoch: 25, Train_Loss: 0.3696362376213074, Test_Loss: 0.3639441430568695 *\n",
      "Epoch: 25, Train_Loss: 0.3669069707393646, Test_Loss: 0.3585868179798126 *\n",
      "Epoch: 25, Train_Loss: 0.3503981828689575, Test_Loss: 0.47889620065689087\n",
      "Epoch: 25, Train_Loss: 0.35987287759780884, Test_Loss: 0.7524881362915039\n",
      "Epoch: 25, Train_Loss: 0.3887551426887512, Test_Loss: 6.362302303314209\n",
      "Epoch: 25, Train_Loss: 0.3584205210208893, Test_Loss: 0.42210230231285095 *\n",
      "Epoch: 25, Train_Loss: 0.3599344789981842, Test_Loss: 0.34527966380119324 *\n",
      "Epoch: 25, Train_Loss: 0.36398255825042725, Test_Loss: 0.3654561936855316\n",
      "Epoch: 25, Train_Loss: 0.40662217140197754, Test_Loss: 0.3592889606952667 *\n",
      "Epoch: 25, Train_Loss: 0.39488595724105835, Test_Loss: 0.36038821935653687\n",
      "Epoch: 25, Train_Loss: 0.35958418250083923, Test_Loss: 0.378966748714447\n",
      "Epoch: 25, Train_Loss: 0.3787813186645508, Test_Loss: 0.565961480140686\n",
      "Epoch: 25, Train_Loss: 0.35744887590408325, Test_Loss: 0.4053823947906494 *\n",
      "Epoch: 25, Train_Loss: 0.40604713559150696, Test_Loss: 0.3466949164867401 *\n",
      "Epoch: 25, Train_Loss: 0.3578847646713257, Test_Loss: 0.37264010310173035\n",
      "Epoch: 25, Train_Loss: 0.34835758805274963, Test_Loss: 0.3613375425338745 *\n",
      "Epoch: 25, Train_Loss: 0.3940814733505249, Test_Loss: 0.3552413284778595 *\n",
      "Epoch: 25, Train_Loss: 0.3710000514984131, Test_Loss: 0.3864733576774597\n",
      "Epoch: 25, Train_Loss: 0.3874410390853882, Test_Loss: 0.3684929609298706 *\n",
      "Epoch: 25, Train_Loss: 0.39516425132751465, Test_Loss: 0.4272330105304718\n",
      "Epoch: 25, Train_Loss: 0.36856991052627563, Test_Loss: 0.40569624304771423 *\n",
      "Epoch: 25, Train_Loss: 0.3634966015815735, Test_Loss: 0.38339632749557495 *\n",
      "Epoch: 25, Train_Loss: 0.3861842155456543, Test_Loss: 0.35916584730148315 *\n",
      "Epoch: 25, Train_Loss: 0.35165756940841675, Test_Loss: 0.35453271865844727 *\n",
      "Epoch: 25, Train_Loss: 0.5822114944458008, Test_Loss: 0.3622586131095886\n",
      "Epoch: 25, Train_Loss: 0.4433818757534027, Test_Loss: 0.35839542746543884 *\n",
      "Epoch: 25, Train_Loss: 0.34830743074417114, Test_Loss: 0.3527357280254364 *\n",
      "Epoch: 25, Train_Loss: 0.3800911009311676, Test_Loss: 0.3521878719329834 *\n",
      "Epoch: 25, Train_Loss: 0.3441118896007538, Test_Loss: 0.3503080904483795 *\n",
      "Epoch: 25, Train_Loss: 0.35126182436943054, Test_Loss: 0.360130250453949\n",
      "Epoch: 25, Train_Loss: 0.37315693497657776, Test_Loss: 0.3533087372779846 *\n",
      "Epoch: 25, Train_Loss: 0.36789393424987793, Test_Loss: 0.34781500697135925 *\n",
      "Epoch: 25, Train_Loss: 0.3485970199108124, Test_Loss: 0.3791492283344269\n",
      "Epoch: 25, Train_Loss: 0.36136671900749207, Test_Loss: 0.36202993988990784 *\n",
      "Epoch: 25, Train_Loss: 0.34767064452171326, Test_Loss: 0.36353686451911926\n",
      "Epoch: 25, Train_Loss: 0.3511182963848114, Test_Loss: 0.44034191966056824\n",
      "Epoch: 25, Train_Loss: 0.3538109362125397, Test_Loss: 0.6071910858154297\n",
      "Epoch: 25, Train_Loss: 0.34437066316604614, Test_Loss: 0.37003612518310547 *\n",
      "Epoch: 25, Train_Loss: 0.3469142019748688, Test_Loss: 0.37738752365112305\n",
      "Epoch: 25, Train_Loss: 0.35296815633773804, Test_Loss: 0.4110880494117737\n",
      "Epoch: 25, Train_Loss: 0.3530121445655823, Test_Loss: 0.5256810784339905\n",
      "Epoch: 25, Train_Loss: 0.35193806886672974, Test_Loss: 0.36754098534584045 *\n",
      "Epoch: 25, Train_Loss: 0.3555523455142975, Test_Loss: 0.44437307119369507\n",
      "Epoch: 25, Train_Loss: 0.3544846177101135, Test_Loss: 0.47269028425216675\n",
      "Epoch: 25, Train_Loss: 0.36970001459121704, Test_Loss: 0.51069575548172\n",
      "Epoch: 25, Train_Loss: 0.36888548731803894, Test_Loss: 0.41398829221725464 *\n",
      "Epoch: 25, Train_Loss: 0.34991925954818726, Test_Loss: 0.3645481765270233 *\n",
      "Epoch: 25, Train_Loss: 0.3542203903198242, Test_Loss: 0.3502315878868103 *\n",
      "Epoch: 25, Train_Loss: 0.35945606231689453, Test_Loss: 0.3581843674182892\n",
      "Epoch: 25, Train_Loss: 0.3496541380882263, Test_Loss: 0.7570860981941223\n",
      "Epoch: 25, Train_Loss: 0.3488803505897522, Test_Loss: 0.8002922534942627\n",
      "Epoch: 25, Train_Loss: 0.3589479327201843, Test_Loss: 0.6992862820625305 *\n",
      "Epoch: 25, Train_Loss: 0.38933703303337097, Test_Loss: 0.8342807292938232\n",
      "Epoch: 25, Train_Loss: 2.7525033950805664, Test_Loss: 0.5239647030830383 *\n",
      "Epoch: 25, Train_Loss: 3.274113655090332, Test_Loss: 0.6206108927726746\n",
      "Epoch: 25, Train_Loss: 0.3668570816516876, Test_Loss: 0.4564891457557678 *\n",
      "Epoch: 25, Train_Loss: 0.34395456314086914, Test_Loss: 0.35173630714416504 *\n",
      "Epoch: 25, Train_Loss: 0.4061887264251709, Test_Loss: 0.35275784134864807\n",
      "Epoch: 25, Train_Loss: 0.4919203817844391, Test_Loss: 0.3909459114074707\n",
      "Epoch: 25, Train_Loss: 0.364859938621521, Test_Loss: 0.5438485145568848\n",
      "Epoch: 25, Train_Loss: 0.3470781445503235, Test_Loss: 0.9722079038619995\n",
      "Epoch: 25, Train_Loss: 0.3588734269142151, Test_Loss: 0.6796334385871887 *\n",
      "Epoch: 25, Train_Loss: 0.43354278802871704, Test_Loss: 1.9412492513656616\n",
      "Epoch: 25, Train_Loss: 0.3644927442073822, Test_Loss: 0.7613474130630493 *\n",
      "Epoch: 25, Train_Loss: 0.35778605937957764, Test_Loss: 0.8071905970573425\n",
      "Epoch: 25, Train_Loss: 1.1125932931900024, Test_Loss: 0.37363237142562866 *\n",
      "Epoch: 25, Train_Loss: 1.1258760690689087, Test_Loss: 0.3943762183189392\n",
      "Epoch: 25, Train_Loss: 0.8032993674278259, Test_Loss: 0.664186954498291\n",
      "Epoch: 25, Train_Loss: 0.5090093612670898, Test_Loss: 1.396815299987793\n",
      "Epoch: 25, Train_Loss: 1.2458105087280273, Test_Loss: 0.6206890344619751 *\n",
      "Epoch: 25, Train_Loss: 1.539015293121338, Test_Loss: 0.4781486988067627 *\n",
      "Epoch: 25, Train_Loss: 0.4244155287742615, Test_Loss: 0.3620963990688324 *\n",
      "Epoch: 25, Train_Loss: 0.3599777817726135, Test_Loss: 0.44464537501335144\n",
      "Epoch: 25, Train_Loss: 0.5099501013755798, Test_Loss: 0.7042080760002136\n",
      "Epoch: 25, Train_Loss: 1.0234932899475098, Test_Loss: 0.5621967911720276 *\n",
      "Epoch: 25, Train_Loss: 0.8564670085906982, Test_Loss: 0.8383026123046875\n",
      "Epoch: 25, Train_Loss: 0.3630392849445343, Test_Loss: 0.5702076554298401 *\n",
      "Epoch: 25, Train_Loss: 0.41232791543006897, Test_Loss: 0.4005264937877655 *\n",
      "Epoch: 25, Train_Loss: 0.3585847020149231, Test_Loss: 0.5737706422805786\n",
      "Epoch: 25, Train_Loss: 0.7903763055801392, Test_Loss: 0.5624963641166687 *\n",
      "Epoch: 25, Train_Loss: 0.37534305453300476, Test_Loss: 0.4719507694244385 *\n",
      "Epoch: 25, Train_Loss: 0.39082103967666626, Test_Loss: 0.7039655447006226\n",
      "Epoch: 25, Train_Loss: 0.3660883903503418, Test_Loss: 0.684944212436676 *\n",
      "Epoch: 25, Train_Loss: 0.41165292263031006, Test_Loss: 0.6076564788818359 *\n",
      "Epoch: 25, Train_Loss: 0.47179466485977173, Test_Loss: 0.4334471821784973 *\n",
      "Epoch: 25, Train_Loss: 0.475816547870636, Test_Loss: 0.3575703799724579 *\n",
      "Epoch: 25, Train_Loss: 0.46281343698501587, Test_Loss: 0.3641939163208008\n",
      "Epoch: 25, Train_Loss: 0.48322659730911255, Test_Loss: 0.38417720794677734\n",
      "Epoch: 25, Train_Loss: 0.4574524760246277, Test_Loss: 0.6824413537979126\n",
      "Epoch: 25, Train_Loss: 0.3978112041950226, Test_Loss: 0.6749963760375977 *\n",
      "Epoch: 25, Train_Loss: 0.49443137645721436, Test_Loss: 0.7069123983383179\n",
      "Epoch: 25, Train_Loss: 0.5108015537261963, Test_Loss: 0.44308292865753174 *\n",
      "Epoch: 25, Train_Loss: 0.3866930305957794, Test_Loss: 0.4637784957885742\n",
      "Epoch: 25, Train_Loss: 0.47882717847824097, Test_Loss: 0.36771392822265625 *\n",
      "Epoch: 25, Train_Loss: 0.443864107131958, Test_Loss: 0.35786619782447815 *\n",
      "Epoch: 25, Train_Loss: 0.36356496810913086, Test_Loss: 0.4161531329154968\n",
      "Epoch: 25, Train_Loss: 0.349176824092865, Test_Loss: 0.359892874956131 *\n",
      "Epoch: 25, Train_Loss: 0.3437005579471588, Test_Loss: 0.3747810423374176\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.3496164083480835, Test_Loss: 0.36932945251464844 *\n",
      "Epoch: 25, Train_Loss: 0.3418903052806854, Test_Loss: 0.4266456365585327\n",
      "Epoch: 25, Train_Loss: 0.3464963734149933, Test_Loss: 0.6214532852172852\n",
      "Epoch: 25, Train_Loss: 0.38805028796195984, Test_Loss: 0.5209920406341553 *\n",
      "Epoch: 25, Train_Loss: 0.36048582196235657, Test_Loss: 0.6726755499839783\n",
      "Epoch: 25, Train_Loss: 0.3837176561355591, Test_Loss: 0.4188235402107239 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.5404292345046997, Test_Loss: 0.41333678364753723 *\n",
      "Epoch: 25, Train_Loss: 0.6035672426223755, Test_Loss: 0.40899187326431274 *\n",
      "Epoch: 25, Train_Loss: 0.3523881435394287, Test_Loss: 0.40282315015792847 *\n",
      "Epoch: 25, Train_Loss: 0.3869020342826843, Test_Loss: 0.4556960165500641\n",
      "Epoch: 25, Train_Loss: 0.4619179368019104, Test_Loss: 2.9236180782318115\n",
      "Epoch: 25, Train_Loss: 0.46663132309913635, Test_Loss: 2.3348889350891113 *\n",
      "Epoch: 25, Train_Loss: 0.6038105487823486, Test_Loss: 0.35723981261253357 *\n",
      "Epoch: 25, Train_Loss: 0.37227344512939453, Test_Loss: 0.3684091866016388\n",
      "Epoch: 25, Train_Loss: 0.5098230838775635, Test_Loss: 0.3714735507965088\n",
      "Epoch: 25, Train_Loss: 0.5730990171432495, Test_Loss: 0.36070647835731506 *\n",
      "Epoch: 25, Train_Loss: 0.5456497669219971, Test_Loss: 0.36673465371131897\n",
      "Epoch: 25, Train_Loss: 0.36345016956329346, Test_Loss: 0.45917195081710815\n",
      "Epoch: 25, Train_Loss: 0.36544615030288696, Test_Loss: 0.41697779297828674 *\n",
      "Epoch: 25, Train_Loss: 0.36114567518234253, Test_Loss: 0.3783071041107178 *\n",
      "Epoch: 25, Train_Loss: 1.0432138442993164, Test_Loss: 0.39936983585357666\n",
      "Epoch: 25, Train_Loss: 0.8898166418075562, Test_Loss: 0.416593074798584\n",
      "Epoch: 25, Train_Loss: 0.35723719000816345, Test_Loss: 0.46218550205230713\n",
      "Epoch: 25, Train_Loss: 0.37563878297805786, Test_Loss: 0.36070409417152405 *\n",
      "Epoch: 25, Train_Loss: 0.347991943359375, Test_Loss: 0.4048965573310852\n",
      "Epoch: 25, Train_Loss: 0.4385857880115509, Test_Loss: 0.35964906215667725 *\n",
      "Epoch: 25, Train_Loss: 0.7260537147521973, Test_Loss: 0.36531826853752136\n",
      "Epoch: 25, Train_Loss: 0.35125723481178284, Test_Loss: 0.3620109260082245 *\n",
      "Epoch: 25, Train_Loss: 0.4056685268878937, Test_Loss: 0.4032595753669739\n",
      "Epoch: 25, Train_Loss: 0.38154909014701843, Test_Loss: 0.378780335187912 *\n",
      "Epoch: 25, Train_Loss: 0.5127114653587341, Test_Loss: 0.34098020195961 *\n",
      "Epoch: 25, Train_Loss: 16.406814575195312, Test_Loss: 0.385984867811203\n",
      "Epoch: 25, Train_Loss: 0.501771092414856, Test_Loss: 0.3774615228176117 *\n",
      "Epoch: 25, Train_Loss: 1.5043686628341675, Test_Loss: 0.4483761191368103\n",
      "Epoch: 25, Train_Loss: 1.2671101093292236, Test_Loss: 0.4673556089401245\n",
      "Epoch: 25, Train_Loss: 0.4271596670150757, Test_Loss: 0.39392125606536865 *\n",
      "Epoch: 25, Train_Loss: 0.612070620059967, Test_Loss: 0.38068991899490356 *\n",
      "Epoch: 25, Train_Loss: 5.959637641906738, Test_Loss: 0.42528727650642395\n",
      "Epoch: 25, Train_Loss: 4.3173956871032715, Test_Loss: 0.43659257888793945\n",
      "Epoch: 25, Train_Loss: 0.4059019982814789, Test_Loss: 0.38322508335113525 *\n",
      "Epoch: 25, Train_Loss: 0.7636300325393677, Test_Loss: 0.5955231189727783\n",
      "Epoch: 25, Train_Loss: 5.331632614135742, Test_Loss: 0.515710711479187 *\n",
      "Epoch: 25, Train_Loss: 0.5629755258560181, Test_Loss: 7.278083324432373\n",
      "Epoch: 25, Train_Loss: 0.360956609249115, Test_Loss: 1.2896629571914673 *\n",
      "Epoch: 25, Train_Loss: 0.37805435061454773, Test_Loss: 0.5583209991455078 *\n",
      "Epoch: 25, Train_Loss: 0.40478986501693726, Test_Loss: 0.6357279419898987\n",
      "Epoch: 25, Train_Loss: 0.4721718728542328, Test_Loss: 0.609439492225647 *\n",
      "Epoch: 25, Train_Loss: 0.3733125925064087, Test_Loss: 0.5262769460678101 *\n",
      "Epoch: 25, Train_Loss: 0.37316152453422546, Test_Loss: 0.5813863277435303\n",
      "Epoch: 25, Train_Loss: 0.343260794878006, Test_Loss: 0.6995490789413452\n",
      "Epoch: 25, Train_Loss: 0.3466205894947052, Test_Loss: 0.9245111346244812\n",
      "Epoch: 25, Train_Loss: 0.45451831817626953, Test_Loss: 0.5578038692474365 *\n",
      "Epoch: 25, Train_Loss: 0.35961222648620605, Test_Loss: 0.5932245850563049\n",
      "Epoch: 25, Train_Loss: 0.3945755362510681, Test_Loss: 0.438754677772522 *\n",
      "Epoch: 25, Train_Loss: 0.49674150347709656, Test_Loss: 0.5721981525421143\n",
      "Epoch: 25, Train_Loss: 0.43068137764930725, Test_Loss: 0.5495406985282898 *\n",
      "Epoch: 25, Train_Loss: 0.3729771077632904, Test_Loss: 0.5092719793319702 *\n",
      "Epoch: 25, Train_Loss: 0.39533817768096924, Test_Loss: 0.531604528427124\n",
      "Epoch: 25, Train_Loss: 0.391189843416214, Test_Loss: 0.4494260549545288 *\n",
      "Epoch: 25, Train_Loss: 0.36710530519485474, Test_Loss: 0.3879956603050232 *\n",
      "Epoch: 25, Train_Loss: 0.34464266896247864, Test_Loss: 0.36011022329330444 *\n",
      "Epoch: 25, Train_Loss: 0.3383135497570038, Test_Loss: 0.3858276903629303\n",
      "Epoch: 25, Train_Loss: 0.34030166268348694, Test_Loss: 0.3673676550388336 *\n",
      "Epoch: 25, Train_Loss: 0.3439979553222656, Test_Loss: 0.35833442211151123 *\n",
      "Epoch: 25, Train_Loss: 0.34304165840148926, Test_Loss: 0.35542213916778564 *\n",
      "Epoch: 25, Train_Loss: 0.33947110176086426, Test_Loss: 0.34794801473617554 *\n",
      "Epoch: 25, Train_Loss: 0.33863478899002075, Test_Loss: 0.3480556011199951\n",
      "Epoch: 25, Train_Loss: 0.3530556261539459, Test_Loss: 0.35888928174972534\n",
      "Epoch: 25, Train_Loss: 0.35613417625427246, Test_Loss: 0.3483300507068634 *\n",
      "Epoch: 25, Train_Loss: 0.41902273893356323, Test_Loss: 0.34211820363998413 *\n",
      "Epoch: 25, Train_Loss: 0.38399460911750793, Test_Loss: 0.38016682863235474\n",
      "Epoch: 25, Train_Loss: 0.3629491329193115, Test_Loss: 0.39332276582717896\n",
      "Epoch: 25, Train_Loss: 5.075018405914307, Test_Loss: 0.34852877259254456 *\n",
      "Epoch: 25, Train_Loss: 4.209815502166748, Test_Loss: 0.3695548176765442\n",
      "Epoch: 25, Train_Loss: 0.3592534065246582, Test_Loss: 0.7860774993896484\n",
      "Epoch: 25, Train_Loss: 0.3787532448768616, Test_Loss: 0.38182333111763 *\n",
      "Epoch: 25, Train_Loss: 0.4296743869781494, Test_Loss: 0.3622643053531647 *\n",
      "Epoch: 25, Train_Loss: 0.39084774255752563, Test_Loss: 0.3823716640472412\n",
      "Epoch: 25, Train_Loss: 0.37009307742118835, Test_Loss: 0.460683137178421\n",
      "Epoch: 25, Train_Loss: 0.3993670344352722, Test_Loss: 0.38002410531044006 *\n",
      "Epoch: 25, Train_Loss: 0.4369772672653198, Test_Loss: 0.44366681575775146\n",
      "Epoch: 25, Train_Loss: 0.5226600170135498, Test_Loss: 0.40219834446907043 *\n",
      "Epoch: 25, Train_Loss: 0.45618361234664917, Test_Loss: 0.5712106227874756\n",
      "Epoch: 25, Train_Loss: 0.35774514079093933, Test_Loss: 0.4418061673641205 *\n",
      "Epoch: 25, Train_Loss: 0.36176010966300964, Test_Loss: 0.3689689636230469 *\n",
      "Epoch: 25, Train_Loss: 0.3562760055065155, Test_Loss: 0.35884448885917664 *\n",
      "Epoch: 25, Train_Loss: 0.47273489832878113, Test_Loss: 0.3607885539531708\n",
      "Epoch: 25, Train_Loss: 0.382952481508255, Test_Loss: 0.5571689605712891\n",
      "Epoch: 25, Train_Loss: 0.3747142553329468, Test_Loss: 0.9959853291511536\n",
      "Epoch: 25, Train_Loss: 0.38191819190979004, Test_Loss: 0.5272933840751648 *\n",
      "Epoch: 25, Train_Loss: 0.36276063323020935, Test_Loss: 0.7391741275787354\n",
      "Epoch: 25, Train_Loss: 0.386991411447525, Test_Loss: 0.6208676099777222 *\n",
      "Epoch: 25, Train_Loss: 0.38982072472572327, Test_Loss: 0.6283371448516846\n",
      "Epoch: 25, Train_Loss: 0.39152753353118896, Test_Loss: 0.48928457498550415 *\n",
      "Epoch: 25, Train_Loss: 0.3404640853404999, Test_Loss: 0.3560589849948883 *\n",
      "Epoch: 25, Train_Loss: 0.33981573581695557, Test_Loss: 0.3435584604740143 *\n",
      "Epoch: 25, Train_Loss: 0.38051357865333557, Test_Loss: 0.3717983663082123\n",
      "Epoch: 25, Train_Loss: 3.6408793926239014, Test_Loss: 0.4640970230102539\n",
      "Epoch: 25, Train_Loss: 1.8499395847320557, Test_Loss: 0.8608324527740479\n",
      "Epoch: 25, Train_Loss: 0.33745694160461426, Test_Loss: 0.589400589466095 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.3927704691886902, Test_Loss: 1.8308255672454834\n",
      "Epoch: 25, Train_Loss: 0.34031352400779724, Test_Loss: 1.026628017425537 *\n",
      "Epoch: 25, Train_Loss: 0.3363475799560547, Test_Loss: 0.8730961084365845 *\n",
      "Epoch: 25, Train_Loss: 0.33588099479675293, Test_Loss: 0.48927146196365356 *\n",
      "Epoch: 25, Train_Loss: 0.34157800674438477, Test_Loss: 0.35101601481437683 *\n",
      "Epoch: 25, Train_Loss: 0.3590491712093353, Test_Loss: 0.475835919380188\n",
      "Epoch: 25, Train_Loss: 0.3500617742538452, Test_Loss: 1.1573152542114258\n",
      "Epoch: 25, Train_Loss: 0.4129515290260315, Test_Loss: 0.9167768955230713 *\n",
      "Epoch: 25, Train_Loss: 0.3361004590988159, Test_Loss: 0.4665243327617645 *\n",
      "Epoch: 25, Train_Loss: 0.33633455634117126, Test_Loss: 0.3609205186367035 *\n",
      "Epoch: 25, Train_Loss: 0.3475190997123718, Test_Loss: 0.400407612323761\n",
      "Epoch: 25, Train_Loss: 0.3760777413845062, Test_Loss: 0.6792848110198975\n",
      "Epoch: 25, Train_Loss: 0.3408721387386322, Test_Loss: 0.551875650882721 *\n",
      "Epoch: 25, Train_Loss: 0.3497360944747925, Test_Loss: 1.1269676685333252\n",
      "Epoch: 25, Train_Loss: 0.3493090569972992, Test_Loss: 0.8156920671463013 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.35056591033935547, Test_Loss: 0.43205419182777405 *\n",
      "Epoch: 25, Train_Loss: 0.33772194385528564, Test_Loss: 0.34196341037750244 *\n",
      "Epoch: 25, Train_Loss: 0.3380758762359619, Test_Loss: 0.33795008063316345 *\n",
      "Epoch: 25, Train_Loss: 0.39470839500427246, Test_Loss: 0.3624247610569\n",
      "Epoch: 25, Train_Loss: 0.3693813383579254, Test_Loss: 0.41250041127204895\n",
      "Epoch: 25, Train_Loss: 0.37704575061798096, Test_Loss: 0.5817164182662964\n",
      "Epoch: 25, Train_Loss: 0.3979836702346802, Test_Loss: 0.5424892902374268 *\n",
      "Epoch: 25, Train_Loss: 0.4416559636592865, Test_Loss: 0.4116557240486145 *\n",
      "Epoch: 25, Train_Loss: 0.3668849468231201, Test_Loss: 0.35056349635124207 *\n",
      "Epoch: 25, Train_Loss: 0.3498331308364868, Test_Loss: 0.36877962946891785\n",
      "Epoch: 25, Train_Loss: 0.3688001036643982, Test_Loss: 0.38549351692199707\n",
      "Epoch: 25, Train_Loss: 0.4046003222465515, Test_Loss: 0.5478143692016602\n",
      "Epoch: 25, Train_Loss: 0.39641308784484863, Test_Loss: 0.8924174308776855\n",
      "Epoch: 25, Train_Loss: 0.34681546688079834, Test_Loss: 0.8402388095855713 *\n",
      "Epoch: 25, Train_Loss: 0.33275023102760315, Test_Loss: 0.43798884749412537 *\n",
      "Epoch: 25, Train_Loss: 0.3352796137332916, Test_Loss: 0.369101881980896 *\n",
      "Epoch: 25, Train_Loss: 0.3343981206417084, Test_Loss: 0.351737916469574 *\n",
      "Epoch: 25, Train_Loss: 0.33576154708862305, Test_Loss: 0.3587605655193329\n",
      "Epoch: 25, Train_Loss: 0.3362472355365753, Test_Loss: 0.37079423666000366\n",
      "Epoch: 25, Train_Loss: 3.8068647384643555, Test_Loss: 0.3548787534236908 *\n",
      "Epoch: 25, Train_Loss: 1.376984715461731, Test_Loss: 0.43041762709617615\n",
      "Epoch: 25, Train_Loss: 0.33466780185699463, Test_Loss: 0.33620771765708923 *\n",
      "Epoch: 25, Train_Loss: 0.33877018094062805, Test_Loss: 0.39016908407211304\n",
      "Epoch: 25, Train_Loss: 0.3379872143268585, Test_Loss: 0.48790228366851807\n",
      "Epoch: 25, Train_Loss: 0.33496806025505066, Test_Loss: 0.6090183258056641\n",
      "Epoch: 25, Train_Loss: 0.33439406752586365, Test_Loss: 0.5428142547607422 *\n",
      "Epoch: 25, Train_Loss: 0.3356896936893463, Test_Loss: 0.35460564494132996 *\n",
      "Epoch: 25, Train_Loss: 0.3341938555240631, Test_Loss: 0.3470630943775177 *\n",
      "Epoch: 25, Train_Loss: 0.33619433641433716, Test_Loss: 0.3465091288089752 *\n",
      "Epoch: 25, Train_Loss: 0.355330228805542, Test_Loss: 0.3450963795185089 *\n",
      "Epoch: 25, Train_Loss: 0.3671723008155823, Test_Loss: 0.3472106456756592\n",
      "Epoch: 25, Train_Loss: 0.38087722659111023, Test_Loss: 1.6832846403121948\n",
      "Epoch: 25, Train_Loss: 0.3760586082935333, Test_Loss: 4.437946796417236\n",
      "Epoch: 25, Train_Loss: 0.3474823534488678, Test_Loss: 0.38029277324676514 *\n",
      "Epoch: 25, Train_Loss: 0.3576584756374359, Test_Loss: 0.35738804936408997 *\n",
      "Epoch: 25, Train_Loss: 0.4235650300979614, Test_Loss: 0.3842749297618866\n",
      "Epoch: 25, Train_Loss: 0.4312836527824402, Test_Loss: 0.3376574218273163 *\n",
      "Epoch: 25, Train_Loss: 0.40461817383766174, Test_Loss: 0.3548920452594757\n",
      "Epoch: 25, Train_Loss: 0.3732078969478607, Test_Loss: 0.40577447414398193\n",
      "Epoch: 25, Train_Loss: 0.3485243022441864, Test_Loss: 0.42734476923942566\n",
      "Epoch: 25, Train_Loss: 0.3318643569946289, Test_Loss: 0.3797462582588196 *\n",
      "Epoch: 25, Train_Loss: 0.3357824981212616, Test_Loss: 0.41750603914260864\n",
      "Epoch: 25, Train_Loss: 0.3345675468444824, Test_Loss: 0.43461453914642334\n",
      "Epoch: 25, Train_Loss: 0.34031346440315247, Test_Loss: 0.4779987931251526\n",
      "Epoch: 25, Train_Loss: 0.3350712060928345, Test_Loss: 0.35998839139938354 *\n",
      "Epoch: 25, Train_Loss: 0.33259421586990356, Test_Loss: 0.3796723186969757\n",
      "Epoch: 25, Train_Loss: 0.33438819646835327, Test_Loss: 0.40268024802207947\n",
      "Epoch: 25, Train_Loss: 0.3462946116924286, Test_Loss: 0.3563195765018463 *\n",
      "Epoch: 25, Train_Loss: 0.39163488149642944, Test_Loss: 0.3652684986591339\n",
      "Epoch: 25, Train_Loss: 0.3830907344818115, Test_Loss: 0.351358026266098 *\n",
      "Epoch: 25, Train_Loss: 0.3791988492012024, Test_Loss: 0.4562267065048218\n",
      "Epoch: 25, Train_Loss: 0.4450438916683197, Test_Loss: 0.34500551223754883 *\n",
      "Epoch: 25, Train_Loss: 0.3917160630226135, Test_Loss: 0.39905843138694763\n",
      "Epoch: 25, Train_Loss: 0.42017674446105957, Test_Loss: 0.3451807200908661 *\n",
      "Epoch: 25, Train_Loss: 0.3424423038959503, Test_Loss: 0.3945632576942444\n",
      "Epoch: 25, Train_Loss: 0.4028352200984955, Test_Loss: 0.3820987641811371 *\n",
      "Epoch: 25, Train_Loss: 0.3602046072483063, Test_Loss: 0.35266897082328796 *\n",
      "Epoch: 25, Train_Loss: 0.5971149206161499, Test_Loss: 0.3383253216743469 *\n",
      "Epoch: 25, Train_Loss: 0.33840256929397583, Test_Loss: 0.365906298160553\n",
      "Epoch: 25, Train_Loss: 0.9492923021316528, Test_Loss: 0.3464808762073517 *\n",
      "Epoch: 25, Train_Loss: 1.739121437072754, Test_Loss: 0.33960670232772827 *\n",
      "Epoch: 25, Train_Loss: 0.40794819593429565, Test_Loss: 0.4235248565673828\n",
      "Epoch: 25, Train_Loss: 0.3667355477809906, Test_Loss: 0.39321228861808777 *\n",
      "Epoch: 25, Train_Loss: 0.35075506567955017, Test_Loss: 4.310393333435059\n",
      "Epoch: 25, Train_Loss: 0.3681924343109131, Test_Loss: 2.402625799179077 *\n",
      "Epoch: 25, Train_Loss: 0.3436281681060791, Test_Loss: 0.3507637083530426 *\n",
      "Epoch: 25, Train_Loss: 0.34009069204330444, Test_Loss: 0.3435319662094116 *\n",
      "Epoch: 25, Train_Loss: 0.3756329417228699, Test_Loss: 0.3401913046836853 *\n",
      "Epoch: 25, Train_Loss: 0.36257779598236084, Test_Loss: 0.3473969101905823\n",
      "Epoch: 25, Train_Loss: 0.36461520195007324, Test_Loss: 0.34829598665237427\n",
      "Epoch: 25, Train_Loss: 0.3509365916252136, Test_Loss: 0.410745233297348\n",
      "Epoch: 25, Train_Loss: 0.3464358448982239, Test_Loss: 0.4528946280479431\n",
      "Epoch: 25, Train_Loss: 0.3392832577228546, Test_Loss: 0.332287460565567 *\n",
      "Epoch: 25, Train_Loss: 0.3466935157775879, Test_Loss: 0.3560570776462555\n",
      "Epoch: 25, Train_Loss: 0.35017675161361694, Test_Loss: 0.3599759042263031\n",
      "Epoch: 25, Train_Loss: 0.3392312526702881, Test_Loss: 0.34811654686927795 *\n",
      "Epoch: 25, Train_Loss: 0.337256520986557, Test_Loss: 0.33880549669265747 *\n",
      "Epoch: 25, Train_Loss: 0.33348357677459717, Test_Loss: 0.3628392815589905\n",
      "Epoch: 25, Train_Loss: 0.3420651853084564, Test_Loss: 0.37752988934516907\n",
      "Epoch: 25, Train_Loss: 0.3391292691230774, Test_Loss: 0.404805988073349\n",
      "Epoch: 25, Train_Loss: 0.34616219997406006, Test_Loss: 0.3621842861175537 *\n",
      "Epoch: 25, Train_Loss: 0.33725517988204956, Test_Loss: 0.37458112835884094\n",
      "Epoch: 25, Train_Loss: 0.3314455449581146, Test_Loss: 0.3443506360054016 *\n",
      "Epoch: 25, Train_Loss: 0.33049625158309937, Test_Loss: 0.337780624628067 *\n",
      "Epoch: 25, Train_Loss: 0.32982560992240906, Test_Loss: 0.33789506554603577\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.3324526846408844, Test_Loss: 0.3373258709907532 *\n",
      "Epoch: 25, Train_Loss: 0.3329929709434509, Test_Loss: 0.34004560112953186\n",
      "Epoch: 25, Train_Loss: 0.3308846354484558, Test_Loss: 0.33891111612319946 *\n",
      "Epoch: 25, Train_Loss: 0.3359714448451996, Test_Loss: 0.3421463966369629\n",
      "Epoch: 25, Train_Loss: 0.3322705924510956, Test_Loss: 0.3415508568286896 *\n",
      "Epoch: 25, Train_Loss: 0.32971516251564026, Test_Loss: 0.3360367715358734 *\n",
      "Epoch: 25, Train_Loss: 0.3366524279117584, Test_Loss: 0.34284964203834534\n",
      "Epoch: 25, Train_Loss: 0.3310074508190155, Test_Loss: 0.378898948431015\n",
      "Epoch: 25, Train_Loss: 0.3312331736087799, Test_Loss: 0.3382570743560791 *\n",
      "Epoch: 25, Train_Loss: 0.34340742230415344, Test_Loss: 0.36717984080314636\n",
      "Epoch: 25, Train_Loss: 0.3339102566242218, Test_Loss: 0.6310697793960571\n",
      "Epoch: 25, Train_Loss: 0.34069663286209106, Test_Loss: 0.3476924002170563 *\n",
      "Epoch: 25, Train_Loss: 0.33083096146583557, Test_Loss: 0.3577471077442169\n",
      "Epoch: 25, Train_Loss: 0.3306528925895691, Test_Loss: 0.41268643736839294\n",
      "Epoch: 25, Train_Loss: 0.3381870687007904, Test_Loss: 0.4863094389438629\n",
      "Epoch: 25, Train_Loss: 0.3416389524936676, Test_Loss: 0.3956949710845947 *\n",
      "Epoch: 25, Train_Loss: 0.3309711217880249, Test_Loss: 0.39123570919036865 *\n",
      "Epoch: 25, Train_Loss: 0.331767737865448, Test_Loss: 0.38098105788230896 *\n",
      "Epoch: 25, Train_Loss: 0.3372564911842346, Test_Loss: 0.5056173801422119\n",
      "Epoch: 25, Train_Loss: 0.38578158617019653, Test_Loss: 0.36118873953819275 *\n",
      "Epoch: 25, Train_Loss: 0.3390924036502838, Test_Loss: 0.3677753210067749\n",
      "Epoch: 25, Train_Loss: 0.3496972918510437, Test_Loss: 0.33678093552589417 *\n",
      "Epoch: 25, Train_Loss: 0.3315136432647705, Test_Loss: 0.34784817695617676\n",
      "Epoch: 25, Train_Loss: 0.35277071595191956, Test_Loss: 0.4967963397502899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.35703808069229126, Test_Loss: 0.8156338930130005\n",
      "Epoch: 25, Train_Loss: 0.33159080147743225, Test_Loss: 0.49465957283973694 *\n",
      "Epoch: 25, Train_Loss: 0.33784228563308716, Test_Loss: 0.8390753269195557\n",
      "Epoch: 25, Train_Loss: 0.3522225618362427, Test_Loss: 0.68109130859375 *\n",
      "Epoch: 25, Train_Loss: 0.4084133505821228, Test_Loss: 0.559357762336731 *\n",
      "Epoch: 25, Train_Loss: 0.3758799135684967, Test_Loss: 0.4897027909755707 *\n",
      "Epoch: 25, Train_Loss: 0.34723418951034546, Test_Loss: 0.3668591380119324 *\n",
      "Epoch: 25, Train_Loss: 0.36616677045822144, Test_Loss: 0.3400963842868805 *\n",
      "Epoch: 25, Train_Loss: 0.3349705934524536, Test_Loss: 0.34637385606765747\n",
      "Epoch: 25, Train_Loss: 0.3657129108905792, Test_Loss: 0.4580341875553131\n",
      "Epoch: 25, Train_Loss: 0.3295688331127167, Test_Loss: 0.7570016384124756\n",
      "Epoch: 25, Train_Loss: 0.33734190464019775, Test_Loss: 0.7051875591278076 *\n",
      "Epoch: 25, Train_Loss: 0.347350537776947, Test_Loss: 1.7234233617782593\n",
      "Epoch: 25, Train_Loss: 0.3432430326938629, Test_Loss: 1.3901399374008179 *\n",
      "Epoch: 25, Train_Loss: 0.3993193507194519, Test_Loss: 0.7400332689285278 *\n",
      "Epoch: 25, Train_Loss: 0.3440707325935364, Test_Loss: 0.5543184876441956 *\n",
      "Epoch: 25, Train_Loss: 0.36897706985473633, Test_Loss: 0.3453277349472046 *\n",
      "Epoch: 25, Train_Loss: 0.3443436026573181, Test_Loss: 0.3897061049938202\n",
      "Epoch: 25, Train_Loss: 0.3731031119823456, Test_Loss: 0.9895479679107666\n",
      "Epoch: 25, Train_Loss: 0.34777048230171204, Test_Loss: 1.1333112716674805\n",
      "Epoch: 25, Train_Loss: 0.6205860376358032, Test_Loss: 0.41201749444007874 *\n",
      "Epoch: 25, Train_Loss: 0.3703935444355011, Test_Loss: 0.38502636551856995 *\n",
      "Epoch: 25, Train_Loss: 0.344394326210022, Test_Loss: 0.3642902076244354 *\n",
      "Epoch: 25, Train_Loss: 0.33736148476600647, Test_Loss: 0.6590544581413269\n",
      "Epoch: 25, Train_Loss: 0.32660195231437683, Test_Loss: 0.48228251934051514 *\n",
      "Epoch: 25, Train_Loss: 0.33173686265945435, Test_Loss: 0.963912844657898\n",
      "Epoch: 25, Train_Loss: 0.33833250403404236, Test_Loss: 0.7852837443351746 *\n",
      "Epoch: 25, Train_Loss: 0.3347183167934418, Test_Loss: 0.5501850247383118 *\n",
      "Epoch: 25, Train_Loss: 0.3318009078502655, Test_Loss: 0.3390659987926483 *\n",
      "Epoch: 25, Train_Loss: 0.3410622775554657, Test_Loss: 0.3401017189025879\n",
      "Epoch: 26, Train_Loss: 0.32931849360466003, Test_Loss: 0.3310670852661133 *\n",
      "Epoch: 26, Train_Loss: 0.33262544870376587, Test_Loss: 0.38083416223526\n",
      "Epoch: 26, Train_Loss: 0.33806195855140686, Test_Loss: 0.6275506019592285\n",
      "Epoch: 26, Train_Loss: 0.329222708940506, Test_Loss: 0.5886818766593933 *\n",
      "Epoch: 26, Train_Loss: 0.3261745274066925, Test_Loss: 0.39422255754470825 *\n",
      "Epoch: 26, Train_Loss: 0.34325870871543884, Test_Loss: 0.35240116715431213 *\n",
      "Epoch: 26, Train_Loss: 0.3338404893875122, Test_Loss: 0.3458012044429779 *\n",
      "Epoch: 26, Train_Loss: 0.3465496003627777, Test_Loss: 0.3498051166534424\n",
      "Epoch: 26, Train_Loss: 0.33177846670150757, Test_Loss: 0.41429224610328674\n",
      "Epoch: 26, Train_Loss: 0.34087514877319336, Test_Loss: 0.6608089804649353\n",
      "Epoch: 26, Train_Loss: 0.34189939498901367, Test_Loss: 0.8263086080551147\n",
      "Epoch: 26, Train_Loss: 0.34821268916130066, Test_Loss: 0.41057074069976807 *\n",
      "Epoch: 26, Train_Loss: 0.3285347521305084, Test_Loss: 0.44609278440475464\n",
      "Epoch: 26, Train_Loss: 0.33918488025665283, Test_Loss: 0.3325822353363037 *\n",
      "Epoch: 26, Train_Loss: 0.3349166810512543, Test_Loss: 0.3402521312236786\n",
      "Epoch: 26, Train_Loss: 0.3361489474773407, Test_Loss: 0.35741958022117615\n",
      "Epoch: 26, Train_Loss: 0.3335671126842499, Test_Loss: 0.34589308500289917 *\n",
      "Epoch: 26, Train_Loss: 0.3401001989841461, Test_Loss: 0.3985741138458252\n",
      "Epoch: 26, Train_Loss: 0.3929816484451294, Test_Loss: 0.33766475319862366 *\n",
      "Epoch: 26, Train_Loss: 2.8864352703094482, Test_Loss: 0.3671134114265442\n",
      "Epoch: 26, Train_Loss: 3.1702375411987305, Test_Loss: 0.40201637148857117\n",
      "Epoch: 26, Train_Loss: 0.34607431292533875, Test_Loss: 0.6594910621643066\n",
      "Epoch: 26, Train_Loss: 0.32753968238830566, Test_Loss: 0.5109195709228516 *\n",
      "Epoch: 26, Train_Loss: 0.39981043338775635, Test_Loss: 0.3486526608467102 *\n",
      "Epoch: 26, Train_Loss: 0.4451935887336731, Test_Loss: 0.3306402564048767 *\n",
      "Epoch: 26, Train_Loss: 0.3446583151817322, Test_Loss: 0.331682413816452\n",
      "Epoch: 26, Train_Loss: 0.3267074525356293, Test_Loss: 0.3321746587753296\n",
      "Epoch: 26, Train_Loss: 0.3580998182296753, Test_Loss: 0.33107244968414307 *\n",
      "Epoch: 26, Train_Loss: 0.3905069828033447, Test_Loss: 0.6517822742462158\n",
      "Epoch: 26, Train_Loss: 0.34009766578674316, Test_Loss: 5.4919281005859375\n",
      "Epoch: 26, Train_Loss: 0.39110761880874634, Test_Loss: 0.4134339690208435 *\n",
      "Epoch: 26, Train_Loss: 1.0981861352920532, Test_Loss: 0.36146360635757446 *\n",
      "Epoch: 26, Train_Loss: 1.0795292854309082, Test_Loss: 0.36610230803489685\n",
      "Epoch: 26, Train_Loss: 0.5101617574691772, Test_Loss: 0.33683446049690247 *\n",
      "Epoch: 26, Train_Loss: 0.43789371848106384, Test_Loss: 0.335141122341156 *\n",
      "Epoch: 26, Train_Loss: 1.5150846242904663, Test_Loss: 0.3752579689025879\n",
      "Epoch: 26, Train_Loss: 1.3246452808380127, Test_Loss: 0.492964506149292\n",
      "Epoch: 26, Train_Loss: 0.3474852740764618, Test_Loss: 0.3873876929283142 *\n",
      "Epoch: 26, Train_Loss: 0.3475223183631897, Test_Loss: 0.40950098633766174\n",
      "Epoch: 26, Train_Loss: 0.5947847962379456, Test_Loss: 0.4278949797153473\n",
      "Epoch: 26, Train_Loss: 1.0888313055038452, Test_Loss: 0.505082368850708\n",
      "Epoch: 26, Train_Loss: 0.9975090026855469, Test_Loss: 0.34720751643180847 *\n",
      "Epoch: 26, Train_Loss: 0.334464967250824, Test_Loss: 0.37024134397506714\n",
      "Epoch: 26, Train_Loss: 0.3810232877731323, Test_Loss: 0.38366034626960754\n",
      "Epoch: 26, Train_Loss: 0.3860464096069336, Test_Loss: 0.35854795575141907 *\n",
      "Epoch: 26, Train_Loss: 0.6688331365585327, Test_Loss: 0.4031711220741272\n",
      "Epoch: 26, Train_Loss: 0.33388379216194153, Test_Loss: 0.44269031286239624\n",
      "Epoch: 26, Train_Loss: 0.389077365398407, Test_Loss: 0.4922751784324646\n",
      "Epoch: 26, Train_Loss: 0.36344313621520996, Test_Loss: 0.3724405765533447 *\n",
      "Epoch: 26, Train_Loss: 0.3766420781612396, Test_Loss: 0.45972925424575806\n",
      "Epoch: 26, Train_Loss: 0.46292662620544434, Test_Loss: 0.37571144104003906 *\n",
      "Epoch: 26, Train_Loss: 0.47349268198013306, Test_Loss: 0.4232679605484009\n",
      "Epoch: 26, Train_Loss: 0.44307741522789, Test_Loss: 0.4523598849773407\n",
      "Epoch: 26, Train_Loss: 0.36881348490715027, Test_Loss: 0.4711732864379883\n",
      "Epoch: 26, Train_Loss: 0.44419190287590027, Test_Loss: 0.41915804147720337 *\n",
      "Epoch: 26, Train_Loss: 0.3827946186065674, Test_Loss: 0.4469672739505768\n",
      "Epoch: 26, Train_Loss: 0.4965096414089203, Test_Loss: 0.6508216857910156\n",
      "Epoch: 26, Train_Loss: 0.5181914567947388, Test_Loss: 0.468600332736969 *\n",
      "Epoch: 26, Train_Loss: 0.35772261023521423, Test_Loss: 0.35815781354904175 *\n",
      "Epoch: 26, Train_Loss: 0.4193871021270752, Test_Loss: 0.4876091182231903\n",
      "Epoch: 26, Train_Loss: 0.39785540103912354, Test_Loss: 3.1272008419036865\n",
      "Epoch: 26, Train_Loss: 0.3547862768173218, Test_Loss: 3.7910654544830322\n",
      "Epoch: 26, Train_Loss: 0.3336055278778076, Test_Loss: 0.3594791889190674 *\n",
      "Epoch: 26, Train_Loss: 0.33462992310523987, Test_Loss: 0.4023095369338989\n",
      "Epoch: 26, Train_Loss: 0.3274718225002289, Test_Loss: 0.36434465646743774 *\n",
      "Epoch: 26, Train_Loss: 0.3257335424423218, Test_Loss: 0.39845678210258484\n",
      "Epoch: 26, Train_Loss: 0.33246204257011414, Test_Loss: 0.3658084571361542 *\n",
      "Epoch: 26, Train_Loss: 0.35135871171951294, Test_Loss: 0.370260089635849\n",
      "Epoch: 26, Train_Loss: 0.34329015016555786, Test_Loss: 0.4616604149341583\n",
      "Epoch: 26, Train_Loss: 0.36343681812286377, Test_Loss: 0.3664996325969696 *\n",
      "Epoch: 26, Train_Loss: 0.4981416165828705, Test_Loss: 0.47188061475753784\n",
      "Epoch: 26, Train_Loss: 0.6655377149581909, Test_Loss: 0.4765748381614685\n",
      "Epoch: 26, Train_Loss: 0.3459794819355011, Test_Loss: 0.5991913080215454\n",
      "Epoch: 26, Train_Loss: 0.38660383224487305, Test_Loss: 0.43729981780052185 *\n",
      "Epoch: 26, Train_Loss: 0.4618713855743408, Test_Loss: 0.427787184715271 *\n",
      "Epoch: 26, Train_Loss: 0.46677130460739136, Test_Loss: 0.3894675374031067 *\n",
      "Epoch: 26, Train_Loss: 0.5336359143257141, Test_Loss: 0.38526153564453125 *\n",
      "Epoch: 26, Train_Loss: 0.3713569641113281, Test_Loss: 0.42860978841781616\n",
      "Epoch: 26, Train_Loss: 0.5454827547073364, Test_Loss: 0.5661258697509766\n",
      "Epoch: 26, Train_Loss: 0.5377722978591919, Test_Loss: 0.4707713723182678 *\n",
      "Epoch: 26, Train_Loss: 0.5160830020904541, Test_Loss: 0.34866365790367126 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 0.3504974842071533, Test_Loss: 0.352329283952713\n",
      "Epoch: 26, Train_Loss: 0.3425089120864868, Test_Loss: 0.35005491971969604 *\n",
      "Epoch: 26, Train_Loss: 0.4478127956390381, Test_Loss: 0.34206539392471313 *\n",
      "Epoch: 26, Train_Loss: 0.8292067050933838, Test_Loss: 0.34471428394317627\n",
      "Epoch: 26, Train_Loss: 0.8729052543640137, Test_Loss: 0.34238290786743164 *\n",
      "Epoch: 26, Train_Loss: 0.34842240810394287, Test_Loss: 0.3507882058620453\n",
      "Epoch: 26, Train_Loss: 0.34682658314704895, Test_Loss: 0.3431389331817627 *\n",
      "Epoch: 26, Train_Loss: 0.3460734784603119, Test_Loss: 0.3413130044937134 *\n",
      "Epoch: 26, Train_Loss: 0.533474326133728, Test_Loss: 0.36996546387672424\n",
      "Epoch: 26, Train_Loss: 0.6264382600784302, Test_Loss: 0.33586037158966064 *\n",
      "Epoch: 26, Train_Loss: 0.33555519580841064, Test_Loss: 0.35535767674446106\n",
      "Epoch: 26, Train_Loss: 0.38298270106315613, Test_Loss: 0.6379244327545166\n",
      "Epoch: 26, Train_Loss: 0.4106549322605133, Test_Loss: 0.3803814947605133 *\n",
      "Epoch: 26, Train_Loss: 2.86733341217041, Test_Loss: 0.35739535093307495 *\n",
      "Epoch: 26, Train_Loss: 13.985088348388672, Test_Loss: 0.3637295961380005\n",
      "Epoch: 26, Train_Loss: 0.779758095741272, Test_Loss: 0.4597119390964508\n",
      "Epoch: 26, Train_Loss: 1.7609105110168457, Test_Loss: 0.43830791115760803 *\n",
      "Epoch: 26, Train_Loss: 0.8209354877471924, Test_Loss: 0.3599291443824768 *\n",
      "Epoch: 26, Train_Loss: 0.3733825981616974, Test_Loss: 0.40656325221061707\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.5685539245605469, Test_Loss: 0.47731471061706543\n",
      "Epoch: 26, Train_Loss: 7.223490238189697, Test_Loss: 0.38907167315483093 *\n",
      "Epoch: 26, Train_Loss: 2.7193479537963867, Test_Loss: 0.36875855922698975 *\n",
      "Epoch: 26, Train_Loss: 0.3606411814689636, Test_Loss: 0.35981324315071106 *\n",
      "Epoch: 26, Train_Loss: 2.0416626930236816, Test_Loss: 0.4061965346336365\n",
      "Epoch: 26, Train_Loss: 4.233255386352539, Test_Loss: 0.5098986625671387\n",
      "Epoch: 26, Train_Loss: 0.5134944915771484, Test_Loss: 0.5843232870101929\n",
      "Epoch: 26, Train_Loss: 0.3318496346473694, Test_Loss: 0.7267943620681763\n",
      "Epoch: 26, Train_Loss: 0.3239871561527252, Test_Loss: 0.8749738931655884\n",
      "Epoch: 26, Train_Loss: 0.33271852135658264, Test_Loss: 0.5864267945289612 *\n",
      "Epoch: 26, Train_Loss: 0.3676624000072479, Test_Loss: 0.646643877029419\n",
      "Epoch: 26, Train_Loss: 0.3403230905532837, Test_Loss: 0.48446816205978394 *\n",
      "Epoch: 26, Train_Loss: 0.32096052169799805, Test_Loss: 0.4449714124202728 *\n",
      "Epoch: 26, Train_Loss: 0.32030051946640015, Test_Loss: 0.37891554832458496 *\n",
      "Epoch: 26, Train_Loss: 0.3298436403274536, Test_Loss: 0.35877639055252075 *\n",
      "Epoch: 26, Train_Loss: 0.3821524679660797, Test_Loss: 0.39099887013435364\n",
      "Epoch: 26, Train_Loss: 0.34306856989860535, Test_Loss: 0.7279332876205444\n",
      "Epoch: 26, Train_Loss: 0.3464258015155792, Test_Loss: 0.7900285124778748\n",
      "Epoch: 26, Train_Loss: 0.5102543234825134, Test_Loss: 1.1924313306808472\n",
      "Epoch: 26, Train_Loss: 0.46326735615730286, Test_Loss: 1.222272515296936\n",
      "Epoch: 26, Train_Loss: 0.368815541267395, Test_Loss: 1.0492757558822632 *\n",
      "Epoch: 26, Train_Loss: 0.3869473338127136, Test_Loss: 0.6754359006881714 *\n",
      "Epoch: 26, Train_Loss: 0.41600579023361206, Test_Loss: 0.3963392376899719 *\n",
      "Epoch: 26, Train_Loss: 0.3263106942176819, Test_Loss: 0.3762696385383606 *\n",
      "Epoch: 26, Train_Loss: 0.3254436254501343, Test_Loss: 0.6647669076919556\n",
      "Epoch: 26, Train_Loss: 0.3193838894367218, Test_Loss: 1.247591495513916\n",
      "Epoch: 26, Train_Loss: 0.32036885619163513, Test_Loss: 0.3694755434989929 *\n",
      "Epoch: 26, Train_Loss: 0.32468685507774353, Test_Loss: 0.36842846870422363 *\n",
      "Epoch: 26, Train_Loss: 0.32438603043556213, Test_Loss: 0.34734657406806946 *\n",
      "Epoch: 26, Train_Loss: 0.32100909948349, Test_Loss: 0.5965619087219238\n",
      "Epoch: 26, Train_Loss: 0.32889366149902344, Test_Loss: 0.6223141551017761\n",
      "Epoch: 26, Train_Loss: 0.33949771523475647, Test_Loss: 0.7029564380645752\n",
      "Epoch: 26, Train_Loss: 0.34281235933303833, Test_Loss: 0.7968277931213379\n",
      "Epoch: 26, Train_Loss: 0.418449342250824, Test_Loss: 0.6328597068786621 *\n",
      "Epoch: 26, Train_Loss: 0.357714980840683, Test_Loss: 0.3309940695762634 *\n",
      "Epoch: 26, Train_Loss: 0.3562906086444855, Test_Loss: 0.37766292691230774\n",
      "Epoch: 26, Train_Loss: 7.2927751541137695, Test_Loss: 0.36790570616722107 *\n",
      "Epoch: 26, Train_Loss: 1.6268374919891357, Test_Loss: 0.38446980714797974\n",
      "Epoch: 26, Train_Loss: 0.33606287837028503, Test_Loss: 0.620711088180542\n",
      "Epoch: 26, Train_Loss: 0.3622889518737793, Test_Loss: 0.48100554943084717 *\n",
      "Epoch: 26, Train_Loss: 0.40314048528671265, Test_Loss: 0.3895918130874634 *\n",
      "Epoch: 26, Train_Loss: 0.3456059694290161, Test_Loss: 0.36930567026138306 *\n",
      "Epoch: 26, Train_Loss: 0.3755195438861847, Test_Loss: 0.33698734641075134 *\n",
      "Epoch: 26, Train_Loss: 0.39401495456695557, Test_Loss: 0.3482447862625122\n",
      "Epoch: 26, Train_Loss: 0.43685850501060486, Test_Loss: 0.4089140295982361\n",
      "Epoch: 26, Train_Loss: 0.47085022926330566, Test_Loss: 0.6953477263450623\n",
      "Epoch: 26, Train_Loss: 0.41660428047180176, Test_Loss: 0.7786055207252502\n",
      "Epoch: 26, Train_Loss: 0.33094561100006104, Test_Loss: 0.4394384026527405 *\n",
      "Epoch: 26, Train_Loss: 0.3498987853527069, Test_Loss: 0.5101149678230286\n",
      "Epoch: 26, Train_Loss: 0.3351696729660034, Test_Loss: 0.36499908566474915 *\n",
      "Epoch: 26, Train_Loss: 0.4338652491569519, Test_Loss: 0.36953258514404297\n",
      "Epoch: 26, Train_Loss: 0.3492538332939148, Test_Loss: 0.3640677034854889 *\n",
      "Epoch: 26, Train_Loss: 0.3618617653846741, Test_Loss: 0.38664859533309937\n",
      "Epoch: 26, Train_Loss: 0.3783999979496002, Test_Loss: 0.370709627866745 *\n",
      "Epoch: 26, Train_Loss: 0.3873494863510132, Test_Loss: 0.35449352860450745 *\n",
      "Epoch: 26, Train_Loss: 0.4089309573173523, Test_Loss: 0.3707982897758484\n",
      "Epoch: 26, Train_Loss: 0.35332444310188293, Test_Loss: 0.38605040311813354\n",
      "Epoch: 26, Train_Loss: 0.4617316722869873, Test_Loss: 0.6722798347473145\n",
      "Epoch: 26, Train_Loss: 0.3462223410606384, Test_Loss: 0.38457635045051575 *\n",
      "Epoch: 26, Train_Loss: 0.332999587059021, Test_Loss: 0.49784135818481445\n",
      "Epoch: 26, Train_Loss: 0.41330578923225403, Test_Loss: 0.33606386184692383 *\n",
      "Epoch: 26, Train_Loss: 4.31844425201416, Test_Loss: 0.33254432678222656 *\n",
      "Epoch: 26, Train_Loss: 0.522354006767273, Test_Loss: 0.3344901204109192\n",
      "Epoch: 26, Train_Loss: 0.3202052414417267, Test_Loss: 0.3348790109157562\n",
      "Epoch: 26, Train_Loss: 0.3596729636192322, Test_Loss: 0.3510141372680664\n",
      "Epoch: 26, Train_Loss: 0.3279075622558594, Test_Loss: 5.730949401855469\n",
      "Epoch: 26, Train_Loss: 0.3206654489040375, Test_Loss: 0.6807346343994141 *\n",
      "Epoch: 26, Train_Loss: 0.32162341475486755, Test_Loss: 0.37133291363716125 *\n",
      "Epoch: 26, Train_Loss: 0.3233431279659271, Test_Loss: 0.3570331931114197 *\n",
      "Epoch: 26, Train_Loss: 0.3368711471557617, Test_Loss: 0.3553749918937683 *\n",
      "Epoch: 26, Train_Loss: 0.3336198031902313, Test_Loss: 0.3370969891548157 *\n",
      "Epoch: 26, Train_Loss: 0.3406834900379181, Test_Loss: 0.4169781506061554\n",
      "Epoch: 26, Train_Loss: 0.318811297416687, Test_Loss: 0.608964204788208\n",
      "Epoch: 26, Train_Loss: 0.3189890682697296, Test_Loss: 0.4200962781906128 *\n",
      "Epoch: 26, Train_Loss: 0.33247673511505127, Test_Loss: 0.4933595359325409\n",
      "Epoch: 26, Train_Loss: 0.3471047282218933, Test_Loss: 0.4592331051826477 *\n",
      "Epoch: 26, Train_Loss: 0.32530099153518677, Test_Loss: 0.5591439008712769\n",
      "Epoch: 26, Train_Loss: 0.33988574147224426, Test_Loss: 0.41501569747924805 *\n",
      "Epoch: 26, Train_Loss: 0.3322635293006897, Test_Loss: 0.4074741005897522 *\n",
      "Epoch: 26, Train_Loss: 0.34117159247398376, Test_Loss: 0.4273122549057007\n",
      "Epoch: 26, Train_Loss: 0.324040025472641, Test_Loss: 0.40686488151550293 *\n",
      "Epoch: 26, Train_Loss: 0.32130178809165955, Test_Loss: 0.3773738741874695 *\n",
      "Epoch: 26, Train_Loss: 0.3765180706977844, Test_Loss: 0.3442634642124176 *\n",
      "Epoch: 26, Train_Loss: 0.35628437995910645, Test_Loss: 0.42964252829551697\n",
      "Epoch: 26, Train_Loss: 0.3676181435585022, Test_Loss: 0.3723122477531433 *\n",
      "Epoch: 26, Train_Loss: 0.38224130868911743, Test_Loss: 0.37807178497314453\n",
      "Epoch: 26, Train_Loss: 0.39359599351882935, Test_Loss: 0.34326252341270447 *\n",
      "Epoch: 26, Train_Loss: 0.3399779498577118, Test_Loss: 0.3331858515739441 *\n",
      "Epoch: 26, Train_Loss: 0.3322044312953949, Test_Loss: 0.39291852712631226\n",
      "Epoch: 26, Train_Loss: 0.33912569284439087, Test_Loss: 0.36829107999801636 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 0.38201627135276794, Test_Loss: 0.3267427682876587 *\n",
      "Epoch: 26, Train_Loss: 0.3748648762702942, Test_Loss: 0.34450170397758484\n",
      "Epoch: 26, Train_Loss: 0.3285360336303711, Test_Loss: 0.3410426378250122 *\n",
      "Epoch: 26, Train_Loss: 0.3193191587924957, Test_Loss: 0.32907113432884216 *\n",
      "Epoch: 26, Train_Loss: 0.32059746980667114, Test_Loss: 0.328888863325119 *\n",
      "Epoch: 26, Train_Loss: 0.3208889365196228, Test_Loss: 0.4499794542789459\n",
      "Epoch: 26, Train_Loss: 0.32300665974617004, Test_Loss: 1.4217631816864014\n",
      "Epoch: 26, Train_Loss: 0.32902273535728455, Test_Loss: 5.333615779876709\n",
      "Epoch: 26, Train_Loss: 3.3184680938720703, Test_Loss: 0.343893438577652 *\n",
      "Epoch: 26, Train_Loss: 0.7538508772850037, Test_Loss: 0.31841841340065 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.32775256037712097, Test_Loss: 0.33627849817276\n",
      "Epoch: 26, Train_Loss: 0.3209981620311737, Test_Loss: 0.3344305753707886 *\n",
      "Epoch: 26, Train_Loss: 0.3204007148742676, Test_Loss: 0.3353121280670166\n",
      "Epoch: 26, Train_Loss: 0.32088908553123474, Test_Loss: 0.3683105409145355\n",
      "Epoch: 26, Train_Loss: 0.32670944929122925, Test_Loss: 0.5085967183113098\n",
      "Epoch: 26, Train_Loss: 0.31876227259635925, Test_Loss: 0.35262763500213623 *\n",
      "Epoch: 26, Train_Loss: 0.3175542652606964, Test_Loss: 0.3256777226924896 *\n",
      "Epoch: 26, Train_Loss: 0.31879061460494995, Test_Loss: 0.3450213670730591\n",
      "Epoch: 26, Train_Loss: 0.3469716012477875, Test_Loss: 0.3515707552433014\n",
      "Epoch: 26, Train_Loss: 0.3525310754776001, Test_Loss: 0.32464799284935 *\n",
      "Epoch: 26, Train_Loss: 0.36770498752593994, Test_Loss: 0.3644839823246002\n",
      "Epoch: 26, Train_Loss: 0.34618085622787476, Test_Loss: 0.3475167155265808 *\n",
      "Epoch: 26, Train_Loss: 0.3233252167701721, Test_Loss: 0.44066816568374634\n",
      "Epoch: 26, Train_Loss: 0.36271363496780396, Test_Loss: 0.36089807748794556 *\n",
      "Epoch: 26, Train_Loss: 0.39868074655532837, Test_Loss: 0.35056209564208984 *\n",
      "Epoch: 26, Train_Loss: 0.40726327896118164, Test_Loss: 0.3510114848613739\n",
      "Epoch: 26, Train_Loss: 0.377423495054245, Test_Loss: 0.3471055030822754 *\n",
      "Epoch: 26, Train_Loss: 0.37657180428504944, Test_Loss: 0.3404616713523865 *\n",
      "Epoch: 26, Train_Loss: 0.3445766568183899, Test_Loss: 0.33390292525291443 *\n",
      "Epoch: 26, Train_Loss: 0.32371431589126587, Test_Loss: 0.334915429353714\n",
      "Epoch: 26, Train_Loss: 0.31731492280960083, Test_Loss: 0.3431869149208069\n",
      "Epoch: 26, Train_Loss: 0.31911930441856384, Test_Loss: 0.3291022777557373 *\n",
      "Epoch: 26, Train_Loss: 0.31836703419685364, Test_Loss: 0.3442809581756592\n",
      "Epoch: 26, Train_Loss: 0.3192044496536255, Test_Loss: 0.3281802833080292 *\n",
      "Epoch: 26, Train_Loss: 0.3199157118797302, Test_Loss: 0.3247717022895813 *\n",
      "Epoch: 26, Train_Loss: 0.3203279376029968, Test_Loss: 0.3862169086933136\n",
      "Epoch: 26, Train_Loss: 0.32497861981391907, Test_Loss: 0.3257497251033783 *\n",
      "Epoch: 26, Train_Loss: 0.3765956163406372, Test_Loss: 0.33488062024116516\n",
      "Epoch: 26, Train_Loss: 0.37145617604255676, Test_Loss: 0.5000022053718567\n",
      "Epoch: 26, Train_Loss: 0.3925417959690094, Test_Loss: 0.48930102586746216 *\n",
      "Epoch: 26, Train_Loss: 0.38871368765830994, Test_Loss: 0.3582536280155182 *\n",
      "Epoch: 26, Train_Loss: 0.38015979528427124, Test_Loss: 0.358047217130661 *\n",
      "Epoch: 26, Train_Loss: 0.40578243136405945, Test_Loss: 0.4422312378883362\n",
      "Epoch: 26, Train_Loss: 0.342235803604126, Test_Loss: 0.4547939896583557\n",
      "Epoch: 26, Train_Loss: 0.38665640354156494, Test_Loss: 0.33376678824424744 *\n",
      "Epoch: 26, Train_Loss: 0.35872241854667664, Test_Loss: 0.383504718542099\n",
      "Epoch: 26, Train_Loss: 0.5561245679855347, Test_Loss: 0.43420225381851196\n",
      "Epoch: 26, Train_Loss: 0.32610002160072327, Test_Loss: 0.44268476963043213\n",
      "Epoch: 26, Train_Loss: 1.2156429290771484, Test_Loss: 0.3921685814857483 *\n",
      "Epoch: 26, Train_Loss: 1.451070785522461, Test_Loss: 0.3303961455821991 *\n",
      "Epoch: 26, Train_Loss: 0.3953280746936798, Test_Loss: 0.3301319181919098 *\n",
      "Epoch: 26, Train_Loss: 0.3479231894016266, Test_Loss: 0.3803582191467285\n",
      "Epoch: 26, Train_Loss: 0.3335433006286621, Test_Loss: 0.6808632016181946\n",
      "Epoch: 26, Train_Loss: 0.3515484631061554, Test_Loss: 0.6922643184661865\n",
      "Epoch: 26, Train_Loss: 0.33418673276901245, Test_Loss: 0.7161542177200317\n",
      "Epoch: 26, Train_Loss: 0.3224417269229889, Test_Loss: 0.6910831928253174 *\n",
      "Epoch: 26, Train_Loss: 0.37129080295562744, Test_Loss: 0.573135495185852 *\n",
      "Epoch: 26, Train_Loss: 0.3426370918750763, Test_Loss: 0.5429375171661377 *\n",
      "Epoch: 26, Train_Loss: 0.3375180661678314, Test_Loss: 0.40646791458129883 *\n",
      "Epoch: 26, Train_Loss: 0.3359436094760895, Test_Loss: 0.3350534439086914 *\n",
      "Epoch: 26, Train_Loss: 0.3591468930244446, Test_Loss: 0.3358490765094757\n",
      "Epoch: 26, Train_Loss: 0.32433998584747314, Test_Loss: 0.37042805552482605\n",
      "Epoch: 26, Train_Loss: 0.33093926310539246, Test_Loss: 0.5792610049247742\n",
      "Epoch: 26, Train_Loss: 0.34439268708229065, Test_Loss: 0.7980709671974182\n",
      "Epoch: 26, Train_Loss: 0.331287682056427, Test_Loss: 0.8895015716552734\n",
      "Epoch: 26, Train_Loss: 0.32534006237983704, Test_Loss: 1.801271915435791\n",
      "Epoch: 26, Train_Loss: 0.32076603174209595, Test_Loss: 0.6850545406341553 *\n",
      "Epoch: 26, Train_Loss: 0.3261651396751404, Test_Loss: 0.8728746771812439\n",
      "Epoch: 26, Train_Loss: 0.3231006860733032, Test_Loss: 0.3426033556461334 *\n",
      "Epoch: 26, Train_Loss: 0.33622485399246216, Test_Loss: 0.3347124755382538 *\n",
      "Epoch: 26, Train_Loss: 0.3175986409187317, Test_Loss: 0.6787034869194031\n",
      "Epoch: 26, Train_Loss: 0.316323459148407, Test_Loss: 1.3630106449127197\n",
      "Epoch: 26, Train_Loss: 0.3151121139526367, Test_Loss: 0.4586120843887329 *\n",
      "Epoch: 26, Train_Loss: 0.31463563442230225, Test_Loss: 0.41521966457366943 *\n",
      "Epoch: 26, Train_Loss: 0.31772181391716003, Test_Loss: 0.33332890272140503 *\n",
      "Epoch: 26, Train_Loss: 0.3144989013671875, Test_Loss: 0.45040881633758545\n",
      "Epoch: 26, Train_Loss: 0.31682494282722473, Test_Loss: 0.6700708270072937\n",
      "Epoch: 26, Train_Loss: 0.31917643547058105, Test_Loss: 0.6003364324569702 *\n",
      "Epoch: 26, Train_Loss: 0.31551286578178406, Test_Loss: 0.834506630897522\n",
      "Epoch: 26, Train_Loss: 0.3177020251750946, Test_Loss: 0.6240506172180176 *\n",
      "Epoch: 26, Train_Loss: 0.3207213878631592, Test_Loss: 0.3342094123363495 *\n",
      "Epoch: 26, Train_Loss: 0.31715452671051025, Test_Loss: 0.3574931025505066\n",
      "Epoch: 26, Train_Loss: 0.31452444195747375, Test_Loss: 0.41029420495033264\n",
      "Epoch: 26, Train_Loss: 0.3312735855579376, Test_Loss: 0.36689186096191406 *\n",
      "Epoch: 26, Train_Loss: 0.3229559659957886, Test_Loss: 0.5350943207740784\n",
      "Epoch: 26, Train_Loss: 0.3193170130252838, Test_Loss: 0.6254467368125916\n",
      "Epoch: 26, Train_Loss: 0.3179348111152649, Test_Loss: 0.5561069250106812 *\n",
      "Epoch: 26, Train_Loss: 0.3178463280200958, Test_Loss: 0.41170448064804077 *\n",
      "Epoch: 26, Train_Loss: 0.33065807819366455, Test_Loss: 0.3338116705417633 *\n",
      "Epoch: 26, Train_Loss: 0.32050150632858276, Test_Loss: 0.3358335793018341\n",
      "Epoch: 26, Train_Loss: 0.32345443964004517, Test_Loss: 0.3718456029891968\n",
      "Epoch: 26, Train_Loss: 0.31838810443878174, Test_Loss: 0.6472136974334717\n",
      "Epoch: 26, Train_Loss: 0.329650342464447, Test_Loss: 0.5927647352218628 *\n",
      "Epoch: 26, Train_Loss: 0.37490808963775635, Test_Loss: 0.5628809332847595 *\n",
      "Epoch: 26, Train_Loss: 0.32625454664230347, Test_Loss: 0.49335575103759766 *\n",
      "Epoch: 26, Train_Loss: 0.332400381565094, Test_Loss: 0.35054123401641846 *\n",
      "Epoch: 26, Train_Loss: 0.31649288535118103, Test_Loss: 0.330360472202301 *\n",
      "Epoch: 26, Train_Loss: 0.34550297260284424, Test_Loss: 0.3274497985839844 *\n",
      "Epoch: 26, Train_Loss: 0.3326105773448944, Test_Loss: 0.34903809428215027\n",
      "Epoch: 26, Train_Loss: 0.32133936882019043, Test_Loss: 0.33934539556503296 *\n",
      "Epoch: 26, Train_Loss: 0.3290751874446869, Test_Loss: 0.37478107213974\n",
      "Epoch: 26, Train_Loss: 0.3693474233150482, Test_Loss: 0.3312292695045471 *\n",
      "Epoch: 26, Train_Loss: 0.40884900093078613, Test_Loss: 0.3717726171016693\n",
      "Epoch: 26, Train_Loss: 0.3638378977775574, Test_Loss: 0.6448961496353149\n",
      "Epoch: 26, Train_Loss: 0.33457669615745544, Test_Loss: 0.37853050231933594 *\n",
      "Epoch: 26, Train_Loss: 0.3514274060726166, Test_Loss: 0.5207518935203552\n",
      "Epoch: 26, Train_Loss: 0.3197346329689026, Test_Loss: 0.3263910412788391 *\n",
      "Epoch: 26, Train_Loss: 0.33490538597106934, Test_Loss: 0.3230338394641876 *\n",
      "Epoch: 26, Train_Loss: 0.316918283700943, Test_Loss: 0.3261585533618927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 0.3264528810977936, Test_Loss: 0.32352831959724426 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.32182806730270386, Test_Loss: 0.32399603724479675\n",
      "Epoch: 26, Train_Loss: 0.3244507312774658, Test_Loss: 4.275960445404053\n",
      "Epoch: 26, Train_Loss: 0.3961600959300995, Test_Loss: 1.937461018562317 *\n",
      "Epoch: 26, Train_Loss: 0.3175390362739563, Test_Loss: 0.36217325925827026 *\n",
      "Epoch: 26, Train_Loss: 0.36879923939704895, Test_Loss: 0.3318602442741394 *\n",
      "Epoch: 26, Train_Loss: 0.3251350522041321, Test_Loss: 0.3463210165500641\n",
      "Epoch: 26, Train_Loss: 0.356118381023407, Test_Loss: 0.31645968556404114 *\n",
      "Epoch: 26, Train_Loss: 0.35208964347839355, Test_Loss: 0.3362743556499481\n",
      "Epoch: 26, Train_Loss: 0.5837826132774353, Test_Loss: 0.4102356731891632\n",
      "Epoch: 26, Train_Loss: 0.32206395268440247, Test_Loss: 0.3601464033126831 *\n",
      "Epoch: 26, Train_Loss: 0.33337917923927307, Test_Loss: 0.3276904225349426 *\n",
      "Epoch: 26, Train_Loss: 0.3114778697490692, Test_Loss: 0.37995147705078125\n",
      "Epoch: 26, Train_Loss: 0.3124551475048065, Test_Loss: 0.37145912647247314 *\n",
      "Epoch: 26, Train_Loss: 0.3174261748790741, Test_Loss: 0.37790170311927795\n",
      "Epoch: 26, Train_Loss: 0.3221748471260071, Test_Loss: 0.41304048895835876\n",
      "Epoch: 26, Train_Loss: 0.31983864307403564, Test_Loss: 0.4493660032749176\n",
      "Epoch: 26, Train_Loss: 0.31730714440345764, Test_Loss: 0.34540852904319763 *\n",
      "Epoch: 26, Train_Loss: 0.33339089155197144, Test_Loss: 0.3300032913684845 *\n",
      "Epoch: 26, Train_Loss: 0.3218909502029419, Test_Loss: 0.3351411521434784\n",
      "Epoch: 26, Train_Loss: 0.3193097412586212, Test_Loss: 0.38307714462280273\n",
      "Epoch: 26, Train_Loss: 0.326111763715744, Test_Loss: 0.35556021332740784 *\n",
      "Epoch: 26, Train_Loss: 0.31486472487449646, Test_Loss: 0.32292598485946655 *\n",
      "Epoch: 26, Train_Loss: 0.31190893054008484, Test_Loss: 0.3341957926750183\n",
      "Epoch: 26, Train_Loss: 0.33379703760147095, Test_Loss: 0.34021031856536865\n",
      "Epoch: 26, Train_Loss: 0.31816035509109497, Test_Loss: 0.35917022824287415\n",
      "Epoch: 26, Train_Loss: 0.332627534866333, Test_Loss: 0.3649422526359558\n",
      "Epoch: 26, Train_Loss: 0.3116404116153717, Test_Loss: 0.3209989666938782 *\n",
      "Epoch: 26, Train_Loss: 0.32561835646629333, Test_Loss: 0.32890087366104126\n",
      "Epoch: 26, Train_Loss: 0.33188334107398987, Test_Loss: 0.3410659730434418\n",
      "Epoch: 26, Train_Loss: 0.338839054107666, Test_Loss: 0.32060694694519043 *\n",
      "Epoch: 26, Train_Loss: 0.31608498096466064, Test_Loss: 0.3163585364818573 *\n",
      "Epoch: 26, Train_Loss: 0.33034396171569824, Test_Loss: 0.428253710269928\n",
      "Epoch: 26, Train_Loss: 0.31490230560302734, Test_Loss: 0.42192143201828003 *\n",
      "Epoch: 26, Train_Loss: 0.32942795753479004, Test_Loss: 6.264115333557129\n",
      "Epoch: 26, Train_Loss: 0.31644344329833984, Test_Loss: 0.45793139934539795 *\n",
      "Epoch: 26, Train_Loss: 0.3267272412776947, Test_Loss: 0.31456416845321655 *\n",
      "Epoch: 26, Train_Loss: 0.45109522342681885, Test_Loss: 0.33191585540771484\n",
      "Epoch: 26, Train_Loss: 3.9599947929382324, Test_Loss: 0.3202208876609802 *\n",
      "Epoch: 26, Train_Loss: 2.193528652191162, Test_Loss: 0.32681453227996826\n",
      "Epoch: 26, Train_Loss: 0.3339368402957916, Test_Loss: 0.3277009427547455\n",
      "Epoch: 26, Train_Loss: 0.31274402141571045, Test_Loss: 0.46589624881744385\n",
      "Epoch: 26, Train_Loss: 0.42318254709243774, Test_Loss: 0.39847055077552795 *\n",
      "Epoch: 26, Train_Loss: 0.42231982946395874, Test_Loss: 0.31010472774505615 *\n",
      "Epoch: 26, Train_Loss: 0.3313857913017273, Test_Loss: 0.3434047996997833\n",
      "Epoch: 26, Train_Loss: 0.31253764033317566, Test_Loss: 0.32255059480667114 *\n",
      "Epoch: 26, Train_Loss: 0.36435800790786743, Test_Loss: 0.3220578134059906 *\n",
      "Epoch: 26, Train_Loss: 0.36428946256637573, Test_Loss: 0.3359140455722809\n",
      "Epoch: 26, Train_Loss: 0.3199692964553833, Test_Loss: 0.3358037769794464 *\n",
      "Epoch: 26, Train_Loss: 0.48373639583587646, Test_Loss: 0.37234655022621155\n",
      "Epoch: 26, Train_Loss: 1.0086171627044678, Test_Loss: 0.3871425688266754\n",
      "Epoch: 26, Train_Loss: 0.9689136743545532, Test_Loss: 0.34784969687461853 *\n",
      "Epoch: 26, Train_Loss: 0.4274652898311615, Test_Loss: 0.36021003127098083\n",
      "Epoch: 26, Train_Loss: 0.4073156714439392, Test_Loss: 0.33072584867477417 *\n",
      "Epoch: 26, Train_Loss: 1.7924069166183472, Test_Loss: 0.32304713129997253 *\n",
      "Epoch: 26, Train_Loss: 1.1565371751785278, Test_Loss: 0.32636162638664246\n",
      "Epoch: 26, Train_Loss: 0.3179476261138916, Test_Loss: 0.3380007743835449\n",
      "Epoch: 26, Train_Loss: 0.31987565755844116, Test_Loss: 0.33561187982559204 *\n",
      "Epoch: 26, Train_Loss: 0.7298590540885925, Test_Loss: 0.32594335079193115 *\n",
      "Epoch: 26, Train_Loss: 0.810634195804596, Test_Loss: 0.3336962163448334\n",
      "Epoch: 26, Train_Loss: 1.010682463645935, Test_Loss: 0.3299332559108734 *\n",
      "Epoch: 26, Train_Loss: 0.31667986512184143, Test_Loss: 0.3242703974246979 *\n",
      "Epoch: 26, Train_Loss: 0.3519803583621979, Test_Loss: 0.3633955419063568\n",
      "Epoch: 26, Train_Loss: 0.44701212644577026, Test_Loss: 0.33937448263168335 *\n",
      "Epoch: 26, Train_Loss: 0.692371129989624, Test_Loss: 0.33906587958335876 *\n",
      "Epoch: 26, Train_Loss: 0.3229106068611145, Test_Loss: 0.3737556040287018\n",
      "Epoch: 26, Train_Loss: 0.36525362730026245, Test_Loss: 0.584730863571167\n",
      "Epoch: 26, Train_Loss: 0.3633662462234497, Test_Loss: 0.3484114408493042 *\n",
      "Epoch: 26, Train_Loss: 0.35039040446281433, Test_Loss: 0.4072360396385193\n",
      "Epoch: 26, Train_Loss: 0.4487357437610626, Test_Loss: 0.5854824781417847\n",
      "Epoch: 26, Train_Loss: 0.4756879508495331, Test_Loss: 0.47179198265075684 *\n",
      "Epoch: 26, Train_Loss: 0.39990317821502686, Test_Loss: 0.3621329069137573 *\n",
      "Epoch: 26, Train_Loss: 0.35324472188949585, Test_Loss: 0.41636136174201965\n",
      "Epoch: 26, Train_Loss: 0.40678781270980835, Test_Loss: 0.3678198456764221 *\n",
      "Epoch: 26, Train_Loss: 0.3768444061279297, Test_Loss: 0.5248239040374756\n",
      "Epoch: 26, Train_Loss: 0.4551329016685486, Test_Loss: 0.38591620326042175 *\n",
      "Epoch: 26, Train_Loss: 0.5607492923736572, Test_Loss: 0.35753294825553894 *\n",
      "Epoch: 26, Train_Loss: 0.34586790204048157, Test_Loss: 0.3371255099773407 *\n",
      "Epoch: 26, Train_Loss: 0.38140660524368286, Test_Loss: 0.5581446886062622\n",
      "Epoch: 26, Train_Loss: 0.40062427520751953, Test_Loss: 0.7233456373214722\n",
      "Epoch: 26, Train_Loss: 0.3355936110019684, Test_Loss: 0.7784035205841064\n",
      "Epoch: 26, Train_Loss: 0.3167589604854584, Test_Loss: 0.5937398076057434 *\n",
      "Epoch: 26, Train_Loss: 0.3381136357784271, Test_Loss: 0.7741912007331848\n",
      "Epoch: 26, Train_Loss: 0.3194802701473236, Test_Loss: 0.602785050868988 *\n",
      "Epoch: 26, Train_Loss: 0.3160310685634613, Test_Loss: 0.526016116142273 *\n",
      "Epoch: 26, Train_Loss: 0.3214133083820343, Test_Loss: 0.45368921756744385 *\n",
      "Epoch: 26, Train_Loss: 0.34104448556900024, Test_Loss: 0.4073900282382965 *\n",
      "Epoch: 26, Train_Loss: 0.3392736315727234, Test_Loss: 0.33971497416496277 *\n",
      "Epoch: 26, Train_Loss: 0.3712984621524811, Test_Loss: 0.3795347809791565\n",
      "Epoch: 26, Train_Loss: 0.4680131673812866, Test_Loss: 0.6759189367294312\n",
      "Epoch: 26, Train_Loss: 0.6187573671340942, Test_Loss: 0.6127612590789795 *\n",
      "Epoch: 26, Train_Loss: 0.3444824814796448, Test_Loss: 0.6147575378417969\n",
      "Epoch: 26, Train_Loss: 0.35601863265037537, Test_Loss: 1.8741092681884766\n",
      "Epoch: 26, Train_Loss: 0.5160473585128784, Test_Loss: 0.9811530709266663 *\n",
      "Epoch: 26, Train_Loss: 0.4621739089488983, Test_Loss: 0.8106509447097778 *\n",
      "Epoch: 26, Train_Loss: 0.4353569746017456, Test_Loss: 0.3737362325191498 *\n",
      "Epoch: 26, Train_Loss: 0.37669235467910767, Test_Loss: 0.33658862113952637 *\n",
      "Epoch: 26, Train_Loss: 0.5520125031471252, Test_Loss: 0.582213282585144\n",
      "Epoch: 26, Train_Loss: 0.5141588449478149, Test_Loss: 1.221920371055603\n",
      "Epoch: 26, Train_Loss: 0.4736505150794983, Test_Loss: 0.8346465826034546 *\n",
      "Epoch: 26, Train_Loss: 0.34277623891830444, Test_Loss: 0.3639284372329712 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.3357677757740021, Test_Loss: 0.3684762716293335\n",
      "Epoch: 26, Train_Loss: 0.595128059387207, Test_Loss: 0.4234643280506134\n",
      "Epoch: 26, Train_Loss: 0.8481863737106323, Test_Loss: 0.8846572637557983\n",
      "Epoch: 26, Train_Loss: 0.6951107382774353, Test_Loss: 0.48021572828292847 *\n",
      "Epoch: 26, Train_Loss: 0.3589094281196594, Test_Loss: 0.6195228099822998\n",
      "Epoch: 26, Train_Loss: 0.35406792163848877, Test_Loss: 0.5039001107215881 *\n",
      "Epoch: 26, Train_Loss: 0.33125388622283936, Test_Loss: 0.3298175036907196 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 0.6378582715988159, Test_Loss: 0.37977251410484314\n",
      "Epoch: 26, Train_Loss: 0.5142210721969604, Test_Loss: 0.514275074005127\n",
      "Epoch: 26, Train_Loss: 0.31950199604034424, Test_Loss: 0.38046693801879883 *\n",
      "Epoch: 26, Train_Loss: 0.3409588932991028, Test_Loss: 0.45071935653686523\n",
      "Epoch: 26, Train_Loss: 0.4067552387714386, Test_Loss: 0.6108286380767822\n",
      "Epoch: 26, Train_Loss: 11.811717987060547, Test_Loss: 0.5947314500808716 *\n",
      "Epoch: 26, Train_Loss: 4.777831077575684, Test_Loss: 0.4257405400276184 *\n",
      "Epoch: 26, Train_Loss: 1.090449571609497, Test_Loss: 0.3409000635147095 *\n",
      "Epoch: 26, Train_Loss: 1.3438076972961426, Test_Loss: 0.3440335690975189\n",
      "Epoch: 26, Train_Loss: 0.4506779909133911, Test_Loss: 0.32724207639694214 *\n",
      "Epoch: 26, Train_Loss: 0.3881365656852722, Test_Loss: 0.5634616613388062\n",
      "Epoch: 26, Train_Loss: 0.941702127456665, Test_Loss: 0.6744676828384399\n",
      "Epoch: 26, Train_Loss: 8.16708755493164, Test_Loss: 0.7352334260940552\n",
      "Epoch: 26, Train_Loss: 1.3962150812149048, Test_Loss: 0.41257160902023315 *\n",
      "Epoch: 26, Train_Loss: 0.40620923042297363, Test_Loss: 0.33916908502578735 *\n",
      "Epoch: 26, Train_Loss: 3.9064581394195557, Test_Loss: 0.32760706543922424 *\n",
      "Epoch: 26, Train_Loss: 2.439728260040283, Test_Loss: 0.332046240568161\n",
      "Epoch: 26, Train_Loss: 0.4236777424812317, Test_Loss: 0.3937927186489105\n",
      "Epoch: 26, Train_Loss: 0.32718950510025024, Test_Loss: 0.3243700861930847 *\n",
      "Epoch: 26, Train_Loss: 0.3229506313800812, Test_Loss: 0.39242488145828247\n",
      "Epoch: 26, Train_Loss: 0.3493572771549225, Test_Loss: 0.33135756850242615 *\n",
      "Epoch: 26, Train_Loss: 0.32779836654663086, Test_Loss: 0.48103272914886475\n",
      "Epoch: 26, Train_Loss: 0.3184731900691986, Test_Loss: 0.6488027572631836\n",
      "Epoch: 26, Train_Loss: 0.3117845952510834, Test_Loss: 0.5063578486442566 *\n",
      "Epoch: 26, Train_Loss: 0.3063182234764099, Test_Loss: 0.5356795191764832\n",
      "Epoch: 26, Train_Loss: 0.31889069080352783, Test_Loss: 0.3476133346557617 *\n",
      "Epoch: 26, Train_Loss: 0.34391388297080994, Test_Loss: 0.32285863161087036 *\n",
      "Epoch: 26, Train_Loss: 0.3280481696128845, Test_Loss: 0.31669533252716064 *\n",
      "Epoch: 26, Train_Loss: 0.3404293358325958, Test_Loss: 0.31848254799842834\n",
      "Epoch: 26, Train_Loss: 0.4943895936012268, Test_Loss: 0.3171496093273163 *\n",
      "Epoch: 26, Train_Loss: 0.47260597348213196, Test_Loss: 2.497858762741089\n",
      "Epoch: 26, Train_Loss: 0.31871578097343445, Test_Loss: 3.415105104446411\n",
      "Epoch: 26, Train_Loss: 0.3243657350540161, Test_Loss: 0.3519686460494995 *\n",
      "Epoch: 26, Train_Loss: 0.38533613085746765, Test_Loss: 0.37027275562286377\n",
      "Epoch: 26, Train_Loss: 0.31237322092056274, Test_Loss: 0.39120763540267944\n",
      "Epoch: 26, Train_Loss: 0.31170177459716797, Test_Loss: 0.3148753345012665 *\n",
      "Epoch: 26, Train_Loss: 0.3074858784675598, Test_Loss: 0.45187604427337646\n",
      "Epoch: 26, Train_Loss: 0.30757397413253784, Test_Loss: 0.5048062205314636\n",
      "Epoch: 26, Train_Loss: 0.3070724606513977, Test_Loss: 0.5501697063446045\n",
      "Epoch: 26, Train_Loss: 0.3091709613800049, Test_Loss: 0.40451914072036743 *\n",
      "Epoch: 26, Train_Loss: 0.3066006600856781, Test_Loss: 0.4650910496711731\n",
      "Epoch: 26, Train_Loss: 0.3082618713378906, Test_Loss: 0.4656490087509155\n",
      "Epoch: 26, Train_Loss: 0.31873926520347595, Test_Loss: 0.525766134262085\n",
      "Epoch: 26, Train_Loss: 0.33248040080070496, Test_Loss: 0.3527546226978302 *\n",
      "Epoch: 26, Train_Loss: 0.347116619348526, Test_Loss: 0.38689714670181274\n",
      "Epoch: 26, Train_Loss: 0.3217437267303467, Test_Loss: 0.3569323420524597 *\n",
      "Epoch: 26, Train_Loss: 0.4470252990722656, Test_Loss: 0.3538206219673157 *\n",
      "Epoch: 27, Train_Loss: 7.501871585845947, Test_Loss: 0.4019484519958496 *\n",
      "Epoch: 27, Train_Loss: 0.5647460222244263, Test_Loss: 0.36470866203308105 *\n",
      "Epoch: 27, Train_Loss: 0.33899980783462524, Test_Loss: 0.5588169097900391\n",
      "Epoch: 27, Train_Loss: 0.39087605476379395, Test_Loss: 0.3839234411716461 *\n",
      "Epoch: 27, Train_Loss: 0.40958961844444275, Test_Loss: 0.5625079870223999\n",
      "Epoch: 27, Train_Loss: 0.37767982482910156, Test_Loss: 0.33713603019714355 *\n",
      "Epoch: 27, Train_Loss: 0.3863328695297241, Test_Loss: 0.5014340281486511\n",
      "Epoch: 27, Train_Loss: 0.3981574773788452, Test_Loss: 0.48709869384765625 *\n",
      "Epoch: 27, Train_Loss: 0.5136635899543762, Test_Loss: 0.3881162405014038 *\n",
      "Epoch: 27, Train_Loss: 0.46291762590408325, Test_Loss: 0.31894394755363464 *\n",
      "Epoch: 27, Train_Loss: 0.4121988117694855, Test_Loss: 0.3648403584957123\n",
      "Epoch: 27, Train_Loss: 0.3089718222618103, Test_Loss: 0.3720434010028839\n",
      "Epoch: 27, Train_Loss: 0.3423146605491638, Test_Loss: 0.3257392644882202 *\n",
      "Epoch: 27, Train_Loss: 0.31808480620384216, Test_Loss: 0.4474736154079437\n",
      "Epoch: 27, Train_Loss: 0.3731154203414917, Test_Loss: 0.3896361291408539 *\n",
      "Epoch: 27, Train_Loss: 0.34121087193489075, Test_Loss: 5.320258617401123\n",
      "Epoch: 27, Train_Loss: 0.3481569290161133, Test_Loss: 1.5017865896224976 *\n",
      "Epoch: 27, Train_Loss: 0.34335318207740784, Test_Loss: 0.318203330039978 *\n",
      "Epoch: 27, Train_Loss: 0.333825945854187, Test_Loss: 0.3315873146057129\n",
      "Epoch: 27, Train_Loss: 0.4155448377132416, Test_Loss: 0.313586562871933 *\n",
      "Epoch: 27, Train_Loss: 0.34331563115119934, Test_Loss: 0.3302789628505707\n",
      "Epoch: 27, Train_Loss: 0.32247394323349, Test_Loss: 0.3171788454055786 *\n",
      "Epoch: 27, Train_Loss: 0.3109521269798279, Test_Loss: 0.40099966526031494\n",
      "Epoch: 27, Train_Loss: 0.307344913482666, Test_Loss: 0.38327720761299133 *\n",
      "Epoch: 27, Train_Loss: 0.41671696305274963, Test_Loss: 0.3088202476501465 *\n",
      "Epoch: 27, Train_Loss: 5.034664154052734, Test_Loss: 0.33218124508857727\n",
      "Epoch: 27, Train_Loss: 0.32007065415382385, Test_Loss: 0.33267274498939514\n",
      "Epoch: 27, Train_Loss: 0.3094874620437622, Test_Loss: 0.3194670081138611 *\n",
      "Epoch: 27, Train_Loss: 0.3149952292442322, Test_Loss: 0.32132264971733093\n",
      "Epoch: 27, Train_Loss: 0.31315216422080994, Test_Loss: 0.324648380279541\n",
      "Epoch: 27, Train_Loss: 0.3080562949180603, Test_Loss: 0.34838762879371643\n",
      "Epoch: 27, Train_Loss: 0.30747562646865845, Test_Loss: 0.38726967573165894\n",
      "Epoch: 27, Train_Loss: 0.3082899749279022, Test_Loss: 0.3370218575000763 *\n",
      "Epoch: 27, Train_Loss: 0.32891082763671875, Test_Loss: 0.3448810577392578\n",
      "Epoch: 27, Train_Loss: 0.3189771771430969, Test_Loss: 0.32187989354133606 *\n",
      "Epoch: 27, Train_Loss: 0.31900686025619507, Test_Loss: 0.34519749879837036\n",
      "Epoch: 27, Train_Loss: 0.30683887004852295, Test_Loss: 0.33876559138298035 *\n",
      "Epoch: 27, Train_Loss: 0.3055269122123718, Test_Loss: 0.3328839838504791 *\n",
      "Epoch: 27, Train_Loss: 0.3304998576641083, Test_Loss: 0.34923163056373596\n",
      "Epoch: 27, Train_Loss: 0.32211777567863464, Test_Loss: 0.3277525007724762 *\n",
      "Epoch: 27, Train_Loss: 0.3150379955768585, Test_Loss: 0.334821879863739\n",
      "Epoch: 27, Train_Loss: 0.3246574401855469, Test_Loss: 0.32354649901390076 *\n",
      "Epoch: 27, Train_Loss: 0.3182690739631653, Test_Loss: 0.313011109828949 *\n",
      "Epoch: 27, Train_Loss: 0.32730066776275635, Test_Loss: 0.316081702709198\n",
      "Epoch: 27, Train_Loss: 0.30644214153289795, Test_Loss: 0.3255983591079712\n",
      "Epoch: 27, Train_Loss: 0.30846312642097473, Test_Loss: 0.33068588376045227\n",
      "Epoch: 27, Train_Loss: 0.36674702167510986, Test_Loss: 0.33680403232574463\n",
      "Epoch: 27, Train_Loss: 0.3935827314853668, Test_Loss: 0.6649249792098999\n",
      "Epoch: 27, Train_Loss: 0.3867346942424774, Test_Loss: 0.31933149695396423 *\n",
      "Epoch: 27, Train_Loss: 0.36012351512908936, Test_Loss: 0.3458015024662018\n",
      "Epoch: 27, Train_Loss: 0.3442932963371277, Test_Loss: 0.3603757321834564\n",
      "Epoch: 27, Train_Loss: 0.33053672313690186, Test_Loss: 0.4811365604400635\n",
      "Epoch: 27, Train_Loss: 0.3209609091281891, Test_Loss: 0.3464782238006592 *\n",
      "Epoch: 27, Train_Loss: 0.3182884752750397, Test_Loss: 0.38515737652778625\n",
      "Epoch: 27, Train_Loss: 0.4454423189163208, Test_Loss: 0.3956623673439026\n",
      "Epoch: 27, Train_Loss: 0.3673188090324402, Test_Loss: 0.5577211976051331\n",
      "Epoch: 27, Train_Loss: 0.3142717182636261, Test_Loss: 0.3564912676811218 *\n",
      "Epoch: 27, Train_Loss: 0.31022679805755615, Test_Loss: 0.3393561840057373 *\n",
      "Epoch: 27, Train_Loss: 0.30786922574043274, Test_Loss: 0.31240230798721313 *\n",
      "Epoch: 27, Train_Loss: 0.3115585744380951, Test_Loss: 0.33441251516342163\n",
      "Epoch: 27, Train_Loss: 0.30790209770202637, Test_Loss: 0.44291189312934875\n",
      "Epoch: 27, Train_Loss: 0.36082005500793457, Test_Loss: 0.9157437086105347\n",
      "Epoch: 27, Train_Loss: 2.9422054290771484, Test_Loss: 0.4760148823261261 *\n",
      "Epoch: 27, Train_Loss: 0.42570602893829346, Test_Loss: 0.8199053406715393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 0.3159021735191345, Test_Loss: 0.6743524670600891 *\n",
      "Epoch: 27, Train_Loss: 0.31087368726730347, Test_Loss: 0.616970419883728 *\n",
      "Epoch: 27, Train_Loss: 0.31054574251174927, Test_Loss: 0.5189598202705383 *\n",
      "Epoch: 27, Train_Loss: 0.3073056936264038, Test_Loss: 0.33203253149986267 *\n",
      "Epoch: 27, Train_Loss: 0.30565837025642395, Test_Loss: 0.31078681349754333 *\n",
      "Epoch: 27, Train_Loss: 0.3048785328865051, Test_Loss: 0.32589566707611084\n",
      "Epoch: 27, Train_Loss: 0.30593594908714294, Test_Loss: 0.40118253231048584\n",
      "Epoch: 27, Train_Loss: 0.30567482113838196, Test_Loss: 1.0110729932785034\n",
      "Epoch: 27, Train_Loss: 0.3336251378059387, Test_Loss: 0.5716785192489624 *\n",
      "Epoch: 27, Train_Loss: 0.3337697982788086, Test_Loss: 1.7274718284606934\n",
      "Epoch: 27, Train_Loss: 0.3826712667942047, Test_Loss: 1.1629434823989868 *\n",
      "Epoch: 27, Train_Loss: 0.34291547536849976, Test_Loss: 1.0951988697052002 *\n",
      "Epoch: 27, Train_Loss: 0.30651015043258667, Test_Loss: 0.5151612162590027 *\n",
      "Epoch: 27, Train_Loss: 0.3764963746070862, Test_Loss: 0.31670433282852173 *\n",
      "Epoch: 27, Train_Loss: 0.3948301076889038, Test_Loss: 0.3882075548171997\n",
      "Epoch: 27, Train_Loss: 0.3918619155883789, Test_Loss: 1.1078084707260132\n",
      "Epoch: 27, Train_Loss: 0.3939015865325928, Test_Loss: 1.0173760652542114 *\n",
      "Epoch: 27, Train_Loss: 0.3727511167526245, Test_Loss: 0.3688121438026428 *\n",
      "Epoch: 27, Train_Loss: 0.3229079246520996, Test_Loss: 0.336039662361145 *\n",
      "Epoch: 27, Train_Loss: 0.3119589686393738, Test_Loss: 0.39286503195762634\n",
      "Epoch: 27, Train_Loss: 0.30492594838142395, Test_Loss: 0.709168553352356\n",
      "Epoch: 27, Train_Loss: 0.3068915605545044, Test_Loss: 0.4828636050224304 *\n",
      "Epoch: 27, Train_Loss: 0.30920687317848206, Test_Loss: 0.7909632921218872\n",
      "Epoch: 27, Train_Loss: 0.307503342628479, Test_Loss: 0.7114524841308594 *\n",
      "Epoch: 27, Train_Loss: 0.30762824416160583, Test_Loss: 0.47618377208709717 *\n",
      "Epoch: 27, Train_Loss: 0.30774539709091187, Test_Loss: 0.3173248767852783 *\n",
      "Epoch: 27, Train_Loss: 0.310993492603302, Test_Loss: 0.3143859803676605 *\n",
      "Epoch: 27, Train_Loss: 0.37807387113571167, Test_Loss: 0.3232603669166565\n",
      "Epoch: 27, Train_Loss: 0.33894020318984985, Test_Loss: 0.3876277804374695\n",
      "Epoch: 27, Train_Loss: 0.3402300775051117, Test_Loss: 0.5754082202911377\n",
      "Epoch: 27, Train_Loss: 0.34843677282333374, Test_Loss: 0.5980862379074097\n",
      "Epoch: 27, Train_Loss: 0.4169386327266693, Test_Loss: 0.4036352038383484 *\n",
      "Epoch: 27, Train_Loss: 0.442528635263443, Test_Loss: 0.32317915558815 *\n",
      "Epoch: 27, Train_Loss: 0.3790915310382843, Test_Loss: 0.3299797475337982\n",
      "Epoch: 27, Train_Loss: 0.3776862621307373, Test_Loss: 0.3446078598499298\n",
      "Epoch: 27, Train_Loss: 0.46196985244750977, Test_Loss: 0.46264296770095825\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 0.4195902347564697, Test_Loss: 0.6397212743759155\n",
      "Epoch: 27, Train_Loss: 0.31315481662750244, Test_Loss: 0.7321804165840149\n",
      "Epoch: 27, Train_Loss: 1.4173659086227417, Test_Loss: 0.36503949761390686 *\n",
      "Epoch: 27, Train_Loss: 1.027349829673767, Test_Loss: 0.37810829281806946\n",
      "Epoch: 27, Train_Loss: 0.3369736969470978, Test_Loss: 0.3232712149620056 *\n",
      "Epoch: 27, Train_Loss: 0.33479371666908264, Test_Loss: 0.3300967514514923\n",
      "Epoch: 27, Train_Loss: 0.31988468766212463, Test_Loss: 0.35336601734161377\n",
      "Epoch: 27, Train_Loss: 0.33881333470344543, Test_Loss: 0.33020222187042236 *\n",
      "Epoch: 27, Train_Loss: 0.3368067443370819, Test_Loss: 0.37964439392089844\n",
      "Epoch: 27, Train_Loss: 0.32002100348472595, Test_Loss: 0.44380098581314087\n",
      "Epoch: 27, Train_Loss: 0.35907503962516785, Test_Loss: 0.36899328231811523 *\n",
      "Epoch: 27, Train_Loss: 0.3328527808189392, Test_Loss: 0.4529491662979126\n",
      "Epoch: 27, Train_Loss: 0.33272048830986023, Test_Loss: 0.5828245282173157\n",
      "Epoch: 27, Train_Loss: 0.40849292278289795, Test_Loss: 0.5507584810256958 *\n",
      "Epoch: 27, Train_Loss: 0.3408772647380829, Test_Loss: 0.3610241413116455 *\n",
      "Epoch: 27, Train_Loss: 0.3106038570404053, Test_Loss: 0.3363885283470154 *\n",
      "Epoch: 27, Train_Loss: 0.31971949338912964, Test_Loss: 0.33727699518203735\n",
      "Epoch: 27, Train_Loss: 0.3321988582611084, Test_Loss: 0.3356778025627136 *\n",
      "Epoch: 27, Train_Loss: 0.31728410720825195, Test_Loss: 0.3419007956981659\n",
      "Epoch: 27, Train_Loss: 0.3102402687072754, Test_Loss: 1.1424291133880615\n",
      "Epoch: 27, Train_Loss: 0.3108168840408325, Test_Loss: 4.4758124351501465\n",
      "Epoch: 27, Train_Loss: 0.3157578408718109, Test_Loss: 0.3527832329273224 *\n",
      "Epoch: 27, Train_Loss: 0.31459304690361023, Test_Loss: 0.33316314220428467 *\n",
      "Epoch: 27, Train_Loss: 0.3190762996673584, Test_Loss: 0.334372878074646\n",
      "Epoch: 27, Train_Loss: 0.3080728352069855, Test_Loss: 0.3122594356536865 *\n",
      "Epoch: 27, Train_Loss: 0.30430349707603455, Test_Loss: 0.31436070799827576\n",
      "Epoch: 27, Train_Loss: 0.301881343126297, Test_Loss: 0.3451889753341675\n",
      "Epoch: 27, Train_Loss: 0.3052847385406494, Test_Loss: 0.39860811829566956\n",
      "Epoch: 27, Train_Loss: 0.30457666516304016, Test_Loss: 0.31070053577423096 *\n",
      "Epoch: 27, Train_Loss: 0.30188700556755066, Test_Loss: 0.3405996561050415\n",
      "Epoch: 27, Train_Loss: 0.3033266067504883, Test_Loss: 0.3446536362171173\n",
      "Epoch: 27, Train_Loss: 0.3055252730846405, Test_Loss: 0.4166049063205719\n",
      "Epoch: 27, Train_Loss: 0.30259329080581665, Test_Loss: 0.3280450403690338 *\n",
      "Epoch: 27, Train_Loss: 0.30486026406288147, Test_Loss: 0.32011061906814575 *\n",
      "Epoch: 27, Train_Loss: 0.307622492313385, Test_Loss: 0.3402004539966583\n",
      "Epoch: 27, Train_Loss: 0.30571845173835754, Test_Loss: 0.35756492614746094\n",
      "Epoch: 27, Train_Loss: 0.30544161796569824, Test_Loss: 0.33632463216781616 *\n",
      "Epoch: 27, Train_Loss: 0.31653982400894165, Test_Loss: 0.32757309079170227 *\n",
      "Epoch: 27, Train_Loss: 0.308843195438385, Test_Loss: 0.44989123940467834\n",
      "Epoch: 27, Train_Loss: 0.30311688780784607, Test_Loss: 0.3419111967086792 *\n",
      "Epoch: 27, Train_Loss: 0.3011605739593506, Test_Loss: 0.3805203139781952\n",
      "Epoch: 27, Train_Loss: 0.3121371865272522, Test_Loss: 0.3242822587490082 *\n",
      "Epoch: 27, Train_Loss: 0.3188897371292114, Test_Loss: 0.35298633575439453\n",
      "Epoch: 27, Train_Loss: 0.30545082688331604, Test_Loss: 0.4145672619342804\n",
      "Epoch: 27, Train_Loss: 0.30619844794273376, Test_Loss: 0.33180615305900574 *\n",
      "Epoch: 27, Train_Loss: 0.30624622106552124, Test_Loss: 0.32083234190940857 *\n",
      "Epoch: 27, Train_Loss: 0.31671571731567383, Test_Loss: 0.3677595257759094\n",
      "Epoch: 27, Train_Loss: 0.3602311909198761, Test_Loss: 0.4267725348472595\n",
      "Epoch: 27, Train_Loss: 0.31399106979370117, Test_Loss: 0.4148494601249695 *\n",
      "Epoch: 27, Train_Loss: 0.31203585863113403, Test_Loss: 0.37382158637046814 *\n",
      "Epoch: 27, Train_Loss: 0.3038921356201172, Test_Loss: 0.41384315490722656\n",
      "Epoch: 27, Train_Loss: 0.33644935488700867, Test_Loss: 3.591003894805908\n",
      "Epoch: 27, Train_Loss: 0.33421915769577026, Test_Loss: 2.8564460277557373 *\n",
      "Epoch: 27, Train_Loss: 0.30802085995674133, Test_Loss: 0.3247784972190857 *\n",
      "Epoch: 27, Train_Loss: 0.3183129131793976, Test_Loss: 0.3097642958164215 *\n",
      "Epoch: 27, Train_Loss: 0.3262690305709839, Test_Loss: 0.3108603358268738\n",
      "Epoch: 27, Train_Loss: 0.3824038505554199, Test_Loss: 0.3236279785633087\n",
      "Epoch: 27, Train_Loss: 0.3685070872306824, Test_Loss: 0.3249436318874359\n",
      "Epoch: 27, Train_Loss: 0.3359067440032959, Test_Loss: 0.4097326993942261\n",
      "Epoch: 27, Train_Loss: 0.33456525206565857, Test_Loss: 0.5585103034973145\n",
      "Epoch: 27, Train_Loss: 0.3070688545703888, Test_Loss: 0.30776989459991455 *\n",
      "Epoch: 27, Train_Loss: 0.3410305976867676, Test_Loss: 0.33155348896980286\n",
      "Epoch: 27, Train_Loss: 0.3051992356777191, Test_Loss: 0.32430899143218994 *\n",
      "Epoch: 27, Train_Loss: 0.3284302353858948, Test_Loss: 0.3216048777103424 *\n",
      "Epoch: 27, Train_Loss: 0.3176786005496979, Test_Loss: 0.3147532045841217 *\n",
      "Epoch: 27, Train_Loss: 0.3204410672187805, Test_Loss: 0.34544140100479126\n",
      "Epoch: 27, Train_Loss: 0.39888978004455566, Test_Loss: 0.35307008028030396\n",
      "Epoch: 27, Train_Loss: 0.3058653175830841, Test_Loss: 0.4473339319229126\n",
      "Epoch: 27, Train_Loss: 0.3529217839241028, Test_Loss: 0.3482040464878082 *\n",
      "Epoch: 27, Train_Loss: 0.3153773546218872, Test_Loss: 0.33129408955574036 *\n",
      "Epoch: 27, Train_Loss: 0.32620659470558167, Test_Loss: 0.32100963592529297 *\n",
      "Epoch: 27, Train_Loss: 0.38381969928741455, Test_Loss: 0.3209759593009949 *\n",
      "Epoch: 27, Train_Loss: 0.510087251663208, Test_Loss: 0.3244571089744568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 0.30539605021476746, Test_Loss: 0.3150205612182617 *\n",
      "Epoch: 27, Train_Loss: 0.3228597044944763, Test_Loss: 0.31365060806274414 *\n",
      "Epoch: 27, Train_Loss: 0.30002906918525696, Test_Loss: 0.31622183322906494\n",
      "Epoch: 27, Train_Loss: 0.30185961723327637, Test_Loss: 0.328071653842926\n",
      "Epoch: 27, Train_Loss: 0.30431652069091797, Test_Loss: 0.32076334953308105 *\n",
      "Epoch: 27, Train_Loss: 0.31237146258354187, Test_Loss: 0.3095943033695221 *\n",
      "Epoch: 27, Train_Loss: 0.3071698546409607, Test_Loss: 0.3120500147342682\n",
      "Epoch: 27, Train_Loss: 0.3047148883342743, Test_Loss: 0.3434049189090729\n",
      "Epoch: 27, Train_Loss: 0.3111017644405365, Test_Loss: 0.3119436204433441 *\n",
      "Epoch: 27, Train_Loss: 0.3106197118759155, Test_Loss: 0.3331298828125\n",
      "Epoch: 27, Train_Loss: 0.30888131260871887, Test_Loss: 0.6270191669464111\n",
      "Epoch: 27, Train_Loss: 0.31191956996917725, Test_Loss: 0.33568844199180603 *\n",
      "Epoch: 27, Train_Loss: 0.3020985424518585, Test_Loss: 0.4183943271636963\n",
      "Epoch: 27, Train_Loss: 0.30050015449523926, Test_Loss: 0.42705851793289185\n",
      "Epoch: 27, Train_Loss: 0.31752651929855347, Test_Loss: 0.4327181279659271\n",
      "Epoch: 27, Train_Loss: 0.31177473068237305, Test_Loss: 0.3834497928619385 *\n",
      "Epoch: 27, Train_Loss: 0.32671919465065, Test_Loss: 0.3383176326751709 *\n",
      "Epoch: 27, Train_Loss: 0.3056049346923828, Test_Loss: 0.41441047191619873\n",
      "Epoch: 27, Train_Loss: 0.313665509223938, Test_Loss: 0.48717862367630005\n",
      "Epoch: 27, Train_Loss: 0.31844472885131836, Test_Loss: 0.3239581882953644 *\n",
      "Epoch: 27, Train_Loss: 0.32721173763275146, Test_Loss: 0.3364439904689789\n",
      "Epoch: 27, Train_Loss: 0.30717387795448303, Test_Loss: 0.3119753897190094 *\n",
      "Epoch: 27, Train_Loss: 0.3181798756122589, Test_Loss: 0.3313309848308563\n",
      "Epoch: 27, Train_Loss: 0.2995155453681946, Test_Loss: 0.4585930109024048\n",
      "Epoch: 27, Train_Loss: 0.3160552382469177, Test_Loss: 0.7714539766311646\n",
      "Epoch: 27, Train_Loss: 0.3090725839138031, Test_Loss: 0.510105550289154 *\n",
      "Epoch: 27, Train_Loss: 0.31690776348114014, Test_Loss: 0.9167392253875732\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 1.192021131515503, Test_Loss: 0.6923387050628662 *\n",
      "Epoch: 27, Train_Loss: 4.351202487945557, Test_Loss: 0.5487569570541382 *\n",
      "Epoch: 27, Train_Loss: 0.9499632120132446, Test_Loss: 0.48989585041999817 *\n",
      "Epoch: 27, Train_Loss: 0.3194587826728821, Test_Loss: 0.3528522253036499 *\n",
      "Epoch: 27, Train_Loss: 0.3041219711303711, Test_Loss: 0.35156479477882385 *\n",
      "Epoch: 27, Train_Loss: 0.3869940936565399, Test_Loss: 0.31406348943710327 *\n",
      "Epoch: 27, Train_Loss: 0.37807533144950867, Test_Loss: 0.4209263324737549\n",
      "Epoch: 27, Train_Loss: 0.31685829162597656, Test_Loss: 0.7492629289627075\n",
      "Epoch: 27, Train_Loss: 0.302513062953949, Test_Loss: 0.7329688668251038 *\n",
      "Epoch: 27, Train_Loss: 0.36996471881866455, Test_Loss: 1.6310964822769165\n",
      "Epoch: 27, Train_Loss: 0.34298983216285706, Test_Loss: 1.457132339477539 *\n",
      "Epoch: 27, Train_Loss: 0.3067995309829712, Test_Loss: 0.8904516696929932 *\n",
      "Epoch: 27, Train_Loss: 0.5658252835273743, Test_Loss: 0.6158902645111084 *\n",
      "Epoch: 27, Train_Loss: 0.9944741725921631, Test_Loss: 0.33960384130477905 *\n",
      "Epoch: 27, Train_Loss: 0.9779192209243774, Test_Loss: 0.33271607756614685 *\n",
      "Epoch: 27, Train_Loss: 0.40357068181037903, Test_Loss: 1.0043342113494873\n",
      "Epoch: 27, Train_Loss: 0.44194793701171875, Test_Loss: 1.2168359756469727\n",
      "Epoch: 27, Train_Loss: 1.744996428489685, Test_Loss: 0.34608498215675354 *\n",
      "Epoch: 27, Train_Loss: 1.0129923820495605, Test_Loss: 0.3357522785663605 *\n",
      "Epoch: 27, Train_Loss: 0.3043385148048401, Test_Loss: 0.34258535504341125\n",
      "Epoch: 27, Train_Loss: 0.3184322416782379, Test_Loss: 0.6818242073059082\n",
      "Epoch: 27, Train_Loss: 0.7246003746986389, Test_Loss: 0.6030463576316833 *\n",
      "Epoch: 27, Train_Loss: 0.7353894710540771, Test_Loss: 0.7420838475227356\n",
      "Epoch: 27, Train_Loss: 0.962470531463623, Test_Loss: 0.6663532257080078 *\n",
      "Epoch: 27, Train_Loss: 0.3131951093673706, Test_Loss: 0.6264063119888306 *\n",
      "Epoch: 27, Train_Loss: 0.3310616612434387, Test_Loss: 0.3169581890106201 *\n",
      "Epoch: 27, Train_Loss: 0.5282840132713318, Test_Loss: 0.42350101470947266\n",
      "Epoch: 27, Train_Loss: 0.5231481790542603, Test_Loss: 0.36612969636917114 *\n",
      "Epoch: 27, Train_Loss: 0.3742041289806366, Test_Loss: 0.37972187995910645\n",
      "Epoch: 27, Train_Loss: 0.41238144040107727, Test_Loss: 0.676327109336853\n",
      "Epoch: 27, Train_Loss: 0.3425021469593048, Test_Loss: 0.6147887706756592 *\n",
      "Epoch: 27, Train_Loss: 0.3546335995197296, Test_Loss: 0.4429543912410736 *\n",
      "Epoch: 27, Train_Loss: 0.41836249828338623, Test_Loss: 0.3401613235473633 *\n",
      "Epoch: 27, Train_Loss: 0.43501824140548706, Test_Loss: 0.3328002691268921 *\n",
      "Epoch: 27, Train_Loss: 0.36832213401794434, Test_Loss: 0.30409660935401917 *\n",
      "Epoch: 27, Train_Loss: 0.3877984881401062, Test_Loss: 0.34854012727737427\n",
      "Epoch: 27, Train_Loss: 0.41524359583854675, Test_Loss: 0.7343201041221619\n",
      "Epoch: 27, Train_Loss: 0.433199018239975, Test_Loss: 0.6756141185760498 *\n",
      "Epoch: 27, Train_Loss: 0.555303156375885, Test_Loss: 0.3693106472492218 *\n",
      "Epoch: 27, Train_Loss: 0.5395268797874451, Test_Loss: 0.43542903661727905\n",
      "Epoch: 27, Train_Loss: 0.3335486650466919, Test_Loss: 0.30842268466949463 *\n",
      "Epoch: 27, Train_Loss: 0.35107746720314026, Test_Loss: 0.3191349506378174\n",
      "Epoch: 27, Train_Loss: 0.35475319623947144, Test_Loss: 0.34393349289894104\n",
      "Epoch: 27, Train_Loss: 0.31487077474594116, Test_Loss: 0.37040287256240845\n",
      "Epoch: 27, Train_Loss: 0.30028730630874634, Test_Loss: 0.31695201992988586 *\n",
      "Epoch: 27, Train_Loss: 0.2990031838417053, Test_Loss: 0.3716903626918793\n",
      "Epoch: 27, Train_Loss: 0.3000781238079071, Test_Loss: 0.3486832082271576 *\n",
      "Epoch: 27, Train_Loss: 0.3022182881832123, Test_Loss: 0.36873140931129456\n",
      "Epoch: 27, Train_Loss: 0.31321632862091064, Test_Loss: 0.6602929830551147\n",
      "Epoch: 27, Train_Loss: 0.32829269766807556, Test_Loss: 0.5014509558677673 *\n",
      "Epoch: 27, Train_Loss: 0.3410215973854065, Test_Loss: 0.44743800163269043 *\n",
      "Epoch: 27, Train_Loss: 0.37550467252731323, Test_Loss: 0.3578343093395233 *\n",
      "Epoch: 27, Train_Loss: 0.46237415075302124, Test_Loss: 0.3678501546382904\n",
      "Epoch: 27, Train_Loss: 0.5024611949920654, Test_Loss: 0.36305397748947144 *\n",
      "Epoch: 27, Train_Loss: 0.31774988770484924, Test_Loss: 0.3705331087112427\n",
      "Epoch: 27, Train_Loss: 0.3485523462295532, Test_Loss: 0.48756885528564453\n",
      "Epoch: 27, Train_Loss: 0.4865434765815735, Test_Loss: 5.285024166107178\n",
      "Epoch: 27, Train_Loss: 0.43328753113746643, Test_Loss: 0.46667391061782837 *\n",
      "Epoch: 27, Train_Loss: 0.34904322028160095, Test_Loss: 0.35342293977737427 *\n",
      "Epoch: 27, Train_Loss: 0.3334522843360901, Test_Loss: 0.32884085178375244 *\n",
      "Epoch: 27, Train_Loss: 0.4792161285877228, Test_Loss: 0.3190556764602661 *\n",
      "Epoch: 27, Train_Loss: 0.47779303789138794, Test_Loss: 0.32193702459335327\n",
      "Epoch: 27, Train_Loss: 0.4651843011379242, Test_Loss: 0.3770996928215027\n",
      "Epoch: 27, Train_Loss: 0.32340168952941895, Test_Loss: 0.4819478392601013\n",
      "Epoch: 27, Train_Loss: 0.32046768069267273, Test_Loss: 0.31004267930984497 *\n",
      "Epoch: 27, Train_Loss: 0.6951305866241455, Test_Loss: 0.3406871259212494\n",
      "Epoch: 27, Train_Loss: 0.9148910045623779, Test_Loss: 0.3556571900844574\n",
      "Epoch: 27, Train_Loss: 0.4667472243309021, Test_Loss: 0.4505084753036499\n",
      "Epoch: 27, Train_Loss: 0.37900131940841675, Test_Loss: 0.31931057572364807 *\n",
      "Epoch: 27, Train_Loss: 0.32247987389564514, Test_Loss: 0.3282279372215271\n",
      "Epoch: 27, Train_Loss: 0.3199032247066498, Test_Loss: 0.32182076573371887 *\n",
      "Epoch: 27, Train_Loss: 0.5671499967575073, Test_Loss: 0.31471285223960876 *\n",
      "Epoch: 27, Train_Loss: 0.36601001024246216, Test_Loss: 0.32480594515800476\n",
      "Epoch: 27, Train_Loss: 0.3179508149623871, Test_Loss: 0.32087594270706177 *\n",
      "Epoch: 27, Train_Loss: 0.31203609704971313, Test_Loss: 0.38276124000549316\n",
      "Epoch: 27, Train_Loss: 0.48310622572898865, Test_Loss: 0.303756445646286 *\n",
      "Epoch: 27, Train_Loss: 15.336018562316895, Test_Loss: 0.3460521697998047\n",
      "Epoch: 27, Train_Loss: 0.746126651763916, Test_Loss: 0.33648771047592163 *\n",
      "Epoch: 27, Train_Loss: 1.2835133075714111, Test_Loss: 0.3913764953613281\n",
      "Epoch: 27, Train_Loss: 1.3923267126083374, Test_Loss: 0.41191524267196655\n",
      "Epoch: 27, Train_Loss: 0.32469621300697327, Test_Loss: 0.3649473190307617 *\n",
      "Epoch: 27, Train_Loss: 0.3992130756378174, Test_Loss: 0.3111249804496765 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 1.687831997871399, Test_Loss: 0.34764721989631653\n",
      "Epoch: 27, Train_Loss: 7.792529582977295, Test_Loss: 0.38439473509788513\n",
      "Epoch: 27, Train_Loss: 0.6079517602920532, Test_Loss: 0.35471129417419434 *\n",
      "Epoch: 27, Train_Loss: 0.4076358377933502, Test_Loss: 0.3215533196926117 *\n",
      "Epoch: 27, Train_Loss: 5.38831090927124, Test_Loss: 0.45009544491767883\n",
      "Epoch: 27, Train_Loss: 0.7922447919845581, Test_Loss: 2.210789442062378\n",
      "Epoch: 27, Train_Loss: 0.3521667718887329, Test_Loss: 4.415111541748047\n",
      "Epoch: 27, Train_Loss: 0.31425169110298157, Test_Loss: 0.3942670226097107 *\n",
      "Epoch: 27, Train_Loss: 0.3463458716869354, Test_Loss: 0.34932398796081543 *\n",
      "Epoch: 27, Train_Loss: 0.39539727568626404, Test_Loss: 0.5034751296043396\n",
      "Epoch: 27, Train_Loss: 0.3132128417491913, Test_Loss: 0.5654556751251221\n",
      "Epoch: 27, Train_Loss: 0.3228161633014679, Test_Loss: 0.38301095366477966 *\n",
      "Epoch: 27, Train_Loss: 0.29750585556030273, Test_Loss: 0.39943230152130127\n",
      "Epoch: 27, Train_Loss: 0.297199010848999, Test_Loss: 0.6373897790908813\n",
      "Epoch: 27, Train_Loss: 0.3006913661956787, Test_Loss: 0.37469810247421265 *\n",
      "Epoch: 27, Train_Loss: 0.33432161808013916, Test_Loss: 0.3410845398902893 *\n",
      "Epoch: 27, Train_Loss: 0.32697010040283203, Test_Loss: 0.3354339599609375 *\n",
      "Epoch: 27, Train_Loss: 0.36926424503326416, Test_Loss: 0.3337375521659851 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 0.36410677433013916, Test_Loss: 0.3640681505203247\n",
      "Epoch: 27, Train_Loss: 0.33664143085479736, Test_Loss: 0.48762789368629456\n",
      "Epoch: 27, Train_Loss: 0.3105596899986267, Test_Loss: 0.533295750617981\n",
      "Epoch: 27, Train_Loss: 0.3232267498970032, Test_Loss: 0.43793320655822754 *\n",
      "Epoch: 27, Train_Loss: 0.39624127745628357, Test_Loss: 0.33264732360839844 *\n",
      "Epoch: 27, Train_Loss: 0.30583837628364563, Test_Loss: 0.3332020342350006\n",
      "Epoch: 27, Train_Loss: 0.2989899516105652, Test_Loss: 0.3460009694099426\n",
      "Epoch: 27, Train_Loss: 0.29654428362846375, Test_Loss: 0.46404778957366943\n",
      "Epoch: 27, Train_Loss: 0.30434587597846985, Test_Loss: 0.4911886751651764\n",
      "Epoch: 27, Train_Loss: 0.30160942673683167, Test_Loss: 0.4564148783683777 *\n",
      "Epoch: 27, Train_Loss: 0.2964510917663574, Test_Loss: 0.4261207580566406 *\n",
      "Epoch: 27, Train_Loss: 0.29482463002204895, Test_Loss: 0.4235234260559082 *\n",
      "Epoch: 27, Train_Loss: 0.30295702815055847, Test_Loss: 0.48256558179855347\n",
      "Epoch: 27, Train_Loss: 0.3103320002555847, Test_Loss: 0.39933210611343384 *\n",
      "Epoch: 27, Train_Loss: 0.32179903984069824, Test_Loss: 0.34128281474113464 *\n",
      "Epoch: 27, Train_Loss: 0.34536829590797424, Test_Loss: 0.3340854346752167 *\n",
      "Epoch: 27, Train_Loss: 0.31222400069236755, Test_Loss: 0.3851483166217804\n",
      "Epoch: 27, Train_Loss: 0.41673147678375244, Test_Loss: 0.34321463108062744 *\n",
      "Epoch: 27, Train_Loss: 7.218565940856934, Test_Loss: 0.33739036321640015 *\n",
      "Epoch: 27, Train_Loss: 0.3932541608810425, Test_Loss: 0.716322660446167\n",
      "Epoch: 27, Train_Loss: 0.34798434376716614, Test_Loss: 0.3874092996120453 *\n",
      "Epoch: 27, Train_Loss: 0.39712801575660706, Test_Loss: 0.4113588333129883\n",
      "Epoch: 27, Train_Loss: 0.45027345418930054, Test_Loss: 0.3852561116218567 *\n",
      "Epoch: 27, Train_Loss: 0.4030629098415375, Test_Loss: 0.5296027660369873\n",
      "Epoch: 27, Train_Loss: 0.3775438070297241, Test_Loss: 0.4098912179470062 *\n",
      "Epoch: 27, Train_Loss: 0.361110657453537, Test_Loss: 0.30675527453422546 *\n",
      "Epoch: 27, Train_Loss: 0.5030637979507446, Test_Loss: 0.38726502656936646\n",
      "Epoch: 27, Train_Loss: 0.4329020082950592, Test_Loss: 0.5315231680870056\n",
      "Epoch: 27, Train_Loss: 0.3559384346008301, Test_Loss: 0.39216408133506775 *\n",
      "Epoch: 27, Train_Loss: 0.2972630560398102, Test_Loss: 0.35196220874786377 *\n",
      "Epoch: 27, Train_Loss: 0.32071423530578613, Test_Loss: 0.3023185133934021 *\n",
      "Epoch: 27, Train_Loss: 0.3095923662185669, Test_Loss: 0.315428763628006\n",
      "Epoch: 27, Train_Loss: 0.35522839426994324, Test_Loss: 0.35213300585746765\n",
      "Epoch: 27, Train_Loss: 0.31551167368888855, Test_Loss: 0.6737725734710693\n",
      "Epoch: 27, Train_Loss: 0.34852492809295654, Test_Loss: 0.6003632545471191 *\n",
      "Epoch: 27, Train_Loss: 0.3214774429798126, Test_Loss: 0.9163165092468262\n",
      "Epoch: 27, Train_Loss: 0.31630682945251465, Test_Loss: 0.6692132949829102 *\n",
      "Epoch: 27, Train_Loss: 0.4209781289100647, Test_Loss: 0.47724997997283936 *\n",
      "Epoch: 27, Train_Loss: 0.3430878818035126, Test_Loss: 0.5921968221664429\n",
      "Epoch: 27, Train_Loss: 0.30623072385787964, Test_Loss: 0.35923251509666443 *\n",
      "Epoch: 27, Train_Loss: 0.29957541823387146, Test_Loss: 0.31930384039878845 *\n",
      "Epoch: 27, Train_Loss: 0.29579755663871765, Test_Loss: 0.319671094417572\n",
      "Epoch: 27, Train_Loss: 0.6688976287841797, Test_Loss: 0.4475160241127014\n",
      "Epoch: 27, Train_Loss: 4.980142116546631, Test_Loss: 0.5418517589569092\n",
      "Epoch: 27, Train_Loss: 0.29872652888298035, Test_Loss: 0.8784185647964478\n",
      "Epoch: 27, Train_Loss: 0.2994803786277771, Test_Loss: 1.097650170326233\n",
      "Epoch: 27, Train_Loss: 0.3011837899684906, Test_Loss: 1.5701472759246826\n",
      "Epoch: 27, Train_Loss: 0.30344709753990173, Test_Loss: 0.6632324457168579 *\n",
      "Epoch: 27, Train_Loss: 0.298098087310791, Test_Loss: 0.7112932205200195\n",
      "Epoch: 27, Train_Loss: 0.2958795428276062, Test_Loss: 0.300291508436203 *\n",
      "Epoch: 27, Train_Loss: 0.301618754863739, Test_Loss: 0.3202623724937439\n",
      "Epoch: 27, Train_Loss: 0.3250184953212738, Test_Loss: 0.7403810620307922\n",
      "Epoch: 27, Train_Loss: 0.31795981526374817, Test_Loss: 1.39266037940979\n",
      "Epoch: 27, Train_Loss: 0.2984240651130676, Test_Loss: 0.3550874590873718 *\n",
      "Epoch: 27, Train_Loss: 0.29605838656425476, Test_Loss: 0.37219077348709106\n",
      "Epoch: 27, Train_Loss: 0.29641371965408325, Test_Loss: 0.2987010180950165 *\n",
      "Epoch: 27, Train_Loss: 0.3136446475982666, Test_Loss: 0.47239646315574646\n",
      "Epoch: 27, Train_Loss: 0.30300089716911316, Test_Loss: 0.6034756898880005\n",
      "Epoch: 27, Train_Loss: 0.2990710735321045, Test_Loss: 0.6830340623855591\n",
      "Epoch: 27, Train_Loss: 0.3100552558898926, Test_Loss: 0.949999213218689\n",
      "Epoch: 27, Train_Loss: 0.3137204945087433, Test_Loss: 0.5722190141677856 *\n",
      "Epoch: 27, Train_Loss: 0.31185948848724365, Test_Loss: 0.3017604649066925 *\n",
      "Epoch: 27, Train_Loss: 0.2950155436992645, Test_Loss: 0.29760098457336426 *\n",
      "Epoch: 27, Train_Loss: 0.29994913935661316, Test_Loss: 0.30247926712036133\n",
      "Epoch: 27, Train_Loss: 0.3289596736431122, Test_Loss: 0.3317696750164032\n",
      "Epoch: 27, Train_Loss: 0.3329920768737793, Test_Loss: 0.5543986558914185\n",
      "Epoch: 27, Train_Loss: 0.3590832054615021, Test_Loss: 0.5905090570449829\n",
      "Epoch: 27, Train_Loss: 0.35946357250213623, Test_Loss: 0.42464420199394226 *\n",
      "Epoch: 27, Train_Loss: 0.34202414751052856, Test_Loss: 0.3680904507637024 *\n",
      "Epoch: 27, Train_Loss: 0.31197166442871094, Test_Loss: 0.3126547634601593 *\n",
      "Epoch: 27, Train_Loss: 0.32091957330703735, Test_Loss: 0.30278706550598145 *\n",
      "Epoch: 27, Train_Loss: 0.30256518721580505, Test_Loss: 0.38449999690055847\n",
      "Epoch: 27, Train_Loss: 0.5177386999130249, Test_Loss: 0.7037901878356934\n",
      "Epoch: 27, Train_Loss: 0.3313996493816376, Test_Loss: 0.7120640277862549\n",
      "Epoch: 27, Train_Loss: 0.29912734031677246, Test_Loss: 0.48164400458335876 *\n",
      "Epoch: 27, Train_Loss: 0.2931177318096161, Test_Loss: 0.37111896276474 *\n",
      "Epoch: 27, Train_Loss: 0.29482483863830566, Test_Loss: 0.3036462068557739 *\n",
      "Epoch: 27, Train_Loss: 0.29402396082878113, Test_Loss: 0.3103400468826294\n",
      "Epoch: 27, Train_Loss: 0.2939653992652893, Test_Loss: 0.3065832853317261 *\n",
      "Epoch: 27, Train_Loss: 0.6881474256515503, Test_Loss: 0.33338692784309387\n",
      "Epoch: 27, Train_Loss: 3.349194049835205, Test_Loss: 0.3317570090293884 *\n",
      "Epoch: 27, Train_Loss: 0.3120787739753723, Test_Loss: 0.3329832851886749\n",
      "Epoch: 27, Train_Loss: 0.300168514251709, Test_Loss: 0.30976569652557373 *\n",
      "Epoch: 27, Train_Loss: 0.29648491740226746, Test_Loss: 0.3454139232635498\n",
      "Epoch: 27, Train_Loss: 0.294575035572052, Test_Loss: 0.6416856646537781\n",
      "Epoch: 27, Train_Loss: 0.2954992353916168, Test_Loss: 0.34395653009414673 *\n",
      "Epoch: 27, Train_Loss: 0.2976034879684448, Test_Loss: 0.474708616733551\n",
      "Epoch: 27, Train_Loss: 0.2928684949874878, Test_Loss: 0.31087642908096313 *\n",
      "Epoch: 27, Train_Loss: 0.2942602336406708, Test_Loss: 0.312689870595932\n",
      "Epoch: 27, Train_Loss: 0.29575395584106445, Test_Loss: 0.3117416203022003 *\n",
      "Epoch: 27, Train_Loss: 0.31753233075141907, Test_Loss: 0.3169502317905426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 0.3306311070919037, Test_Loss: 0.31333523988723755 *\n",
      "Epoch: 27, Train_Loss: 0.34484225511550903, Test_Loss: 4.72498083114624\n",
      "Epoch: 27, Train_Loss: 0.3223711848258972, Test_Loss: 1.0045357942581177 *\n",
      "Epoch: 27, Train_Loss: 0.29745519161224365, Test_Loss: 0.3371759057044983 *\n",
      "Epoch: 27, Train_Loss: 0.38996416330337524, Test_Loss: 0.30234256386756897 *\n",
      "Epoch: 27, Train_Loss: 0.3670925498008728, Test_Loss: 0.333132803440094\n",
      "Epoch: 27, Train_Loss: 0.3586743175983429, Test_Loss: 0.3011609613895416 *\n",
      "Epoch: 27, Train_Loss: 0.3777766227722168, Test_Loss: 0.3512013554573059\n",
      "Epoch: 27, Train_Loss: 0.34621861577033997, Test_Loss: 0.6568240523338318\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 0.2976625859737396, Test_Loss: 0.5242899656295776 *\n",
      "Epoch: 27, Train_Loss: 0.2947331964969635, Test_Loss: 0.539024829864502\n",
      "Epoch: 27, Train_Loss: 0.294170618057251, Test_Loss: 0.5494675636291504\n",
      "Epoch: 27, Train_Loss: 0.297510027885437, Test_Loss: 0.5927042961120605\n",
      "Epoch: 27, Train_Loss: 0.3013158440589905, Test_Loss: 0.45779281854629517 *\n",
      "Epoch: 27, Train_Loss: 0.29646340012550354, Test_Loss: 0.3855482041835785 *\n",
      "Epoch: 27, Train_Loss: 0.29209062457084656, Test_Loss: 0.46005958318710327\n",
      "Epoch: 27, Train_Loss: 0.30054980516433716, Test_Loss: 0.4646221399307251\n",
      "Epoch: 27, Train_Loss: 0.3191349506378174, Test_Loss: 0.3252219557762146 *\n",
      "Epoch: 27, Train_Loss: 0.3574479818344116, Test_Loss: 0.3065859377384186 *\n",
      "Epoch: 27, Train_Loss: 0.32133185863494873, Test_Loss: 0.38605231046676636\n",
      "Epoch: 27, Train_Loss: 0.3633800148963928, Test_Loss: 0.3440055549144745 *\n",
      "Epoch: 27, Train_Loss: 0.33812353014945984, Test_Loss: 0.33824560046195984 *\n",
      "Epoch: 27, Train_Loss: 0.34421154856681824, Test_Loss: 0.35665154457092285\n",
      "Epoch: 27, Train_Loss: 0.36413344740867615, Test_Loss: 0.3147454261779785 *\n",
      "Epoch: 27, Train_Loss: 0.3625222146511078, Test_Loss: 0.325601726770401\n",
      "Epoch: 27, Train_Loss: 0.3314058482646942, Test_Loss: 0.3440014719963074\n",
      "Epoch: 27, Train_Loss: 0.4844529628753662, Test_Loss: 0.30450552701950073 *\n",
      "Epoch: 27, Train_Loss: 0.3057425618171692, Test_Loss: 0.30784547328948975\n",
      "Epoch: 27, Train_Loss: 0.3135671317577362, Test_Loss: 0.33605876564979553\n",
      "Epoch: 27, Train_Loss: 1.6124483346939087, Test_Loss: 0.3085583448410034 *\n",
      "Epoch: 27, Train_Loss: 0.6898088455200195, Test_Loss: 0.29888761043548584 *\n",
      "Epoch: 27, Train_Loss: 0.3130560517311096, Test_Loss: 0.3756667971611023\n",
      "Epoch: 27, Train_Loss: 0.32581889629364014, Test_Loss: 0.6751109957695007\n",
      "Epoch: 27, Train_Loss: 0.3063298463821411, Test_Loss: 5.546842098236084\n",
      "Epoch: 27, Train_Loss: 0.32492074370384216, Test_Loss: 0.30753788352012634 *\n",
      "Epoch: 27, Train_Loss: 0.30049777030944824, Test_Loss: 0.29462572932243347 *\n",
      "Epoch: 27, Train_Loss: 0.3187556862831116, Test_Loss: 0.31267619132995605\n",
      "Epoch: 27, Train_Loss: 0.3681214451789856, Test_Loss: 0.30675071477890015 *\n",
      "Epoch: 27, Train_Loss: 0.328411728143692, Test_Loss: 0.3313189446926117\n",
      "Epoch: 27, Train_Loss: 0.3249571919441223, Test_Loss: 0.3005006015300751 *\n",
      "Epoch: 27, Train_Loss: 0.3176525831222534, Test_Loss: 0.39294102787971497\n",
      "Epoch: 27, Train_Loss: 0.3117199242115021, Test_Loss: 0.3323153853416443 *\n",
      "Epoch: 27, Train_Loss: 0.312652587890625, Test_Loss: 0.2940000891685486 *\n",
      "Epoch: 27, Train_Loss: 0.30508139729499817, Test_Loss: 0.32160648703575134\n",
      "Epoch: 27, Train_Loss: 0.3165189325809479, Test_Loss: 0.307156503200531 *\n",
      "Epoch: 27, Train_Loss: 0.3040432929992676, Test_Loss: 0.30090659856796265 *\n",
      "Epoch: 27, Train_Loss: 0.29321032762527466, Test_Loss: 0.3276543915271759\n",
      "Epoch: 27, Train_Loss: 0.3007643222808838, Test_Loss: 0.3521272540092468\n",
      "Epoch: 27, Train_Loss: 0.3133088946342468, Test_Loss: 0.35537153482437134\n",
      "Epoch: 27, Train_Loss: 0.30627191066741943, Test_Loss: 0.3581687808036804\n",
      "Epoch: 27, Train_Loss: 0.2976682782173157, Test_Loss: 0.3270704448223114 *\n",
      "Epoch: 27, Train_Loss: 0.29310086369514465, Test_Loss: 0.328466534614563\n",
      "Epoch: 27, Train_Loss: 0.29587000608444214, Test_Loss: 0.29581886529922485 *\n",
      "Epoch: 27, Train_Loss: 0.29056018590927124, Test_Loss: 0.2932001054286957 *\n",
      "Epoch: 27, Train_Loss: 0.2941378951072693, Test_Loss: 0.29680243134498596\n",
      "Epoch: 27, Train_Loss: 0.29481789469718933, Test_Loss: 0.29897844791412354\n",
      "Epoch: 27, Train_Loss: 0.2968307435512543, Test_Loss: 0.29593873023986816 *\n",
      "Epoch: 27, Train_Loss: 0.29615792632102966, Test_Loss: 0.2939433455467224 *\n",
      "Epoch: 27, Train_Loss: 0.29262542724609375, Test_Loss: 0.29677608609199524\n",
      "Epoch: 27, Train_Loss: 0.2930136024951935, Test_Loss: 0.2974437177181244\n",
      "Epoch: 27, Train_Loss: 0.2938072383403778, Test_Loss: 0.33459407091140747\n",
      "Epoch: 27, Train_Loss: 0.2973870635032654, Test_Loss: 0.2968190908432007 *\n",
      "Epoch: 27, Train_Loss: 0.2957669198513031, Test_Loss: 0.30780303478240967\n",
      "Epoch: 28, Train_Loss: 0.2942391037940979, Test_Loss: 0.35922104120254517 *\n",
      "Epoch: 28, Train_Loss: 0.3060421347618103, Test_Loss: 0.3971249759197235\n",
      "Epoch: 28, Train_Loss: 0.2954533100128174, Test_Loss: 0.49779200553894043\n",
      "Epoch: 28, Train_Loss: 0.2931559383869171, Test_Loss: 0.3096550703048706 *\n",
      "Epoch: 28, Train_Loss: 0.2915438711643219, Test_Loss: 0.3597797453403473\n",
      "Epoch: 28, Train_Loss: 0.29607218503952026, Test_Loss: 0.4087793231010437\n",
      "Epoch: 28, Train_Loss: 0.3006671965122223, Test_Loss: 0.5404751896858215\n",
      "Epoch: 28, Train_Loss: 0.29351457953453064, Test_Loss: 0.3175170421600342 *\n",
      "Epoch: 28, Train_Loss: 0.2936117947101593, Test_Loss: 0.3763500452041626\n",
      "Epoch: 28, Train_Loss: 0.29245907068252563, Test_Loss: 0.3946138918399811\n",
      "Epoch: 28, Train_Loss: 0.33466532826423645, Test_Loss: 0.3902835547924042 *\n",
      "Epoch: 28, Train_Loss: 0.36039599776268005, Test_Loss: 0.35897740721702576 *\n",
      "Epoch: 28, Train_Loss: 0.3066806495189667, Test_Loss: 0.3070937991142273 *\n",
      "Epoch: 28, Train_Loss: 0.29968324303627014, Test_Loss: 0.3008975088596344 *\n",
      "Epoch: 28, Train_Loss: 0.2955833375453949, Test_Loss: 0.3143955171108246\n",
      "Epoch: 28, Train_Loss: 0.32068246603012085, Test_Loss: 0.6356167793273926\n",
      "Epoch: 28, Train_Loss: 0.3052957355976105, Test_Loss: 0.6777478456497192\n",
      "Epoch: 28, Train_Loss: 0.2978383004665375, Test_Loss: 0.7450153827667236\n",
      "Epoch: 28, Train_Loss: 0.3024863004684448, Test_Loss: 1.1258710622787476\n",
      "Epoch: 28, Train_Loss: 0.3173184096813202, Test_Loss: 0.47238689661026 *\n",
      "Epoch: 28, Train_Loss: 0.38791969418525696, Test_Loss: 0.4917968511581421\n",
      "Epoch: 28, Train_Loss: 0.3390456438064575, Test_Loss: 0.40563127398490906 *\n",
      "Epoch: 28, Train_Loss: 0.3176490068435669, Test_Loss: 0.303959459066391 *\n",
      "Epoch: 28, Train_Loss: 0.30523446202278137, Test_Loss: 0.3049981892108917\n",
      "Epoch: 28, Train_Loss: 0.30744826793670654, Test_Loss: 0.36857813596725464\n",
      "Epoch: 28, Train_Loss: 0.3115420639514923, Test_Loss: 0.5210040211677551\n",
      "Epoch: 28, Train_Loss: 0.2965400815010071, Test_Loss: 0.7969775795936584\n",
      "Epoch: 28, Train_Loss: 0.31211721897125244, Test_Loss: 0.6634243726730347 *\n",
      "Epoch: 28, Train_Loss: 0.2996184527873993, Test_Loss: 2.2746939659118652\n",
      "Epoch: 28, Train_Loss: 0.30883073806762695, Test_Loss: 0.8521043062210083 *\n",
      "Epoch: 28, Train_Loss: 0.4090842008590698, Test_Loss: 0.7286121845245361 *\n",
      "Epoch: 28, Train_Loss: 0.29802295565605164, Test_Loss: 0.31082186102867126 *\n",
      "Epoch: 28, Train_Loss: 0.3230631947517395, Test_Loss: 0.3099364936351776 *\n",
      "Epoch: 28, Train_Loss: 0.3195697069168091, Test_Loss: 0.6688196659088135\n",
      "Epoch: 28, Train_Loss: 0.32647591829299927, Test_Loss: 1.563001275062561\n",
      "Epoch: 28, Train_Loss: 0.3931753933429718, Test_Loss: 0.5449880361557007 *\n",
      "Epoch: 28, Train_Loss: 0.49956342577934265, Test_Loss: 0.37660062313079834 *\n",
      "Epoch: 28, Train_Loss: 0.29503846168518066, Test_Loss: 0.2999900281429291 *\n",
      "Epoch: 28, Train_Loss: 0.32368898391723633, Test_Loss: 0.40162110328674316\n",
      "Epoch: 28, Train_Loss: 0.2900923490524292, Test_Loss: 0.6395876407623291\n",
      "Epoch: 28, Train_Loss: 0.2908749282360077, Test_Loss: 0.5681520700454712 *\n",
      "Epoch: 28, Train_Loss: 0.29305005073547363, Test_Loss: 0.9421597719192505\n",
      "Epoch: 28, Train_Loss: 0.30315056443214417, Test_Loss: 0.6181626319885254 *\n",
      "Epoch: 28, Train_Loss: 0.2968161702156067, Test_Loss: 0.29941070079803467 *\n",
      "Epoch: 28, Train_Loss: 0.3039657175540924, Test_Loss: 0.2962702810764313 *\n",
      "Epoch: 28, Train_Loss: 0.2974803149700165, Test_Loss: 0.2958803176879883 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.2997733950614929, Test_Loss: 0.3150995969772339\n",
      "Epoch: 28, Train_Loss: 0.3002103269100189, Test_Loss: 0.46599215269088745\n",
      "Epoch: 28, Train_Loss: 0.2976510226726532, Test_Loss: 0.6874936819076538\n",
      "Epoch: 28, Train_Loss: 0.2928428053855896, Test_Loss: 0.5437614917755127 *\n",
      "Epoch: 28, Train_Loss: 0.2903650403022766, Test_Loss: 0.3611075282096863 *\n",
      "Epoch: 28, Train_Loss: 0.3021431863307953, Test_Loss: 0.3040894865989685 *\n",
      "Epoch: 28, Train_Loss: 0.2995874583721161, Test_Loss: 0.3058982491493225\n",
      "Epoch: 28, Train_Loss: 0.31872260570526123, Test_Loss: 0.35153695940971375\n",
      "Epoch: 28, Train_Loss: 0.2937365472316742, Test_Loss: 0.6468979120254517\n",
      "Epoch: 28, Train_Loss: 0.31063127517700195, Test_Loss: 0.5965014696121216 *\n",
      "Epoch: 28, Train_Loss: 0.3048608899116516, Test_Loss: 0.6488701105117798\n",
      "Epoch: 28, Train_Loss: 0.31590577960014343, Test_Loss: 0.3554686903953552 *\n",
      "Epoch: 28, Train_Loss: 0.2981211841106415, Test_Loss: 0.30329495668411255 *\n",
      "Epoch: 28, Train_Loss: 0.3108171224594116, Test_Loss: 0.30805131793022156\n",
      "Epoch: 28, Train_Loss: 0.2958430349826813, Test_Loss: 0.30113685131073 *\n",
      "Epoch: 28, Train_Loss: 0.3108975291252136, Test_Loss: 0.31887510418891907\n",
      "Epoch: 28, Train_Loss: 0.3057313561439514, Test_Loss: 0.3360700011253357\n",
      "Epoch: 28, Train_Loss: 0.3165149986743927, Test_Loss: 0.34489718079566956\n",
      "Epoch: 28, Train_Loss: 2.0432915687561035, Test_Loss: 0.29669690132141113 *\n",
      "Epoch: 28, Train_Loss: 4.032151699066162, Test_Loss: 0.3453284204006195\n",
      "Epoch: 28, Train_Loss: 0.32564258575439453, Test_Loss: 0.5608700513839722\n",
      "Epoch: 28, Train_Loss: 0.30313044786453247, Test_Loss: 0.3989422917366028 *\n",
      "Epoch: 28, Train_Loss: 0.3047243058681488, Test_Loss: 0.4963119626045227\n",
      "Epoch: 28, Train_Loss: 0.40593045949935913, Test_Loss: 0.3010654151439667 *\n",
      "Epoch: 28, Train_Loss: 0.33208268880844116, Test_Loss: 0.30211061239242554\n",
      "Epoch: 28, Train_Loss: 0.3013920187950134, Test_Loss: 0.3029814064502716\n",
      "Epoch: 28, Train_Loss: 0.29117846488952637, Test_Loss: 0.3094474971294403\n",
      "Epoch: 28, Train_Loss: 0.36063289642333984, Test_Loss: 0.3017011880874634 *\n",
      "Epoch: 28, Train_Loss: 0.31533315777778625, Test_Loss: 3.488088846206665\n",
      "Epoch: 28, Train_Loss: 0.2993989586830139, Test_Loss: 2.596237897872925 *\n",
      "Epoch: 28, Train_Loss: 0.7777778506278992, Test_Loss: 0.29607218503952026 *\n",
      "Epoch: 28, Train_Loss: 1.3759220838546753, Test_Loss: 0.2937205135822296 *\n",
      "Epoch: 28, Train_Loss: 0.9826679825782776, Test_Loss: 0.29320085048675537 *\n",
      "Epoch: 28, Train_Loss: 0.35659974813461304, Test_Loss: 0.29863354563713074\n",
      "Epoch: 28, Train_Loss: 0.6618673801422119, Test_Loss: 0.29622983932495117 *\n",
      "Epoch: 28, Train_Loss: 2.3415045738220215, Test_Loss: 0.3291800916194916\n",
      "Epoch: 28, Train_Loss: 0.8642286062240601, Test_Loss: 0.34991902112960815\n",
      "Epoch: 28, Train_Loss: 0.293749064207077, Test_Loss: 0.3195493817329407 *\n",
      "Epoch: 28, Train_Loss: 0.3169611096382141, Test_Loss: 0.36376434564590454\n",
      "Epoch: 28, Train_Loss: 0.8053679466247559, Test_Loss: 0.35628852248191833 *\n",
      "Epoch: 28, Train_Loss: 0.760333240032196, Test_Loss: 0.4150844216346741\n",
      "Epoch: 28, Train_Loss: 0.7336137294769287, Test_Loss: 0.30660319328308105 *\n",
      "Epoch: 28, Train_Loss: 0.3111949563026428, Test_Loss: 0.39931949973106384\n",
      "Epoch: 28, Train_Loss: 0.3060651123523712, Test_Loss: 0.3503535985946655 *\n",
      "Epoch: 28, Train_Loss: 0.5586403012275696, Test_Loss: 0.3210841715335846 *\n",
      "Epoch: 28, Train_Loss: 0.474731981754303, Test_Loss: 0.32720404863357544\n",
      "Epoch: 28, Train_Loss: 0.37693551182746887, Test_Loss: 0.401571124792099\n",
      "Epoch: 28, Train_Loss: 0.3831576108932495, Test_Loss: 0.3609854280948639 *\n",
      "Epoch: 28, Train_Loss: 0.32059577107429504, Test_Loss: 0.3222943842411041 *\n",
      "Epoch: 28, Train_Loss: 0.34725528955459595, Test_Loss: 0.3857945203781128\n",
      "Epoch: 28, Train_Loss: 0.382083922624588, Test_Loss: 0.33849239349365234 *\n",
      "Epoch: 28, Train_Loss: 0.4964885413646698, Test_Loss: 0.3537062406539917\n",
      "Epoch: 28, Train_Loss: 0.34682369232177734, Test_Loss: 0.41976165771484375\n",
      "Epoch: 28, Train_Loss: 0.384117066860199, Test_Loss: 0.3402741849422455 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 0.3699641823768616, Test_Loss: 0.3532821536064148\n",
      "Epoch: 28, Train_Loss: 0.41326093673706055, Test_Loss: 0.44741466641426086\n",
      "Epoch: 28, Train_Loss: 0.44993895292282104, Test_Loss: 0.5728684663772583\n",
      "Epoch: 28, Train_Loss: 0.5406191945075989, Test_Loss: 0.3479157090187073 *\n",
      "Epoch: 28, Train_Loss: 0.33406805992126465, Test_Loss: 0.4235164225101471\n",
      "Epoch: 28, Train_Loss: 0.3379594087600708, Test_Loss: 0.401967853307724 *\n",
      "Epoch: 28, Train_Loss: 0.3399580717086792, Test_Loss: 6.591166973114014\n",
      "Epoch: 28, Train_Loss: 0.31458550691604614, Test_Loss: 0.9232665300369263 *\n",
      "Epoch: 28, Train_Loss: 0.29021844267845154, Test_Loss: 0.3235551714897156 *\n",
      "Epoch: 28, Train_Loss: 0.3011170029640198, Test_Loss: 0.3885311782360077\n",
      "Epoch: 28, Train_Loss: 0.29258105158805847, Test_Loss: 0.39434516429901123\n",
      "Epoch: 28, Train_Loss: 0.2956964373588562, Test_Loss: 0.3416261672973633 *\n",
      "Epoch: 28, Train_Loss: 0.35153529047966003, Test_Loss: 0.35693228244781494\n",
      "Epoch: 28, Train_Loss: 0.31839025020599365, Test_Loss: 0.5069479942321777\n",
      "Epoch: 28, Train_Loss: 0.34668898582458496, Test_Loss: 0.5825613737106323\n",
      "Epoch: 28, Train_Loss: 0.3775760233402252, Test_Loss: 0.30331355333328247 *\n",
      "Epoch: 28, Train_Loss: 0.5011224746704102, Test_Loss: 0.352701872587204\n",
      "Epoch: 28, Train_Loss: 0.3643997609615326, Test_Loss: 0.32693803310394287 *\n",
      "Epoch: 28, Train_Loss: 0.31706011295318604, Test_Loss: 0.3511736989021301\n",
      "Epoch: 28, Train_Loss: 0.40690624713897705, Test_Loss: 0.36022311449050903\n",
      "Epoch: 28, Train_Loss: 0.45353370904922485, Test_Loss: 0.3887901306152344\n",
      "Epoch: 28, Train_Loss: 0.48627591133117676, Test_Loss: 0.3867872357368469 *\n",
      "Epoch: 28, Train_Loss: 0.33221644163131714, Test_Loss: 0.379502534866333 *\n",
      "Epoch: 28, Train_Loss: 0.32416248321533203, Test_Loss: 0.4187910854816437\n",
      "Epoch: 28, Train_Loss: 0.49031201004981995, Test_Loss: 0.332093209028244 *\n",
      "Epoch: 28, Train_Loss: 0.4844602942466736, Test_Loss: 0.34389716386795044\n",
      "Epoch: 28, Train_Loss: 0.3788198232650757, Test_Loss: 0.42759639024734497\n",
      "Epoch: 28, Train_Loss: 0.3136903941631317, Test_Loss: 0.3606467843055725 *\n",
      "Epoch: 28, Train_Loss: 0.30506691336631775, Test_Loss: 0.3796461224555969\n",
      "Epoch: 28, Train_Loss: 0.8104983568191528, Test_Loss: 0.37871211767196655 *\n",
      "Epoch: 28, Train_Loss: 0.8290101289749146, Test_Loss: 0.3714504539966583 *\n",
      "Epoch: 28, Train_Loss: 0.3882611095905304, Test_Loss: 0.368306040763855 *\n",
      "Epoch: 28, Train_Loss: 0.3446972072124481, Test_Loss: 0.3548116087913513 *\n",
      "Epoch: 28, Train_Loss: 0.2933086156845093, Test_Loss: 0.32459449768066406 *\n",
      "Epoch: 28, Train_Loss: 0.2959650158882141, Test_Loss: 0.37824538350105286\n",
      "Epoch: 28, Train_Loss: 0.5691922903060913, Test_Loss: 0.34708914160728455 *\n",
      "Epoch: 28, Train_Loss: 0.3151400089263916, Test_Loss: 0.30159521102905273 *\n",
      "Epoch: 28, Train_Loss: 0.31732049584388733, Test_Loss: 0.31510406732559204\n",
      "Epoch: 28, Train_Loss: 0.337327241897583, Test_Loss: 0.6233545541763306\n",
      "Epoch: 28, Train_Loss: 0.5946384072303772, Test_Loss: 0.30690282583236694 *\n",
      "Epoch: 28, Train_Loss: 15.50654411315918, Test_Loss: 0.31030768156051636\n",
      "Epoch: 28, Train_Loss: 0.44524312019348145, Test_Loss: 0.34343624114990234\n",
      "Epoch: 28, Train_Loss: 1.3542563915252686, Test_Loss: 0.4049524664878845\n",
      "Epoch: 28, Train_Loss: 1.406596064567566, Test_Loss: 0.34813570976257324 *\n",
      "Epoch: 28, Train_Loss: 0.31660956144332886, Test_Loss: 0.3942960798740387\n",
      "Epoch: 28, Train_Loss: 0.39293617010116577, Test_Loss: 0.3658122718334198 *\n",
      "Epoch: 28, Train_Loss: 2.4405641555786133, Test_Loss: 0.49920451641082764\n",
      "Epoch: 28, Train_Loss: 6.776268482208252, Test_Loss: 0.38565942645072937 *\n",
      "Epoch: 28, Train_Loss: 0.368558794260025, Test_Loss: 0.3294548988342285 *\n",
      "Epoch: 28, Train_Loss: 0.43582069873809814, Test_Loss: 0.3467431664466858\n",
      "Epoch: 28, Train_Loss: 5.293931007385254, Test_Loss: 0.3463132381439209 *\n",
      "Epoch: 28, Train_Loss: 0.46791350841522217, Test_Loss: 0.6488962173461914\n",
      "Epoch: 28, Train_Loss: 0.3435918390750885, Test_Loss: 0.694985032081604\n",
      "Epoch: 28, Train_Loss: 0.32032933831214905, Test_Loss: 0.4717215597629547 *\n",
      "Epoch: 28, Train_Loss: 0.3658866584300995, Test_Loss: 0.5646079778671265\n",
      "Epoch: 28, Train_Loss: 0.3905647397041321, Test_Loss: 0.7028673887252808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.3036004602909088, Test_Loss: 0.5010721683502197 *\n",
      "Epoch: 28, Train_Loss: 0.33058586716651917, Test_Loss: 0.45326489210128784 *\n",
      "Epoch: 28, Train_Loss: 0.29306793212890625, Test_Loss: 0.39544999599456787 *\n",
      "Epoch: 28, Train_Loss: 0.28878912329673767, Test_Loss: 0.31909748911857605 *\n",
      "Epoch: 28, Train_Loss: 0.31645601987838745, Test_Loss: 0.4196538031101227\n",
      "Epoch: 28, Train_Loss: 0.3544939458370209, Test_Loss: 0.5014702677726746\n",
      "Epoch: 28, Train_Loss: 0.3284941017627716, Test_Loss: 1.030480980873108\n",
      "Epoch: 28, Train_Loss: 0.38147521018981934, Test_Loss: 0.58553147315979 *\n",
      "Epoch: 28, Train_Loss: 0.3248358368873596, Test_Loss: 1.1597154140472412\n",
      "Epoch: 28, Train_Loss: 0.2966187000274658, Test_Loss: 1.1885459423065186\n",
      "Epoch: 28, Train_Loss: 0.30353599786758423, Test_Loss: 1.1901917457580566\n",
      "Epoch: 28, Train_Loss: 0.3227198123931885, Test_Loss: 0.421018123626709 *\n",
      "Epoch: 28, Train_Loss: 0.4442134499549866, Test_Loss: 0.3020762801170349 *\n",
      "Epoch: 28, Train_Loss: 0.2935180068016052, Test_Loss: 0.4456193447113037\n",
      "Epoch: 28, Train_Loss: 0.28889739513397217, Test_Loss: 0.916399359703064\n",
      "Epoch: 28, Train_Loss: 0.290006548166275, Test_Loss: 0.9786096215248108\n",
      "Epoch: 28, Train_Loss: 0.29037684202194214, Test_Loss: 0.339388370513916 *\n",
      "Epoch: 28, Train_Loss: 0.2926582396030426, Test_Loss: 0.30898410081863403 *\n",
      "Epoch: 28, Train_Loss: 0.28831198811531067, Test_Loss: 0.390415757894516\n",
      "Epoch: 28, Train_Loss: 0.2862728536128998, Test_Loss: 0.7288991212844849\n",
      "Epoch: 28, Train_Loss: 0.2983303964138031, Test_Loss: 0.5156359672546387 *\n",
      "Epoch: 28, Train_Loss: 0.3086211383342743, Test_Loss: 0.8753710985183716\n",
      "Epoch: 28, Train_Loss: 0.3111797571182251, Test_Loss: 0.6903051137924194 *\n",
      "Epoch: 28, Train_Loss: 0.32638856768608093, Test_Loss: 0.33887070417404175 *\n",
      "Epoch: 28, Train_Loss: 0.3152758777141571, Test_Loss: 0.2950061857700348 *\n",
      "Epoch: 28, Train_Loss: 0.9402300119400024, Test_Loss: 0.290046364068985 *\n",
      "Epoch: 28, Train_Loss: 6.0803141593933105, Test_Loss: 0.3200376331806183\n",
      "Epoch: 28, Train_Loss: 0.3220376968383789, Test_Loss: 0.39932259917259216\n",
      "Epoch: 28, Train_Loss: 0.3613092303276062, Test_Loss: 0.4431159794330597\n",
      "Epoch: 28, Train_Loss: 0.48511719703674316, Test_Loss: 0.495965838432312\n",
      "Epoch: 28, Train_Loss: 0.5260401964187622, Test_Loss: 0.38513514399528503 *\n",
      "Epoch: 28, Train_Loss: 0.4970148503780365, Test_Loss: 0.3041037917137146 *\n",
      "Epoch: 28, Train_Loss: 0.3660464286804199, Test_Loss: 0.3153909146785736\n",
      "Epoch: 28, Train_Loss: 0.3384847939014435, Test_Loss: 0.31324875354766846 *\n",
      "Epoch: 28, Train_Loss: 0.48970723152160645, Test_Loss: 0.4900878667831421\n",
      "Epoch: 28, Train_Loss: 0.35367080569267273, Test_Loss: 0.6030911803245544\n",
      "Epoch: 28, Train_Loss: 0.3206271231174469, Test_Loss: 0.7967416644096375\n",
      "Epoch: 28, Train_Loss: 0.3058921992778778, Test_Loss: 0.41902071237564087 *\n",
      "Epoch: 28, Train_Loss: 0.31002363562583923, Test_Loss: 0.313396692276001 *\n",
      "Epoch: 28, Train_Loss: 0.35557639598846436, Test_Loss: 0.3038361966609955 *\n",
      "Epoch: 28, Train_Loss: 0.41764378547668457, Test_Loss: 0.31322869658470154\n",
      "Epoch: 28, Train_Loss: 0.3179493546485901, Test_Loss: 0.3281703293323517\n",
      "Epoch: 28, Train_Loss: 0.32689881324768066, Test_Loss: 0.29534804821014404 *\n",
      "Epoch: 28, Train_Loss: 0.3002982437610626, Test_Loss: 0.3459797203540802\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 0.3054284155368805, Test_Loss: 0.3060263395309448 *\n",
      "Epoch: 28, Train_Loss: 0.37767869234085083, Test_Loss: 0.4172833561897278\n",
      "Epoch: 28, Train_Loss: 0.33273011445999146, Test_Loss: 0.5661340951919556\n",
      "Epoch: 28, Train_Loss: 0.3169093430042267, Test_Loss: 0.5272380113601685 *\n",
      "Epoch: 28, Train_Loss: 0.3689540922641754, Test_Loss: 0.5378720760345459\n",
      "Epoch: 28, Train_Loss: 0.32352304458618164, Test_Loss: 0.33503827452659607 *\n",
      "Epoch: 28, Train_Loss: 1.2674574851989746, Test_Loss: 0.32210344076156616 *\n",
      "Epoch: 28, Train_Loss: 4.599573612213135, Test_Loss: 0.3145800530910492 *\n",
      "Epoch: 28, Train_Loss: 0.2903040051460266, Test_Loss: 0.31695863604545593\n",
      "Epoch: 28, Train_Loss: 0.2923325300216675, Test_Loss: 0.30157193541526794 *\n",
      "Epoch: 28, Train_Loss: 0.2932370603084564, Test_Loss: 1.8269069194793701\n",
      "Epoch: 28, Train_Loss: 0.29197123646736145, Test_Loss: 3.9429244995117188\n",
      "Epoch: 28, Train_Loss: 0.2860686779022217, Test_Loss: 0.3009694218635559 *\n",
      "Epoch: 28, Train_Loss: 0.2870354950428009, Test_Loss: 0.2979373633861542 *\n",
      "Epoch: 28, Train_Loss: 0.2945355176925659, Test_Loss: 0.3001790940761566\n",
      "Epoch: 28, Train_Loss: 0.31511688232421875, Test_Loss: 0.28942033648490906 *\n",
      "Epoch: 28, Train_Loss: 0.3029908537864685, Test_Loss: 0.2948073446750641\n",
      "Epoch: 28, Train_Loss: 0.2879149913787842, Test_Loss: 0.3757789731025696\n",
      "Epoch: 28, Train_Loss: 0.28677982091903687, Test_Loss: 0.4052344858646393\n",
      "Epoch: 28, Train_Loss: 0.2836967408657074, Test_Loss: 0.36853229999542236 *\n",
      "Epoch: 28, Train_Loss: 0.2997882664203644, Test_Loss: 0.3860304355621338\n",
      "Epoch: 28, Train_Loss: 0.285483717918396, Test_Loss: 0.37839242815971375 *\n",
      "Epoch: 28, Train_Loss: 0.2869710922241211, Test_Loss: 0.4687926471233368\n",
      "Epoch: 28, Train_Loss: 0.30403509736061096, Test_Loss: 0.2993435263633728 *\n",
      "Epoch: 28, Train_Loss: 0.30768731236457825, Test_Loss: 0.2977061867713928 *\n",
      "Epoch: 28, Train_Loss: 0.2963365912437439, Test_Loss: 0.30280691385269165\n",
      "Epoch: 28, Train_Loss: 0.28569260239601135, Test_Loss: 0.29251331090927124 *\n",
      "Epoch: 28, Train_Loss: 0.2993745505809784, Test_Loss: 0.2907704710960388 *\n",
      "Epoch: 28, Train_Loss: 0.3301418125629425, Test_Loss: 0.28697776794433594 *\n",
      "Epoch: 28, Train_Loss: 0.33189940452575684, Test_Loss: 0.3350268602371216\n",
      "Epoch: 28, Train_Loss: 0.3255561590194702, Test_Loss: 0.2877921760082245 *\n",
      "Epoch: 28, Train_Loss: 0.3211858570575714, Test_Loss: 0.3160872757434845\n",
      "Epoch: 28, Train_Loss: 0.3304899334907532, Test_Loss: 0.28570377826690674 *\n",
      "Epoch: 28, Train_Loss: 0.30956631898880005, Test_Loss: 0.2973136007785797\n",
      "Epoch: 28, Train_Loss: 0.3299086391925812, Test_Loss: 0.2909824252128601 *\n",
      "Epoch: 28, Train_Loss: 0.2936159074306488, Test_Loss: 0.28788837790489197 *\n",
      "Epoch: 28, Train_Loss: 0.4191969037055969, Test_Loss: 0.28514206409454346 *\n",
      "Epoch: 28, Train_Loss: 0.30133339762687683, Test_Loss: 0.2868611812591553\n",
      "Epoch: 28, Train_Loss: 0.2872002124786377, Test_Loss: 0.2918233275413513\n",
      "Epoch: 28, Train_Loss: 0.2848016023635864, Test_Loss: 0.2880709767341614 *\n",
      "Epoch: 28, Train_Loss: 0.28625938296318054, Test_Loss: 0.3549172282218933\n",
      "Epoch: 28, Train_Loss: 0.2855828106403351, Test_Loss: 0.3570300340652466\n",
      "Epoch: 28, Train_Loss: 0.285449355840683, Test_Loss: 4.72025203704834\n",
      "Epoch: 28, Train_Loss: 1.3291609287261963, Test_Loss: 2.2745018005371094 *\n",
      "Epoch: 28, Train_Loss: 2.992941379547119, Test_Loss: 0.3027208745479584 *\n",
      "Epoch: 28, Train_Loss: 0.29808464646339417, Test_Loss: 0.3254818320274353\n",
      "Epoch: 28, Train_Loss: 0.3003443777561188, Test_Loss: 0.31270745396614075 *\n",
      "Epoch: 28, Train_Loss: 0.2874845564365387, Test_Loss: 0.30891916155815125 *\n",
      "Epoch: 28, Train_Loss: 0.28611278533935547, Test_Loss: 0.30142292380332947 *\n",
      "Epoch: 28, Train_Loss: 0.28839111328125, Test_Loss: 0.3744734525680542\n",
      "Epoch: 28, Train_Loss: 0.2911156415939331, Test_Loss: 0.3660832345485687 *\n",
      "Epoch: 28, Train_Loss: 0.28694239258766174, Test_Loss: 0.28733929991722107 *\n",
      "Epoch: 28, Train_Loss: 0.2906876802444458, Test_Loss: 0.31435635685920715\n",
      "Epoch: 28, Train_Loss: 0.29324331879615784, Test_Loss: 0.3239097595214844\n",
      "Epoch: 28, Train_Loss: 0.3028123676776886, Test_Loss: 0.31694525480270386 *\n",
      "Epoch: 28, Train_Loss: 0.3248272240161896, Test_Loss: 0.29374054074287415 *\n",
      "Epoch: 28, Train_Loss: 0.33073297142982483, Test_Loss: 0.3209623694419861\n",
      "Epoch: 28, Train_Loss: 0.3135856091976166, Test_Loss: 0.3350810110569\n",
      "Epoch: 28, Train_Loss: 0.28794875741004944, Test_Loss: 0.3651033937931061\n",
      "Epoch: 28, Train_Loss: 0.37673142552375793, Test_Loss: 0.3143773376941681 *\n",
      "Epoch: 28, Train_Loss: 0.3340807557106018, Test_Loss: 0.32898515462875366\n",
      "Epoch: 28, Train_Loss: 0.32786792516708374, Test_Loss: 0.29650741815567017 *\n",
      "Epoch: 28, Train_Loss: 0.3615463376045227, Test_Loss: 0.33788228034973145\n",
      "Epoch: 28, Train_Loss: 0.35291028022766113, Test_Loss: 0.3553773760795593\n",
      "Epoch: 28, Train_Loss: 0.29328593611717224, Test_Loss: 0.332888126373291 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.2888333797454834, Test_Loss: 0.32945600152015686 *\n",
      "Epoch: 28, Train_Loss: 0.2844710350036621, Test_Loss: 0.3185366988182068 *\n",
      "Epoch: 28, Train_Loss: 0.29079991579055786, Test_Loss: 0.32837462425231934\n",
      "Epoch: 28, Train_Loss: 0.2899464964866638, Test_Loss: 0.29952892661094666 *\n",
      "Epoch: 28, Train_Loss: 0.2825033366680145, Test_Loss: 0.2950761914253235 *\n",
      "Epoch: 28, Train_Loss: 0.2815926969051361, Test_Loss: 0.2951449155807495\n",
      "Epoch: 28, Train_Loss: 0.28896012902259827, Test_Loss: 0.3193141222000122\n",
      "Epoch: 28, Train_Loss: 0.30459749698638916, Test_Loss: 0.28670167922973633 *\n",
      "Epoch: 28, Train_Loss: 0.3486301898956299, Test_Loss: 0.3153142035007477\n",
      "Epoch: 28, Train_Loss: 0.36198848485946655, Test_Loss: 0.6296213269233704\n",
      "Epoch: 28, Train_Loss: 0.4381103515625, Test_Loss: 0.3095012903213501 *\n",
      "Epoch: 28, Train_Loss: 0.3240380883216858, Test_Loss: 0.30734381079673767 *\n",
      "Epoch: 28, Train_Loss: 0.32995086908340454, Test_Loss: 0.3263992965221405\n",
      "Epoch: 28, Train_Loss: 0.35952436923980713, Test_Loss: 0.4224919080734253\n",
      "Epoch: 28, Train_Loss: 0.3531666100025177, Test_Loss: 0.3435426652431488 *\n",
      "Epoch: 28, Train_Loss: 0.3102532625198364, Test_Loss: 0.34468790888786316\n",
      "Epoch: 28, Train_Loss: 0.5087350606918335, Test_Loss: 0.3814050555229187\n",
      "Epoch: 28, Train_Loss: 0.300910085439682, Test_Loss: 0.506603479385376\n",
      "Epoch: 28, Train_Loss: 0.3222503662109375, Test_Loss: 0.3207513093948364 *\n",
      "Epoch: 28, Train_Loss: 1.5563018321990967, Test_Loss: 0.36983585357666016\n",
      "Epoch: 28, Train_Loss: 0.5454796552658081, Test_Loss: 0.3073878884315491 *\n",
      "Epoch: 28, Train_Loss: 0.33683085441589355, Test_Loss: 0.30751028656959534\n",
      "Epoch: 28, Train_Loss: 0.30194729566574097, Test_Loss: 0.43058857321739197\n",
      "Epoch: 28, Train_Loss: 0.30697473883628845, Test_Loss: 0.807574987411499\n",
      "Epoch: 28, Train_Loss: 0.305860310792923, Test_Loss: 0.4416941702365875 *\n",
      "Epoch: 28, Train_Loss: 0.3250301480293274, Test_Loss: 0.7500638961791992\n",
      "Epoch: 28, Train_Loss: 0.30785638093948364, Test_Loss: 0.6413522958755493 *\n",
      "Epoch: 28, Train_Loss: 0.3243604004383087, Test_Loss: 0.5330113172531128 *\n",
      "Epoch: 28, Train_Loss: 0.3061933219432831, Test_Loss: 0.45136645436286926 *\n",
      "Epoch: 28, Train_Loss: 0.30005699396133423, Test_Loss: 0.3150482773780823 *\n",
      "Epoch: 28, Train_Loss: 0.30414560437202454, Test_Loss: 0.29334500432014465 *\n",
      "Epoch: 28, Train_Loss: 0.29164719581604004, Test_Loss: 0.3009706735610962\n",
      "Epoch: 28, Train_Loss: 0.29812124371528625, Test_Loss: 0.4053031802177429\n",
      "Epoch: 28, Train_Loss: 0.29941242933273315, Test_Loss: 0.7561817169189453\n",
      "Epoch: 28, Train_Loss: 0.32366183400154114, Test_Loss: 0.6438237428665161 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 0.2909534275531769, Test_Loss: 1.495691180229187\n",
      "Epoch: 28, Train_Loss: 0.28455397486686707, Test_Loss: 1.3011457920074463 *\n",
      "Epoch: 28, Train_Loss: 0.29205578565597534, Test_Loss: 0.6575495600700378 *\n",
      "Epoch: 28, Train_Loss: 0.2920725643634796, Test_Loss: 0.483358770608902 *\n",
      "Epoch: 28, Train_Loss: 0.2886619567871094, Test_Loss: 0.3112175166606903 *\n",
      "Epoch: 28, Train_Loss: 0.2915833592414856, Test_Loss: 0.3469855487346649\n",
      "Epoch: 28, Train_Loss: 0.2859390676021576, Test_Loss: 0.8043017387390137\n",
      "Epoch: 28, Train_Loss: 0.28280729055404663, Test_Loss: 0.9912135601043701\n",
      "Epoch: 28, Train_Loss: 0.28184130787849426, Test_Loss: 0.3506225645542145 *\n",
      "Epoch: 28, Train_Loss: 0.29129064083099365, Test_Loss: 0.32845863699913025 *\n",
      "Epoch: 28, Train_Loss: 0.28341928124427795, Test_Loss: 0.32962167263031006\n",
      "Epoch: 28, Train_Loss: 0.2813868820667267, Test_Loss: 0.624041736125946\n",
      "Epoch: 28, Train_Loss: 0.28533005714416504, Test_Loss: 0.4378725290298462 *\n",
      "Epoch: 28, Train_Loss: 0.28385505080223083, Test_Loss: 0.8282713890075684\n",
      "Epoch: 28, Train_Loss: 0.28220734000205994, Test_Loss: 0.6775034666061401 *\n",
      "Epoch: 28, Train_Loss: 0.2845519185066223, Test_Loss: 0.4899895191192627 *\n",
      "Epoch: 28, Train_Loss: 0.2831546366214752, Test_Loss: 0.30102095007896423 *\n",
      "Epoch: 28, Train_Loss: 0.2860410213470459, Test_Loss: 0.30626413226127625\n",
      "Epoch: 28, Train_Loss: 0.28715384006500244, Test_Loss: 0.29928305745124817 *\n",
      "Epoch: 28, Train_Loss: 0.291085422039032, Test_Loss: 0.33241185545921326\n",
      "Epoch: 28, Train_Loss: 0.2906269431114197, Test_Loss: 0.6053810119628906\n",
      "Epoch: 28, Train_Loss: 0.28089088201522827, Test_Loss: 0.5419340133666992 *\n",
      "Epoch: 28, Train_Loss: 0.28253698348999023, Test_Loss: 0.3664018213748932 *\n",
      "Epoch: 28, Train_Loss: 0.2900000512599945, Test_Loss: 0.3016709089279175 *\n",
      "Epoch: 28, Train_Loss: 0.2949431836605072, Test_Loss: 0.31008023023605347\n",
      "Epoch: 28, Train_Loss: 0.2845170497894287, Test_Loss: 0.3008074164390564 *\n",
      "Epoch: 28, Train_Loss: 0.2894754409790039, Test_Loss: 0.38330087065696716\n",
      "Epoch: 28, Train_Loss: 0.2846510410308838, Test_Loss: 0.6706984639167786\n",
      "Epoch: 28, Train_Loss: 0.31691086292266846, Test_Loss: 0.705572247505188\n",
      "Epoch: 28, Train_Loss: 0.3436279296875, Test_Loss: 0.3592986464500427 *\n",
      "Epoch: 28, Train_Loss: 0.2909107208251953, Test_Loss: 0.403822124004364\n",
      "Epoch: 28, Train_Loss: 0.2887660562992096, Test_Loss: 0.28789249062538147 *\n",
      "Epoch: 28, Train_Loss: 0.2863292396068573, Test_Loss: 0.29612046480178833\n",
      "Epoch: 28, Train_Loss: 0.31209835410118103, Test_Loss: 0.3071492910385132\n",
      "Epoch: 28, Train_Loss: 0.3047142028808594, Test_Loss: 0.3043982982635498 *\n",
      "Epoch: 28, Train_Loss: 0.29335492849349976, Test_Loss: 0.3551265597343445\n",
      "Epoch: 28, Train_Loss: 0.2938076853752136, Test_Loss: 0.2978210747241974 *\n",
      "Epoch: 28, Train_Loss: 0.29976916313171387, Test_Loss: 0.3458481431007385\n",
      "Epoch: 28, Train_Loss: 0.3527447283267975, Test_Loss: 0.4147714376449585\n",
      "Epoch: 28, Train_Loss: 0.31426802277565, Test_Loss: 0.571700930595398\n",
      "Epoch: 28, Train_Loss: 0.3212178349494934, Test_Loss: 0.46767064929008484 *\n",
      "Epoch: 28, Train_Loss: 0.2971447706222534, Test_Loss: 0.31355923414230347 *\n",
      "Epoch: 28, Train_Loss: 0.3140755295753479, Test_Loss: 0.2948268949985504 *\n",
      "Epoch: 28, Train_Loss: 0.2929706573486328, Test_Loss: 0.2952433228492737\n",
      "Epoch: 28, Train_Loss: 0.28607940673828125, Test_Loss: 0.296339213848114\n",
      "Epoch: 28, Train_Loss: 0.3083374798297882, Test_Loss: 0.29355061054229736 *\n",
      "Epoch: 28, Train_Loss: 0.2908661961555481, Test_Loss: 0.7124279141426086\n",
      "Epoch: 28, Train_Loss: 0.29522213339805603, Test_Loss: 5.187490463256836\n",
      "Epoch: 28, Train_Loss: 0.3608805239200592, Test_Loss: 0.3661363422870636 *\n",
      "Epoch: 28, Train_Loss: 0.30255576968193054, Test_Loss: 0.3213399350643158 *\n",
      "Epoch: 28, Train_Loss: 0.3145115375518799, Test_Loss: 0.30651435256004333 *\n",
      "Epoch: 28, Train_Loss: 0.312020480632782, Test_Loss: 0.2991948127746582 *\n",
      "Epoch: 28, Train_Loss: 0.2886379063129425, Test_Loss: 0.28747719526290894 *\n",
      "Epoch: 28, Train_Loss: 0.4076694846153259, Test_Loss: 0.3510472774505615\n",
      "Epoch: 28, Train_Loss: 0.4362764060497284, Test_Loss: 0.5134083032608032\n",
      "Epoch: 28, Train_Loss: 0.2828206419944763, Test_Loss: 0.3193589746952057 *\n",
      "Epoch: 28, Train_Loss: 0.31484243273735046, Test_Loss: 0.3546167016029358\n",
      "Epoch: 28, Train_Loss: 0.2801893651485443, Test_Loss: 0.3659239411354065\n",
      "Epoch: 28, Train_Loss: 0.2804199755191803, Test_Loss: 0.6219093799591064\n",
      "Epoch: 28, Train_Loss: 0.29660704731941223, Test_Loss: 0.32850199937820435 *\n",
      "Epoch: 28, Train_Loss: 0.311860591173172, Test_Loss: 0.3234773278236389 *\n",
      "Epoch: 28, Train_Loss: 0.2890873849391937, Test_Loss: 0.3551807105541229\n",
      "Epoch: 28, Train_Loss: 0.2870175540447235, Test_Loss: 0.3008701205253601 *\n",
      "Epoch: 28, Train_Loss: 0.2810884416103363, Test_Loss: 0.2952900528907776 *\n",
      "Epoch: 28, Train_Loss: 0.2875771224498749, Test_Loss: 0.2861853837966919 *\n",
      "Epoch: 28, Train_Loss: 0.2841712236404419, Test_Loss: 0.3698830306529999\n",
      "Epoch: 28, Train_Loss: 0.2826204001903534, Test_Loss: 0.28343725204467773 *\n",
      "Epoch: 28, Train_Loss: 0.28295236825942993, Test_Loss: 0.32835865020751953\n",
      "Epoch: 28, Train_Loss: 0.2819584608078003, Test_Loss: 0.29337769746780396 *\n",
      "Epoch: 28, Train_Loss: 0.29216518998146057, Test_Loss: 0.2991393208503723\n",
      "Epoch: 28, Train_Loss: 0.28689491748809814, Test_Loss: 0.31172847747802734\n",
      "Epoch: 28, Train_Loss: 0.30013346672058105, Test_Loss: 0.29922130703926086 *\n",
      "Epoch: 28, Train_Loss: 0.2920137345790863, Test_Loss: 0.2827160954475403 *\n",
      "Epoch: 28, Train_Loss: 0.29560935497283936, Test_Loss: 0.2856976389884949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.2912468910217285, Test_Loss: 0.29693615436553955\n",
      "Epoch: 28, Train_Loss: 0.29901647567749023, Test_Loss: 0.28844112157821655 *\n",
      "Epoch: 28, Train_Loss: 0.2897000014781952, Test_Loss: 0.3008672595024109\n",
      "Epoch: 28, Train_Loss: 0.3036402761936188, Test_Loss: 0.41980570554733276\n",
      "Epoch: 28, Train_Loss: 0.2828001379966736, Test_Loss: 3.172828197479248\n",
      "Epoch: 28, Train_Loss: 0.2910238206386566, Test_Loss: 3.7230594158172607\n",
      "Epoch: 28, Train_Loss: 0.29956352710723877, Test_Loss: 0.3072279393672943 *\n",
      "Epoch: 28, Train_Loss: 0.31570398807525635, Test_Loss: 0.28750693798065186 *\n",
      "Epoch: 28, Train_Loss: 2.415151357650757, Test_Loss: 0.29797598719596863\n",
      "Epoch: 28, Train_Loss: 3.3440544605255127, Test_Loss: 0.3069233298301697\n",
      "Epoch: 28, Train_Loss: 0.28910747170448303, Test_Loss: 0.2997879683971405 *\n",
      "Epoch: 28, Train_Loss: 0.2885585427284241, Test_Loss: 0.37081441283226013\n",
      "Epoch: 28, Train_Loss: 0.3050963282585144, Test_Loss: 0.4839640259742737\n",
      "Epoch: 28, Train_Loss: 0.40360718965530396, Test_Loss: 0.2863940894603729 *\n",
      "Epoch: 28, Train_Loss: 0.3057272136211395, Test_Loss: 0.3057177662849426\n",
      "Epoch: 28, Train_Loss: 0.29501834511756897, Test_Loss: 0.3010851740837097 *\n",
      "Epoch: 28, Train_Loss: 0.2844601273536682, Test_Loss: 0.3050483763217926\n",
      "Epoch: 28, Train_Loss: 0.3763872981071472, Test_Loss: 0.28964051604270935 *\n",
      "Epoch: 28, Train_Loss: 0.30404040217399597, Test_Loss: 0.3526896834373474\n",
      "Epoch: 28, Train_Loss: 0.2952617108821869, Test_Loss: 0.3431951105594635 *\n",
      "Epoch: 28, Train_Loss: 0.8304568529129028, Test_Loss: 0.39991074800491333\n",
      "Epoch: 28, Train_Loss: 0.7508846521377563, Test_Loss: 0.3030545711517334 *\n",
      "Epoch: 28, Train_Loss: 0.8719080686569214, Test_Loss: 0.31491002440452576\n",
      "Epoch: 28, Train_Loss: 0.43097785115242004, Test_Loss: 0.28964412212371826 *\n",
      "Epoch: 28, Train_Loss: 0.8253330588340759, Test_Loss: 0.31980612874031067\n",
      "Epoch: 28, Train_Loss: 1.411870002746582, Test_Loss: 0.39053255319595337\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 0.5740334987640381, Test_Loss: 0.35297954082489014 *\n",
      "Epoch: 28, Train_Loss: 0.2843776345252991, Test_Loss: 0.3370111584663391 *\n",
      "Epoch: 28, Train_Loss: 0.30979475378990173, Test_Loss: 0.3755547106266022\n",
      "Epoch: 28, Train_Loss: 0.7819889783859253, Test_Loss: 0.38040438294410706\n",
      "Epoch: 28, Train_Loss: 0.6632372140884399, Test_Loss: 0.339232861995697 *\n",
      "Epoch: 28, Train_Loss: 0.47857171297073364, Test_Loss: 0.3054814338684082 *\n",
      "Epoch: 28, Train_Loss: 0.3275030553340912, Test_Loss: 0.3061935305595398\n",
      "Epoch: 28, Train_Loss: 0.29047220945358276, Test_Loss: 0.34699368476867676\n",
      "Epoch: 28, Train_Loss: 0.5644540786743164, Test_Loss: 0.2826330363750458 *\n",
      "Epoch: 28, Train_Loss: 0.3770188093185425, Test_Loss: 0.3155113160610199\n",
      "Epoch: 28, Train_Loss: 0.3251057267189026, Test_Loss: 0.6115257143974304\n",
      "Epoch: 28, Train_Loss: 0.3149580955505371, Test_Loss: 0.3254912197589874 *\n",
      "Epoch: 28, Train_Loss: 0.33766284584999084, Test_Loss: 0.3279878795146942\n",
      "Epoch: 28, Train_Loss: 0.36146920919418335, Test_Loss: 0.4411170482635498\n",
      "Epoch: 28, Train_Loss: 0.41003087162971497, Test_Loss: 0.49915415048599243\n",
      "Epoch: 28, Train_Loss: 0.41318315267562866, Test_Loss: 0.4048663079738617 *\n",
      "Epoch: 28, Train_Loss: 0.31348052620887756, Test_Loss: 0.3009624779224396 *\n",
      "Epoch: 28, Train_Loss: 0.3912655711174011, Test_Loss: 0.3483286201953888\n",
      "Epoch: 28, Train_Loss: 0.3391854763031006, Test_Loss: 0.4845302104949951\n",
      "Epoch: 28, Train_Loss: 0.4094405174255371, Test_Loss: 0.35317671298980713 *\n",
      "Epoch: 28, Train_Loss: 0.4034782648086548, Test_Loss: 0.3070808947086334 *\n",
      "Epoch: 28, Train_Loss: 0.402704656124115, Test_Loss: 0.29732391238212585 *\n",
      "Epoch: 28, Train_Loss: 0.33576685190200806, Test_Loss: 0.3324154317378998\n",
      "Epoch: 28, Train_Loss: 0.33839893341064453, Test_Loss: 0.47798416018486023\n",
      "Epoch: 28, Train_Loss: 0.3188938498497009, Test_Loss: 0.5928073525428772\n",
      "Epoch: 28, Train_Loss: 0.2900742292404175, Test_Loss: 0.639024555683136\n",
      "Epoch: 28, Train_Loss: 0.2782086133956909, Test_Loss: 0.7756919860839844\n",
      "Epoch: 28, Train_Loss: 0.28219765424728394, Test_Loss: 0.5761220455169678 *\n",
      "Epoch: 28, Train_Loss: 0.2801830470561981, Test_Loss: 0.762610912322998\n",
      "Epoch: 28, Train_Loss: 0.28730088472366333, Test_Loss: 0.5315697193145752 *\n",
      "Epoch: 28, Train_Loss: 0.31494247913360596, Test_Loss: 0.35780036449432373 *\n",
      "Epoch: 28, Train_Loss: 0.30519503355026245, Test_Loss: 0.3994656801223755\n",
      "Epoch: 28, Train_Loss: 0.32135218381881714, Test_Loss: 0.3216407299041748 *\n",
      "Epoch: 28, Train_Loss: 0.46468979120254517, Test_Loss: 0.3719596266746521\n",
      "Epoch: 28, Train_Loss: 0.43160638213157654, Test_Loss: 0.6638841032981873\n",
      "Epoch: 28, Train_Loss: 0.301788330078125, Test_Loss: 0.7251311540603638\n",
      "Epoch: 28, Train_Loss: 0.31336498260498047, Test_Loss: 1.2327628135681152\n",
      "Epoch: 28, Train_Loss: 0.34747615456581116, Test_Loss: 1.4801855087280273\n",
      "Epoch: 28, Train_Loss: 0.4586009979248047, Test_Loss: 0.6793698072433472 *\n",
      "Epoch: 28, Train_Loss: 0.503818154335022, Test_Loss: 0.5899132490158081 *\n",
      "Epoch: 28, Train_Loss: 0.3370991051197052, Test_Loss: 0.3229130208492279 *\n",
      "Epoch: 28, Train_Loss: 0.393695592880249, Test_Loss: 0.3076183497905731 *\n",
      "Epoch: 28, Train_Loss: 0.440708726644516, Test_Loss: 0.6535542011260986\n",
      "Epoch: 28, Train_Loss: 0.440136194229126, Test_Loss: 1.1735429763793945\n",
      "Epoch: 28, Train_Loss: 0.3460969030857086, Test_Loss: 0.3262263536453247 *\n",
      "Epoch: 28, Train_Loss: 0.29971787333488464, Test_Loss: 0.3149879574775696 *\n",
      "Epoch: 28, Train_Loss: 0.2897433936595917, Test_Loss: 0.3109671473503113 *\n",
      "Epoch: 28, Train_Loss: 0.8109122514724731, Test_Loss: 0.5572701692581177\n",
      "Epoch: 28, Train_Loss: 0.8482124209403992, Test_Loss: 0.5819005370140076\n",
      "Epoch: 28, Train_Loss: 0.3118114173412323, Test_Loss: 0.6807326078414917\n",
      "Epoch: 28, Train_Loss: 0.32706785202026367, Test_Loss: 0.604897677898407 *\n",
      "Epoch: 28, Train_Loss: 0.2845439314842224, Test_Loss: 0.5891793966293335 *\n",
      "Epoch: 28, Train_Loss: 0.3042386770248413, Test_Loss: 0.2891445457935333 *\n",
      "Epoch: 28, Train_Loss: 0.6098176836967468, Test_Loss: 0.3740047812461853\n",
      "Epoch: 29, Train_Loss: 0.28883111476898193, Test_Loss: 0.3453906774520874 *\n",
      "Epoch: 29, Train_Loss: 0.33178234100341797, Test_Loss: 0.31916147470474243 *\n",
      "Epoch: 29, Train_Loss: 0.39587515592575073, Test_Loss: 0.5870746970176697\n",
      "Epoch: 29, Train_Loss: 0.7715342044830322, Test_Loss: 0.5528241395950317 *\n",
      "Epoch: 29, Train_Loss: 15.244927406311035, Test_Loss: 0.40512722730636597 *\n",
      "Epoch: 29, Train_Loss: 0.41162508726119995, Test_Loss: 0.33619388937950134 *\n",
      "Epoch: 29, Train_Loss: 1.350379228591919, Test_Loss: 0.30922406911849976 *\n",
      "Epoch: 29, Train_Loss: 1.4220681190490723, Test_Loss: 0.2875131070613861 *\n",
      "Epoch: 29, Train_Loss: 0.3402475118637085, Test_Loss: 0.31161829829216003\n",
      "Epoch: 29, Train_Loss: 0.508100688457489, Test_Loss: 0.6330991983413696\n",
      "Epoch: 29, Train_Loss: 3.241103410720825, Test_Loss: 0.6044498682022095 *\n",
      "Epoch: 29, Train_Loss: 6.161326885223389, Test_Loss: 0.3776503801345825 *\n",
      "Epoch: 29, Train_Loss: 0.3226308226585388, Test_Loss: 0.4121316075325012\n",
      "Epoch: 29, Train_Loss: 0.39779844880104065, Test_Loss: 0.2877517640590668 *\n",
      "Epoch: 29, Train_Loss: 5.090463638305664, Test_Loss: 0.32729995250701904\n",
      "Epoch: 29, Train_Loss: 0.4481719434261322, Test_Loss: 0.34751245379447937\n",
      "Epoch: 29, Train_Loss: 0.34316587448120117, Test_Loss: 0.40296798944473267\n",
      "Epoch: 29, Train_Loss: 0.2850603461265564, Test_Loss: 0.4014952480792999 *\n",
      "Epoch: 29, Train_Loss: 0.29235899448394775, Test_Loss: 0.4011135697364807 *\n",
      "Epoch: 29, Train_Loss: 0.3509962260723114, Test_Loss: 0.39284104108810425 *\n",
      "Epoch: 29, Train_Loss: 0.2780689597129822, Test_Loss: 0.4321848154067993\n",
      "Epoch: 29, Train_Loss: 0.30545660853385925, Test_Loss: 0.8349247574806213\n",
      "Epoch: 29, Train_Loss: 0.2828354239463806, Test_Loss: 0.38889431953430176 *\n",
      "Epoch: 29, Train_Loss: 0.28018879890441895, Test_Loss: 0.5238484740257263\n",
      "Epoch: 29, Train_Loss: 0.31643015146255493, Test_Loss: 0.31329110264778137 *\n",
      "Epoch: 29, Train_Loss: 0.3032129406929016, Test_Loss: 0.3206178545951843\n",
      "Epoch: 29, Train_Loss: 0.3545026183128357, Test_Loss: 0.3114202320575714 *\n",
      "Epoch: 29, Train_Loss: 0.3478144109249115, Test_Loss: 0.3266759514808655\n",
      "Epoch: 29, Train_Loss: 0.3246769905090332, Test_Loss: 0.4201962351799011\n",
      "Epoch: 29, Train_Loss: 0.28819501399993896, Test_Loss: 4.8984222412109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 0.3102888762950897, Test_Loss: 0.6572307348251343 *\n",
      "Epoch: 29, Train_Loss: 0.34026166796684265, Test_Loss: 0.48137637972831726 *\n",
      "Epoch: 29, Train_Loss: 0.35474440455436707, Test_Loss: 0.48471254110336304\n",
      "Epoch: 29, Train_Loss: 0.2808782756328583, Test_Loss: 0.479131281375885 *\n",
      "Epoch: 29, Train_Loss: 0.27567407488822937, Test_Loss: 0.32139095664024353 *\n",
      "Epoch: 29, Train_Loss: 0.27606281638145447, Test_Loss: 0.5077746510505676\n",
      "Epoch: 29, Train_Loss: 0.2818549871444702, Test_Loss: 0.9019594192504883\n",
      "Epoch: 29, Train_Loss: 0.27868714928627014, Test_Loss: 0.335851788520813 *\n",
      "Epoch: 29, Train_Loss: 0.27589115500450134, Test_Loss: 0.4746192991733551\n",
      "Epoch: 29, Train_Loss: 0.2766449451446533, Test_Loss: 0.4935265779495239\n",
      "Epoch: 29, Train_Loss: 0.2902289927005768, Test_Loss: 0.6416351795196533\n",
      "Epoch: 29, Train_Loss: 0.3023020923137665, Test_Loss: 0.42403846979141235 *\n",
      "Epoch: 29, Train_Loss: 0.32272869348526, Test_Loss: 0.4236200153827667 *\n",
      "Epoch: 29, Train_Loss: 0.3218362331390381, Test_Loss: 0.3706728219985962 *\n",
      "Epoch: 29, Train_Loss: 0.3144555985927582, Test_Loss: 0.30578601360321045 *\n",
      "Epoch: 29, Train_Loss: 2.288544178009033, Test_Loss: 0.286823034286499 *\n",
      "Epoch: 29, Train_Loss: 5.883293151855469, Test_Loss: 0.2983887195587158\n",
      "Epoch: 29, Train_Loss: 0.29246455430984497, Test_Loss: 0.48803189396858215\n",
      "Epoch: 29, Train_Loss: 0.30707675218582153, Test_Loss: 0.3344475030899048 *\n",
      "Epoch: 29, Train_Loss: 0.3609342873096466, Test_Loss: 0.3678642511367798\n",
      "Epoch: 29, Train_Loss: 0.352800577878952, Test_Loss: 0.3596911132335663 *\n",
      "Epoch: 29, Train_Loss: 0.3318851590156555, Test_Loss: 0.45021936297416687\n",
      "Epoch: 29, Train_Loss: 0.326382577419281, Test_Loss: 0.5503706336021423\n",
      "Epoch: 29, Train_Loss: 0.37356793880462646, Test_Loss: 0.5008996725082397 *\n",
      "Epoch: 29, Train_Loss: 0.4708622097969055, Test_Loss: 0.341889888048172 *\n",
      "Epoch: 29, Train_Loss: 0.3508784770965576, Test_Loss: 0.38215816020965576\n",
      "Epoch: 29, Train_Loss: 0.3087565004825592, Test_Loss: 0.4384286403656006\n",
      "Epoch: 29, Train_Loss: 0.3078855872154236, Test_Loss: 0.3523014783859253 *\n",
      "Epoch: 29, Train_Loss: 0.3033590018749237, Test_Loss: 0.314087837934494 *\n",
      "Epoch: 29, Train_Loss: 0.37063297629356384, Test_Loss: 0.577545166015625\n",
      "Epoch: 29, Train_Loss: 0.38502728939056396, Test_Loss: 1.7408320903778076\n",
      "Epoch: 29, Train_Loss: 0.3358740210533142, Test_Loss: 5.673079967498779\n",
      "Epoch: 29, Train_Loss: 0.314249187707901, Test_Loss: 0.34935933351516724 *\n",
      "Epoch: 29, Train_Loss: 0.2895243167877197, Test_Loss: 0.2873818278312683 *\n",
      "Epoch: 29, Train_Loss: 0.324665904045105, Test_Loss: 0.3857828378677368\n",
      "Epoch: 29, Train_Loss: 0.39175575971603394, Test_Loss: 0.530292809009552\n",
      "Epoch: 29, Train_Loss: 0.40517812967300415, Test_Loss: 0.32385340332984924 *\n",
      "Epoch: 29, Train_Loss: 0.2843194305896759, Test_Loss: 0.3415078818798065\n",
      "Epoch: 29, Train_Loss: 0.2825641632080078, Test_Loss: 0.43944233655929565\n",
      "Epoch: 29, Train_Loss: 0.2841736376285553, Test_Loss: 0.2943919599056244 *\n",
      "Epoch: 29, Train_Loss: 1.8219965696334839, Test_Loss: 0.2913319170475006 *\n",
      "Epoch: 29, Train_Loss: 3.742863416671753, Test_Loss: 0.3108060359954834\n",
      "Epoch: 29, Train_Loss: 0.2846863567829132, Test_Loss: 0.3364937901496887\n",
      "Epoch: 29, Train_Loss: 0.3148781657218933, Test_Loss: 0.2859773337841034 *\n",
      "Epoch: 29, Train_Loss: 0.2856906056404114, Test_Loss: 0.44300758838653564\n",
      "Epoch: 29, Train_Loss: 0.2780548930168152, Test_Loss: 0.4168892502784729 *\n",
      "Epoch: 29, Train_Loss: 0.2764666676521301, Test_Loss: 0.37891918420791626 *\n",
      "Epoch: 29, Train_Loss: 0.27653247117996216, Test_Loss: 0.32718294858932495 *\n",
      "Epoch: 29, Train_Loss: 0.29471760988235474, Test_Loss: 0.3322600722312927\n",
      "Epoch: 29, Train_Loss: 0.29110652208328247, Test_Loss: 0.29932427406311035 *\n",
      "Epoch: 29, Train_Loss: 0.30453041195869446, Test_Loss: 0.3386818766593933\n",
      "Epoch: 29, Train_Loss: 0.27532345056533813, Test_Loss: 0.4317205846309662\n",
      "Epoch: 29, Train_Loss: 0.2731896638870239, Test_Loss: 0.35722053050994873 *\n",
      "Epoch: 29, Train_Loss: 0.2875373959541321, Test_Loss: 0.35186153650283813 *\n",
      "Epoch: 29, Train_Loss: 0.30767524242401123, Test_Loss: 0.35090333223342896 *\n",
      "Epoch: 29, Train_Loss: 0.27849456667900085, Test_Loss: 0.3336278796195984 *\n",
      "Epoch: 29, Train_Loss: 0.2780471742153168, Test_Loss: 0.32704678177833557 *\n",
      "Epoch: 29, Train_Loss: 0.2832987308502197, Test_Loss: 0.3145838677883148 *\n",
      "Epoch: 29, Train_Loss: 0.2880153954029083, Test_Loss: 0.2956240773200989 *\n",
      "Epoch: 29, Train_Loss: 0.27585315704345703, Test_Loss: 0.3369424641132355\n",
      "Epoch: 29, Train_Loss: 0.27397990226745605, Test_Loss: 0.2883761525154114 *\n",
      "Epoch: 29, Train_Loss: 0.3250577747821808, Test_Loss: 0.2871525287628174 *\n",
      "Epoch: 29, Train_Loss: 0.31230607628822327, Test_Loss: 0.4903532862663269\n",
      "Epoch: 29, Train_Loss: 0.3227304518222809, Test_Loss: 0.44224846363067627 *\n",
      "Epoch: 29, Train_Loss: 0.3031335473060608, Test_Loss: 0.32145988941192627 *\n",
      "Epoch: 29, Train_Loss: 0.3611616790294647, Test_Loss: 0.31336894631385803 *\n",
      "Epoch: 29, Train_Loss: 0.3134061098098755, Test_Loss: 0.3748272657394409\n",
      "Epoch: 29, Train_Loss: 0.2898346781730652, Test_Loss: 0.4268767833709717\n",
      "Epoch: 29, Train_Loss: 0.31792065501213074, Test_Loss: 0.2854412794113159 *\n",
      "Epoch: 29, Train_Loss: 0.2942756712436676, Test_Loss: 0.3738301396369934\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 29\n",
      "Epoch: 29, Train_Loss: 0.38600558042526245, Test_Loss: 0.49585962295532227\n",
      "Epoch: 29, Train_Loss: 0.2903491258621216, Test_Loss: 0.441709041595459 *\n",
      "Epoch: 29, Train_Loss: 0.2739661931991577, Test_Loss: 0.3512822985649109 *\n",
      "Epoch: 29, Train_Loss: 0.27325668931007385, Test_Loss: 0.2870871424674988 *\n",
      "Epoch: 29, Train_Loss: 0.27415770292282104, Test_Loss: 0.30950552225112915\n",
      "Epoch: 29, Train_Loss: 0.2775734066963196, Test_Loss: 0.32745081186294556\n",
      "Epoch: 29, Train_Loss: 0.2754743993282318, Test_Loss: 0.6533305644989014\n",
      "Epoch: 29, Train_Loss: 2.0733351707458496, Test_Loss: 0.6607813239097595\n",
      "Epoch: 29, Train_Loss: 2.158479690551758, Test_Loss: 0.6518491506576538 *\n",
      "Epoch: 29, Train_Loss: 0.284222275018692, Test_Loss: 0.6520241498947144\n",
      "Epoch: 29, Train_Loss: 0.2784245014190674, Test_Loss: 0.45019519329071045 *\n",
      "Epoch: 29, Train_Loss: 0.27655228972435, Test_Loss: 0.5785616636276245\n",
      "Epoch: 29, Train_Loss: 0.2765829861164093, Test_Loss: 0.3841180205345154 *\n",
      "Epoch: 29, Train_Loss: 0.2722240388393402, Test_Loss: 0.2827598452568054 *\n",
      "Epoch: 29, Train_Loss: 0.276518315076828, Test_Loss: 0.29247432947158813\n",
      "Epoch: 29, Train_Loss: 0.27142706513404846, Test_Loss: 0.3550433814525604\n",
      "Epoch: 29, Train_Loss: 0.2740952968597412, Test_Loss: 0.4941312074661255\n",
      "Epoch: 29, Train_Loss: 0.28871509432792664, Test_Loss: 0.8291720747947693\n",
      "Epoch: 29, Train_Loss: 0.3037795424461365, Test_Loss: 0.814463198184967 *\n",
      "Epoch: 29, Train_Loss: 0.3189818859100342, Test_Loss: 1.4412150382995605\n",
      "Epoch: 29, Train_Loss: 0.32122036814689636, Test_Loss: 0.6328468322753906 *\n",
      "Epoch: 29, Train_Loss: 0.2986322343349457, Test_Loss: 0.759987473487854\n",
      "Epoch: 29, Train_Loss: 0.278062641620636, Test_Loss: 0.2806394696235657 *\n",
      "Epoch: 29, Train_Loss: 0.3890461325645447, Test_Loss: 0.2804001271724701 *\n",
      "Epoch: 29, Train_Loss: 0.3175033628940582, Test_Loss: 0.6547518968582153\n",
      "Epoch: 29, Train_Loss: 0.3030376434326172, Test_Loss: 1.0948386192321777\n",
      "Epoch: 29, Train_Loss: 0.34876227378845215, Test_Loss: 0.3930283784866333 *\n",
      "Epoch: 29, Train_Loss: 0.3059263527393341, Test_Loss: 0.3919414281845093 *\n",
      "Epoch: 29, Train_Loss: 0.27328386902809143, Test_Loss: 0.2813147306442261 *\n",
      "Epoch: 29, Train_Loss: 0.2771803140640259, Test_Loss: 0.38021525740623474\n",
      "Epoch: 29, Train_Loss: 0.27636951208114624, Test_Loss: 0.5914201736450195\n",
      "Epoch: 29, Train_Loss: 0.28499242663383484, Test_Loss: 0.6319255828857422\n",
      "Epoch: 29, Train_Loss: 0.27618086338043213, Test_Loss: 0.7663090229034424\n",
      "Epoch: 29, Train_Loss: 0.2717687785625458, Test_Loss: 0.5719768404960632 *\n",
      "Epoch: 29, Train_Loss: 0.27287188172340393, Test_Loss: 0.278545081615448 *\n",
      "Epoch: 29, Train_Loss: 0.27992957830429077, Test_Loss: 0.28261420130729675\n",
      "Epoch: 29, Train_Loss: 0.30815690755844116, Test_Loss: 0.2854325771331787\n",
      "Epoch: 29, Train_Loss: 0.33517077565193176, Test_Loss: 0.30781790614128113\n",
      "Epoch: 29, Train_Loss: 0.30670225620269775, Test_Loss: 0.5011245012283325\n",
      "Epoch: 29, Train_Loss: 0.3633658289909363, Test_Loss: 0.4989432692527771 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 0.32733163237571716, Test_Loss: 0.4337972402572632 *\n",
      "Epoch: 29, Train_Loss: 0.33904075622558594, Test_Loss: 0.3328128457069397 *\n",
      "Epoch: 29, Train_Loss: 0.31697991490364075, Test_Loss: 0.2824621796607971 *\n",
      "Epoch: 29, Train_Loss: 0.3462933897972107, Test_Loss: 0.30413341522216797\n",
      "Epoch: 29, Train_Loss: 0.30149045586586, Test_Loss: 0.3631439208984375\n",
      "Epoch: 29, Train_Loss: 0.5402365326881409, Test_Loss: 0.6371109485626221\n",
      "Epoch: 29, Train_Loss: 0.29410067200660706, Test_Loss: 0.5649136304855347 *\n",
      "Epoch: 29, Train_Loss: 0.4593893885612488, Test_Loss: 0.5358383655548096 *\n",
      "Epoch: 29, Train_Loss: 1.584600567817688, Test_Loss: 0.35596877336502075 *\n",
      "Epoch: 29, Train_Loss: 0.4224703311920166, Test_Loss: 0.2960674464702606 *\n",
      "Epoch: 29, Train_Loss: 0.35673490166664124, Test_Loss: 0.2923256754875183 *\n",
      "Epoch: 29, Train_Loss: 0.30046573281288147, Test_Loss: 0.28393784165382385 *\n",
      "Epoch: 29, Train_Loss: 0.3179832100868225, Test_Loss: 0.3096241056919098\n",
      "Epoch: 29, Train_Loss: 0.28012850880622864, Test_Loss: 0.29995661973953247 *\n",
      "Epoch: 29, Train_Loss: 0.27246180176734924, Test_Loss: 0.3040819764137268\n",
      "Epoch: 29, Train_Loss: 0.3048025369644165, Test_Loss: 0.2909278869628906 *\n",
      "Epoch: 29, Train_Loss: 0.306906133890152, Test_Loss: 0.3919810652732849\n",
      "Epoch: 29, Train_Loss: 0.2982856333255768, Test_Loss: 0.6532192230224609\n",
      "Epoch: 29, Train_Loss: 0.29435351490974426, Test_Loss: 0.32828301191329956 *\n",
      "Epoch: 29, Train_Loss: 0.29266712069511414, Test_Loss: 0.49091005325317383\n",
      "Epoch: 29, Train_Loss: 0.2786366045475006, Test_Loss: 0.2923596501350403 *\n",
      "Epoch: 29, Train_Loss: 0.2851925194263458, Test_Loss: 0.28665691614151 *\n",
      "Epoch: 29, Train_Loss: 0.2888299226760864, Test_Loss: 0.2890816330909729\n",
      "Epoch: 29, Train_Loss: 0.29236823320388794, Test_Loss: 0.290717750787735\n",
      "Epoch: 29, Train_Loss: 0.28258252143859863, Test_Loss: 0.28674769401550293 *\n",
      "Epoch: 29, Train_Loss: 0.2728968858718872, Test_Loss: 4.474855899810791\n",
      "Epoch: 29, Train_Loss: 0.28618761897087097, Test_Loss: 1.6190803050994873 *\n",
      "Epoch: 29, Train_Loss: 0.28090086579322815, Test_Loss: 0.2977880835533142 *\n",
      "Epoch: 29, Train_Loss: 0.28611791133880615, Test_Loss: 0.30598607659339905\n",
      "Epoch: 29, Train_Loss: 0.28553518652915955, Test_Loss: 0.3112910985946655\n",
      "Epoch: 29, Train_Loss: 0.2738439440727234, Test_Loss: 0.27825406193733215 *\n",
      "Epoch: 29, Train_Loss: 0.2720663249492645, Test_Loss: 0.31392738223075867\n",
      "Epoch: 29, Train_Loss: 0.2729271650314331, Test_Loss: 0.45343419909477234\n",
      "Epoch: 29, Train_Loss: 0.2781985104084015, Test_Loss: 0.44634896516799927 *\n",
      "Epoch: 29, Train_Loss: 0.2728935778141022, Test_Loss: 0.3076203763484955 *\n",
      "Epoch: 29, Train_Loss: 0.27505993843078613, Test_Loss: 0.38202401995658875\n",
      "Epoch: 29, Train_Loss: 0.2788512110710144, Test_Loss: 0.40402930974960327\n",
      "Epoch: 29, Train_Loss: 0.2720511555671692, Test_Loss: 0.4492519497871399\n",
      "Epoch: 29, Train_Loss: 0.27546918392181396, Test_Loss: 0.3336718678474426 *\n",
      "Epoch: 29, Train_Loss: 0.27566254138946533, Test_Loss: 0.3979055881500244\n",
      "Epoch: 29, Train_Loss: 0.2746768593788147, Test_Loss: 0.28777581453323364 *\n",
      "Epoch: 29, Train_Loss: 0.27454274892807007, Test_Loss: 0.3032701313495636\n",
      "Epoch: 29, Train_Loss: 0.2811899185180664, Test_Loss: 0.3158368170261383\n",
      "Epoch: 29, Train_Loss: 0.27411291003227234, Test_Loss: 0.4401366710662842\n",
      "Epoch: 29, Train_Loss: 0.2872982621192932, Test_Loss: 0.32745829224586487 *\n",
      "Epoch: 29, Train_Loss: 0.2724915146827698, Test_Loss: 0.2841509282588959 *\n",
      "Epoch: 29, Train_Loss: 0.2714245617389679, Test_Loss: 0.3553433418273926\n",
      "Epoch: 29, Train_Loss: 0.28317201137542725, Test_Loss: 0.2997877299785614 *\n",
      "Epoch: 29, Train_Loss: 0.2841408848762512, Test_Loss: 0.31981223821640015\n",
      "Epoch: 29, Train_Loss: 0.2750297486782074, Test_Loss: 0.32040315866470337\n",
      "Epoch: 29, Train_Loss: 0.27421095967292786, Test_Loss: 0.27921897172927856 *\n",
      "Epoch: 29, Train_Loss: 0.27629581093788147, Test_Loss: 0.29853832721710205\n",
      "Epoch: 29, Train_Loss: 0.3103632628917694, Test_Loss: 0.30083635449409485\n",
      "Epoch: 29, Train_Loss: 0.3126733899116516, Test_Loss: 0.3126583993434906\n",
      "Epoch: 29, Train_Loss: 0.2857467830181122, Test_Loss: 0.27512261271476746 *\n",
      "Epoch: 29, Train_Loss: 0.2760462164878845, Test_Loss: 0.4468841254711151\n",
      "Epoch: 29, Train_Loss: 0.28050535917282104, Test_Loss: 0.46653521060943604\n",
      "Epoch: 29, Train_Loss: 0.3034974932670593, Test_Loss: 6.7439045906066895\n",
      "Epoch: 29, Train_Loss: 0.296093612909317, Test_Loss: 0.41512423753738403 *\n",
      "Epoch: 29, Train_Loss: 0.28120529651641846, Test_Loss: 0.2774144411087036 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 29\n",
      "Epoch: 29, Train_Loss: 0.28172925114631653, Test_Loss: 0.33546531200408936\n",
      "Epoch: 29, Train_Loss: 0.323896199464798, Test_Loss: 0.34114018082618713\n",
      "Epoch: 29, Train_Loss: 0.32810086011886597, Test_Loss: 0.2964344620704651 *\n",
      "Epoch: 29, Train_Loss: 0.28921031951904297, Test_Loss: 0.30587708950042725\n",
      "Epoch: 29, Train_Loss: 0.3104671239852905, Test_Loss: 0.43283921480178833\n",
      "Epoch: 29, Train_Loss: 0.28302422165870667, Test_Loss: 0.337613582611084 *\n",
      "Epoch: 29, Train_Loss: 0.3072715103626251, Test_Loss: 0.2729273736476898 *\n",
      "Epoch: 29, Train_Loss: 0.2846231758594513, Test_Loss: 0.29940083622932434\n",
      "Epoch: 29, Train_Loss: 0.27372226119041443, Test_Loss: 0.321858286857605\n",
      "Epoch: 29, Train_Loss: 0.30212873220443726, Test_Loss: 0.28267940878868103 *\n",
      "Epoch: 29, Train_Loss: 0.2780972719192505, Test_Loss: 0.3436910808086395\n",
      "Epoch: 29, Train_Loss: 0.30729952454566956, Test_Loss: 0.3330395817756653 *\n",
      "Epoch: 29, Train_Loss: 0.3293244242668152, Test_Loss: 0.34328770637512207\n",
      "Epoch: 29, Train_Loss: 0.29965126514434814, Test_Loss: 0.3287907838821411 *\n",
      "Epoch: 29, Train_Loss: 0.2981564700603485, Test_Loss: 0.3270103931427002 *\n",
      "Epoch: 29, Train_Loss: 0.3026430606842041, Test_Loss: 0.306596577167511 *\n",
      "Epoch: 29, Train_Loss: 0.2740064859390259, Test_Loss: 0.2932494878768921 *\n",
      "Epoch: 29, Train_Loss: 0.47834891080856323, Test_Loss: 0.32802289724349976\n",
      "Epoch: 29, Train_Loss: 0.3774179518222809, Test_Loss: 0.31320592761039734 *\n",
      "Epoch: 29, Train_Loss: 0.2738928198814392, Test_Loss: 0.30204129219055176 *\n",
      "Epoch: 29, Train_Loss: 0.3092825710773468, Test_Loss: 0.3123266100883484\n",
      "Epoch: 29, Train_Loss: 0.27179232239723206, Test_Loss: 0.30203473567962646 *\n",
      "Epoch: 29, Train_Loss: 0.2706725001335144, Test_Loss: 0.29855528473854065 *\n",
      "Epoch: 29, Train_Loss: 0.2811499536037445, Test_Loss: 0.2943669557571411 *\n",
      "Epoch: 29, Train_Loss: 0.28534945845603943, Test_Loss: 0.27450892329216003 *\n",
      "Epoch: 29, Train_Loss: 0.27459096908569336, Test_Loss: 0.30364757776260376\n",
      "Epoch: 29, Train_Loss: 0.27750104665756226, Test_Loss: 0.2874610722064972 *\n",
      "Epoch: 29, Train_Loss: 0.27241435647010803, Test_Loss: 0.2897781431674957\n",
      "Epoch: 29, Train_Loss: 0.27544790506362915, Test_Loss: 0.33416664600372314\n",
      "Epoch: 29, Train_Loss: 0.27677276730537415, Test_Loss: 0.5739027261734009\n",
      "Epoch: 29, Train_Loss: 0.27088674902915955, Test_Loss: 0.2918064296245575 *\n",
      "Epoch: 29, Train_Loss: 0.272196888923645, Test_Loss: 0.3129708468914032\n",
      "Epoch: 29, Train_Loss: 0.2752676010131836, Test_Loss: 0.37284055352211\n",
      "Epoch: 29, Train_Loss: 0.2819536030292511, Test_Loss: 0.43101686239242554\n",
      "Epoch: 29, Train_Loss: 0.2819056212902069, Test_Loss: 0.3044261336326599 *\n",
      "Epoch: 29, Train_Loss: 0.285050630569458, Test_Loss: 0.370017409324646\n",
      "Epoch: 29, Train_Loss: 0.28741753101348877, Test_Loss: 0.3976234793663025\n",
      "Epoch: 29, Train_Loss: 0.2866223156452179, Test_Loss: 0.4991591274738312\n",
      "Epoch: 29, Train_Loss: 0.28018271923065186, Test_Loss: 0.3358406126499176 *\n",
      "Epoch: 29, Train_Loss: 0.28162693977355957, Test_Loss: 0.28152936697006226 *\n",
      "Epoch: 29, Train_Loss: 0.2801734507083893, Test_Loss: 0.2836611568927765\n",
      "Epoch: 29, Train_Loss: 0.29380813241004944, Test_Loss: 0.2990202009677887\n",
      "Epoch: 29, Train_Loss: 0.2752699553966522, Test_Loss: 0.5709638595581055\n",
      "Epoch: 29, Train_Loss: 0.27580446004867554, Test_Loss: 0.7526727318763733\n",
      "Epoch: 29, Train_Loss: 0.2868365943431854, Test_Loss: 0.5308552980422974 *\n",
      "Epoch: 29, Train_Loss: 0.32574111223220825, Test_Loss: 0.7826287746429443\n",
      "Epoch: 29, Train_Loss: 2.47050404548645, Test_Loss: 0.5019822120666504 *\n",
      "Epoch: 29, Train_Loss: 3.1220388412475586, Test_Loss: 0.4399014711380005 *\n",
      "Epoch: 29, Train_Loss: 0.28184974193573, Test_Loss: 0.38756120204925537 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 0.2693921625614166, Test_Loss: 0.3094237446784973 *\n",
      "Epoch: 29, Train_Loss: 0.3061847388744354, Test_Loss: 0.2835831642150879 *\n",
      "Epoch: 29, Train_Loss: 0.3950565457344055, Test_Loss: 0.30356740951538086\n",
      "Epoch: 29, Train_Loss: 0.29590147733688354, Test_Loss: 0.5153613090515137\n",
      "Epoch: 29, Train_Loss: 0.27743399143218994, Test_Loss: 0.6814941763877869\n",
      "Epoch: 29, Train_Loss: 0.28326210379600525, Test_Loss: 0.5428426265716553 *\n",
      "Epoch: 29, Train_Loss: 0.36613622307777405, Test_Loss: 1.7323561906814575\n",
      "Epoch: 29, Train_Loss: 0.28705641627311707, Test_Loss: 0.9058734178543091 *\n",
      "Epoch: 29, Train_Loss: 0.2813434600830078, Test_Loss: 0.6997716426849365 *\n",
      "Epoch: 29, Train_Loss: 0.715031623840332, Test_Loss: 0.3183351755142212 *\n",
      "Epoch: 29, Train_Loss: 0.6482163667678833, Test_Loss: 0.2781485617160797 *\n",
      "Epoch: 29, Train_Loss: 0.6949721574783325, Test_Loss: 0.5026682019233704\n",
      "Epoch: 29, Train_Loss: 0.4179513454437256, Test_Loss: 1.0406967401504517\n",
      "Epoch: 29, Train_Loss: 1.0759507417678833, Test_Loss: 0.6784482002258301 *\n",
      "Epoch: 29, Train_Loss: 1.2473547458648682, Test_Loss: 0.38658788800239563 *\n",
      "Epoch: 29, Train_Loss: 0.39880552887916565, Test_Loss: 0.28827565908432007 *\n",
      "Epoch: 29, Train_Loss: 0.28011322021484375, Test_Loss: 0.3620040714740753\n",
      "Epoch: 29, Train_Loss: 0.38203302025794983, Test_Loss: 0.6174547672271729\n",
      "Epoch: 29, Train_Loss: 0.8343787789344788, Test_Loss: 0.479228138923645 *\n",
      "Epoch: 29, Train_Loss: 0.7201026678085327, Test_Loss: 0.6020087599754333\n",
      "Epoch: 29, Train_Loss: 0.3042669892311096, Test_Loss: 0.4633866548538208 *\n",
      "Epoch: 29, Train_Loss: 0.3190862536430359, Test_Loss: 0.28780636191368103 *\n",
      "Epoch: 29, Train_Loss: 0.2806676924228668, Test_Loss: 0.3339872360229492\n",
      "Epoch: 29, Train_Loss: 0.5957906246185303, Test_Loss: 0.383188396692276\n",
      "Epoch: 29, Train_Loss: 0.30731016397476196, Test_Loss: 0.3245116174221039 *\n",
      "Epoch: 29, Train_Loss: 0.3210974931716919, Test_Loss: 0.4507315158843994\n",
      "Epoch: 29, Train_Loss: 0.2895432114601135, Test_Loss: 0.5852791666984558\n",
      "Epoch: 29, Train_Loss: 0.3339509069919586, Test_Loss: 0.5437976717948914 *\n",
      "Epoch: 29, Train_Loss: 0.37326884269714355, Test_Loss: 0.33766308426856995 *\n",
      "Epoch: 29, Train_Loss: 0.40202149748802185, Test_Loss: 0.28345903754234314 *\n",
      "Epoch: 29, Train_Loss: 0.3792109191417694, Test_Loss: 0.2932899296283722\n",
      "Epoch: 29, Train_Loss: 0.29401665925979614, Test_Loss: 0.31241345405578613\n",
      "Epoch: 29, Train_Loss: 0.38962140679359436, Test_Loss: 0.652895450592041\n",
      "Epoch: 29, Train_Loss: 0.3174322545528412, Test_Loss: 0.6411629915237427 *\n",
      "Epoch: 29, Train_Loss: 0.36653563380241394, Test_Loss: 0.7739144563674927\n",
      "Epoch: 29, Train_Loss: 0.40942102670669556, Test_Loss: 0.3572983741760254 *\n",
      "Epoch: 29, Train_Loss: 0.32655462622642517, Test_Loss: 0.40478044748306274\n",
      "Epoch: 29, Train_Loss: 0.348258912563324, Test_Loss: 0.2788178026676178 *\n",
      "Epoch: 29, Train_Loss: 0.33345499634742737, Test_Loss: 0.2710229158401489 *\n",
      "Epoch: 29, Train_Loss: 0.2950281798839569, Test_Loss: 0.3215904235839844\n",
      "Epoch: 29, Train_Loss: 0.2775152027606964, Test_Loss: 0.28328654170036316 *\n",
      "Epoch: 29, Train_Loss: 0.2707464098930359, Test_Loss: 0.30144134163856506\n",
      "Epoch: 29, Train_Loss: 0.27510830760002136, Test_Loss: 0.284890353679657 *\n",
      "Epoch: 29, Train_Loss: 0.27354851365089417, Test_Loss: 0.4053111672401428\n",
      "Epoch: 29, Train_Loss: 0.27459031343460083, Test_Loss: 0.5658828020095825\n",
      "Epoch: 29, Train_Loss: 0.29661157727241516, Test_Loss: 0.45316410064697266 *\n",
      "Epoch: 29, Train_Loss: 0.2936422824859619, Test_Loss: 0.5135589838027954\n",
      "Epoch: 29, Train_Loss: 0.2977994382381439, Test_Loss: 0.3035351634025574 *\n",
      "Epoch: 29, Train_Loss: 0.46895039081573486, Test_Loss: 0.2967441976070404 *\n",
      "Epoch: 29, Train_Loss: 0.4568190276622772, Test_Loss: 0.2948713004589081 *\n",
      "Epoch: 29, Train_Loss: 0.27756989002227783, Test_Loss: 0.2997223734855652\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 29\n",
      "Epoch: 29, Train_Loss: 0.3157654106616974, Test_Loss: 0.3031056523323059\n",
      "Epoch: 29, Train_Loss: 0.3425620496273041, Test_Loss: 2.5438899993896484\n",
      "Epoch: 29, Train_Loss: 0.4271090030670166, Test_Loss: 2.972993850708008\n",
      "Epoch: 29, Train_Loss: 0.5315971374511719, Test_Loss: 0.3322561979293823 *\n",
      "Epoch: 29, Train_Loss: 0.28929147124290466, Test_Loss: 0.34304264187812805\n",
      "Epoch: 29, Train_Loss: 0.4570416808128357, Test_Loss: 0.3322320878505707 *\n",
      "Epoch: 29, Train_Loss: 0.42528003454208374, Test_Loss: 0.27943891286849976 *\n",
      "Epoch: 29, Train_Loss: 0.41692179441452026, Test_Loss: 0.3063763976097107\n",
      "Epoch: 29, Train_Loss: 0.29214945435523987, Test_Loss: 0.4474557042121887\n",
      "Epoch: 29, Train_Loss: 0.28005942702293396, Test_Loss: 0.46591296792030334\n",
      "Epoch: 29, Train_Loss: 0.28070729970932007, Test_Loss: 0.2978816628456116 *\n",
      "Epoch: 29, Train_Loss: 0.744978666305542, Test_Loss: 0.3641091287136078\n",
      "Epoch: 29, Train_Loss: 0.8127939701080322, Test_Loss: 0.37929797172546387\n",
      "Epoch: 29, Train_Loss: 0.28941288590431213, Test_Loss: 0.5143848657608032\n",
      "Epoch: 29, Train_Loss: 0.31339535117149353, Test_Loss: 0.2833712697029114 *\n",
      "Epoch: 29, Train_Loss: 0.27201059460639954, Test_Loss: 0.3099229633808136\n",
      "Epoch: 29, Train_Loss: 0.3400455117225647, Test_Loss: 0.2802363932132721 *\n",
      "Epoch: 29, Train_Loss: 0.6076520681381226, Test_Loss: 0.2991010546684265\n",
      "Epoch: 29, Train_Loss: 0.278423935174942, Test_Loss: 0.29287397861480713 *\n",
      "Epoch: 29, Train_Loss: 0.3385896384716034, Test_Loss: 0.3555249571800232\n",
      "Epoch: 29, Train_Loss: 0.33079037070274353, Test_Loss: 0.38650384545326233\n",
      "Epoch: 29, Train_Loss: 0.6496264934539795, Test_Loss: 0.2736104428768158 *\n",
      "Epoch: 29, Train_Loss: 15.353629112243652, Test_Loss: 0.35388076305389404\n",
      "Epoch: 29, Train_Loss: 0.473819375038147, Test_Loss: 0.2915493845939636 *\n",
      "Epoch: 29, Train_Loss: 1.2376620769500732, Test_Loss: 0.32541245222091675\n",
      "Epoch: 29, Train_Loss: 1.1305665969848633, Test_Loss: 0.3603648245334625\n",
      "Epoch: 29, Train_Loss: 0.33823153376579285, Test_Loss: 0.3023298382759094 *\n",
      "Epoch: 29, Train_Loss: 0.6021753549575806, Test_Loss: 0.302283376455307 *\n",
      "Epoch: 29, Train_Loss: 4.372265815734863, Test_Loss: 0.3872194290161133\n",
      "Epoch: 29, Train_Loss: 4.383659839630127, Test_Loss: 0.4381345212459564\n",
      "Epoch: 29, Train_Loss: 0.35319823026657104, Test_Loss: 0.3185817003250122 *\n",
      "Epoch: 29, Train_Loss: 0.5987700819969177, Test_Loss: 0.4928054213523865\n",
      "Epoch: 29, Train_Loss: 4.8602986335754395, Test_Loss: 0.5443154573440552\n",
      "Epoch: 29, Train_Loss: 0.476606547832489, Test_Loss: 6.250608921051025\n",
      "Epoch: 29, Train_Loss: 0.2896551489830017, Test_Loss: 1.8029913902282715 *\n",
      "Epoch: 29, Train_Loss: 0.2728149890899658, Test_Loss: 0.4073638319969177 *\n",
      "Epoch: 29, Train_Loss: 0.28307896852493286, Test_Loss: 0.6101695895195007\n",
      "Epoch: 29, Train_Loss: 0.4125054180622101, Test_Loss: 0.717798113822937\n",
      "Epoch: 29, Train_Loss: 0.2687700688838959, Test_Loss: 0.6596977114677429 *\n",
      "Epoch: 29, Train_Loss: 0.27457675337791443, Test_Loss: 0.44049423933029175 *\n",
      "Epoch: 29, Train_Loss: 0.2684441804885864, Test_Loss: 0.5210493803024292\n",
      "Epoch: 29, Train_Loss: 0.2677679657936096, Test_Loss: 1.0537817478179932\n",
      "Epoch: 29, Train_Loss: 0.31983092427253723, Test_Loss: 0.507224977016449 *\n",
      "Epoch: 29, Train_Loss: 0.28416335582733154, Test_Loss: 0.3831019401550293 *\n",
      "Epoch: 29, Train_Loss: 0.33874550461769104, Test_Loss: 0.35822778940200806 *\n",
      "Epoch: 29, Train_Loss: 0.3481515049934387, Test_Loss: 0.39233648777008057\n",
      "Epoch: 29, Train_Loss: 0.2994939088821411, Test_Loss: 0.48643529415130615\n",
      "Epoch: 29, Train_Loss: 0.27799683809280396, Test_Loss: 0.7523360848426819\n",
      "Epoch: 29, Train_Loss: 0.30545735359191895, Test_Loss: 0.4808002710342407 *\n",
      "Epoch: 29, Train_Loss: 0.3591575622558594, Test_Loss: 0.3606444001197815 *\n",
      "Epoch: 29, Train_Loss: 0.3114379644393921, Test_Loss: 0.37520140409469604\n",
      "Epoch: 29, Train_Loss: 0.27093517780303955, Test_Loss: 0.3395734429359436 *\n",
      "Epoch: 29, Train_Loss: 0.2660078704357147, Test_Loss: 0.3424839675426483\n",
      "Epoch: 29, Train_Loss: 0.26685458421707153, Test_Loss: 0.4315980076789856\n",
      "Epoch: 29, Train_Loss: 0.26900714635849, Test_Loss: 0.37537938356399536 *\n",
      "Epoch: 29, Train_Loss: 0.26857617497444153, Test_Loss: 0.36079567670822144 *\n",
      "Epoch: 29, Train_Loss: 0.26559412479400635, Test_Loss: 0.339143842458725 *\n",
      "Epoch: 29, Train_Loss: 0.27086693048477173, Test_Loss: 0.3089849650859833 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 0.281120240688324, Test_Loss: 0.318662166595459\n",
      "Epoch: 29, Train_Loss: 0.29984354972839355, Test_Loss: 0.2870849668979645 *\n",
      "Epoch: 29, Train_Loss: 0.3337591588497162, Test_Loss: 0.2794218957424164 *\n",
      "Epoch: 29, Train_Loss: 0.30150336027145386, Test_Loss: 0.288227915763855\n",
      "Epoch: 29, Train_Loss: 0.3473482131958008, Test_Loss: 0.3038846254348755\n",
      "Epoch: 29, Train_Loss: 4.2725605964660645, Test_Loss: 0.2823484241962433 *\n",
      "Epoch: 29, Train_Loss: 4.2063093185424805, Test_Loss: 0.29215264320373535\n",
      "Epoch: 29, Train_Loss: 0.28644147515296936, Test_Loss: 0.7001162171363831\n",
      "Epoch: 29, Train_Loss: 0.2912457287311554, Test_Loss: 0.3043895959854126 *\n",
      "Epoch: 29, Train_Loss: 0.3381488025188446, Test_Loss: 0.3394913673400879\n",
      "Epoch: 29, Train_Loss: 0.3197595775127411, Test_Loss: 0.3428856134414673\n",
      "Epoch: 29, Train_Loss: 0.36952462792396545, Test_Loss: 0.3699306845664978\n",
      "Epoch: 29, Train_Loss: 0.31790030002593994, Test_Loss: 0.31689658761024475 *\n",
      "Epoch: 29, Train_Loss: 0.3219284117221832, Test_Loss: 0.3552004098892212\n",
      "Epoch: 29, Train_Loss: 0.38587599992752075, Test_Loss: 0.33708083629608154 *\n",
      "Epoch: 29, Train_Loss: 0.33171236515045166, Test_Loss: 0.6216487884521484\n",
      "Epoch: 29, Train_Loss: 0.2925584316253662, Test_Loss: 0.4228247404098511 *\n",
      "Epoch: 29, Train_Loss: 0.2884279489517212, Test_Loss: 0.322068989276886 *\n",
      "Epoch: 29, Train_Loss: 0.28865909576416016, Test_Loss: 0.2949545979499817 *\n",
      "Epoch: 29, Train_Loss: 0.31241291761398315, Test_Loss: 0.34327495098114014\n",
      "Epoch: 29, Train_Loss: 0.29724690318107605, Test_Loss: 0.4434107542037964\n",
      "Epoch: 29, Train_Loss: 0.3112996816635132, Test_Loss: 0.8065983057022095\n",
      "Epoch: 29, Train_Loss: 0.31790876388549805, Test_Loss: 0.4225962460041046 *\n",
      "Epoch: 29, Train_Loss: 0.3315572142601013, Test_Loss: 0.6258394718170166\n",
      "Epoch: 29, Train_Loss: 0.3611810505390167, Test_Loss: 0.7306915521621704\n",
      "Epoch: 29, Train_Loss: 0.32666394114494324, Test_Loss: 0.49793529510498047 *\n",
      "Epoch: 29, Train_Loss: 0.37072503566741943, Test_Loss: 0.470874547958374 *\n",
      "Epoch: 29, Train_Loss: 0.2712318003177643, Test_Loss: 0.3002364933490753 *\n",
      "Epoch: 29, Train_Loss: 0.26888757944107056, Test_Loss: 0.2712613642215729 *\n",
      "Epoch: 29, Train_Loss: 0.297044962644577, Test_Loss: 0.2949436604976654\n",
      "Epoch: 29, Train_Loss: 2.573617935180664, Test_Loss: 0.3858304023742676\n",
      "Epoch: 29, Train_Loss: 2.264300584793091, Test_Loss: 0.7351268529891968\n",
      "Epoch: 29, Train_Loss: 0.2708376944065094, Test_Loss: 0.5345849990844727 *\n",
      "Epoch: 29, Train_Loss: 0.306569904088974, Test_Loss: 1.466930627822876\n",
      "Epoch: 29, Train_Loss: 0.28321799635887146, Test_Loss: 1.1190868616104126 *\n",
      "Epoch: 29, Train_Loss: 0.26729604601860046, Test_Loss: 0.7535651326179504 *\n",
      "Epoch: 29, Train_Loss: 0.27019843459129333, Test_Loss: 0.4566635489463806 *\n",
      "Epoch: 29, Train_Loss: 0.26903674006462097, Test_Loss: 0.2762802839279175 *\n",
      "Epoch: 29, Train_Loss: 0.2876075506210327, Test_Loss: 0.35536783933639526\n",
      "Epoch: 29, Train_Loss: 0.27789053320884705, Test_Loss: 0.8263131976127625\n",
      "Epoch: 29, Train_Loss: 0.292418897151947, Test_Loss: 0.8866614103317261\n",
      "Epoch: 29, Train_Loss: 0.2650265395641327, Test_Loss: 0.3647928237915039 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 29\n",
      "Epoch: 29, Train_Loss: 0.2662573456764221, Test_Loss: 0.30398741364479065 *\n",
      "Epoch: 29, Train_Loss: 0.2765752375125885, Test_Loss: 0.34227150678634644\n",
      "Epoch: 29, Train_Loss: 0.2869223952293396, Test_Loss: 0.5965371131896973\n",
      "Epoch: 29, Train_Loss: 0.2651611864566803, Test_Loss: 0.43734753131866455 *\n",
      "Epoch: 29, Train_Loss: 0.27412471175193787, Test_Loss: 0.6833880543708801\n",
      "Epoch: 29, Train_Loss: 0.27610698342323303, Test_Loss: 0.6806455850601196 *\n",
      "Epoch: 29, Train_Loss: 0.27718251943588257, Test_Loss: 0.3981373608112335 *\n",
      "Epoch: 29, Train_Loss: 0.2643626630306244, Test_Loss: 0.27533429861068726 *\n",
      "Epoch: 29, Train_Loss: 0.2653008699417114, Test_Loss: 0.2759380042552948\n",
      "Epoch: 29, Train_Loss: 0.3416062891483307, Test_Loss: 0.27320727705955505 *\n",
      "Epoch: 29, Train_Loss: 0.3078305721282959, Test_Loss: 0.33394038677215576\n",
      "Epoch: 29, Train_Loss: 0.3113754093647003, Test_Loss: 0.569807767868042\n",
      "Epoch: 29, Train_Loss: 0.3336213529109955, Test_Loss: 0.5462642908096313 *\n",
      "Epoch: 29, Train_Loss: 0.3302956819534302, Test_Loss: 0.33939480781555176 *\n",
      "Epoch: 29, Train_Loss: 0.2966195344924927, Test_Loss: 0.2885078489780426 *\n",
      "Epoch: 29, Train_Loss: 0.2767336964607239, Test_Loss: 0.28968125581741333\n",
      "Epoch: 29, Train_Loss: 0.2965947687625885, Test_Loss: 0.2904456853866577\n",
      "Epoch: 29, Train_Loss: 0.3140762746334076, Test_Loss: 0.45057523250579834\n",
      "Epoch: 29, Train_Loss: 0.3173636496067047, Test_Loss: 0.5329347848892212\n",
      "Epoch: 29, Train_Loss: 0.2781822085380554, Test_Loss: 0.7300199270248413\n",
      "Epoch: 29, Train_Loss: 0.2640348970890045, Test_Loss: 0.3662751019001007 *\n",
      "Epoch: 29, Train_Loss: 0.26673614978790283, Test_Loss: 0.3007974922657013 *\n",
      "Epoch: 29, Train_Loss: 0.2737194001674652, Test_Loss: 0.27052390575408936 *\n",
      "Epoch: 29, Train_Loss: 0.2686505913734436, Test_Loss: 0.27823498845100403\n",
      "Epoch: 29, Train_Loss: 0.2670525014400482, Test_Loss: 0.28742867708206177\n",
      "Epoch: 29, Train_Loss: 2.4529693126678467, Test_Loss: 0.2830139696598053 *\n",
      "Epoch: 29, Train_Loss: 1.438444972038269, Test_Loss: 0.32291504740715027\n",
      "Epoch: 29, Train_Loss: 0.2772161662578583, Test_Loss: 0.2673661410808563 *\n",
      "Epoch: 29, Train_Loss: 0.26804542541503906, Test_Loss: 0.3829689025878906\n",
      "Epoch: 29, Train_Loss: 0.26869744062423706, Test_Loss: 0.517488956451416\n",
      "Epoch: 29, Train_Loss: 0.26841503381729126, Test_Loss: 0.5070289373397827 *\n",
      "Epoch: 29, Train_Loss: 0.26705411076545715, Test_Loss: 0.4739740490913391 *\n",
      "Epoch: 29, Train_Loss: 0.2652386426925659, Test_Loss: 0.2910947799682617 *\n",
      "Epoch: 29, Train_Loss: 0.2630245089530945, Test_Loss: 0.27599987387657166 *\n",
      "Epoch: 29, Train_Loss: 0.2651914060115814, Test_Loss: 0.2761572301387787\n",
      "Epoch: 29, Train_Loss: 0.28756028413772583, Test_Loss: 0.2747611403465271 *\n",
      "Epoch: 29, Train_Loss: 0.2990109920501709, Test_Loss: 0.2764872908592224\n",
      "Epoch: 29, Train_Loss: 0.3023962080478668, Test_Loss: 1.240138053894043\n",
      "Epoch: 29, Train_Loss: 0.30741938948631287, Test_Loss: 4.848221302032471\n",
      "Epoch: 29, Train_Loss: 0.28183844685554504, Test_Loss: 0.30416226387023926 *\n",
      "Epoch: 29, Train_Loss: 0.28997230529785156, Test_Loss: 0.3077141344547272\n",
      "Epoch: 29, Train_Loss: 0.32908865809440613, Test_Loss: 0.3395701050758362\n",
      "Epoch: 29, Train_Loss: 0.29104122519493103, Test_Loss: 0.27244704961776733 *\n",
      "Epoch: 29, Train_Loss: 0.3145099878311157, Test_Loss: 0.27324479818344116\n",
      "Epoch: 29, Train_Loss: 0.31235092878341675, Test_Loss: 0.4534676671028137\n",
      "Epoch: 29, Train_Loss: 0.29900187253952026, Test_Loss: 0.5157965421676636\n",
      "Epoch: 29, Train_Loss: 0.2631074786186218, Test_Loss: 0.2973855435848236 *\n",
      "Epoch: 29, Train_Loss: 0.2691960334777832, Test_Loss: 0.28275638818740845 *\n",
      "Epoch: 29, Train_Loss: 0.2647908926010132, Test_Loss: 0.4138457775115967\n",
      "Epoch: 29, Train_Loss: 0.27190303802490234, Test_Loss: 0.6659772396087646\n",
      "Epoch: 29, Train_Loss: 0.270440012216568, Test_Loss: 0.30081015825271606 *\n",
      "Epoch: 29, Train_Loss: 0.2638549208641052, Test_Loss: 0.31481942534446716\n",
      "Epoch: 29, Train_Loss: 0.26687002182006836, Test_Loss: 0.31502270698547363\n",
      "Epoch: 29, Train_Loss: 0.26884910464286804, Test_Loss: 0.31964775919914246\n",
      "Epoch: 30, Train_Loss: 0.30531951785087585, Test_Loss: 0.31469860672950745 *\n",
      "Epoch: 30, Train_Loss: 0.31501153111457825, Test_Loss: 0.2693641781806946 *\n",
      "Epoch: 30, Train_Loss: 0.2958505153656006, Test_Loss: 0.43163058161735535\n",
      "Epoch: 30, Train_Loss: 0.38892996311187744, Test_Loss: 0.2685871422290802 *\n",
      "Epoch: 30, Train_Loss: 0.3047761023044586, Test_Loss: 0.3503234088420868\n",
      "Epoch: 30, Train_Loss: 0.3276752829551697, Test_Loss: 0.27109837532043457 *\n",
      "Epoch: 30, Train_Loss: 0.2766551375389099, Test_Loss: 0.30439072847366333\n",
      "Epoch: 30, Train_Loss: 0.32525402307510376, Test_Loss: 0.30950385332107544\n",
      "Epoch: 30, Train_Loss: 0.2883702516555786, Test_Loss: 0.27500206232070923 *\n",
      "Epoch: 30, Train_Loss: 0.5371788144111633, Test_Loss: 0.26816442608833313 *\n",
      "Epoch: 30, Train_Loss: 0.27860328555107117, Test_Loss: 0.29229989647865295\n",
      "Epoch: 30, Train_Loss: 0.8088281154632568, Test_Loss: 0.2999005913734436\n",
      "Epoch: 30, Train_Loss: 1.3745747804641724, Test_Loss: 0.28764283657073975 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.33571070432662964, Test_Loss: 0.30288267135620117\n",
      "Epoch: 30, Train_Loss: 0.2929896116256714, Test_Loss: 0.4078606963157654\n",
      "Epoch: 30, Train_Loss: 0.2839690148830414, Test_Loss: 3.913691520690918\n",
      "Epoch: 30, Train_Loss: 0.29996711015701294, Test_Loss: 2.933382272720337 *\n",
      "Epoch: 30, Train_Loss: 0.26687654852867126, Test_Loss: 0.30166494846343994 *\n",
      "Epoch: 30, Train_Loss: 0.26413118839263916, Test_Loss: 0.30132994055747986 *\n",
      "Epoch: 30, Train_Loss: 0.2989134192466736, Test_Loss: 0.31567883491516113\n",
      "Epoch: 30, Train_Loss: 0.29849475622177124, Test_Loss: 0.3218139410018921\n",
      "Epoch: 30, Train_Loss: 0.29453593492507935, Test_Loss: 0.2828744351863861 *\n",
      "Epoch: 30, Train_Loss: 0.2811780571937561, Test_Loss: 0.34340426325798035\n",
      "Epoch: 30, Train_Loss: 0.2865696847438812, Test_Loss: 0.43267011642456055\n",
      "Epoch: 30, Train_Loss: 0.2689243257045746, Test_Loss: 0.26690205931663513 *\n",
      "Epoch: 30, Train_Loss: 0.2839532196521759, Test_Loss: 0.3307558298110962\n",
      "Epoch: 30, Train_Loss: 0.2788820266723633, Test_Loss: 0.3303435146808624 *\n",
      "Epoch: 30, Train_Loss: 0.27358725666999817, Test_Loss: 0.36221519112586975\n",
      "Epoch: 30, Train_Loss: 0.269844651222229, Test_Loss: 0.2843852937221527 *\n",
      "Epoch: 30, Train_Loss: 0.26712408661842346, Test_Loss: 0.3849847912788391\n",
      "Epoch: 30, Train_Loss: 0.2767655551433563, Test_Loss: 0.3458457887172699 *\n",
      "Epoch: 30, Train_Loss: 0.27228406071662903, Test_Loss: 0.35614630579948425\n",
      "Epoch: 30, Train_Loss: 0.285684734582901, Test_Loss: 0.3485133945941925 *\n",
      "Epoch: 30, Train_Loss: 0.26369509100914, Test_Loss: 0.34666430950164795 *\n",
      "Epoch: 30, Train_Loss: 0.261761337518692, Test_Loss: 0.2828029692173004 *\n",
      "Epoch: 30, Train_Loss: 0.2634046971797943, Test_Loss: 0.30920711159706116\n",
      "Epoch: 30, Train_Loss: 0.2652982175350189, Test_Loss: 0.37082594633102417\n",
      "Epoch: 30, Train_Loss: 0.268047571182251, Test_Loss: 0.3380139470100403 *\n",
      "Epoch: 30, Train_Loss: 0.26164305210113525, Test_Loss: 0.32461485266685486 *\n",
      "Epoch: 30, Train_Loss: 0.2635265588760376, Test_Loss: 0.3074236512184143 *\n",
      "Epoch: 30, Train_Loss: 0.2651843726634979, Test_Loss: 0.325547456741333\n",
      "Epoch: 30, Train_Loss: 0.2646876573562622, Test_Loss: 0.2876105010509491 *\n",
      "Epoch: 30, Train_Loss: 0.26313677430152893, Test_Loss: 0.2801477313041687 *\n",
      "Epoch: 30, Train_Loss: 0.26707005500793457, Test_Loss: 0.268620103597641 *\n",
      "Epoch: 30, Train_Loss: 0.26583847403526306, Test_Loss: 0.2926543056964874\n",
      "Epoch: 30, Train_Loss: 0.2642267346382141, Test_Loss: 0.2749370038509369 *\n",
      "Epoch: 30, Train_Loss: 0.2769860625267029, Test_Loss: 0.30503395199775696\n",
      "Epoch: 30, Train_Loss: 0.2650119364261627, Test_Loss: 0.5285511016845703\n",
      "Epoch: 30, Train_Loss: 0.2741677165031433, Test_Loss: 0.29491114616394043 *\n",
      "Epoch: 30, Train_Loss: 0.26120585203170776, Test_Loss: 0.2935210168361664 *\n",
      "Epoch: 30, Train_Loss: 0.26279136538505554, Test_Loss: 0.39688777923583984\n",
      "Epoch: 30, Train_Loss: 0.2736899256706238, Test_Loss: 0.4034692049026489\n",
      "Epoch: 30, Train_Loss: 0.27107304334640503, Test_Loss: 0.36772894859313965 *\n",
      "Epoch: 30, Train_Loss: 0.2672625184059143, Test_Loss: 0.30527380108833313 *\n",
      "Epoch: 30, Train_Loss: 0.263327419757843, Test_Loss: 0.3381490111351013\n",
      "Epoch: 30, Train_Loss: 0.26949894428253174, Test_Loss: 0.5121890306472778\n",
      "Epoch: 30, Train_Loss: 0.30793994665145874, Test_Loss: 0.32362693548202515 *\n",
      "Epoch: 30, Train_Loss: 0.27877867221832275, Test_Loss: 0.3074377477169037 *\n",
      "Epoch: 30, Train_Loss: 0.2793414890766144, Test_Loss: 0.27583855390548706 *\n",
      "Epoch: 30, Train_Loss: 0.26176178455352783, Test_Loss: 0.28121116757392883\n",
      "Epoch: 30, Train_Loss: 0.2832319140434265, Test_Loss: 0.4054086208343506\n",
      "Epoch: 30, Train_Loss: 0.29273563623428345, Test_Loss: 0.685643196105957\n",
      "Epoch: 30, Train_Loss: 0.27154168486595154, Test_Loss: 0.48278576135635376 *\n",
      "Epoch: 30, Train_Loss: 0.2707597315311432, Test_Loss: 0.8065891265869141\n",
      "Epoch: 30, Train_Loss: 0.2864009141921997, Test_Loss: 0.7268987894058228 *\n",
      "Epoch: 30, Train_Loss: 0.3389434814453125, Test_Loss: 0.5101224184036255 *\n",
      "Epoch: 30, Train_Loss: 0.31201761960983276, Test_Loss: 0.455257773399353 *\n",
      "Epoch: 30, Train_Loss: 0.28718769550323486, Test_Loss: 0.3127385377883911 *\n",
      "Epoch: 30, Train_Loss: 0.31890803575515747, Test_Loss: 0.28143417835235596 *\n",
      "Epoch: 30, Train_Loss: 0.2660647928714752, Test_Loss: 0.282644122838974\n",
      "Epoch: 30, Train_Loss: 0.2933087944984436, Test_Loss: 0.4044300317764282\n",
      "Epoch: 30, Train_Loss: 0.2637907564640045, Test_Loss: 0.6205414533615112\n",
      "Epoch: 30, Train_Loss: 0.27024489641189575, Test_Loss: 0.7124342918395996\n",
      "Epoch: 30, Train_Loss: 0.27095726132392883, Test_Loss: 1.5352451801300049\n",
      "Epoch: 30, Train_Loss: 0.2641010880470276, Test_Loss: 1.3321374654769897 *\n",
      "Epoch: 30, Train_Loss: 0.3212384283542633, Test_Loss: 0.5731610059738159 *\n",
      "Epoch: 30, Train_Loss: 0.283968448638916, Test_Loss: 0.5550107955932617 *\n",
      "Epoch: 30, Train_Loss: 0.3110082149505615, Test_Loss: 0.281602144241333 *\n",
      "Epoch: 30, Train_Loss: 0.2679206132888794, Test_Loss: 0.3059682250022888\n",
      "Epoch: 30, Train_Loss: 0.29591232538223267, Test_Loss: 0.7669568061828613\n",
      "Epoch: 30, Train_Loss: 0.2719961106777191, Test_Loss: 1.0178548097610474\n",
      "Epoch: 30, Train_Loss: 0.5629991292953491, Test_Loss: 0.32425767183303833 *\n",
      "Epoch: 30, Train_Loss: 0.31084492802619934, Test_Loss: 0.31760677695274353 *\n",
      "Epoch: 30, Train_Loss: 0.27637308835983276, Test_Loss: 0.2856939435005188 *\n",
      "Epoch: 30, Train_Loss: 0.27861571311950684, Test_Loss: 0.5265412330627441\n",
      "Epoch: 30, Train_Loss: 0.26374903321266174, Test_Loss: 0.46935659646987915 *\n",
      "Epoch: 30, Train_Loss: 0.26424625515937805, Test_Loss: 0.7274148464202881\n",
      "Epoch: 30, Train_Loss: 0.27017641067504883, Test_Loss: 0.6047496795654297 *\n",
      "Epoch: 30, Train_Loss: 0.2704451382160187, Test_Loss: 0.5330668091773987 *\n",
      "Epoch: 30, Train_Loss: 0.2658337652683258, Test_Loss: 0.2790300250053406 *\n",
      "Epoch: 30, Train_Loss: 0.2696177661418915, Test_Loss: 0.3090355694293976\n",
      "Epoch: 30, Train_Loss: 0.2662902772426605, Test_Loss: 0.2810625433921814 *\n",
      "Epoch: 30, Train_Loss: 0.26494473218917847, Test_Loss: 0.29892653226852417\n",
      "Epoch: 30, Train_Loss: 0.2722008228302002, Test_Loss: 0.6225319504737854\n",
      "Epoch: 30, Train_Loss: 0.2647826373577118, Test_Loss: 0.6102409958839417 *\n",
      "Epoch: 30, Train_Loss: 0.2609066665172577, Test_Loss: 0.34446415305137634 *\n",
      "Epoch: 30, Train_Loss: 0.27478134632110596, Test_Loss: 0.2892889380455017 *\n",
      "Epoch: 30, Train_Loss: 0.2763482332229614, Test_Loss: 0.2846701443195343 *\n",
      "Epoch: 30, Train_Loss: 0.2744244933128357, Test_Loss: 0.2819523513317108 *\n",
      "Epoch: 30, Train_Loss: 0.2655991017818451, Test_Loss: 0.35407015681266785\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.27771279215812683, Test_Loss: 0.6494717001914978\n",
      "Epoch: 30, Train_Loss: 0.27274033427238464, Test_Loss: 0.7267006635665894\n",
      "Epoch: 30, Train_Loss: 0.2711045742034912, Test_Loss: 0.3247406482696533 *\n",
      "Epoch: 30, Train_Loss: 0.2609594464302063, Test_Loss: 0.4641905128955841\n",
      "Epoch: 30, Train_Loss: 0.2762601673603058, Test_Loss: 0.264914333820343 *\n",
      "Epoch: 30, Train_Loss: 0.2707306444644928, Test_Loss: 0.2689895033836365\n",
      "Epoch: 30, Train_Loss: 0.2685425281524658, Test_Loss: 0.2755494713783264\n",
      "Epoch: 30, Train_Loss: 0.2626704275608063, Test_Loss: 0.2832481861114502\n",
      "Epoch: 30, Train_Loss: 0.27690762281417847, Test_Loss: 0.30875006318092346\n",
      "Epoch: 30, Train_Loss: 0.33367496728897095, Test_Loss: 0.27558422088623047 *\n",
      "Epoch: 30, Train_Loss: 2.682889938354492, Test_Loss: 0.2912680506706238\n",
      "Epoch: 30, Train_Loss: 3.096625566482544, Test_Loss: 0.3566778600215912\n",
      "Epoch: 30, Train_Loss: 0.2739610970020294, Test_Loss: 0.5867734551429749\n",
      "Epoch: 30, Train_Loss: 0.26036152243614197, Test_Loss: 0.39384031295776367 *\n",
      "Epoch: 30, Train_Loss: 0.31813400983810425, Test_Loss: 0.3237420916557312 *\n",
      "Epoch: 30, Train_Loss: 0.36610713601112366, Test_Loss: 0.2655702531337738 *\n",
      "Epoch: 30, Train_Loss: 0.27841174602508545, Test_Loss: 0.2689734399318695\n",
      "Epoch: 30, Train_Loss: 0.2641626000404358, Test_Loss: 0.2678072154521942 *\n",
      "Epoch: 30, Train_Loss: 0.2925657629966736, Test_Loss: 0.27097803354263306\n",
      "Epoch: 30, Train_Loss: 0.32937273383140564, Test_Loss: 0.4671545624732971\n",
      "Epoch: 30, Train_Loss: 0.27113839983940125, Test_Loss: 5.4830732345581055\n",
      "Epoch: 30, Train_Loss: 0.2982991933822632, Test_Loss: 0.403575599193573 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.6939531564712524, Test_Loss: 0.28831747174263 *\n",
      "Epoch: 30, Train_Loss: 0.660401463508606, Test_Loss: 0.29589980840682983\n",
      "Epoch: 30, Train_Loss: 0.47740164399147034, Test_Loss: 0.308681845664978\n",
      "Epoch: 30, Train_Loss: 0.3876586854457855, Test_Loss: 0.265038400888443 *\n",
      "Epoch: 30, Train_Loss: 1.3202311992645264, Test_Loss: 0.3745189309120178\n",
      "Epoch: 30, Train_Loss: 1.121000051498413, Test_Loss: 0.4990442097187042\n",
      "Epoch: 30, Train_Loss: 0.2789990305900574, Test_Loss: 0.32346415519714355 *\n",
      "Epoch: 30, Train_Loss: 0.26557669043540955, Test_Loss: 0.2850138247013092 *\n",
      "Epoch: 30, Train_Loss: 0.5247901082038879, Test_Loss: 0.3952476680278778\n",
      "Epoch: 30, Train_Loss: 0.7614653706550598, Test_Loss: 0.565278172492981\n",
      "Epoch: 30, Train_Loss: 0.9065465927124023, Test_Loss: 0.32679691910743713 *\n",
      "Epoch: 30, Train_Loss: 0.2765383720397949, Test_Loss: 0.37916994094848633\n",
      "Epoch: 30, Train_Loss: 0.32172709703445435, Test_Loss: 0.36441969871520996 *\n",
      "Epoch: 30, Train_Loss: 0.31360477209091187, Test_Loss: 0.2832023501396179 *\n",
      "Epoch: 30, Train_Loss: 0.5351014137268066, Test_Loss: 0.34752020239830017\n",
      "Epoch: 30, Train_Loss: 0.26885464787483215, Test_Loss: 0.2904873192310333 *\n",
      "Epoch: 30, Train_Loss: 0.3301401734352112, Test_Loss: 0.44156643748283386\n",
      "Epoch: 30, Train_Loss: 0.28482165932655334, Test_Loss: 0.30686870217323303 *\n",
      "Epoch: 30, Train_Loss: 0.3219614028930664, Test_Loss: 0.358728289604187\n",
      "Epoch: 30, Train_Loss: 0.36113759875297546, Test_Loss: 0.29246786236763 *\n",
      "Epoch: 30, Train_Loss: 0.3962498903274536, Test_Loss: 0.30270370841026306\n",
      "Epoch: 30, Train_Loss: 0.3481118679046631, Test_Loss: 0.3551817238330841\n",
      "Epoch: 30, Train_Loss: 0.2892897129058838, Test_Loss: 0.3159736692905426 *\n",
      "Epoch: 30, Train_Loss: 0.43645715713500977, Test_Loss: 0.2910689413547516 *\n",
      "Epoch: 30, Train_Loss: 0.32810088992118835, Test_Loss: 0.3126178979873657\n",
      "Epoch: 30, Train_Loss: 0.3607238829135895, Test_Loss: 0.47375592589378357\n",
      "Epoch: 30, Train_Loss: 0.4420453906059265, Test_Loss: 0.3899692893028259 *\n",
      "Epoch: 30, Train_Loss: 0.29699692130088806, Test_Loss: 0.27437347173690796 *\n",
      "Epoch: 30, Train_Loss: 0.3248269855976105, Test_Loss: 0.44678670167922974\n",
      "Epoch: 30, Train_Loss: 0.3267133831977844, Test_Loss: 2.526928663253784\n",
      "Epoch: 30, Train_Loss: 0.28699690103530884, Test_Loss: 4.1843180656433105\n",
      "Epoch: 30, Train_Loss: 0.26940909028053284, Test_Loss: 0.3075745701789856 *\n",
      "Epoch: 30, Train_Loss: 0.2637569010257721, Test_Loss: 0.27714741230010986 *\n",
      "Epoch: 30, Train_Loss: 0.26324042677879333, Test_Loss: 0.34564727544784546\n",
      "Epoch: 30, Train_Loss: 0.2644493877887726, Test_Loss: 0.39896243810653687\n",
      "Epoch: 30, Train_Loss: 0.26747947931289673, Test_Loss: 0.29361841082572937 *\n",
      "Epoch: 30, Train_Loss: 0.29061758518218994, Test_Loss: 0.31250953674316406\n",
      "Epoch: 30, Train_Loss: 0.280167818069458, Test_Loss: 0.5617564916610718\n",
      "Epoch: 30, Train_Loss: 0.30520278215408325, Test_Loss: 0.26906001567840576 *\n",
      "Epoch: 30, Train_Loss: 0.48727884888648987, Test_Loss: 0.31438302993774414\n",
      "Epoch: 30, Train_Loss: 0.5625806450843811, Test_Loss: 0.4043251872062683\n",
      "Epoch: 30, Train_Loss: 0.26917293667793274, Test_Loss: 0.4880564510822296\n",
      "Epoch: 30, Train_Loss: 0.3260520398616791, Test_Loss: 0.3275662064552307 *\n",
      "Epoch: 30, Train_Loss: 0.37063711881637573, Test_Loss: 0.4529496431350708\n",
      "Epoch: 30, Train_Loss: 0.4091814458370209, Test_Loss: 0.39228111505508423 *\n",
      "Epoch: 30, Train_Loss: 0.5044263005256653, Test_Loss: 0.34613722562789917 *\n",
      "Epoch: 30, Train_Loss: 0.30962300300598145, Test_Loss: 0.37086614966392517\n",
      "Epoch: 30, Train_Loss: 0.5454180240631104, Test_Loss: 0.47766557335853577\n",
      "Epoch: 30, Train_Loss: 0.39068442583084106, Test_Loss: 0.3230319023132324 *\n",
      "Epoch: 30, Train_Loss: 0.4229238033294678, Test_Loss: 0.30195844173431396 *\n",
      "Epoch: 30, Train_Loss: 0.28121379017829895, Test_Loss: 0.3435399532318115\n",
      "Epoch: 30, Train_Loss: 0.27560171484947205, Test_Loss: 0.3094470500946045 *\n",
      "Epoch: 30, Train_Loss: 0.3798050284385681, Test_Loss: 0.3290078639984131\n",
      "Epoch: 30, Train_Loss: 0.7772512435913086, Test_Loss: 0.3083166480064392 *\n",
      "Epoch: 30, Train_Loss: 0.8645315170288086, Test_Loss: 0.2997797727584839 *\n",
      "Epoch: 30, Train_Loss: 0.29014530777931213, Test_Loss: 0.2900066673755646 *\n",
      "Epoch: 30, Train_Loss: 0.28823864459991455, Test_Loss: 0.27534303069114685 *\n",
      "Epoch: 30, Train_Loss: 0.2644021213054657, Test_Loss: 0.2839745879173279\n",
      "Epoch: 30, Train_Loss: 0.4327714443206787, Test_Loss: 0.30495038628578186\n",
      "Epoch: 30, Train_Loss: 0.5701885223388672, Test_Loss: 0.26885083317756653 *\n",
      "Epoch: 30, Train_Loss: 0.2725350856781006, Test_Loss: 0.28442034125328064\n",
      "Epoch: 30, Train_Loss: 0.327576220035553, Test_Loss: 0.5008046627044678\n",
      "Epoch: 30, Train_Loss: 0.34368860721588135, Test_Loss: 0.33100512623786926 *\n",
      "Epoch: 30, Train_Loss: 1.484302282333374, Test_Loss: 0.2930663228034973 *\n",
      "Epoch: 30, Train_Loss: 14.084052085876465, Test_Loss: 0.31987687945365906\n",
      "Epoch: 30, Train_Loss: 0.5693553686141968, Test_Loss: 0.41227370500564575\n",
      "Epoch: 30, Train_Loss: 1.2139006853103638, Test_Loss: 0.4065009355545044 *\n",
      "Epoch: 30, Train_Loss: 0.8326072692871094, Test_Loss: 0.2833452820777893 *\n",
      "Epoch: 30, Train_Loss: 0.3396166265010834, Test_Loss: 0.3170287609100342\n",
      "Epoch: 30, Train_Loss: 0.6922813653945923, Test_Loss: 0.4212028384208679\n",
      "Epoch: 30, Train_Loss: 5.592060089111328, Test_Loss: 0.33418792486190796 *\n",
      "Epoch: 30, Train_Loss: 2.7535102367401123, Test_Loss: 0.31194034218788147 *\n",
      "Epoch: 30, Train_Loss: 0.3359893560409546, Test_Loss: 0.29632148146629333 *\n",
      "Epoch: 30, Train_Loss: 1.5080070495605469, Test_Loss: 0.31506603956222534\n",
      "Epoch: 30, Train_Loss: 4.225985527038574, Test_Loss: 0.3871203660964966\n",
      "Epoch: 30, Train_Loss: 0.4585825204849243, Test_Loss: 0.5260705947875977\n",
      "Epoch: 30, Train_Loss: 0.2822904884815216, Test_Loss: 0.7992582321166992\n",
      "Epoch: 30, Train_Loss: 0.2660314440727234, Test_Loss: 0.7670900821685791 *\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.27762970328330994, Test_Loss: 0.554438591003418 *\n",
      "Epoch: 30, Train_Loss: 0.29748111963272095, Test_Loss: 0.5287997722625732 *\n",
      "Epoch: 30, Train_Loss: 0.2624865472316742, Test_Loss: 0.41934657096862793 *\n",
      "Epoch: 30, Train_Loss: 0.2578570246696472, Test_Loss: 0.316532701253891 *\n",
      "Epoch: 30, Train_Loss: 0.2579842805862427, Test_Loss: 0.29182013869285583 *\n",
      "Epoch: 30, Train_Loss: 0.25863757729530334, Test_Loss: 0.311596155166626\n",
      "Epoch: 30, Train_Loss: 0.2823479175567627, Test_Loss: 0.36510324478149414\n",
      "Epoch: 30, Train_Loss: 0.27625542879104614, Test_Loss: 0.6944373846054077\n",
      "Epoch: 30, Train_Loss: 0.29033470153808594, Test_Loss: 0.7008695602416992\n",
      "Epoch: 30, Train_Loss: 0.33994218707084656, Test_Loss: 1.0493322610855103\n",
      "Epoch: 30, Train_Loss: 0.3551831543445587, Test_Loss: 1.314889669418335\n",
      "Epoch: 30, Train_Loss: 0.26337772607803345, Test_Loss: 0.55950528383255 *\n",
      "Epoch: 30, Train_Loss: 0.2883330285549164, Test_Loss: 0.608071506023407\n",
      "Epoch: 30, Train_Loss: 0.3637073040008545, Test_Loss: 0.2806192636489868 *\n",
      "Epoch: 30, Train_Loss: 0.27109381556510925, Test_Loss: 0.2777591347694397 *\n",
      "Epoch: 30, Train_Loss: 0.26356151700019836, Test_Loss: 0.5823278427124023\n",
      "Epoch: 30, Train_Loss: 0.2577287554740906, Test_Loss: 1.181891918182373\n",
      "Epoch: 30, Train_Loss: 0.2589797079563141, Test_Loss: 0.3243348300457001 *\n",
      "Epoch: 30, Train_Loss: 0.2622966468334198, Test_Loss: 0.3075639605522156 *\n",
      "Epoch: 30, Train_Loss: 0.26047447323799133, Test_Loss: 0.2802358865737915 *\n",
      "Epoch: 30, Train_Loss: 0.257606565952301, Test_Loss: 0.4524347186088562\n",
      "Epoch: 30, Train_Loss: 0.2619393467903137, Test_Loss: 0.5982833504676819\n",
      "Epoch: 30, Train_Loss: 0.2868771255016327, Test_Loss: 0.5719519257545471 *\n",
      "Epoch: 30, Train_Loss: 0.2903827428817749, Test_Loss: 0.6052513122558594\n",
      "Epoch: 30, Train_Loss: 0.3354962468147278, Test_Loss: 0.5435020923614502 *\n",
      "Epoch: 30, Train_Loss: 0.28341755270957947, Test_Loss: 0.2659139931201935 *\n",
      "Epoch: 30, Train_Loss: 0.3272039592266083, Test_Loss: 0.2762661278247833\n",
      "Epoch: 30, Train_Loss: 6.2740397453308105, Test_Loss: 0.28315049409866333\n",
      "Epoch: 30, Train_Loss: 1.880637288093567, Test_Loss: 0.3002605736255646\n",
      "Epoch: 30, Train_Loss: 0.2757585644721985, Test_Loss: 0.5015009641647339\n",
      "Epoch: 30, Train_Loss: 0.28946101665496826, Test_Loss: 0.45070987939834595 *\n",
      "Epoch: 30, Train_Loss: 0.33854228258132935, Test_Loss: 0.31994107365608215 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.3006865084171295, Test_Loss: 0.30957961082458496 *\n",
      "Epoch: 30, Train_Loss: 0.3408811688423157, Test_Loss: 0.2730294167995453 *\n",
      "Epoch: 30, Train_Loss: 0.2812235355377197, Test_Loss: 0.2870541214942932\n",
      "Epoch: 30, Train_Loss: 0.3386158049106598, Test_Loss: 0.33752885460853577\n",
      "Epoch: 30, Train_Loss: 0.3977164626121521, Test_Loss: 0.6579223275184631\n",
      "Epoch: 30, Train_Loss: 0.323573499917984, Test_Loss: 0.5301036834716797 *\n",
      "Epoch: 30, Train_Loss: 0.27356046438217163, Test_Loss: 0.4286012053489685 *\n",
      "Epoch: 30, Train_Loss: 0.28129205107688904, Test_Loss: 0.40056702494621277 *\n",
      "Epoch: 30, Train_Loss: 0.27094385027885437, Test_Loss: 0.2999635636806488 *\n",
      "Epoch: 30, Train_Loss: 0.2967110872268677, Test_Loss: 0.3109031915664673\n",
      "Epoch: 30, Train_Loss: 0.285941481590271, Test_Loss: 0.29042139649391174 *\n",
      "Epoch: 30, Train_Loss: 0.29657602310180664, Test_Loss: 0.32412979006767273\n",
      "Epoch: 30, Train_Loss: 0.33141568303108215, Test_Loss: 0.31151148676872253 *\n",
      "Epoch: 30, Train_Loss: 0.3329245150089264, Test_Loss: 0.29468753933906555 *\n",
      "Epoch: 30, Train_Loss: 0.3459242880344391, Test_Loss: 0.32446449995040894\n",
      "Epoch: 30, Train_Loss: 0.2996770739555359, Test_Loss: 0.4156205654144287\n",
      "Epoch: 30, Train_Loss: 0.36043885350227356, Test_Loss: 0.6943740844726562\n",
      "Epoch: 30, Train_Loss: 0.26176002621650696, Test_Loss: 0.3041955530643463 *\n",
      "Epoch: 30, Train_Loss: 0.2580864429473877, Test_Loss: 0.4415096938610077\n",
      "Epoch: 30, Train_Loss: 0.4611855149269104, Test_Loss: 0.2614859342575073 *\n",
      "Epoch: 30, Train_Loss: 3.3321938514709473, Test_Loss: 0.26383867859840393\n",
      "Epoch: 30, Train_Loss: 0.6725763082504272, Test_Loss: 0.260432630777359 *\n",
      "Epoch: 30, Train_Loss: 0.25811854004859924, Test_Loss: 0.26504698395729065\n",
      "Epoch: 30, Train_Loss: 0.2932981848716736, Test_Loss: 0.2767840027809143\n",
      "Epoch: 30, Train_Loss: 0.2659144699573517, Test_Loss: 6.2817840576171875\n",
      "Epoch: 30, Train_Loss: 0.26002973318099976, Test_Loss: 0.9589598178863525 *\n",
      "Epoch: 30, Train_Loss: 0.26452264189720154, Test_Loss: 0.3705074191093445 *\n",
      "Epoch: 30, Train_Loss: 0.26192906498908997, Test_Loss: 0.3248688280582428 *\n",
      "Epoch: 30, Train_Loss: 0.278523325920105, Test_Loss: 0.36365145444869995\n",
      "Epoch: 30, Train_Loss: 0.2749203145503998, Test_Loss: 0.26760685443878174 *\n",
      "Epoch: 30, Train_Loss: 0.26976969838142395, Test_Loss: 0.49538642168045044\n",
      "Epoch: 30, Train_Loss: 0.25781944394111633, Test_Loss: 0.8665632009506226\n",
      "Epoch: 30, Train_Loss: 0.2611466944217682, Test_Loss: 0.6858211755752563 *\n",
      "Epoch: 30, Train_Loss: 0.2665649652481079, Test_Loss: 0.5580009818077087 *\n",
      "Epoch: 30, Train_Loss: 0.2790694534778595, Test_Loss: 0.6285214424133301\n",
      "Epoch: 30, Train_Loss: 0.2610721290111542, Test_Loss: 0.7230972051620483\n",
      "Epoch: 30, Train_Loss: 0.2802124321460724, Test_Loss: 0.7091050148010254 *\n",
      "Epoch: 30, Train_Loss: 0.2676866352558136, Test_Loss: 0.4508301019668579 *\n",
      "Epoch: 30, Train_Loss: 0.2748732566833496, Test_Loss: 0.42982062697410583 *\n",
      "Epoch: 30, Train_Loss: 0.25793981552124023, Test_Loss: 0.4430994987487793\n",
      "Epoch: 30, Train_Loss: 0.2568095326423645, Test_Loss: 0.4145796298980713 *\n",
      "Epoch: 30, Train_Loss: 0.3293285071849823, Test_Loss: 0.33248305320739746 *\n",
      "Epoch: 30, Train_Loss: 0.2964063882827759, Test_Loss: 0.5517943501472473\n",
      "Epoch: 30, Train_Loss: 0.3002038300037384, Test_Loss: 0.3718358278274536 *\n",
      "Epoch: 30, Train_Loss: 0.3866044282913208, Test_Loss: 0.33729422092437744 *\n",
      "Epoch: 30, Train_Loss: 0.31487423181533813, Test_Loss: 0.36175909638404846\n",
      "Epoch: 30, Train_Loss: 0.282562792301178, Test_Loss: 0.3086914122104645 *\n",
      "Epoch: 30, Train_Loss: 0.2650274336338043, Test_Loss: 0.3206087350845337\n",
      "Epoch: 30, Train_Loss: 0.27917245030403137, Test_Loss: 0.3465149700641632\n",
      "Epoch: 30, Train_Loss: 0.31965765357017517, Test_Loss: 0.2853870093822479 *\n",
      "Epoch: 30, Train_Loss: 0.3019508123397827, Test_Loss: 0.2912805378437042\n",
      "Epoch: 30, Train_Loss: 0.2665731906890869, Test_Loss: 0.30801746249198914\n",
      "Epoch: 30, Train_Loss: 0.2580379843711853, Test_Loss: 0.2748473882675171 *\n",
      "Epoch: 30, Train_Loss: 0.25644731521606445, Test_Loss: 0.2641048729419708 *\n",
      "Epoch: 30, Train_Loss: 0.25908851623535156, Test_Loss: 0.40339022874832153\n",
      "Epoch: 30, Train_Loss: 0.2573467791080475, Test_Loss: 0.8702138662338257\n",
      "Epoch: 30, Train_Loss: 0.2641051113605499, Test_Loss: 5.792257308959961\n",
      "Epoch: 30, Train_Loss: 2.104706287384033, Test_Loss: 0.2797764539718628 *\n",
      "Epoch: 30, Train_Loss: 0.920903205871582, Test_Loss: 0.2618699073791504 *\n",
      "Epoch: 30, Train_Loss: 0.2715120315551758, Test_Loss: 0.29221072793006897\n",
      "Epoch: 30, Train_Loss: 0.2600312829017639, Test_Loss: 0.2819085121154785 *\n",
      "Epoch: 30, Train_Loss: 0.25918176770210266, Test_Loss: 0.2798020839691162 *\n",
      "Epoch: 30, Train_Loss: 0.2676999568939209, Test_Loss: 0.27714648842811584 *\n",
      "Epoch: 30, Train_Loss: 0.2601333260536194, Test_Loss: 0.3991166353225708\n",
      "Epoch: 30, Train_Loss: 0.25789016485214233, Test_Loss: 0.30147650837898254 *\n",
      "Epoch: 30, Train_Loss: 0.256286084651947, Test_Loss: 0.25918763875961304 *\n",
      "Epoch: 30, Train_Loss: 0.2570544183254242, Test_Loss: 0.283822625875473\n",
      "Epoch: 30, Train_Loss: 0.28543829917907715, Test_Loss: 0.3136991262435913\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.29438894987106323, Test_Loss: 0.27203369140625 *\n",
      "Epoch: 30, Train_Loss: 0.28969913721084595, Test_Loss: 0.37796467542648315\n",
      "Epoch: 30, Train_Loss: 0.2836343050003052, Test_Loss: 0.33191972970962524 *\n",
      "Epoch: 30, Train_Loss: 0.26261913776397705, Test_Loss: 0.3443628251552582\n",
      "Epoch: 30, Train_Loss: 0.2975758910179138, Test_Loss: 0.31321120262145996 *\n",
      "Epoch: 30, Train_Loss: 0.3034185469150543, Test_Loss: 0.31665319204330444\n",
      "Epoch: 30, Train_Loss: 0.29025959968566895, Test_Loss: 0.2826763987541199 *\n",
      "Epoch: 30, Train_Loss: 0.3397248387336731, Test_Loss: 0.29494789242744446\n",
      "Epoch: 30, Train_Loss: 0.2898216247558594, Test_Loss: 0.39736080169677734\n",
      "Epoch: 30, Train_Loss: 0.2842380404472351, Test_Loss: 0.3607815206050873 *\n",
      "Epoch: 30, Train_Loss: 0.25517570972442627, Test_Loss: 0.3437831401824951 *\n",
      "Epoch: 30, Train_Loss: 0.26247385144233704, Test_Loss: 0.36336252093315125\n",
      "Epoch: 30, Train_Loss: 0.25777068734169006, Test_Loss: 0.3064157962799072 *\n",
      "Epoch: 30, Train_Loss: 0.2581841051578522, Test_Loss: 0.28896045684814453 *\n",
      "Epoch: 30, Train_Loss: 0.25926703214645386, Test_Loss: 0.285870224237442 *\n",
      "Epoch: 30, Train_Loss: 0.2581387162208557, Test_Loss: 0.2633547782897949 *\n",
      "Epoch: 30, Train_Loss: 0.2582101821899414, Test_Loss: 0.2868835926055908\n",
      "Epoch: 30, Train_Loss: 0.2628512680530548, Test_Loss: 0.266256183385849 *\n",
      "Epoch: 30, Train_Loss: 0.3182316720485687, Test_Loss: 0.27829474210739136\n",
      "Epoch: 30, Train_Loss: 0.3098934590816498, Test_Loss: 0.38378193974494934\n",
      "Epoch: 30, Train_Loss: 0.29615992307662964, Test_Loss: 0.5187904834747314\n",
      "Epoch: 30, Train_Loss: 0.35223522782325745, Test_Loss: 0.3013933598995209 *\n",
      "Epoch: 30, Train_Loss: 0.30796217918395996, Test_Loss: 0.33183956146240234\n",
      "Epoch: 30, Train_Loss: 0.31391972303390503, Test_Loss: 0.3540381193161011\n",
      "Epoch: 30, Train_Loss: 0.2794393301010132, Test_Loss: 0.41378194093704224\n",
      "Epoch: 30, Train_Loss: 0.31536826491355896, Test_Loss: 0.2789503335952759 *\n",
      "Epoch: 30, Train_Loss: 0.29454439878463745, Test_Loss: 0.34903812408447266\n",
      "Epoch: 30, Train_Loss: 0.5342505574226379, Test_Loss: 0.4170762598514557\n",
      "Epoch: 30, Train_Loss: 0.27830418944358826, Test_Loss: 0.42522966861724854\n",
      "Epoch: 30, Train_Loss: 1.1094279289245605, Test_Loss: 0.3463478088378906 *\n",
      "Epoch: 30, Train_Loss: 1.3487496376037598, Test_Loss: 0.2791953384876251 *\n",
      "Epoch: 30, Train_Loss: 0.32253777980804443, Test_Loss: 0.278719425201416 *\n",
      "Epoch: 30, Train_Loss: 0.28953784704208374, Test_Loss: 0.29679569602012634\n",
      "Epoch: 30, Train_Loss: 0.27634143829345703, Test_Loss: 0.6743230223655701\n",
      "Epoch: 30, Train_Loss: 0.28308066725730896, Test_Loss: 0.6678663492202759 *\n",
      "Epoch: 30, Train_Loss: 0.25614604353904724, Test_Loss: 0.5650134086608887 *\n",
      "Epoch: 30, Train_Loss: 0.26054647564888, Test_Loss: 0.7599267959594727\n",
      "Epoch: 30, Train_Loss: 0.3047245740890503, Test_Loss: 0.4323815703392029 *\n",
      "Epoch: 30, Train_Loss: 0.28195875883102417, Test_Loss: 0.4746359586715698\n",
      "Epoch: 30, Train_Loss: 0.27829739451408386, Test_Loss: 0.36817920207977295 *\n",
      "Epoch: 30, Train_Loss: 0.27117836475372314, Test_Loss: 0.2721496820449829 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.28150710463523865, Test_Loss: 0.2689684331417084 *\n",
      "Epoch: 30, Train_Loss: 0.263375848531723, Test_Loss: 0.3117562234401703\n",
      "Epoch: 30, Train_Loss: 0.27664607763290405, Test_Loss: 0.4993717074394226\n",
      "Epoch: 30, Train_Loss: 0.26987066864967346, Test_Loss: 0.742526650428772\n",
      "Epoch: 30, Train_Loss: 0.26268789172172546, Test_Loss: 0.6445409059524536 *\n",
      "Epoch: 30, Train_Loss: 0.265550822019577, Test_Loss: 1.7105581760406494\n",
      "Epoch: 30, Train_Loss: 0.25486379861831665, Test_Loss: 0.7236504554748535 *\n",
      "Epoch: 30, Train_Loss: 0.26803332567214966, Test_Loss: 0.8204807043075562\n",
      "Epoch: 30, Train_Loss: 0.26574620604515076, Test_Loss: 0.2826625406742096 *\n",
      "Epoch: 30, Train_Loss: 0.2763758897781372, Test_Loss: 0.2617887556552887 *\n",
      "Epoch: 30, Train_Loss: 0.25411397218704224, Test_Loss: 0.5563699007034302\n",
      "Epoch: 30, Train_Loss: 0.258614182472229, Test_Loss: 1.0708835124969482\n",
      "Epoch: 30, Train_Loss: 0.2557699382305145, Test_Loss: 0.47892963886260986 *\n",
      "Epoch: 30, Train_Loss: 0.2578737139701843, Test_Loss: 0.3580935597419739 *\n",
      "Epoch: 30, Train_Loss: 0.25768062472343445, Test_Loss: 0.27295681834220886 *\n",
      "Epoch: 30, Train_Loss: 0.2545076310634613, Test_Loss: 0.342195063829422\n",
      "Epoch: 30, Train_Loss: 0.25778985023498535, Test_Loss: 0.6303766965866089\n",
      "Epoch: 30, Train_Loss: 0.2589181661605835, Test_Loss: 0.48974913358688354 *\n",
      "Epoch: 30, Train_Loss: 0.2552493214607239, Test_Loss: 0.6866773366928101\n",
      "Epoch: 30, Train_Loss: 0.2559964954853058, Test_Loss: 0.5183982253074646 *\n",
      "Epoch: 30, Train_Loss: 0.2605612576007843, Test_Loss: 0.2704923152923584 *\n",
      "Epoch: 30, Train_Loss: 0.25821980834007263, Test_Loss: 0.2697075605392456 *\n",
      "Epoch: 30, Train_Loss: 0.2560037672519684, Test_Loss: 0.2882026433944702\n",
      "Epoch: 30, Train_Loss: 0.26666197180747986, Test_Loss: 0.28610295057296753 *\n",
      "Epoch: 30, Train_Loss: 0.2616402208805084, Test_Loss: 0.4739137291908264\n",
      "Epoch: 30, Train_Loss: 0.2653527557849884, Test_Loss: 0.5922645330429077\n",
      "Epoch: 30, Train_Loss: 0.25691303610801697, Test_Loss: 0.5094895362854004 *\n",
      "Epoch: 30, Train_Loss: 0.2556779086589813, Test_Loss: 0.32478493452072144 *\n",
      "Epoch: 30, Train_Loss: 0.26487424969673157, Test_Loss: 0.27327385544776917 *\n",
      "Epoch: 30, Train_Loss: 0.26385658979415894, Test_Loss: 0.28481411933898926\n",
      "Epoch: 30, Train_Loss: 0.26321884989738464, Test_Loss: 0.2990371882915497\n",
      "Epoch: 30, Train_Loss: 0.25479018688201904, Test_Loss: 0.6294902563095093\n",
      "Epoch: 30, Train_Loss: 0.26333609223365784, Test_Loss: 0.5157254934310913 *\n",
      "Epoch: 30, Train_Loss: 0.30519723892211914, Test_Loss: 0.652554988861084\n",
      "Epoch: 30, Train_Loss: 0.2663716971874237, Test_Loss: 0.3555782437324524 *\n",
      "Epoch: 30, Train_Loss: 0.2756219506263733, Test_Loss: 0.3100327253341675 *\n",
      "Epoch: 30, Train_Loss: 0.2533591687679291, Test_Loss: 0.266157329082489 *\n",
      "Epoch: 30, Train_Loss: 0.27850285172462463, Test_Loss: 0.2601987421512604 *\n",
      "Epoch: 30, Train_Loss: 0.290265291929245, Test_Loss: 0.27843374013900757\n",
      "Epoch: 30, Train_Loss: 0.2618917226791382, Test_Loss: 0.2781793475151062 *\n",
      "Epoch: 30, Train_Loss: 0.25971725583076477, Test_Loss: 0.2898902893066406\n",
      "Epoch: 30, Train_Loss: 0.28762251138687134, Test_Loss: 0.2592676281929016 *\n",
      "Epoch: 30, Train_Loss: 0.34920811653137207, Test_Loss: 0.35827749967575073\n",
      "Epoch: 30, Train_Loss: 0.29947909712791443, Test_Loss: 0.6202074885368347\n",
      "Epoch: 30, Train_Loss: 0.275366872549057, Test_Loss: 0.34524890780448914 *\n",
      "Epoch: 30, Train_Loss: 0.29941004514694214, Test_Loss: 0.45184797048568726\n",
      "Epoch: 30, Train_Loss: 0.2603837251663208, Test_Loss: 0.26404136419296265 *\n",
      "Epoch: 30, Train_Loss: 0.28312069177627563, Test_Loss: 0.2610098719596863 *\n",
      "Epoch: 30, Train_Loss: 0.2557131052017212, Test_Loss: 0.26385512948036194\n",
      "Epoch: 30, Train_Loss: 0.2684418261051178, Test_Loss: 0.2634614408016205 *\n",
      "Epoch: 30, Train_Loss: 0.25736355781555176, Test_Loss: 0.26055508852005005 *\n",
      "Epoch: 30, Train_Loss: 0.2602522373199463, Test_Loss: 4.15249490737915\n",
      "Epoch: 30, Train_Loss: 0.3334546983242035, Test_Loss: 2.2879834175109863 *\n",
      "Epoch: 30, Train_Loss: 0.26068487763404846, Test_Loss: 0.28806787729263306 *\n",
      "Epoch: 30, Train_Loss: 0.30276674032211304, Test_Loss: 0.2848609983921051 *\n",
      "Epoch: 30, Train_Loss: 0.264845609664917, Test_Loss: 0.2987499535083771\n",
      "Epoch: 30, Train_Loss: 0.2865666151046753, Test_Loss: 0.25817203521728516 *\n",
      "Epoch: 30, Train_Loss: 0.2919405996799469, Test_Loss: 0.2612236440181732\n",
      "Epoch: 30, Train_Loss: 0.5481525659561157, Test_Loss: 0.47473716735839844\n",
      "Model saved at location save_model/self_driving_car_model_new.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.26425716280937195, Test_Loss: 0.3887573778629303 *\n",
      "Epoch: 30, Train_Loss: 0.2745649218559265, Test_Loss: 0.28323519229888916 *\n",
      "Epoch: 30, Train_Loss: 0.2535611093044281, Test_Loss: 0.36777263879776\n",
      "Epoch: 30, Train_Loss: 0.2541652023792267, Test_Loss: 0.3379543423652649 *\n",
      "Epoch: 30, Train_Loss: 0.258316308259964, Test_Loss: 0.5326217412948608\n",
      "Epoch: 30, Train_Loss: 0.26169806718826294, Test_Loss: 0.3078392744064331 *\n",
      "Epoch: 30, Train_Loss: 0.2633371949195862, Test_Loss: 0.37980955839157104\n",
      "Epoch: 30, Train_Loss: 0.2586580812931061, Test_Loss: 0.2877148687839508 *\n",
      "Epoch: 30, Train_Loss: 0.2623341381549835, Test_Loss: 0.2901225686073303\n",
      "Epoch: 30, Train_Loss: 0.25788548588752747, Test_Loss: 0.3016943335533142\n",
      "Epoch: 30, Train_Loss: 0.25988078117370605, Test_Loss: 0.35896992683410645\n",
      "Epoch: 30, Train_Loss: 0.26651501655578613, Test_Loss: 0.2924593389034271 *\n",
      "Epoch: 30, Train_Loss: 0.2579820156097412, Test_Loss: 0.25949886441230774 *\n",
      "Epoch: 30, Train_Loss: 0.254536509513855, Test_Loss: 0.2952810525894165\n",
      "Epoch: 30, Train_Loss: 0.2703763544559479, Test_Loss: 0.2673865854740143 *\n",
      "Epoch: 30, Train_Loss: 0.2647421061992645, Test_Loss: 0.2737852931022644\n",
      "Epoch: 30, Train_Loss: 0.2742491066455841, Test_Loss: 0.30621978640556335\n",
      "Epoch: 30, Train_Loss: 0.25487828254699707, Test_Loss: 0.26641973853111267 *\n",
      "Epoch: 30, Train_Loss: 0.2652835249900818, Test_Loss: 0.26557695865631104 *\n",
      "Epoch: 30, Train_Loss: 0.26528802514076233, Test_Loss: 0.26853102445602417\n",
      "Epoch: 30, Train_Loss: 0.27874502539634705, Test_Loss: 0.27856045961380005\n",
      "Epoch: 30, Train_Loss: 0.26053062081336975, Test_Loss: 0.25255846977233887 *\n",
      "Epoch: 30, Train_Loss: 0.2720062732696533, Test_Loss: 0.36399948596954346\n",
      "Epoch: 30, Train_Loss: 0.25352978706359863, Test_Loss: 0.3336527645587921 *\n",
      "Epoch: 30, Train_Loss: 0.26481738686561584, Test_Loss: 6.064342498779297\n",
      "Epoch: 30, Train_Loss: 0.2587648630142212, Test_Loss: 0.5428712368011475 *\n",
      "Epoch: 30, Train_Loss: 0.26894259452819824, Test_Loss: 0.2671428322792053 *\n",
      "Epoch: 30, Train_Loss: 0.37448105216026306, Test_Loss: 0.2919505536556244\n",
      "Epoch: 30, Train_Loss: 3.642855167388916, Test_Loss: 0.2768743932247162 *\n",
      "Epoch: 30, Train_Loss: 2.445547342300415, Test_Loss: 0.2727501392364502 *\n",
      "Epoch: 30, Train_Loss: 0.2655290961265564, Test_Loss: 0.2658267617225647 *\n",
      "Epoch: 30, Train_Loss: 0.25510963797569275, Test_Loss: 0.3733334541320801\n",
      "Epoch: 30, Train_Loss: 0.33765625953674316, Test_Loss: 0.3209230303764343 *\n",
      "Epoch: 30, Train_Loss: 0.3525018095970154, Test_Loss: 0.25371742248535156 *\n",
      "Epoch: 30, Train_Loss: 0.27734652161598206, Test_Loss: 0.2737882137298584\n",
      "Epoch: 30, Train_Loss: 0.25706541538238525, Test_Loss: 0.2837287187576294\n",
      "Epoch: 30, Train_Loss: 0.30302783846855164, Test_Loss: 0.2620798945426941 *\n",
      "Epoch: 30, Train_Loss: 0.29175323247909546, Test_Loss: 0.27225106954574585\n",
      "Epoch: 30, Train_Loss: 0.26032745838165283, Test_Loss: 0.2740127444267273\n",
      "Epoch: 30, Train_Loss: 0.3893548250198364, Test_Loss: 0.2977685034275055\n",
      "Epoch: 30, Train_Loss: 0.7274126410484314, Test_Loss: 0.33870115876197815\n",
      "Epoch: 30, Train_Loss: 0.8069194555282593, Test_Loss: 0.3098905086517334 *\n",
      "Epoch: 30, Train_Loss: 0.35471269488334656, Test_Loss: 0.3183133006095886\n",
      "Epoch: 30, Train_Loss: 0.3286306858062744, Test_Loss: 0.25693395733833313 *\n",
      "Epoch: 30, Train_Loss: 1.626624345779419, Test_Loss: 0.27080827951431274\n",
      "Epoch: 30, Train_Loss: 1.0268436670303345, Test_Loss: 0.2759200632572174\n",
      "Epoch: 30, Train_Loss: 0.2687426209449768, Test_Loss: 0.286801815032959\n",
      "Epoch: 30, Train_Loss: 0.2687467336654663, Test_Loss: 0.2967146039009094\n",
      "Epoch: 30, Train_Loss: 0.6453881859779358, Test_Loss: 0.2939731776714325 *\n",
      "Epoch: 30, Train_Loss: 0.7142072916030884, Test_Loss: 0.2782396674156189 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.9886375069618225, Test_Loss: 0.26422902941703796 *\n",
      "Epoch: 30, Train_Loss: 0.26115304231643677, Test_Loss: 0.2612317204475403 *\n",
      "Epoch: 30, Train_Loss: 0.28128740191459656, Test_Loss: 0.2776297628879547\n",
      "Epoch: 30, Train_Loss: 0.3697853684425354, Test_Loss: 0.2754495441913605 *\n"
     ]
    }
   ],
   "source": [
    "SAVEDIR = \"save_model/\"\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "lr = 1e-4   #learning rate = 0.0001\n",
    "\n",
    "L2NormConst = 0.001\n",
    "train_vars = tf.trainable_variables()\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(y_true, y_predicted))) + tf.add_n([tf.nn.l2_loss(w) for w in train_vars]) * L2NormConst\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "print('TRAINING SELF DRIVING CAR MODEL')\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "epoch_number, train_loss, test_loss,  = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_avg_loss = 0\n",
    "    test_avg_loss = 0\n",
    "    te_loss_old = 10000  #any big number can be given\n",
    "    ##dropout = 0.5\n",
    "    for i in range(int(len(x)/batch_size)):\n",
    "        train_batch_x, train_batch_y = loadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 0.5})\n",
    "        tr_loss = loss.eval(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 0.5})\n",
    "        train_avg_loss += tr_loss / batch_size\n",
    "    \n",
    "        test_batch_x, test_batch_y = loadTestBatch(batch_size)\n",
    "        te_loss_new = loss.eval(feed_dict = {x_input: test_batch_x, y_true: test_batch_y, keep_prob: 0.5})\n",
    "        test_avg_loss += te_loss_new / batch_size\n",
    "        \n",
    "        if te_loss_new < te_loss_old:\n",
    "            \n",
    "            print(\"Epoch: {}, Train_Loss: {}, Test_Loss: {} *\".format(epoch+1, tr_loss, te_loss_new))\n",
    "        else:\n",
    "            print(\"Epoch: {}, Train_Loss: {}, Test_Loss: {}\".format(epoch+1, tr_loss, te_loss_new))\n",
    "        te_loss_old = te_loss_new\n",
    "        \n",
    "        if (i+1) % batch_size == 0:\n",
    "            if not os.path.exists(SAVEDIR):\n",
    "                os.makedirs(SAVEDIR)\n",
    "            save_path = os.path.join(SAVEDIR, \"self_driving_car_model_new.ckpt\")\n",
    "            saver.save(sess = sess, save_path = save_path)\n",
    "            print(\"Model saved at location {} at epoch {}\".format(save_path, epoch + 1))\n",
    "        \n",
    "    epoch_number.append(epoch)\n",
    "    train_loss.append(train_avg_loss)\n",
    "    test_loss.append(test_avg_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_frame = pd.DataFrame(columns = [\"Epoch\", \"Train Loss\", \"Test Loss\"])\n",
    "log_frame[\"Epoch\"] = epoch_number\n",
    "log_frame[\"Train Loss\"] = train_loss\n",
    "log_frame[\"Test Loss\"] = test_loss\n",
    "log_frame.to_csv(os.path.join(SAVEDIR, \"log.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>31.096509</td>\n",
       "      <td>30.758518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28.237732</td>\n",
       "      <td>28.102454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>27.235888</td>\n",
       "      <td>26.641113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>25.113611</td>\n",
       "      <td>25.132473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24.012218</td>\n",
       "      <td>23.353619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>21.609449</td>\n",
       "      <td>21.471747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>19.758644</td>\n",
       "      <td>19.638440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>18.186861</td>\n",
       "      <td>17.646539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>15.834687</td>\n",
       "      <td>15.667052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>14.384864</td>\n",
       "      <td>13.888145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>12.099833</td>\n",
       "      <td>12.081557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>11.108097</td>\n",
       "      <td>10.468595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>9.193144</td>\n",
       "      <td>9.136619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>7.997546</td>\n",
       "      <td>7.865208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>7.288189</td>\n",
       "      <td>6.755052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>5.977243</td>\n",
       "      <td>5.920544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>5.602363</td>\n",
       "      <td>5.128174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>4.510043</td>\n",
       "      <td>4.495645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>4.461168</td>\n",
       "      <td>4.046359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>3.697977</td>\n",
       "      <td>3.593919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>3.340056</td>\n",
       "      <td>3.233539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>3.400370</td>\n",
       "      <td>3.041043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2.823179</td>\n",
       "      <td>2.795093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>3.004162</td>\n",
       "      <td>2.660086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2.422638</td>\n",
       "      <td>2.544963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2.718110</td>\n",
       "      <td>2.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2.301414</td>\n",
       "      <td>2.329843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>2.196320</td>\n",
       "      <td>2.280292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>2.440208</td>\n",
       "      <td>2.277881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2.003629</td>\n",
       "      <td>2.165855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch  Train Loss  Test Loss\n",
       "0       0   31.096509  30.758518\n",
       "1       1   28.237732  28.102454\n",
       "2       2   27.235888  26.641113\n",
       "3       3   25.113611  25.132473\n",
       "4       4   24.012218  23.353619\n",
       "5       5   21.609449  21.471747\n",
       "6       6   19.758644  19.638440\n",
       "7       7   18.186861  17.646539\n",
       "8       8   15.834687  15.667052\n",
       "9       9   14.384864  13.888145\n",
       "10     10   12.099833  12.081557\n",
       "11     11   11.108097  10.468595\n",
       "12     12    9.193144   9.136619\n",
       "13     13    7.997546   7.865208\n",
       "14     14    7.288189   6.755052\n",
       "15     15    5.977243   5.920544\n",
       "16     16    5.602363   5.128174\n",
       "17     17    4.510043   4.495645\n",
       "18     18    4.461168   4.046359\n",
       "19     19    3.697977   3.593919\n",
       "20     20    3.340056   3.233539\n",
       "21     21    3.400370   3.041043\n",
       "22     22    2.823179   2.795093\n",
       "23     23    3.004162   2.660086\n",
       "24     24    2.422638   2.544963\n",
       "25     25    2.718110   2.417300\n",
       "26     26    2.301414   2.329843\n",
       "27     27    2.196320   2.280292\n",
       "28     28    2.440208   2.277881\n",
       "29     29    2.003629   2.165855"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.read_csv('save_model/log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Train and test loss')\n",
    "plt.plot(loss_df['Train Loss'], color='blue', label='train')\n",
    "plt.plot(loss_df['Test Loss'], color='orange', label='test')\n",
    "filename = 'self_driving_car'\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(filename + '_plot.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save_model/self_driving_car_model_new.ckpt\n",
      "press q to quit\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"save_model/self_driving_car_model_new.ckpt\")\n",
    "\n",
    "print('press q to quit')\n",
    "\n",
    "predicted_degrees = []\n",
    "img = cv2.imread('steering_wheel_image.png', 0) \n",
    "rows, cols = img.shape\n",
    "\n",
    "i = 0\n",
    "while(cv2.waitKey(30) != ord(\"q\")):\n",
    "    full_image = cv2.imread(test_x[i])\n",
    "    cv2.imshow('Frame Window', full_image)\n",
    "    image = ((cv2.resize(full_image[-150:], (200, 66)) / 255.0).reshape((1, 66, 200, 3)))\n",
    "    degrees = sess.run(y_predicted, feed_dict = {x_input: image, keep_prob: 1.0})[0][0] *180 / pi \n",
    "    #predicted degrees from radians to degrees.\n",
    "    #print(\"Predicted degrees: \"+str(degrees))\n",
    "    predicted_degrees.append(degrees)\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2), -degrees, 1) \n",
    "    dst = cv2.warpAffine(src = img, M = M, dsize = (cols, rows)) \n",
    "    cv2.imshow(\"Steering Wheel\", dst)\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- The updated model of Sully Chen's is performing and works to some extent but it is not as good as the initial one.\n",
    "- Modifications to the original implementation: Used linear activation function (ELU) and changed drop out rate of Adam optimizer to 1e-4\n",
    "\n",
    "- The model may have performed better if it had run for more than 30 epochs.\n",
    "\n",
    "\n",
    "\n",
    "- Maybe the atan function in the output layer makes a lot of difference in the performance.\n",
    " \n",
    "- Maybe the lower performance is due to the `learning rate` and `dropout` rate changes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
